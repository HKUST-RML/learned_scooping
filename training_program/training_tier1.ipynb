{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Construct customized ResNet\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, pcpt_block, pcpt_layers, scoop_block, scoop_layers, h, w, pcpt_is_upsample=0, scoop_is_upsample=0):\n",
    "        self.inplanes = 64\n",
    "        self.pcpt_is_upsample = pcpt_is_upsample\n",
    "        super(ResNet, self).__init__()\n",
    "        self.pcpt_conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.pcpt_bn1 = nn.BatchNorm2d(64)\n",
    "        self.pcpt_relu = nn.ReLU(inplace=True)\n",
    "        self.pcpt_maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.pcpt_upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.pcpt_layer1 = self._make_layer(pcpt_block, 128, pcpt_layers[0])\n",
    "        self.pcpt_layer2 = self._make_layer(pcpt_block, 256, pcpt_layers[1])\n",
    "        self.pcpt_layer3 = self._make_layer(pcpt_block, 512, pcpt_layers[2])\n",
    "\n",
    "        self.inplanes = 512\n",
    "        self.scoop_is_upsample = scoop_is_upsample\n",
    "        self.scoop_upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.scoop_layer1 = self._make_layer(scoop_block, 256, scoop_layers[0])\n",
    "        self.scoop_layer2 = self._make_layer(scoop_block, 128, scoop_layers[1])\n",
    "        self.scoop_layer3 = self._make_layer(scoop_block, 64, scoop_layers[2])\n",
    "        self.scoop_conv1 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.scoop_bn1 = nn.BatchNorm2d(1)\n",
    "        self.scoop_conv2 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.scoop_bn2 = nn.BatchNorm2d(3)\n",
    "        self.scoop_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.scoop_head = nn.Linear(h*w, 1)\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pcpt_conv1(x)\n",
    "        x = self.pcpt_bn1(x)\n",
    "        x = self.pcpt_relu(x)\n",
    "        x = self.pcpt_maxpool(x)\n",
    "\n",
    "        x = self.pcpt_layer1(x)\n",
    "        x = self.pcpt_maxpool(x)\n",
    "        x = self.pcpt_layer2(x)\n",
    "        x = self.pcpt_layer3(x)\n",
    "\n",
    "        x = self.scoop_layer1(x)\n",
    "        x = self.scoop_layer2(x)\n",
    "        x = self.scoop_upsample(x)\n",
    "        x = self.scoop_layer3(x)\n",
    "        x = self.scoop_upsample(x)\n",
    "\n",
    "        x = self.scoop_conv2(x)\n",
    "        x = self.scoop_bn2(x)\n",
    "        x = self.scoop_relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22124, 200, 200, 4)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "import scipy.ndimage\n",
    "BATCH_SIZE =  32\n",
    "NUM_EPOCH = 50\n",
    "\n",
    "# make data\n",
    "\n",
    "data_input = np.load(\"data_20210715/input_data_array_20210715.npy\")/255.0\n",
    "#normalization=np.ones(data_input.shape)\n",
    "#normalization[:,:,:]=[255,255,255,0.08]\n",
    "#data_input = data_input/normalization\n",
    "#print(data_input)\n",
    "print(data_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "'''\n",
    "input_array_mean = np.mean(np.mean(np.mean(data_input, axis=0), axis=0), axis=0)\n",
    "input_array_std = np.mean(np.mean(np.std(data_input, axis=0), axis=0), axis=0)\n",
    "print('input_array_mean', input_array_mean)\n",
    "print('input_array_std', input_array_std)\n",
    "'''\n",
    "\n",
    "#input_array_mean=np.ones(data_input.shape)\n",
    "#input_array_mean[:,:,:]=[0.54569177,0.48216905,0.49853667,0.42829879]\n",
    "#input_array_std=np.ones(data_input.shape)\n",
    "#input_array_std[:,:,:]=[0.31950833,0.29141234,0.30389949,0.20277719]\n",
    "\n",
    "#data_input = (data_input-input_array_mean)/input_array_std\n",
    "\n",
    "temp_index_set = random.sample(range(len(data_input)), 50)\n",
    "#print(temp_index_set)\n",
    "\n",
    "temp_index_set_other = list(set(range(len(data_input))).difference(set(temp_index_set)))\n",
    "#print(temp_index_set_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "data_input = torch.from_numpy(data_input).permute(0,3,1,2)\n",
    "\n",
    "train_data_input = data_input[temp_index_set_other, :, :, :]\n",
    "val_data_input = data_input[temp_index_set, :, :, :]\n",
    "htmap_h = data_input.shape[2]\n",
    "htmap_w = data_input.shape[3]\n",
    "del data_input\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22124, 200, 200)\n",
      "517031 262824989\n",
      "Class weights: tensor([  0.0000, 509.3351,   1.0020], device='cuda:0')\n",
      "step1\n",
      "step2\n",
      "(22074, 200, 200) uint8\n",
      "(50, 200, 200) uint8\n",
      "step3\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "data_label = np.load(\"data_20210715/label_data_array_20210715.npy\")\n",
    "print(data_label.shape)\n",
    "data_label[data_label==0]=2\n",
    "data_label[data_label==255]=0\n",
    "data_label[(data_label==128)]=1\n",
    "#print(data_label[np.logical_and(np.logical_and((data_label!=0),(data_label!=1)),(data_label!=2))])\n",
    "\n",
    "\n",
    "good_cnt = (data_label==1).sum()\n",
    "bad_cnt = (data_label==2).sum()\n",
    "print(good_cnt, bad_cnt)\n",
    "total = good_cnt + bad_cnt\n",
    "\n",
    "weights = [0, total/good_cnt, total/bad_cnt]\n",
    "class_weights = torch.FloatTensor(weights)\n",
    "if torch.cuda.is_available():\n",
    "    class_weights = class_weights.cuda()\n",
    "print('Class weights:', class_weights)\n",
    "\n",
    "train_data_label = data_label[temp_index_set_other, :, :]\n",
    "print('step1')\n",
    "val_data_label = data_label[temp_index_set, :, :]\n",
    "del data_label\n",
    "gc.collect()\n",
    "print('step2')\n",
    "print(train_data_label.shape, train_data_label.dtype)\n",
    "print(val_data_label.shape, val_data_label.dtype)\n",
    "train_data_label = torch.from_numpy(train_data_label)\n",
    "val_data_label = torch.from_numpy(val_data_label)\n",
    "\n",
    "print('step3')\n",
    "train_data_label = train_data_label.long()\n",
    "val_data_label = val_data_label.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step4\n",
      "step5\n"
     ]
    }
   ],
   "source": [
    "train_torch_dataset = Data.TensorDataset(train_data_input, train_data_label)\n",
    "print('step4')\n",
    "loader = Data.DataLoader(dataset=train_torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print('step5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (pcpt_conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (pcpt_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pcpt_relu): ReLU(inplace=True)\n",
      "  (pcpt_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pcpt_upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (pcpt_layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pcpt_layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pcpt_layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (scoop_layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (scoop_layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_conv1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (scoop_bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (scoop_conv2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (scoop_bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (scoop_relu): ReLU(inplace=True)\n",
      "  (scoop_head): Linear(in_features=40000, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(pcpt_block=BasicBlock, pcpt_layers=[1,5,1], scoop_block=BasicBlock, scoop_layers=[1,5,1], h=htmap_h, w=htmap_w).cuda()     # define the network\n",
    "print(net)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   step:  0   train loss:  1.1575491428375244  val loss:  1.719327688217163\n",
      "min_val_loss_print 1.719327688217163\n",
      "epoch:  0   step:  1   train loss:  1.176862120628357  val loss:  1.4551018476486206\n",
      "min_val_loss_print 1.4551018476486206\n",
      "epoch:  0   step:  2   train loss:  0.9605910778045654  val loss:  1.293009877204895\n",
      "min_val_loss_print 1.293009877204895\n",
      "epoch:  0   step:  3   train loss:  1.0098267793655396  val loss:  1.3510037660598755\n",
      "epoch:  0   step:  4   train loss:  1.1145790815353394  val loss:  1.3881511688232422\n",
      "epoch:  0   step:  5   train loss:  1.03610098361969  val loss:  1.3610345125198364\n",
      "epoch:  0   step:  6   train loss:  1.0199413299560547  val loss:  1.310666799545288\n",
      "epoch:  0   step:  7   train loss:  0.9649294018745422  val loss:  1.297982096672058\n",
      "epoch:  0   step:  8   train loss:  0.9855425953865051  val loss:  1.2694835662841797\n",
      "min_val_loss_print 1.2694835662841797\n",
      "epoch:  0   step:  9   train loss:  0.88344806432724  val loss:  1.2344201803207397\n",
      "min_val_loss_print 1.2344201803207397\n",
      "epoch:  0   step:  10   train loss:  0.829477846622467  val loss:  1.2055009603500366\n",
      "min_val_loss_print 1.2055009603500366\n",
      "epoch:  0   step:  11   train loss:  0.8387848138809204  val loss:  1.1610743999481201\n",
      "min_val_loss_print 1.1610743999481201\n",
      "epoch:  0   step:  12   train loss:  0.9590335488319397  val loss:  1.093934416770935\n",
      "min_val_loss_print 1.093934416770935\n",
      "epoch:  0   step:  13   train loss:  0.9868746399879456  val loss:  1.0333589315414429\n",
      "min_val_loss_print 1.0333589315414429\n",
      "epoch:  0   step:  14   train loss:  0.7108268141746521  val loss:  0.9886952042579651\n",
      "min_val_loss_print 0.9886952042579651\n",
      "epoch:  0   step:  15   train loss:  0.8511576652526855  val loss:  0.9529000520706177\n",
      "min_val_loss_print 0.9529000520706177\n",
      "epoch:  0   step:  16   train loss:  0.7828068733215332  val loss:  0.927743673324585\n",
      "min_val_loss_print 0.927743673324585\n",
      "epoch:  0   step:  17   train loss:  0.6796391010284424  val loss:  0.9236078262329102\n",
      "min_val_loss_print 0.9236078262329102\n",
      "epoch:  0   step:  18   train loss:  0.7412221431732178  val loss:  0.938227117061615\n",
      "epoch:  0   step:  19   train loss:  0.902863085269928  val loss:  0.9602562189102173\n",
      "epoch:  0   step:  20   train loss:  0.795388400554657  val loss:  0.971480131149292\n",
      "epoch:  0   step:  21   train loss:  1.0064468383789062  val loss:  0.9737274050712585\n",
      "epoch:  0   step:  22   train loss:  0.6920163631439209  val loss:  0.9621883630752563\n",
      "epoch:  0   step:  23   train loss:  0.9851900339126587  val loss:  0.9367321729660034\n",
      "epoch:  0   step:  24   train loss:  0.8446158170700073  val loss:  0.9130269289016724\n",
      "min_val_loss_print 0.9130269289016724\n",
      "epoch:  0   step:  25   train loss:  0.8820773959159851  val loss:  0.8909907937049866\n",
      "min_val_loss_print 0.8909907937049866\n",
      "epoch:  0   step:  26   train loss:  0.6418757438659668  val loss:  0.8818337917327881\n",
      "min_val_loss_print 0.8818337917327881\n",
      "epoch:  0   step:  27   train loss:  0.7596719861030579  val loss:  0.868218183517456\n",
      "min_val_loss_print 0.868218183517456\n",
      "epoch:  0   step:  28   train loss:  0.5370975136756897  val loss:  0.9069320559501648\n",
      "epoch:  0   step:  29   train loss:  0.8817638754844666  val loss:  1.0244945287704468\n",
      "epoch:  0   step:  30   train loss:  0.5741832852363586  val loss:  1.1166603565216064\n",
      "epoch:  0   step:  31   train loss:  0.43670129776000977  val loss:  1.2087165117263794\n",
      "epoch:  0   step:  32   train loss:  0.8512665033340454  val loss:  1.213971495628357\n",
      "epoch:  0   step:  33   train loss:  0.5014265775680542  val loss:  1.1772854328155518\n",
      "epoch:  0   step:  34   train loss:  0.8493999242782593  val loss:  1.1650668382644653\n",
      "epoch:  0   step:  35   train loss:  0.3389348089694977  val loss:  1.1222591400146484\n",
      "epoch:  0   step:  36   train loss:  0.6518878936767578  val loss:  1.081593632698059\n",
      "epoch:  0   step:  37   train loss:  0.8332449793815613  val loss:  1.0135979652404785\n",
      "epoch:  0   step:  38   train loss:  0.7237334251403809  val loss:  0.9362286329269409\n",
      "epoch:  0   step:  39   train loss:  0.928379237651825  val loss:  0.8632920980453491\n",
      "min_val_loss_print 0.8632920980453491\n",
      "epoch:  0   step:  40   train loss:  0.6467531323432922  val loss:  0.799418568611145\n",
      "min_val_loss_print 0.799418568611145\n",
      "epoch:  0   step:  41   train loss:  0.6070922613143921  val loss:  0.7448210716247559\n",
      "min_val_loss_print 0.7448210716247559\n",
      "epoch:  0   step:  42   train loss:  0.5064850449562073  val loss:  0.7199722528457642\n",
      "min_val_loss_print 0.7199722528457642\n",
      "epoch:  0   step:  43   train loss:  0.624352216720581  val loss:  0.7091907262802124\n",
      "min_val_loss_print 0.7091907262802124\n",
      "epoch:  0   step:  44   train loss:  0.5497662425041199  val loss:  0.7078831195831299\n",
      "min_val_loss_print 0.7078831195831299\n",
      "epoch:  0   step:  45   train loss:  0.7638164162635803  val loss:  0.7105798125267029\n",
      "epoch:  0   step:  46   train loss:  0.4953138828277588  val loss:  0.712020993232727\n",
      "epoch:  0   step:  47   train loss:  0.7138798236846924  val loss:  0.7083006501197815\n",
      "epoch:  0   step:  48   train loss:  0.5654960870742798  val loss:  0.7032402753829956\n",
      "min_val_loss_print 0.7032402753829956\n",
      "epoch:  0   step:  49   train loss:  0.5672503113746643  val loss:  0.7027204036712646\n",
      "min_val_loss_print 0.7027204036712646\n",
      "epoch:  0   step:  50   train loss:  0.6938140392303467  val loss:  0.7013908624649048\n",
      "min_val_loss_print 0.7013908624649048\n",
      "epoch:  0   step:  51   train loss:  0.524920642375946  val loss:  0.696485161781311\n",
      "min_val_loss_print 0.696485161781311\n",
      "epoch:  0   step:  52   train loss:  0.6266857385635376  val loss:  0.704247236251831\n",
      "epoch:  0   step:  53   train loss:  0.9260559678077698  val loss:  0.707359790802002\n",
      "epoch:  0   step:  54   train loss:  0.374073326587677  val loss:  0.719933271408081\n",
      "epoch:  0   step:  55   train loss:  0.4643392860889435  val loss:  0.7231190800666809\n",
      "epoch:  0   step:  56   train loss:  0.4786739647388458  val loss:  0.7238746881484985\n",
      "epoch:  0   step:  57   train loss:  0.7048437595367432  val loss:  0.7275078296661377\n",
      "epoch:  0   step:  58   train loss:  0.4861365556716919  val loss:  0.7263695001602173\n",
      "epoch:  0   step:  59   train loss:  0.5782078504562378  val loss:  0.721190869808197\n",
      "epoch:  0   step:  60   train loss:  0.6500124335289001  val loss:  0.7095460295677185\n",
      "epoch:  0   step:  61   train loss:  0.4074278175830841  val loss:  0.6893901228904724\n",
      "min_val_loss_print 0.6893901228904724\n",
      "epoch:  0   step:  62   train loss:  0.6716671586036682  val loss:  0.6758500933647156\n",
      "min_val_loss_print 0.6758500933647156\n",
      "epoch:  0   step:  63   train loss:  0.482684463262558  val loss:  0.6562287211418152\n",
      "min_val_loss_print 0.6562287211418152\n",
      "epoch:  0   step:  64   train loss:  0.6239277124404907  val loss:  0.6435726881027222\n",
      "min_val_loss_print 0.6435726881027222\n",
      "epoch:  0   step:  65   train loss:  0.48877260088920593  val loss:  0.6288375854492188\n",
      "min_val_loss_print 0.6288375854492188\n",
      "epoch:  0   step:  66   train loss:  0.5447642207145691  val loss:  0.624500572681427\n",
      "min_val_loss_print 0.624500572681427\n",
      "epoch:  0   step:  67   train loss:  0.7581914067268372  val loss:  0.6194096803665161\n",
      "min_val_loss_print 0.6194096803665161\n",
      "epoch:  0   step:  68   train loss:  0.5457506775856018  val loss:  0.6185072660446167\n",
      "min_val_loss_print 0.6185072660446167\n",
      "epoch:  0   step:  69   train loss:  0.8085669875144958  val loss:  0.6209996342658997\n",
      "epoch:  0   step:  70   train loss:  0.35540011525154114  val loss:  0.6313081979751587\n",
      "epoch:  0   step:  71   train loss:  0.7782040238380432  val loss:  0.6312670111656189\n",
      "epoch:  0   step:  72   train loss:  0.621441662311554  val loss:  0.6361609697341919\n",
      "epoch:  0   step:  73   train loss:  0.5627393126487732  val loss:  0.6383547782897949\n",
      "epoch:  0   step:  74   train loss:  0.658291220664978  val loss:  0.6406238079071045\n",
      "epoch:  0   step:  75   train loss:  0.36043858528137207  val loss:  0.6403886079788208\n",
      "epoch:  0   step:  76   train loss:  0.4255014955997467  val loss:  0.6383835673332214\n",
      "epoch:  0   step:  77   train loss:  0.41921257972717285  val loss:  0.6369837522506714\n",
      "epoch:  0   step:  78   train loss:  0.6143878698348999  val loss:  0.623606264591217\n",
      "epoch:  0   step:  79   train loss:  0.3929661512374878  val loss:  0.6098366379737854\n",
      "min_val_loss_print 0.6098366379737854\n",
      "epoch:  0   step:  80   train loss:  0.4109434187412262  val loss:  0.6092143058776855\n",
      "min_val_loss_print 0.6092143058776855\n",
      "epoch:  0   step:  81   train loss:  0.5635203123092651  val loss:  0.606440007686615\n",
      "min_val_loss_print 0.606440007686615\n",
      "epoch:  0   step:  82   train loss:  0.3750287592411041  val loss:  0.6124849915504456\n",
      "epoch:  0   step:  83   train loss:  0.8600097298622131  val loss:  0.642760694026947\n",
      "epoch:  0   step:  84   train loss:  0.6302417516708374  val loss:  0.6609402894973755\n",
      "epoch:  0   step:  85   train loss:  0.6915343403816223  val loss:  0.7050789594650269\n",
      "epoch:  0   step:  86   train loss:  0.8292025327682495  val loss:  0.7400752305984497\n",
      "epoch:  0   step:  87   train loss:  0.4512600004673004  val loss:  0.7646806240081787\n",
      "epoch:  0   step:  88   train loss:  0.46221035718917847  val loss:  0.7794408202171326\n",
      "epoch:  0   step:  89   train loss:  0.5981292128562927  val loss:  0.7940433025360107\n",
      "epoch:  0   step:  90   train loss:  0.9047603607177734  val loss:  0.8236820101737976\n",
      "epoch:  0   step:  91   train loss:  0.503309428691864  val loss:  0.8616177439689636\n",
      "epoch:  0   step:  92   train loss:  0.46219754219055176  val loss:  0.8854779601097107\n",
      "epoch:  0   step:  93   train loss:  0.44874581694602966  val loss:  0.8951887488365173\n",
      "epoch:  0   step:  94   train loss:  0.5782493352890015  val loss:  0.8894422650337219\n",
      "epoch:  0   step:  95   train loss:  0.6418582201004028  val loss:  0.8677647709846497\n",
      "epoch:  0   step:  96   train loss:  0.5092521905899048  val loss:  0.8301703929901123\n",
      "epoch:  0   step:  97   train loss:  0.4436730742454529  val loss:  0.7912195920944214\n",
      "epoch:  0   step:  98   train loss:  0.6084856390953064  val loss:  0.7453745603561401\n",
      "epoch:  0   step:  99   train loss:  0.4884149730205536  val loss:  0.7059252858161926\n",
      "epoch:  0   step:  100   train loss:  0.7377011179924011  val loss:  0.6740933656692505\n",
      "epoch:  0   step:  101   train loss:  0.6074985265731812  val loss:  0.6479701995849609\n",
      "epoch:  0   step:  102   train loss:  0.3352397084236145  val loss:  0.6304159164428711\n",
      "epoch:  0   step:  103   train loss:  0.4535045921802521  val loss:  0.6148934960365295\n",
      "epoch:  0   step:  104   train loss:  0.5670012831687927  val loss:  0.6048780083656311\n",
      "min_val_loss_print 0.6048780083656311\n",
      "epoch:  0   step:  105   train loss:  0.6256288886070251  val loss:  0.5978638529777527\n",
      "min_val_loss_print 0.5978638529777527\n",
      "epoch:  0   step:  106   train loss:  0.5713072419166565  val loss:  0.5935642719268799\n",
      "min_val_loss_print 0.5935642719268799\n",
      "epoch:  0   step:  107   train loss:  0.5051181316375732  val loss:  0.5888431668281555\n",
      "min_val_loss_print 0.5888431668281555\n",
      "epoch:  0   step:  108   train loss:  0.565339207649231  val loss:  0.5862902402877808\n",
      "min_val_loss_print 0.5862902402877808\n",
      "epoch:  0   step:  109   train loss:  0.6220130324363708  val loss:  0.5823989510536194\n",
      "min_val_loss_print 0.5823989510536194\n",
      "epoch:  0   step:  110   train loss:  0.7090945839881897  val loss:  0.5734161734580994\n",
      "min_val_loss_print 0.5734161734580994\n",
      "epoch:  0   step:  111   train loss:  0.42858991026878357  val loss:  0.5606316924095154\n",
      "min_val_loss_print 0.5606316924095154\n",
      "epoch:  0   step:  112   train loss:  0.5328304767608643  val loss:  0.5650426149368286\n",
      "epoch:  0   step:  113   train loss:  0.4935337007045746  val loss:  0.5674489140510559\n",
      "epoch:  0   step:  114   train loss:  0.5677644610404968  val loss:  0.572542667388916\n",
      "epoch:  0   step:  115   train loss:  0.6130639910697937  val loss:  0.5827794671058655\n",
      "epoch:  0   step:  116   train loss:  0.554631233215332  val loss:  0.5955891609191895\n",
      "epoch:  0   step:  117   train loss:  0.6064577102661133  val loss:  0.6042646169662476\n",
      "epoch:  0   step:  118   train loss:  0.5523253083229065  val loss:  0.6154623031616211\n",
      "epoch:  0   step:  119   train loss:  0.5072489380836487  val loss:  0.6187963485717773\n",
      "epoch:  0   step:  120   train loss:  0.567406177520752  val loss:  0.6169981956481934\n",
      "epoch:  0   step:  121   train loss:  0.4053184986114502  val loss:  0.6185522079467773\n",
      "epoch:  0   step:  122   train loss:  0.5644432306289673  val loss:  0.6212400197982788\n",
      "epoch:  0   step:  123   train loss:  0.35804516077041626  val loss:  0.6184826493263245\n",
      "epoch:  0   step:  124   train loss:  0.4793088138103485  val loss:  0.6186462640762329\n",
      "epoch:  0   step:  125   train loss:  0.42996910214424133  val loss:  0.6227397918701172\n",
      "epoch:  0   step:  126   train loss:  0.4537489116191864  val loss:  0.6250790357589722\n",
      "epoch:  0   step:  127   train loss:  0.6877375245094299  val loss:  0.6228939890861511\n",
      "epoch:  0   step:  128   train loss:  0.449121356010437  val loss:  0.6240774989128113\n",
      "epoch:  0   step:  129   train loss:  0.470461368560791  val loss:  0.6214239001274109\n",
      "epoch:  0   step:  130   train loss:  0.367810994386673  val loss:  0.6308670043945312\n",
      "epoch:  0   step:  131   train loss:  0.44380053877830505  val loss:  0.6462711691856384\n",
      "epoch:  0   step:  132   train loss:  0.6637690663337708  val loss:  0.6505184173583984\n",
      "epoch:  0   step:  133   train loss:  0.5965543389320374  val loss:  0.6549922227859497\n",
      "epoch:  0   step:  134   train loss:  0.5988923907279968  val loss:  0.6546958684921265\n",
      "epoch:  0   step:  135   train loss:  0.6562133431434631  val loss:  0.6541364789009094\n",
      "epoch:  0   step:  136   train loss:  0.4565018117427826  val loss:  0.6542683243751526\n",
      "epoch:  0   step:  137   train loss:  0.42120248079299927  val loss:  0.6549789905548096\n",
      "epoch:  0   step:  138   train loss:  0.536704957485199  val loss:  0.6514050364494324\n",
      "epoch:  0   step:  139   train loss:  0.44142672419548035  val loss:  0.6501926183700562\n",
      "epoch:  0   step:  140   train loss:  0.7763267755508423  val loss:  0.6429187655448914\n",
      "epoch:  0   step:  141   train loss:  0.4966522753238678  val loss:  0.6298224329948425\n",
      "epoch:  0   step:  142   train loss:  0.2979567348957062  val loss:  0.6175105571746826\n",
      "epoch:  0   step:  143   train loss:  0.42121753096580505  val loss:  0.6105058789253235\n",
      "epoch:  0   step:  144   train loss:  0.7286813259124756  val loss:  0.6048241257667542\n",
      "epoch:  0   step:  145   train loss:  0.3997243642807007  val loss:  0.5946178436279297\n",
      "epoch:  0   step:  146   train loss:  0.7780540585517883  val loss:  0.5811059474945068\n",
      "epoch:  0   step:  147   train loss:  0.6164406538009644  val loss:  0.5708150267601013\n",
      "epoch:  0   step:  148   train loss:  0.5210047364234924  val loss:  0.5637895464897156\n",
      "epoch:  0   step:  149   train loss:  0.5271094441413879  val loss:  0.5626356601715088\n",
      "epoch:  0   step:  150   train loss:  0.32000112533569336  val loss:  0.5605225563049316\n",
      "min_val_loss_print 0.5605225563049316\n",
      "epoch:  0   step:  151   train loss:  0.4020307660102844  val loss:  0.5569316744804382\n",
      "min_val_loss_print 0.5569316744804382\n",
      "epoch:  0   step:  152   train loss:  0.4017220437526703  val loss:  0.5567039847373962\n",
      "min_val_loss_print 0.5567039847373962\n",
      "epoch:  0   step:  153   train loss:  0.5558532476425171  val loss:  0.55426424741745\n",
      "min_val_loss_print 0.55426424741745\n",
      "epoch:  0   step:  154   train loss:  0.440700888633728  val loss:  0.561411440372467\n",
      "epoch:  0   step:  155   train loss:  0.4552396237850189  val loss:  0.5644717812538147\n",
      "epoch:  0   step:  156   train loss:  0.40070846676826477  val loss:  0.5681102871894836\n",
      "epoch:  0   step:  157   train loss:  0.3795751929283142  val loss:  0.5731403231620789\n",
      "epoch:  0   step:  158   train loss:  0.537606418132782  val loss:  0.5792009234428406\n",
      "epoch:  0   step:  159   train loss:  0.4899020791053772  val loss:  0.583132266998291\n",
      "epoch:  0   step:  160   train loss:  0.5291927456855774  val loss:  0.5900416970252991\n",
      "epoch:  0   step:  161   train loss:  0.3899328112602234  val loss:  0.6042442917823792\n",
      "epoch:  0   step:  162   train loss:  0.5984816551208496  val loss:  0.6239005923271179\n",
      "epoch:  0   step:  163   train loss:  0.4085577130317688  val loss:  0.6565697193145752\n",
      "epoch:  0   step:  164   train loss:  0.3726227581501007  val loss:  0.6942466497421265\n",
      "epoch:  0   step:  165   train loss:  0.40565261244773865  val loss:  0.7254440784454346\n",
      "epoch:  0   step:  166   train loss:  0.6668563485145569  val loss:  0.7471585869789124\n",
      "epoch:  0   step:  167   train loss:  0.6574552059173584  val loss:  0.766193687915802\n",
      "epoch:  0   step:  168   train loss:  0.4655378460884094  val loss:  0.7795387506484985\n",
      "epoch:  0   step:  169   train loss:  0.5733668804168701  val loss:  0.7812976837158203\n",
      "epoch:  0   step:  170   train loss:  0.7851952910423279  val loss:  0.7795108556747437\n",
      "epoch:  0   step:  171   train loss:  0.37203502655029297  val loss:  0.7691968679428101\n",
      "epoch:  0   step:  172   train loss:  0.4476683437824249  val loss:  0.7483723759651184\n",
      "epoch:  0   step:  173   train loss:  0.43165916204452515  val loss:  0.7334168553352356\n",
      "epoch:  0   step:  174   train loss:  0.4611550569534302  val loss:  0.7185812592506409\n",
      "epoch:  0   step:  175   train loss:  0.6261492967605591  val loss:  0.692458987236023\n",
      "epoch:  0   step:  176   train loss:  0.6375312805175781  val loss:  0.667999804019928\n",
      "epoch:  0   step:  177   train loss:  0.3647984266281128  val loss:  0.64174485206604\n",
      "epoch:  0   step:  178   train loss:  0.44434836506843567  val loss:  0.6198565363883972\n",
      "epoch:  0   step:  179   train loss:  0.5299102663993835  val loss:  0.5934997797012329\n",
      "epoch:  0   step:  180   train loss:  0.6354460716247559  val loss:  0.5803965926170349\n",
      "epoch:  0   step:  181   train loss:  0.4797709286212921  val loss:  0.5808407664299011\n",
      "epoch:  0   step:  182   train loss:  0.44359254837036133  val loss:  0.5796375274658203\n",
      "epoch:  0   step:  183   train loss:  0.4027539789676666  val loss:  0.5909019708633423\n",
      "epoch:  0   step:  184   train loss:  0.4373624324798584  val loss:  0.6025669574737549\n",
      "epoch:  0   step:  185   train loss:  0.4794902205467224  val loss:  0.6100090742111206\n",
      "epoch:  0   step:  186   train loss:  0.46158140897750854  val loss:  0.6124540567398071\n",
      "epoch:  0   step:  187   train loss:  0.3331153690814972  val loss:  0.6151216626167297\n",
      "epoch:  0   step:  188   train loss:  0.5878466963768005  val loss:  0.6042330861091614\n",
      "epoch:  0   step:  189   train loss:  0.550436794757843  val loss:  0.5844036936759949\n",
      "epoch:  0   step:  190   train loss:  0.4191308915615082  val loss:  0.5656325221061707\n",
      "epoch:  0   step:  191   train loss:  0.7490454912185669  val loss:  0.5463789701461792\n",
      "min_val_loss_print 0.5463789701461792\n",
      "epoch:  0   step:  192   train loss:  0.4254913926124573  val loss:  0.5342304110527039\n",
      "min_val_loss_print 0.5342304110527039\n",
      "epoch:  0   step:  193   train loss:  0.4783999025821686  val loss:  0.5286221504211426\n",
      "min_val_loss_print 0.5286221504211426\n",
      "epoch:  0   step:  194   train loss:  0.5038044452667236  val loss:  0.5216938257217407\n",
      "min_val_loss_print 0.5216938257217407\n",
      "epoch:  0   step:  195   train loss:  0.4198598265647888  val loss:  0.5176323056221008\n",
      "min_val_loss_print 0.5176323056221008\n",
      "epoch:  0   step:  196   train loss:  0.48406246304512024  val loss:  0.5163488984107971\n",
      "min_val_loss_print 0.5163488984107971\n",
      "epoch:  0   step:  197   train loss:  0.32374927401542664  val loss:  0.5151717662811279\n",
      "min_val_loss_print 0.5151717662811279\n",
      "epoch:  0   step:  198   train loss:  0.568038821220398  val loss:  0.5140627026557922\n",
      "min_val_loss_print 0.5140627026557922\n",
      "epoch:  0   step:  199   train loss:  0.649498462677002  val loss:  0.5098461508750916\n",
      "min_val_loss_print 0.5098461508750916\n",
      "epoch:  0   step:  200   train loss:  0.48840615153312683  val loss:  0.5060166716575623\n",
      "min_val_loss_print 0.5060166716575623\n",
      "epoch:  0   step:  201   train loss:  0.38999417424201965  val loss:  0.5046709775924683\n",
      "min_val_loss_print 0.5046709775924683\n",
      "epoch:  0   step:  202   train loss:  0.5131030678749084  val loss:  0.5031100511550903\n",
      "min_val_loss_print 0.5031100511550903\n",
      "epoch:  0   step:  203   train loss:  0.4464886784553528  val loss:  0.5058264136314392\n",
      "epoch:  0   step:  204   train loss:  0.49745863676071167  val loss:  0.5108623504638672\n",
      "epoch:  0   step:  205   train loss:  0.3462768495082855  val loss:  0.5149672627449036\n",
      "epoch:  0   step:  206   train loss:  0.40416064858436584  val loss:  0.5198109745979309\n",
      "epoch:  0   step:  207   train loss:  0.4822036027908325  val loss:  0.522099494934082\n",
      "epoch:  0   step:  208   train loss:  0.45844191312789917  val loss:  0.5308509469032288\n",
      "epoch:  0   step:  209   train loss:  0.5150741338729858  val loss:  0.5399243831634521\n",
      "epoch:  0   step:  210   train loss:  0.3921559154987335  val loss:  0.538969874382019\n",
      "epoch:  0   step:  211   train loss:  0.36292943358421326  val loss:  0.5411167144775391\n",
      "epoch:  0   step:  212   train loss:  0.35690703988075256  val loss:  0.5427839159965515\n",
      "epoch:  0   step:  213   train loss:  0.38744619488716125  val loss:  0.5451785326004028\n",
      "epoch:  0   step:  214   train loss:  0.5770752429962158  val loss:  0.5389037132263184\n",
      "epoch:  0   step:  215   train loss:  0.5977053046226501  val loss:  0.5344091057777405\n",
      "epoch:  0   step:  216   train loss:  0.299701452255249  val loss:  0.5309275984764099\n",
      "epoch:  0   step:  217   train loss:  0.4212699234485626  val loss:  0.5278803706169128\n",
      "epoch:  0   step:  218   train loss:  0.5604565739631653  val loss:  0.5300902128219604\n",
      "epoch:  0   step:  219   train loss:  0.5597714781761169  val loss:  0.5339338779449463\n",
      "epoch:  0   step:  220   train loss:  0.4749223291873932  val loss:  0.5311971306800842\n",
      "epoch:  0   step:  221   train loss:  0.4523486793041229  val loss:  0.5381224155426025\n",
      "epoch:  0   step:  222   train loss:  0.5062081217765808  val loss:  0.5402224063873291\n",
      "epoch:  0   step:  223   train loss:  0.38574445247650146  val loss:  0.5434331297874451\n",
      "epoch:  0   step:  224   train loss:  0.401103138923645  val loss:  0.5425316691398621\n",
      "epoch:  0   step:  225   train loss:  0.6523774266242981  val loss:  0.5486600995063782\n",
      "epoch:  0   step:  226   train loss:  0.4015621542930603  val loss:  0.5436375737190247\n",
      "epoch:  0   step:  227   train loss:  0.4520566165447235  val loss:  0.542446494102478\n",
      "epoch:  0   step:  228   train loss:  0.5607847571372986  val loss:  0.5497896671295166\n",
      "epoch:  0   step:  229   train loss:  0.3843243718147278  val loss:  0.5518390536308289\n",
      "epoch:  0   step:  230   train loss:  0.40768229961395264  val loss:  0.5582888722419739\n",
      "epoch:  0   step:  231   train loss:  0.35282662510871887  val loss:  0.5640941262245178\n",
      "epoch:  0   step:  232   train loss:  0.30015307664871216  val loss:  0.5712928175926208\n",
      "epoch:  0   step:  233   train loss:  0.6244615316390991  val loss:  0.5702815055847168\n",
      "epoch:  0   step:  234   train loss:  0.36211153864860535  val loss:  0.5685166120529175\n",
      "epoch:  0   step:  235   train loss:  0.32163289189338684  val loss:  0.5648534893989563\n",
      "epoch:  0   step:  236   train loss:  0.4857465624809265  val loss:  0.5590192675590515\n",
      "epoch:  0   step:  237   train loss:  0.47229334712028503  val loss:  0.5529561042785645\n",
      "epoch:  0   step:  238   train loss:  0.282396525144577  val loss:  0.5451640486717224\n",
      "epoch:  0   step:  239   train loss:  0.48868271708488464  val loss:  0.5335882306098938\n",
      "epoch:  0   step:  240   train loss:  0.31380555033683777  val loss:  0.5279932618141174\n",
      "epoch:  0   step:  241   train loss:  0.5589736104011536  val loss:  0.5225195288658142\n",
      "epoch:  0   step:  242   train loss:  0.5496186017990112  val loss:  0.5213136672973633\n",
      "epoch:  0   step:  243   train loss:  0.6100444197654724  val loss:  0.5149528384208679\n",
      "epoch:  0   step:  244   train loss:  0.4207514822483063  val loss:  0.5118603706359863\n",
      "epoch:  0   step:  245   train loss:  0.3841898739337921  val loss:  0.5113175511360168\n",
      "epoch:  0   step:  246   train loss:  0.5869876146316528  val loss:  0.5123993158340454\n",
      "epoch:  0   step:  247   train loss:  0.3811991810798645  val loss:  0.5163803696632385\n",
      "epoch:  0   step:  248   train loss:  0.661853551864624  val loss:  0.5150156021118164\n",
      "epoch:  0   step:  249   train loss:  0.5123856067657471  val loss:  0.5083183646202087\n",
      "epoch:  0   step:  250   train loss:  0.3386799097061157  val loss:  0.5047001242637634\n",
      "epoch:  0   step:  251   train loss:  0.3649532198905945  val loss:  0.503898024559021\n",
      "epoch:  0   step:  252   train loss:  0.3233691453933716  val loss:  0.5048359632492065\n",
      "epoch:  0   step:  253   train loss:  0.4493255913257599  val loss:  0.5093386769294739\n",
      "epoch:  0   step:  254   train loss:  0.5485516786575317  val loss:  0.5166445970535278\n",
      "epoch:  0   step:  255   train loss:  0.4963187277317047  val loss:  0.5295579433441162\n",
      "epoch:  0   step:  338   train loss:  0.47058939933776855  val loss:  0.4957995116710663\n",
      "epoch:  0   step:  339   train loss:  0.3659369945526123  val loss:  0.49727991223335266\n",
      "epoch:  0   step:  340   train loss:  0.2867101728916168  val loss:  0.496044784784317\n",
      "epoch:  0   step:  341   train loss:  0.36418619751930237  val loss:  0.5028823614120483\n",
      "epoch:  0   step:  342   train loss:  0.35127198696136475  val loss:  0.5060031414031982\n",
      "epoch:  0   step:  343   train loss:  0.264066219329834  val loss:  0.5101384520530701\n",
      "epoch:  0   step:  344   train loss:  0.35946035385131836  val loss:  0.5136425495147705\n",
      "epoch:  0   step:  345   train loss:  0.6417446732521057  val loss:  0.511654794216156\n",
      "epoch:  0   step:  346   train loss:  0.5425505042076111  val loss:  0.5121253132820129\n",
      "epoch:  0   step:  347   train loss:  0.5620180368423462  val loss:  0.5127357244491577\n",
      "epoch:  0   step:  348   train loss:  0.4892309606075287  val loss:  0.5115191340446472\n",
      "epoch:  0   step:  349   train loss:  0.3185492753982544  val loss:  0.5054286122322083\n",
      "epoch:  0   step:  350   train loss:  0.3917204737663269  val loss:  0.5055291652679443\n",
      "epoch:  0   step:  351   train loss:  0.37484869360923767  val loss:  0.49981489777565\n",
      "epoch:  0   step:  352   train loss:  0.6637812256813049  val loss:  0.5055137276649475\n",
      "epoch:  0   step:  353   train loss:  0.37103453278541565  val loss:  0.5196609497070312\n",
      "epoch:  0   step:  354   train loss:  0.28222769498825073  val loss:  0.5255876183509827\n",
      "epoch:  0   step:  355   train loss:  0.3785253167152405  val loss:  0.5343791842460632\n",
      "epoch:  0   step:  356   train loss:  0.27814966440200806  val loss:  0.540278434753418\n",
      "epoch:  0   step:  357   train loss:  0.33764782547950745  val loss:  0.5457977056503296\n",
      "epoch:  0   step:  358   train loss:  0.5215363502502441  val loss:  0.5471618175506592\n",
      "epoch:  0   step:  359   train loss:  0.529927134513855  val loss:  0.5474457144737244\n",
      "epoch:  0   step:  360   train loss:  0.3093859851360321  val loss:  0.5505722761154175\n",
      "epoch:  0   step:  361   train loss:  0.4101569950580597  val loss:  0.5512120723724365\n",
      "epoch:  0   step:  362   train loss:  0.33057671785354614  val loss:  0.5480474233627319\n",
      "epoch:  0   step:  363   train loss:  0.39644065499305725  val loss:  0.5457527041435242\n",
      "epoch:  0   step:  364   train loss:  0.40603742003440857  val loss:  0.5423005223274231\n",
      "epoch:  0   step:  365   train loss:  0.41995856165885925  val loss:  0.5362677574157715\n",
      "epoch:  0   step:  366   train loss:  0.24238573014736176  val loss:  0.5300755500793457\n",
      "epoch:  0   step:  367   train loss:  0.38120365142822266  val loss:  0.5258114337921143\n",
      "epoch:  0   step:  368   train loss:  0.35561540722846985  val loss:  0.5170149803161621\n",
      "epoch:  0   step:  369   train loss:  0.2591187059879303  val loss:  0.5141869187355042\n",
      "epoch:  0   step:  370   train loss:  0.531571090221405  val loss:  0.5100486874580383\n",
      "epoch:  0   step:  371   train loss:  0.3739892840385437  val loss:  0.5043298006057739\n",
      "epoch:  0   step:  372   train loss:  0.41602304577827454  val loss:  0.4947556257247925\n",
      "epoch:  0   step:  373   train loss:  0.30251389741897583  val loss:  0.48248592019081116\n",
      "epoch:  0   step:  374   train loss:  0.6182671785354614  val loss:  0.4771938920021057\n",
      "epoch:  0   step:  375   train loss:  0.42913371324539185  val loss:  0.47531628608703613\n",
      "epoch:  0   step:  376   train loss:  0.4116988182067871  val loss:  0.47187092900276184\n",
      "epoch:  0   step:  377   train loss:  0.41505083441734314  val loss:  0.4718048572540283\n",
      "epoch:  0   step:  378   train loss:  0.3075157105922699  val loss:  0.478895366191864\n",
      "epoch:  0   step:  379   train loss:  0.3168843984603882  val loss:  0.48172271251678467\n",
      "epoch:  0   step:  380   train loss:  0.39079785346984863  val loss:  0.4905503988265991\n",
      "epoch:  0   step:  381   train loss:  0.48809465765953064  val loss:  0.4934232532978058\n",
      "epoch:  0   step:  382   train loss:  0.4147266447544098  val loss:  0.4986959397792816\n",
      "epoch:  0   step:  383   train loss:  0.3034202754497528  val loss:  0.5101823210716248\n",
      "epoch:  0   step:  384   train loss:  0.4176594018936157  val loss:  0.5170681476593018\n",
      "epoch:  0   step:  385   train loss:  0.4880351424217224  val loss:  0.5211095213890076\n",
      "epoch:  0   step:  386   train loss:  0.48823150992393494  val loss:  0.5190490484237671\n",
      "epoch:  0   step:  387   train loss:  0.3637540340423584  val loss:  0.5184018611907959\n",
      "epoch:  0   step:  388   train loss:  0.2864278554916382  val loss:  0.5199803113937378\n",
      "epoch:  0   step:  389   train loss:  0.403683066368103  val loss:  0.5208261609077454\n",
      "epoch:  0   step:  390   train loss:  0.6027299165725708  val loss:  0.5185025334358215\n",
      "epoch:  0   step:  391   train loss:  0.3321225643157959  val loss:  0.5143266320228577\n",
      "epoch:  0   step:  392   train loss:  0.33970385789871216  val loss:  0.5112746953964233\n",
      "epoch:  0   step:  393   train loss:  0.38202136754989624  val loss:  0.5124337077140808\n",
      "epoch:  0   step:  394   train loss:  0.393137663602829  val loss:  0.5118770003318787\n",
      "epoch:  0   step:  395   train loss:  0.3149835765361786  val loss:  0.5106260180473328\n",
      "epoch:  0   step:  396   train loss:  0.33582645654678345  val loss:  0.511197566986084\n",
      "epoch:  0   step:  397   train loss:  0.4748534560203552  val loss:  0.5114026069641113\n",
      "epoch:  0   step:  398   train loss:  0.3637637495994568  val loss:  0.5094018578529358\n",
      "epoch:  0   step:  399   train loss:  0.49162226915359497  val loss:  0.5073704123497009\n",
      "epoch:  0   step:  400   train loss:  0.5879745483398438  val loss:  0.504959762096405\n",
      "epoch:  0   step:  401   train loss:  0.49003729224205017  val loss:  0.4959685504436493\n",
      "epoch:  0   step:  402   train loss:  0.6212009787559509  val loss:  0.48548680543899536\n",
      "epoch:  0   step:  403   train loss:  0.4559839069843292  val loss:  0.479879230260849\n",
      "epoch:  0   step:  404   train loss:  0.47115248441696167  val loss:  0.4675810635089874\n",
      "min_val_loss_print 0.4675810635089874\n",
      "epoch:  0   step:  405   train loss:  0.4155583083629608  val loss:  0.46399545669555664\n",
      "min_val_loss_print 0.46399545669555664\n",
      "epoch:  0   step:  406   train loss:  0.3368869423866272  val loss:  0.4587875008583069\n",
      "min_val_loss_print 0.4587875008583069\n",
      "epoch:  0   step:  407   train loss:  0.62845778465271  val loss:  0.45458322763442993\n",
      "min_val_loss_print 0.45458322763442993\n",
      "epoch:  0   step:  408   train loss:  0.5385671257972717  val loss:  0.4560963213443756\n",
      "epoch:  0   step:  409   train loss:  0.5011519193649292  val loss:  0.4620577394962311\n",
      "epoch:  0   step:  410   train loss:  0.33522868156433105  val loss:  0.4617200791835785\n",
      "epoch:  0   step:  411   train loss:  0.30047693848609924  val loss:  0.4619564712047577\n",
      "epoch:  0   step:  412   train loss:  0.3242291212081909  val loss:  0.4626539945602417\n",
      "epoch:  0   step:  413   train loss:  0.5006421804428101  val loss:  0.46545276045799255\n",
      "epoch:  0   step:  414   train loss:  0.69297856092453  val loss:  0.47575774788856506\n",
      "epoch:  0   step:  415   train loss:  0.345619261264801  val loss:  0.4860302209854126\n",
      "epoch:  0   step:  416   train loss:  0.35085421800613403  val loss:  0.4950413703918457\n",
      "epoch:  0   step:  417   train loss:  0.7001646757125854  val loss:  0.5081173181533813\n",
      "epoch:  0   step:  418   train loss:  0.3382118046283722  val loss:  0.5174787044525146\n",
      "epoch:  0   step:  419   train loss:  0.4089788496494293  val loss:  0.5211698412895203\n",
      "epoch:  0   step:  420   train loss:  0.24578820168972015  val loss:  0.5228481888771057\n",
      "epoch:  0   step:  421   train loss:  0.47450584173202515  val loss:  0.5226194858551025\n",
      "epoch:  0   step:  422   train loss:  0.4353797137737274  val loss:  0.5145005583763123\n",
      "epoch:  0   step:  423   train loss:  0.6138688921928406  val loss:  0.5115576386451721\n",
      "epoch:  0   step:  424   train loss:  0.4958137273788452  val loss:  0.5051334500312805\n",
      "epoch:  0   step:  425   train loss:  0.37190940976142883  val loss:  0.4990772306919098\n",
      "epoch:  0   step:  426   train loss:  0.3528706133365631  val loss:  0.49646565318107605\n",
      "epoch:  0   step:  427   train loss:  0.32647427916526794  val loss:  0.4964420199394226\n",
      "epoch:  0   step:  428   train loss:  0.467663049697876  val loss:  0.5023044943809509\n",
      "epoch:  0   step:  429   train loss:  0.37862953543663025  val loss:  0.5062634944915771\n",
      "epoch:  0   step:  430   train loss:  0.40877828001976013  val loss:  0.5066035985946655\n",
      "epoch:  0   step:  431   train loss:  0.6087685227394104  val loss:  0.5092799663543701\n",
      "epoch:  0   step:  432   train loss:  0.5485599637031555  val loss:  0.5067939162254333\n",
      "epoch:  0   step:  433   train loss:  0.38724783062934875  val loss:  0.5032232999801636\n",
      "epoch:  0   step:  434   train loss:  0.29532909393310547  val loss:  0.4973459839820862\n",
      "epoch:  0   step:  435   train loss:  0.3096004128456116  val loss:  0.4920494854450226\n",
      "epoch:  0   step:  436   train loss:  0.3461816608905792  val loss:  0.4886730909347534\n",
      "epoch:  0   step:  437   train loss:  0.4882456958293915  val loss:  0.48542556166648865\n",
      "epoch:  0   step:  438   train loss:  0.3080297112464905  val loss:  0.4907509386539459\n",
      "epoch:  0   step:  439   train loss:  0.40391162037849426  val loss:  0.49690327048301697\n",
      "epoch:  0   step:  440   train loss:  0.34829699993133545  val loss:  0.5003376007080078\n",
      "epoch:  0   step:  441   train loss:  0.5385830402374268  val loss:  0.5008764266967773\n",
      "epoch:  0   step:  442   train loss:  0.2598504424095154  val loss:  0.501441478729248\n",
      "epoch:  0   step:  443   train loss:  0.2985803186893463  val loss:  0.5039151310920715\n",
      "epoch:  0   step:  444   train loss:  0.4198254644870758  val loss:  0.5061075091362\n",
      "epoch:  0   step:  445   train loss:  0.2884286642074585  val loss:  0.5041307806968689\n",
      "epoch:  0   step:  446   train loss:  0.259729266166687  val loss:  0.5048704743385315\n",
      "epoch:  0   step:  447   train loss:  0.28370964527130127  val loss:  0.5096312165260315\n",
      "epoch:  0   step:  448   train loss:  0.3493114709854126  val loss:  0.5110484957695007\n",
      "epoch:  0   step:  449   train loss:  0.5559653639793396  val loss:  0.5196211934089661\n",
      "epoch:  0   step:  450   train loss:  0.43378210067749023  val loss:  0.5265516638755798\n",
      "epoch:  0   step:  451   train loss:  1.3715020418167114  val loss:  0.5373589396476746\n",
      "epoch:  0   step:  452   train loss:  0.5066351294517517  val loss:  0.5470196604728699\n",
      "epoch:  0   step:  453   train loss:  0.3880748152732849  val loss:  0.5522854328155518\n",
      "epoch:  0   step:  454   train loss:  0.45877233147621155  val loss:  0.5585097074508667\n",
      "epoch:  0   step:  455   train loss:  0.35200896859169006  val loss:  0.564501941204071\n",
      "epoch:  0   step:  456   train loss:  0.5662875175476074  val loss:  0.5638700723648071\n",
      "epoch:  0   step:  457   train loss:  1.2243032455444336  val loss:  0.5571545362472534\n",
      "epoch:  0   step:  458   train loss:  0.44068461656570435  val loss:  0.5531186461448669\n",
      "epoch:  0   step:  459   train loss:  0.3986908793449402  val loss:  0.5475934147834778\n",
      "epoch:  0   step:  460   train loss:  0.4422725737094879  val loss:  0.5386691689491272\n",
      "epoch:  0   step:  461   train loss:  0.40444281697273254  val loss:  0.5342302322387695\n",
      "epoch:  0   step:  462   train loss:  0.3682405948638916  val loss:  0.5273526906967163\n",
      "epoch:  0   step:  463   train loss:  0.44797199964523315  val loss:  0.5179885029792786\n",
      "epoch:  0   step:  464   train loss:  0.42971110343933105  val loss:  0.5122395753860474\n",
      "epoch:  0   step:  465   train loss:  0.49585387110710144  val loss:  0.5135425329208374\n",
      "epoch:  0   step:  466   train loss:  0.5749388933181763  val loss:  0.5127048492431641\n",
      "epoch:  0   step:  467   train loss:  0.2566836178302765  val loss:  0.508437991142273\n",
      "epoch:  0   step:  468   train loss:  0.2790852189064026  val loss:  0.5006189942359924\n",
      "epoch:  0   step:  469   train loss:  0.4549809396266937  val loss:  0.4949582815170288\n",
      "epoch:  0   step:  470   train loss:  0.5309592485427856  val loss:  0.48645463585853577\n",
      "epoch:  0   step:  471   train loss:  0.4696768522262573  val loss:  0.4800577759742737\n",
      "epoch:  0   step:  472   train loss:  0.3803083002567291  val loss:  0.4713175296783447\n",
      "epoch:  0   step:  473   train loss:  0.29718849062919617  val loss:  0.4703020751476288\n",
      "epoch:  0   step:  474   train loss:  0.5006893873214722  val loss:  0.4667392075061798\n",
      "epoch:  0   step:  475   train loss:  0.3769749402999878  val loss:  0.4653918445110321\n",
      "epoch:  0   step:  476   train loss:  0.29673442244529724  val loss:  0.4645441770553589\n",
      "epoch:  0   step:  477   train loss:  0.29588645696640015  val loss:  0.47017917037010193\n",
      "epoch:  0   step:  478   train loss:  0.3534022569656372  val loss:  0.4730929136276245\n",
      "epoch:  0   step:  479   train loss:  0.39772313833236694  val loss:  0.47691142559051514\n",
      "epoch:  0   step:  480   train loss:  0.4625563323497772  val loss:  0.49593326449394226\n",
      "epoch:  0   step:  481   train loss:  0.23997189104557037  val loss:  0.5135540962219238\n",
      "epoch:  0   step:  482   train loss:  0.5319883823394775  val loss:  0.531959056854248\n",
      "epoch:  0   step:  483   train loss:  0.41241827607154846  val loss:  0.5447597503662109\n",
      "epoch:  0   step:  484   train loss:  0.45895928144454956  val loss:  0.5472866296768188\n",
      "epoch:  0   step:  485   train loss:  0.41858962178230286  val loss:  0.5422512888908386\n",
      "epoch:  0   step:  486   train loss:  0.33697831630706787  val loss:  0.5327118039131165\n",
      "epoch:  0   step:  487   train loss:  0.47719624638557434  val loss:  0.524936854839325\n",
      "epoch:  0   step:  488   train loss:  0.4261021018028259  val loss:  0.5134996771812439\n",
      "epoch:  0   step:  489   train loss:  0.33338552713394165  val loss:  0.5044332146644592\n",
      "epoch:  0   step:  490   train loss:  0.2468121498823166  val loss:  0.4983234107494354\n",
      "epoch:  0   step:  491   train loss:  0.2112392634153366  val loss:  0.4963175654411316\n",
      "epoch:  0   step:  492   train loss:  0.289816677570343  val loss:  0.4915618300437927\n",
      "epoch:  0   step:  493   train loss:  0.5551891326904297  val loss:  0.48761603236198425\n",
      "epoch:  0   step:  494   train loss:  0.30740782618522644  val loss:  0.4920330345630646\n",
      "epoch:  0   step:  495   train loss:  0.49668198823928833  val loss:  0.5019485950469971\n",
      "epoch:  0   step:  496   train loss:  0.5286470651626587  val loss:  0.5091003775596619\n",
      "epoch:  0   step:  497   train loss:  0.30489906668663025  val loss:  0.5178619027137756\n",
      "epoch:  0   step:  498   train loss:  0.4393397271633148  val loss:  0.5173880457878113\n",
      "epoch:  0   step:  499   train loss:  0.48029032349586487  val loss:  0.5363322496414185\n",
      "epoch:  0   step:  500   train loss:  0.5640417337417603  val loss:  0.5531531572341919\n",
      "epoch:  0   step:  501   train loss:  0.30697762966156006  val loss:  0.5692043900489807\n",
      "epoch:  0   step:  502   train loss:  0.4611855745315552  val loss:  0.5786572694778442\n",
      "epoch:  0   step:  503   train loss:  0.5800507068634033  val loss:  0.5827164649963379\n",
      "epoch:  0   step:  504   train loss:  0.5464615225791931  val loss:  0.5877135992050171\n",
      "epoch:  0   step:  505   train loss:  0.4109175205230713  val loss:  0.5861057043075562\n",
      "epoch:  0   step:  506   train loss:  0.41360315680503845  val loss:  0.5844899415969849\n",
      "epoch:  0   step:  507   train loss:  0.4390885531902313  val loss:  0.5816670656204224\n",
      "epoch:  0   step:  508   train loss:  0.4074045419692993  val loss:  0.578631579875946\n",
      "epoch:  0   step:  509   train loss:  0.6226834058761597  val loss:  0.5730635523796082\n",
      "epoch:  0   step:  510   train loss:  0.3869921565055847  val loss:  0.5695881247520447\n",
      "epoch:  0   step:  511   train loss:  0.3392348885536194  val loss:  0.570836067199707\n",
      "epoch:  0   step:  512   train loss:  0.523530900478363  val loss:  0.5685319900512695\n",
      "epoch:  0   step:  513   train loss:  0.4025556445121765  val loss:  0.5650887489318848\n",
      "epoch:  0   step:  514   train loss:  0.4818190336227417  val loss:  0.5608019232749939\n",
      "epoch:  0   step:  515   train loss:  0.6179917454719543  val loss:  0.552568793296814\n",
      "epoch:  0   step:  516   train loss:  0.2694891095161438  val loss:  0.5458422899246216\n",
      "epoch:  0   step:  517   train loss:  0.503633439540863  val loss:  0.5446592569351196\n",
      "epoch:  0   step:  518   train loss:  0.42425331473350525  val loss:  0.5392208695411682\n",
      "epoch:  0   step:  519   train loss:  0.417849063873291  val loss:  0.5307713150978088\n",
      "epoch:  0   step:  520   train loss:  0.6555154323577881  val loss:  0.5245810151100159\n",
      "epoch:  0   step:  521   train loss:  0.5149545073509216  val loss:  0.5129725337028503\n",
      "epoch:  0   step:  522   train loss:  0.42350903153419495  val loss:  0.5097377896308899\n",
      "epoch:  0   step:  523   train loss:  0.4244099259376526  val loss:  0.5034288763999939\n",
      "epoch:  0   step:  524   train loss:  0.6143388748168945  val loss:  0.5003473162651062\n",
      "epoch:  0   step:  525   train loss:  0.3834889233112335  val loss:  0.49726688861846924\n",
      "epoch:  0   step:  526   train loss:  0.3492380976676941  val loss:  0.4966139495372772\n",
      "epoch:  0   step:  527   train loss:  0.38378918170928955  val loss:  0.4966428577899933\n",
      "epoch:  0   step:  528   train loss:  0.43550240993499756  val loss:  0.49551814794540405\n",
      "epoch:  0   step:  529   train loss:  0.6382520198822021  val loss:  0.4948013424873352\n",
      "epoch:  0   step:  530   train loss:  0.2887254059314728  val loss:  0.49255791306495667\n",
      "epoch:  0   step:  531   train loss:  0.2713281214237213  val loss:  0.4906555414199829\n",
      "epoch:  0   step:  532   train loss:  0.4035584032535553  val loss:  0.4921361207962036\n",
      "epoch:  0   step:  533   train loss:  0.3360897898674011  val loss:  0.49233490228652954\n",
      "epoch:  0   step:  534   train loss:  0.41806623339653015  val loss:  0.4880947768688202\n",
      "epoch:  0   step:  535   train loss:  0.4045470654964447  val loss:  0.48506107926368713\n",
      "epoch:  0   step:  536   train loss:  0.4318179786205292  val loss:  0.47306931018829346\n",
      "epoch:  0   step:  537   train loss:  0.35576850175857544  val loss:  0.46171823143959045\n",
      "epoch:  0   step:  538   train loss:  0.30500802397727966  val loss:  0.4522063434123993\n",
      "min_val_loss_print 0.4522063434123993\n",
      "epoch:  0   step:  539   train loss:  0.4246569573879242  val loss:  0.4475284814834595\n",
      "min_val_loss_print 0.4475284814834595\n",
      "epoch:  0   step:  540   train loss:  0.3364212214946747  val loss:  0.44959545135498047\n",
      "epoch:  0   step:  541   train loss:  0.2881857752799988  val loss:  0.4504740238189697\n",
      "epoch:  0   step:  542   train loss:  0.38181570172309875  val loss:  0.4564204514026642\n",
      "epoch:  0   step:  543   train loss:  0.5667328238487244  val loss:  0.46157753467559814\n",
      "epoch:  0   step:  544   train loss:  0.3462531268596649  val loss:  0.46580222249031067\n",
      "epoch:  0   step:  545   train loss:  0.38129597902297974  val loss:  0.4692871570587158\n",
      "epoch:  0   step:  546   train loss:  0.35871586203575134  val loss:  0.475372850894928\n",
      "epoch:  0   step:  547   train loss:  0.4057402014732361  val loss:  0.4783601760864258\n",
      "epoch:  0   step:  548   train loss:  0.37771040201187134  val loss:  0.4779806435108185\n",
      "epoch:  0   step:  549   train loss:  0.39906954765319824  val loss:  0.4773603677749634\n",
      "epoch:  0   step:  550   train loss:  0.62327641248703  val loss:  0.47745481133461\n",
      "epoch:  0   step:  551   train loss:  0.7768986821174622  val loss:  0.4743729829788208\n",
      "epoch:  0   step:  552   train loss:  0.3518717586994171  val loss:  0.4778686463832855\n",
      "epoch:  0   step:  553   train loss:  0.333407998085022  val loss:  0.4757518470287323\n",
      "epoch:  0   step:  554   train loss:  0.35098496079444885  val loss:  0.47463536262512207\n",
      "epoch:  0   step:  555   train loss:  0.32581475377082825  val loss:  0.4727962017059326\n",
      "epoch:  0   step:  556   train loss:  0.42150259017944336  val loss:  0.4710314869880676\n",
      "epoch:  0   step:  557   train loss:  0.25383251905441284  val loss:  0.4699385464191437\n",
      "epoch:  0   step:  558   train loss:  0.4218061864376068  val loss:  0.47204214334487915\n",
      "epoch:  0   step:  559   train loss:  0.34976446628570557  val loss:  0.471625417470932\n",
      "epoch:  0   step:  560   train loss:  0.31111547350883484  val loss:  0.4699704945087433\n",
      "epoch:  0   step:  561   train loss:  0.41634178161621094  val loss:  0.4701720178127289\n",
      "epoch:  0   step:  562   train loss:  0.37221792340278625  val loss:  0.4662920832633972\n",
      "epoch:  0   step:  563   train loss:  0.5913594961166382  val loss:  0.46017709374427795\n",
      "epoch:  0   step:  564   train loss:  0.3414458632469177  val loss:  0.44807299971580505\n",
      "epoch:  0   step:  565   train loss:  0.33615338802337646  val loss:  0.4429536759853363\n",
      "min_val_loss_print 0.4429536759853363\n",
      "epoch:  0   step:  566   train loss:  0.40654125809669495  val loss:  0.4375770092010498\n",
      "min_val_loss_print 0.4375770092010498\n",
      "epoch:  0   step:  567   train loss:  0.320813924074173  val loss:  0.43286794424057007\n",
      "min_val_loss_print 0.43286794424057007\n",
      "epoch:  0   step:  568   train loss:  0.3921312093734741  val loss:  0.4279290437698364\n",
      "min_val_loss_print 0.4279290437698364\n",
      "epoch:  0   step:  569   train loss:  0.4131729006767273  val loss:  0.4238297939300537\n",
      "min_val_loss_print 0.4238297939300537\n",
      "epoch:  0   step:  570   train loss:  0.29806360602378845  val loss:  0.41963809728622437\n",
      "min_val_loss_print 0.41963809728622437\n",
      "epoch:  0   step:  571   train loss:  0.6742900013923645  val loss:  0.4180246591567993\n",
      "min_val_loss_print 0.4180246591567993\n",
      "epoch:  0   step:  572   train loss:  0.35426607728004456  val loss:  0.41718459129333496\n",
      "min_val_loss_print 0.41718459129333496\n",
      "epoch:  0   step:  573   train loss:  0.4397752583026886  val loss:  0.41755160689353943\n",
      "epoch:  0   step:  574   train loss:  0.46103349328041077  val loss:  0.41570794582366943\n",
      "min_val_loss_print 0.41570794582366943\n",
      "epoch:  0   step:  575   train loss:  0.5568439960479736  val loss:  0.4151371717453003\n",
      "min_val_loss_print 0.4151371717453003\n",
      "epoch:  0   step:  576   train loss:  0.3657098114490509  val loss:  0.417908251285553\n",
      "epoch:  0   step:  577   train loss:  0.287184476852417  val loss:  0.4189261496067047\n",
      "epoch:  0   step:  578   train loss:  0.3385068476200104  val loss:  0.4270954132080078\n",
      "epoch:  0   step:  579   train loss:  0.5205148458480835  val loss:  0.42580676078796387\n",
      "epoch:  0   step:  580   train loss:  0.2827923595905304  val loss:  0.42846062779426575\n",
      "epoch:  0   step:  581   train loss:  0.4850936830043793  val loss:  0.4285832643508911\n",
      "epoch:  0   step:  582   train loss:  0.40596047043800354  val loss:  0.4355441927909851\n",
      "epoch:  0   step:  583   train loss:  0.3259337246417999  val loss:  0.44652336835861206\n",
      "epoch:  0   step:  584   train loss:  0.5108644962310791  val loss:  0.45347848534584045\n",
      "epoch:  0   step:  585   train loss:  0.4358551800251007  val loss:  0.4566490352153778\n",
      "epoch:  0   step:  586   train loss:  0.5116761326789856  val loss:  0.4615505635738373\n",
      "epoch:  0   step:  587   train loss:  0.39469286799430847  val loss:  0.4652858078479767\n",
      "epoch:  0   step:  588   train loss:  0.5045280456542969  val loss:  0.46880611777305603\n",
      "epoch:  0   step:  589   train loss:  0.31744584441185  val loss:  0.4710398316383362\n",
      "epoch:  0   step:  590   train loss:  0.465956449508667  val loss:  0.47000595927238464\n",
      "epoch:  0   step:  591   train loss:  0.24943676590919495  val loss:  0.46974071860313416\n",
      "epoch:  0   step:  592   train loss:  0.6072210073471069  val loss:  0.4745345115661621\n",
      "epoch:  0   step:  593   train loss:  0.4612797200679779  val loss:  0.4767772853374481\n",
      "epoch:  0   step:  594   train loss:  0.2919810116291046  val loss:  0.4752115309238434\n",
      "epoch:  0   step:  595   train loss:  0.36346563696861267  val loss:  0.47266650199890137\n",
      "epoch:  0   step:  596   train loss:  0.3766976296901703  val loss:  0.4707557260990143\n",
      "epoch:  0   step:  597   train loss:  0.3304350674152374  val loss:  0.47006839513778687\n",
      "epoch:  0   step:  598   train loss:  0.4285616874694824  val loss:  0.4660108685493469\n",
      "epoch:  0   step:  599   train loss:  0.3573647439479828  val loss:  0.4644533097743988\n",
      "epoch:  0   step:  600   train loss:  0.3443388044834137  val loss:  0.4595317542552948\n",
      "epoch:  0   step:  601   train loss:  0.4343504309654236  val loss:  0.456632137298584\n",
      "epoch:  0   step:  602   train loss:  0.48439207673072815  val loss:  0.45197319984436035\n",
      "epoch:  0   step:  603   train loss:  0.377338171005249  val loss:  0.44916579127311707\n",
      "epoch:  0   step:  604   train loss:  0.44099122285842896  val loss:  0.4451455771923065\n",
      "epoch:  0   step:  605   train loss:  0.21592003107070923  val loss:  0.4411655068397522\n",
      "epoch:  0   step:  606   train loss:  0.44683629274368286  val loss:  0.4384525716304779\n",
      "epoch:  0   step:  607   train loss:  0.41248008608818054  val loss:  0.43792957067489624\n",
      "epoch:  0   step:  608   train loss:  0.5311902165412903  val loss:  0.4349138140678406\n",
      "epoch:  0   step:  609   train loss:  0.3799685537815094  val loss:  0.43229421973228455\n",
      "epoch:  0   step:  610   train loss:  0.48908665776252747  val loss:  0.4322779178619385\n",
      "epoch:  0   step:  611   train loss:  0.3193756937980652  val loss:  0.4323170483112335\n",
      "epoch:  0   step:  612   train loss:  0.409302681684494  val loss:  0.43102163076400757\n",
      "epoch:  0   step:  613   train loss:  0.29670631885528564  val loss:  0.42664167284965515\n",
      "epoch:  0   step:  614   train loss:  0.4754011631011963  val loss:  0.4237579107284546\n",
      "epoch:  0   step:  615   train loss:  0.3316645324230194  val loss:  0.4271128177642822\n",
      "epoch:  0   step:  616   train loss:  0.3374485671520233  val loss:  0.42961254715919495\n",
      "epoch:  0   step:  617   train loss:  0.28919970989227295  val loss:  0.4279584288597107\n",
      "epoch:  0   step:  618   train loss:  0.5487626791000366  val loss:  0.42738789319992065\n",
      "epoch:  0   step:  619   train loss:  0.32758012413978577  val loss:  0.4308307468891144\n",
      "epoch:  0   step:  620   train loss:  0.2841176390647888  val loss:  0.43426433205604553\n",
      "epoch:  0   step:  621   train loss:  0.3929955065250397  val loss:  0.4346439242362976\n",
      "epoch:  0   step:  622   train loss:  0.36042219400405884  val loss:  0.44244006276130676\n",
      "epoch:  0   step:  623   train loss:  0.4414348900318146  val loss:  0.452061265707016\n",
      "epoch:  0   step:  624   train loss:  1.1987905502319336  val loss:  0.4828258752822876\n",
      "epoch:  0   step:  625   train loss:  0.49837833642959595  val loss:  0.4999067187309265\n",
      "epoch:  0   step:  626   train loss:  0.3259703814983368  val loss:  0.5106992721557617\n",
      "epoch:  0   step:  627   train loss:  0.3886687457561493  val loss:  0.5164337158203125\n",
      "epoch:  0   step:  628   train loss:  0.3083265721797943  val loss:  0.5141946077346802\n",
      "epoch:  0   step:  629   train loss:  0.447982519865036  val loss:  0.5135180354118347\n",
      "epoch:  0   step:  630   train loss:  0.45075085759162903  val loss:  0.5059409141540527\n",
      "epoch:  0   step:  631   train loss:  0.34708383679389954  val loss:  0.5020487904548645\n",
      "epoch:  0   step:  632   train loss:  0.5322982668876648  val loss:  0.4991922676563263\n",
      "epoch:  0   step:  633   train loss:  0.48967692255973816  val loss:  0.49825266003608704\n",
      "epoch:  0   step:  634   train loss:  0.41670188307762146  val loss:  0.4964461028575897\n",
      "epoch:  0   step:  635   train loss:  0.4211699664592743  val loss:  0.49553975462913513\n",
      "epoch:  0   step:  636   train loss:  0.42348942160606384  val loss:  0.4935217499732971\n",
      "epoch:  0   step:  637   train loss:  0.453617662191391  val loss:  0.4885702431201935\n",
      "epoch:  0   step:  638   train loss:  0.4603176712989807  val loss:  0.48608145117759705\n",
      "epoch:  0   step:  639   train loss:  0.37512752413749695  val loss:  0.47948533296585083\n",
      "epoch:  0   step:  640   train loss:  0.42074820399284363  val loss:  0.4832024872303009\n",
      "epoch:  0   step:  641   train loss:  0.5819099545478821  val loss:  0.4887349009513855\n",
      "epoch:  0   step:  642   train loss:  0.3560175895690918  val loss:  0.4975743889808655\n",
      "epoch:  0   step:  643   train loss:  0.3973820209503174  val loss:  0.5012830495834351\n",
      "epoch:  0   step:  644   train loss:  0.4987756311893463  val loss:  0.5000514984130859\n",
      "epoch:  0   step:  645   train loss:  0.4103625416755676  val loss:  0.505523681640625\n",
      "epoch:  0   step:  646   train loss:  0.3762844204902649  val loss:  0.5105146765708923\n",
      "epoch:  0   step:  647   train loss:  0.4142582416534424  val loss:  0.5105299353599548\n",
      "epoch:  0   step:  648   train loss:  0.1605372577905655  val loss:  0.5099332332611084\n",
      "epoch:  0   step:  649   train loss:  0.4132511019706726  val loss:  0.5106152296066284\n",
      "epoch:  0   step:  650   train loss:  0.45138511061668396  val loss:  0.5101799964904785\n",
      "epoch:  0   step:  651   train loss:  0.5169011354446411  val loss:  0.5055671334266663\n",
      "epoch:  0   step:  652   train loss:  0.3118217885494232  val loss:  0.49617239832878113\n",
      "epoch:  0   step:  653   train loss:  0.4739305377006531  val loss:  0.4853527247905731\n",
      "epoch:  0   step:  654   train loss:  0.4055573344230652  val loss:  0.4767158031463623\n",
      "epoch:  0   step:  655   train loss:  0.41138848662376404  val loss:  0.4778370261192322\n",
      "epoch:  0   step:  656   train loss:  0.4536594748497009  val loss:  0.4783475697040558\n",
      "epoch:  0   step:  657   train loss:  0.27200403809547424  val loss:  0.47801336646080017\n",
      "epoch:  0   step:  658   train loss:  0.3744053542613983  val loss:  0.4835435152053833\n",
      "epoch:  0   step:  659   train loss:  0.38507676124572754  val loss:  0.4859539270401001\n",
      "epoch:  0   step:  660   train loss:  0.5671948790550232  val loss:  0.48289725184440613\n",
      "epoch:  0   step:  661   train loss:  0.22177596390247345  val loss:  0.4818010628223419\n",
      "epoch:  0   step:  662   train loss:  0.3336074650287628  val loss:  0.48126494884490967\n",
      "epoch:  0   step:  663   train loss:  0.43385809659957886  val loss:  0.4787927567958832\n",
      "epoch:  0   step:  664   train loss:  0.40119069814682007  val loss:  0.4766528308391571\n",
      "epoch:  0   step:  665   train loss:  0.5713410377502441  val loss:  0.4721904993057251\n",
      "epoch:  0   step:  666   train loss:  0.4217663109302521  val loss:  0.4713751971721649\n",
      "epoch:  0   step:  667   train loss:  0.43061351776123047  val loss:  0.4707016944885254\n",
      "epoch:  0   step:  668   train loss:  0.37963154911994934  val loss:  0.4715740978717804\n",
      "epoch:  0   step:  669   train loss:  0.49201780557632446  val loss:  0.4742251932621002\n",
      "epoch:  0   step:  670   train loss:  0.26332637667655945  val loss:  0.47385329008102417\n",
      "epoch:  0   step:  671   train loss:  0.4588656425476074  val loss:  0.47655636072158813\n",
      "epoch:  0   step:  672   train loss:  0.32194453477859497  val loss:  0.4747127294540405\n",
      "epoch:  0   step:  673   train loss:  0.32662612199783325  val loss:  0.47670474648475647\n",
      "epoch:  0   step:  674   train loss:  0.5145057439804077  val loss:  0.4746887981891632\n",
      "epoch:  0   step:  675   train loss:  0.46958497166633606  val loss:  0.4746450185775757\n",
      "epoch:  0   step:  676   train loss:  0.5903559327125549  val loss:  0.4727237820625305\n",
      "epoch:  0   step:  677   train loss:  0.5139907002449036  val loss:  0.4713393449783325\n",
      "epoch:  0   step:  678   train loss:  0.3530822992324829  val loss:  0.4729480445384979\n",
      "epoch:  0   step:  679   train loss:  0.29242855310440063  val loss:  0.4723266661167145\n",
      "epoch:  0   step:  680   train loss:  0.4749716818332672  val loss:  0.4737459719181061\n",
      "epoch:  0   step:  681   train loss:  0.33855774998664856  val loss:  0.476731538772583\n",
      "epoch:  0   step:  682   train loss:  0.4146260619163513  val loss:  0.481892853975296\n",
      "epoch:  0   step:  683   train loss:  0.5043533444404602  val loss:  0.4838305711746216\n",
      "epoch:  0   step:  684   train loss:  0.42909756302833557  val loss:  0.4855276644229889\n",
      "epoch:  0   step:  685   train loss:  0.47302430868148804  val loss:  0.4874408543109894\n",
      "epoch:  0   step:  686   train loss:  0.241336852312088  val loss:  0.48651623725891113\n",
      "epoch:  0   step:  687   train loss:  0.2834254801273346  val loss:  0.487244576215744\n",
      "epoch:  0   step:  688   train loss:  0.2393726408481598  val loss:  0.4923938810825348\n",
      "epoch:  0   step:  689   train loss:  0.48257777094841003  val loss:  0.49588286876678467\n",
      "epoch:  1   step:  0   train loss:  0.37707507610321045  val loss:  0.49027639627456665\n",
      "epoch:  1   step:  1   train loss:  0.6765031814575195  val loss:  0.48697429895401\n",
      "epoch:  1   step:  2   train loss:  0.3652714490890503  val loss:  0.4860800802707672\n",
      "epoch:  1   step:  3   train loss:  0.3062354028224945  val loss:  0.487881064414978\n",
      "epoch:  1   step:  4   train loss:  0.44082292914390564  val loss:  0.4875184893608093\n",
      "epoch:  1   step:  5   train loss:  0.3552740216255188  val loss:  0.48799267411231995\n",
      "epoch:  1   step:  6   train loss:  0.5087474584579468  val loss:  0.4865987002849579\n",
      "epoch:  1   step:  7   train loss:  0.2839260995388031  val loss:  0.48591309785842896\n",
      "epoch:  1   step:  8   train loss:  0.3087071478366852  val loss:  0.4852658808231354\n",
      "epoch:  1   step:  9   train loss:  0.22553816437721252  val loss:  0.4846145510673523\n",
      "epoch:  1   step:  10   train loss:  0.4214453399181366  val loss:  0.4830784499645233\n",
      "epoch:  1   step:  11   train loss:  0.3160005807876587  val loss:  0.48277807235717773\n",
      "epoch:  1   step:  12   train loss:  0.45196518301963806  val loss:  0.4821465313434601\n",
      "epoch:  1   step:  13   train loss:  0.3448159396648407  val loss:  0.4770561158657074\n",
      "epoch:  1   step:  14   train loss:  0.3946748375892639  val loss:  0.472351610660553\n",
      "epoch:  1   step:  15   train loss:  0.41511836647987366  val loss:  0.4727173149585724\n",
      "epoch:  1   step:  16   train loss:  0.46744731068611145  val loss:  0.4715765416622162\n",
      "epoch:  1   step:  17   train loss:  0.4964558482170105  val loss:  0.4733363389968872\n",
      "epoch:  1   step:  18   train loss:  0.5191073417663574  val loss:  0.47331079840660095\n",
      "epoch:  1   step:  19   train loss:  0.4068344235420227  val loss:  0.471099853515625\n",
      "epoch:  1   step:  20   train loss:  0.348501056432724  val loss:  0.4677099287509918\n",
      "epoch:  1   step:  21   train loss:  0.23501645028591156  val loss:  0.4603580832481384\n",
      "epoch:  1   step:  22   train loss:  0.3352322578430176  val loss:  0.4605599343776703\n",
      "epoch:  1   step:  23   train loss:  0.29085052013397217  val loss:  0.45862844586372375\n",
      "epoch:  1   step:  24   train loss:  0.4084457755088806  val loss:  0.4605235457420349\n",
      "epoch:  1   step:  25   train loss:  0.24191582202911377  val loss:  0.46318140625953674\n",
      "epoch:  1   step:  26   train loss:  0.2843453884124756  val loss:  0.4651768207550049\n",
      "epoch:  1   step:  27   train loss:  0.2528495490550995  val loss:  0.46516552567481995\n",
      "epoch:  1   step:  28   train loss:  0.3709316551685333  val loss:  0.4635044038295746\n",
      "epoch:  1   step:  29   train loss:  0.21386156976222992  val loss:  0.46098941564559937\n",
      "epoch:  1   step:  30   train loss:  0.5101835131645203  val loss:  0.45746663212776184\n",
      "epoch:  1   step:  31   train loss:  0.21021848917007446  val loss:  0.4557197093963623\n",
      "epoch:  1   step:  32   train loss:  0.307021826505661  val loss:  0.4517858922481537\n",
      "epoch:  1   step:  33   train loss:  0.4653521180152893  val loss:  0.4500839114189148\n",
      "epoch:  1   step:  34   train loss:  0.4389175772666931  val loss:  0.44804710149765015\n",
      "epoch:  1   step:  35   train loss:  0.32007038593292236  val loss:  0.44467249512672424\n",
      "epoch:  1   step:  36   train loss:  0.21573790907859802  val loss:  0.4410814046859741\n",
      "epoch:  1   step:  37   train loss:  0.3120253384113312  val loss:  0.4424995183944702\n",
      "epoch:  1   step:  38   train loss:  0.4154313802719116  val loss:  0.4457342326641083\n",
      "epoch:  1   step:  39   train loss:  0.4796355664730072  val loss:  0.4433136284351349\n",
      "epoch:  1   step:  40   train loss:  0.3065922260284424  val loss:  0.4436967074871063\n",
      "epoch:  1   step:  41   train loss:  0.2921704649925232  val loss:  0.4505876302719116\n",
      "epoch:  1   step:  42   train loss:  0.39225488901138306  val loss:  0.459868848323822\n",
      "epoch:  1   step:  43   train loss:  0.45708751678466797  val loss:  0.46011030673980713\n",
      "epoch:  1   step:  44   train loss:  0.44672244787216187  val loss:  0.46279168128967285\n",
      "epoch:  1   step:  45   train loss:  0.30445900559425354  val loss:  0.46507495641708374\n",
      "epoch:  1   step:  46   train loss:  0.4417102336883545  val loss:  0.46248701214790344\n",
      "epoch:  1   step:  47   train loss:  0.365970641374588  val loss:  0.4679950773715973\n",
      "epoch:  1   step:  48   train loss:  0.4741070866584778  val loss:  0.4682788848876953\n",
      "epoch:  1   step:  49   train loss:  0.237565815448761  val loss:  0.46454331278800964\n",
      "epoch:  1   step:  50   train loss:  0.21244922280311584  val loss:  0.4590555429458618\n",
      "epoch:  1   step:  51   train loss:  0.3634730875492096  val loss:  0.45654812455177307\n",
      "epoch:  1   step:  52   train loss:  0.32992881536483765  val loss:  0.45348796248435974\n",
      "epoch:  1   step:  53   train loss:  0.21770144999027252  val loss:  0.4523145258426666\n",
      "epoch:  1   step:  54   train loss:  0.3021698296070099  val loss:  0.4523371458053589\n",
      "epoch:  1   step:  55   train loss:  0.23651838302612305  val loss:  0.4492851197719574\n",
      "epoch:  1   step:  56   train loss:  0.3788334131240845  val loss:  0.4504370093345642\n",
      "epoch:  1   step:  57   train loss:  0.35020360350608826  val loss:  0.44684070348739624\n",
      "epoch:  1   step:  58   train loss:  0.6421630382537842  val loss:  0.4438226521015167\n",
      "epoch:  1   step:  59   train loss:  0.23697346448898315  val loss:  0.43808209896087646\n",
      "epoch:  1   step:  60   train loss:  0.40709564089775085  val loss:  0.43455269932746887\n",
      "epoch:  1   step:  61   train loss:  0.32011398673057556  val loss:  0.43242064118385315\n",
      "epoch:  1   step:  62   train loss:  0.2827756404876709  val loss:  0.4302408695220947\n",
      "epoch:  1   step:  63   train loss:  0.24674314260482788  val loss:  0.42919090390205383\n",
      "epoch:  1   step:  64   train loss:  0.37217462062835693  val loss:  0.4314643442630768\n",
      "epoch:  1   step:  65   train loss:  0.29921290278434753  val loss:  0.4342900514602661\n",
      "epoch:  1   step:  66   train loss:  0.2718859910964966  val loss:  0.43904343247413635\n",
      "epoch:  1   step:  67   train loss:  0.29534250497817993  val loss:  0.4455867409706116\n",
      "epoch:  1   step:  68   train loss:  0.16432179510593414  val loss:  0.44913509488105774\n",
      "epoch:  1   step:  69   train loss:  0.44734498858451843  val loss:  0.45448046922683716\n",
      "epoch:  1   step:  70   train loss:  0.29028791189193726  val loss:  0.4600539803504944\n",
      "epoch:  1   step:  71   train loss:  0.2946307361125946  val loss:  0.4645751416683197\n",
      "epoch:  1   step:  72   train loss:  0.46205198764801025  val loss:  0.46559938788414\n",
      "epoch:  1   step:  73   train loss:  0.5135265588760376  val loss:  0.4646861255168915\n",
      "epoch:  1   step:  74   train loss:  0.22396819293498993  val loss:  0.4629308879375458\n",
      "epoch:  1   step:  75   train loss:  0.32884570956230164  val loss:  0.4606502056121826\n",
      "epoch:  1   step:  76   train loss:  0.2810938358306885  val loss:  0.46276384592056274\n",
      "epoch:  1   step:  77   train loss:  0.538322389125824  val loss:  0.4650040566921234\n",
      "epoch:  1   step:  78   train loss:  0.47765734791755676  val loss:  0.46390005946159363\n",
      "epoch:  1   step:  79   train loss:  0.40585577487945557  val loss:  0.4642205238342285\n",
      "epoch:  1   step:  80   train loss:  0.3395581543445587  val loss:  0.4648522734642029\n",
      "epoch:  1   step:  81   train loss:  0.4699557423591614  val loss:  0.46064919233322144\n",
      "epoch:  1   step:  82   train loss:  0.2900503873825073  val loss:  0.4548947811126709\n",
      "epoch:  1   step:  83   train loss:  0.3252682089805603  val loss:  0.4504599869251251\n",
      "epoch:  1   step:  84   train loss:  0.37560611963272095  val loss:  0.4462263286113739\n",
      "epoch:  1   step:  85   train loss:  0.4370500445365906  val loss:  0.43957704305648804\n",
      "epoch:  1   step:  86   train loss:  0.23636797070503235  val loss:  0.4374563992023468\n",
      "epoch:  1   step:  87   train loss:  0.3794313371181488  val loss:  0.4360806941986084\n",
      "epoch:  1   step:  88   train loss:  0.2971952259540558  val loss:  0.43732336163520813\n",
      "epoch:  1   step:  89   train loss:  0.28822484612464905  val loss:  0.439594566822052\n",
      "epoch:  1   step:  90   train loss:  0.30781441926956177  val loss:  0.43836894631385803\n",
      "epoch:  1   step:  91   train loss:  0.33973848819732666  val loss:  0.43866050243377686\n",
      "epoch:  1   step:  92   train loss:  0.3571980595588684  val loss:  0.4361385405063629\n",
      "epoch:  1   step:  93   train loss:  0.46622806787490845  val loss:  0.4338333010673523\n",
      "epoch:  1   step:  94   train loss:  0.25869208574295044  val loss:  0.43328988552093506\n",
      "epoch:  1   step:  95   train loss:  0.3969300389289856  val loss:  0.43062496185302734\n",
      "epoch:  1   step:  96   train loss:  0.4025607407093048  val loss:  0.4311857223510742\n",
      "epoch:  1   step:  97   train loss:  0.42654988169670105  val loss:  0.42860186100006104\n",
      "epoch:  1   step:  98   train loss:  0.26187044382095337  val loss:  0.42827269434928894\n",
      "epoch:  1   step:  99   train loss:  0.33350157737731934  val loss:  0.427868515253067\n",
      "epoch:  1   step:  100   train loss:  0.2835170030593872  val loss:  0.43239447474479675\n",
      "epoch:  1   step:  101   train loss:  0.4197516441345215  val loss:  0.4336543679237366\n",
      "epoch:  1   step:  102   train loss:  0.42382001876831055  val loss:  0.43365970253944397\n",
      "epoch:  1   step:  103   train loss:  0.42927223443984985  val loss:  0.43116870522499084\n",
      "epoch:  1   step:  104   train loss:  0.30512338876724243  val loss:  0.42772868275642395\n",
      "epoch:  1   step:  105   train loss:  0.3040139079093933  val loss:  0.4312252998352051\n",
      "epoch:  1   step:  106   train loss:  0.30431461334228516  val loss:  0.4369087517261505\n",
      "epoch:  1   step:  107   train loss:  0.15931394696235657  val loss:  0.43874281644821167\n",
      "epoch:  1   step:  108   train loss:  0.2679063379764557  val loss:  0.44288870692253113\n",
      "epoch:  1   step:  109   train loss:  0.43750372529029846  val loss:  0.4434143602848053\n",
      "epoch:  1   step:  110   train loss:  0.4277639389038086  val loss:  0.4452935457229614\n",
      "epoch:  1   step:  111   train loss:  0.29018646478652954  val loss:  0.4439409673213959\n",
      "epoch:  1   step:  112   train loss:  0.34417062997817993  val loss:  0.44758567214012146\n",
      "epoch:  1   step:  113   train loss:  0.37783971428871155  val loss:  0.44687312841415405\n",
      "epoch:  1   step:  114   train loss:  0.36231181025505066  val loss:  0.44589364528656006\n",
      "epoch:  1   step:  115   train loss:  0.43969178199768066  val loss:  0.4467373192310333\n",
      "epoch:  1   step:  116   train loss:  0.5159309506416321  val loss:  0.4456963837146759\n",
      "epoch:  1   step:  117   train loss:  0.34130725264549255  val loss:  0.44139304757118225\n",
      "epoch:  1   step:  118   train loss:  0.3128255009651184  val loss:  0.4383397400379181\n",
      "epoch:  1   step:  119   train loss:  0.5077579617500305  val loss:  0.4325401782989502\n",
      "epoch:  1   step:  120   train loss:  0.5663283467292786  val loss:  0.423480361700058\n",
      "epoch:  1   step:  121   train loss:  0.42089858651161194  val loss:  0.41735658049583435\n",
      "epoch:  1   step:  122   train loss:  0.35720598697662354  val loss:  0.41654619574546814\n",
      "epoch:  1   step:  123   train loss:  0.23968301713466644  val loss:  0.41377711296081543\n",
      "min_val_loss_print 0.41377711296081543\n",
      "epoch:  1   step:  124   train loss:  0.25299468636512756  val loss:  0.4087423086166382\n",
      "min_val_loss_print 0.4087423086166382\n",
      "epoch:  1   step:  125   train loss:  0.4952620565891266  val loss:  0.41016751527786255\n",
      "epoch:  1   step:  126   train loss:  0.3035334646701813  val loss:  0.4073171317577362\n",
      "min_val_loss_print 0.4073171317577362\n",
      "epoch:  1   step:  127   train loss:  0.2440594583749771  val loss:  0.41021934151649475\n",
      "epoch:  1   step:  128   train loss:  0.28113293647766113  val loss:  0.41196146607398987\n",
      "epoch:  1   step:  129   train loss:  0.46742382645606995  val loss:  0.4142846167087555\n",
      "epoch:  1   step:  130   train loss:  0.2952203154563904  val loss:  0.41362208127975464\n",
      "epoch:  1   step:  131   train loss:  0.36631256341934204  val loss:  0.4142123758792877\n",
      "epoch:  1   step:  132   train loss:  0.37596747279167175  val loss:  0.4165228605270386\n",
      "epoch:  1   step:  133   train loss:  0.27319350838661194  val loss:  0.4143911898136139\n",
      "epoch:  1   step:  134   train loss:  0.4035918712615967  val loss:  0.4139145016670227\n",
      "epoch:  1   step:  135   train loss:  0.22664421796798706  val loss:  0.4136679172515869\n",
      "epoch:  1   step:  136   train loss:  0.3937493860721588  val loss:  0.41294771432876587\n",
      "epoch:  1   step:  137   train loss:  0.294849693775177  val loss:  0.4127052426338196\n",
      "epoch:  1   step:  138   train loss:  1.4522236585617065  val loss:  0.42505791783332825\n",
      "epoch:  1   step:  139   train loss:  0.27567294239997864  val loss:  0.427528977394104\n",
      "epoch:  1   step:  140   train loss:  0.35269635915756226  val loss:  0.43125542998313904\n",
      "epoch:  1   step:  141   train loss:  0.43090739846229553  val loss:  0.43765491247177124\n",
      "epoch:  1   step:  142   train loss:  0.3333408832550049  val loss:  0.44521406292915344\n",
      "epoch:  1   step:  143   train loss:  0.397946298122406  val loss:  0.4510730803012848\n",
      "epoch:  1   step:  144   train loss:  0.21402478218078613  val loss:  0.4532872438430786\n",
      "epoch:  1   step:  145   train loss:  0.2887163758277893  val loss:  0.45664554834365845\n",
      "epoch:  1   step:  146   train loss:  0.32463720440864563  val loss:  0.45464104413986206\n",
      "epoch:  1   step:  147   train loss:  0.5427358746528625  val loss:  0.45401477813720703\n",
      "epoch:  1   step:  148   train loss:  0.37615570425987244  val loss:  0.45189061760902405\n",
      "epoch:  1   step:  149   train loss:  0.33595186471939087  val loss:  0.4468306601047516\n",
      "epoch:  1   step:  150   train loss:  0.3684804141521454  val loss:  0.44534409046173096\n",
      "epoch:  1   step:  151   train loss:  0.3920355439186096  val loss:  0.4397144019603729\n",
      "epoch:  1   step:  152   train loss:  0.31407612562179565  val loss:  0.4349260628223419\n",
      "epoch:  1   step:  153   train loss:  0.29216429591178894  val loss:  0.4311765432357788\n",
      "epoch:  1   step:  154   train loss:  0.2464657872915268  val loss:  0.4249509274959564\n",
      "epoch:  1   step:  155   train loss:  0.4659835398197174  val loss:  0.42564865946769714\n",
      "epoch:  1   step:  156   train loss:  0.26131170988082886  val loss:  0.42435795068740845\n",
      "epoch:  1   step:  157   train loss:  0.3184029459953308  val loss:  0.42189425230026245\n",
      "epoch:  1   step:  158   train loss:  0.3726235330104828  val loss:  0.4234425127506256\n",
      "epoch:  1   step:  159   train loss:  0.48203322291374207  val loss:  0.4210064113140106\n",
      "epoch:  1   step:  160   train loss:  0.4161810278892517  val loss:  0.4188033640384674\n",
      "epoch:  1   step:  161   train loss:  0.6449640989303589  val loss:  0.415984183549881\n",
      "epoch:  1   step:  162   train loss:  0.4189901351928711  val loss:  0.41882842779159546\n",
      "epoch:  1   step:  163   train loss:  0.5288487076759338  val loss:  0.4192788302898407\n",
      "epoch:  1   step:  164   train loss:  0.35813194513320923  val loss:  0.42274975776672363\n",
      "epoch:  1   step:  165   train loss:  0.477340430021286  val loss:  0.4228106737136841\n",
      "epoch:  1   step:  166   train loss:  0.3733031153678894  val loss:  0.42276838421821594\n",
      "epoch:  1   step:  167   train loss:  0.34606459736824036  val loss:  0.42370814085006714\n",
      "epoch:  1   step:  168   train loss:  0.41302165389060974  val loss:  0.41815605759620667\n",
      "epoch:  1   step:  169   train loss:  0.41974619030952454  val loss:  0.4172324240207672\n",
      "epoch:  1   step:  170   train loss:  0.312924861907959  val loss:  0.425004780292511\n",
      "epoch:  1   step:  171   train loss:  0.402041494846344  val loss:  0.4346185028553009\n",
      "epoch:  1   step:  172   train loss:  0.38895106315612793  val loss:  0.4431288242340088\n",
      "epoch:  1   step:  173   train loss:  0.4462946355342865  val loss:  0.4535384774208069\n",
      "epoch:  1   step:  174   train loss:  0.3362407982349396  val loss:  0.4594290256500244\n",
      "epoch:  1   step:  175   train loss:  0.4350636303424835  val loss:  0.4605887234210968\n",
      "epoch:  1   step:  176   train loss:  0.28855448961257935  val loss:  0.4611448347568512\n",
      "epoch:  1   step:  177   train loss:  0.3519739508628845  val loss:  0.4632886052131653\n",
      "epoch:  1   step:  178   train loss:  0.3170807361602783  val loss:  0.4689025282859802\n",
      "epoch:  1   step:  179   train loss:  0.20099832117557526  val loss:  0.47459766268730164\n",
      "epoch:  1   step:  180   train loss:  0.2644006609916687  val loss:  0.4804343283176422\n",
      "epoch:  1   step:  181   train loss:  0.36047235131263733  val loss:  0.4848671853542328\n",
      "epoch:  1   step:  182   train loss:  0.3863986134529114  val loss:  0.49220824241638184\n",
      "epoch:  1   step:  183   train loss:  0.28501972556114197  val loss:  0.49590742588043213\n",
      "epoch:  1   step:  184   train loss:  0.48067447543144226  val loss:  0.5031407475471497\n",
      "epoch:  1   step:  185   train loss:  0.28846341371536255  val loss:  0.5080916881561279\n",
      "epoch:  1   step:  186   train loss:  0.30101075768470764  val loss:  0.5130173563957214\n",
      "epoch:  1   step:  187   train loss:  0.48190778493881226  val loss:  0.5157599449157715\n",
      "epoch:  1   step:  188   train loss:  0.28191742300987244  val loss:  0.5168363451957703\n",
      "epoch:  1   step:  189   train loss:  0.21377888321876526  val loss:  0.513526976108551\n",
      "epoch:  1   step:  190   train loss:  0.405129611492157  val loss:  0.5105172395706177\n",
      "epoch:  1   step:  191   train loss:  0.3373154401779175  val loss:  0.5075693130493164\n",
      "epoch:  1   step:  192   train loss:  0.35460740327835083  val loss:  0.5055884122848511\n",
      "epoch:  1   step:  193   train loss:  0.2636408507823944  val loss:  0.5046773552894592\n",
      "epoch:  1   step:  194   train loss:  0.2807312607765198  val loss:  0.4823768138885498\n",
      "epoch:  1   step:  195   train loss:  0.28462958335876465  val loss:  0.46859025955200195\n",
      "epoch:  1   step:  196   train loss:  0.2760601341724396  val loss:  0.45717233419418335\n",
      "epoch:  1   step:  197   train loss:  0.5098646283149719  val loss:  0.4444873034954071\n",
      "epoch:  1   step:  198   train loss:  0.41782569885253906  val loss:  0.43501022458076477\n",
      "epoch:  1   step:  199   train loss:  0.34395474195480347  val loss:  0.4262230098247528\n",
      "epoch:  1   step:  200   train loss:  0.2602585554122925  val loss:  0.4230121970176697\n",
      "epoch:  1   step:  201   train loss:  0.267579048871994  val loss:  0.42048779129981995\n",
      "epoch:  1   step:  202   train loss:  0.39568737149238586  val loss:  0.41997966170310974\n",
      "epoch:  1   step:  203   train loss:  0.6122939586639404  val loss:  0.4154666066169739\n",
      "epoch:  1   step:  204   train loss:  0.3655112385749817  val loss:  0.4113583564758301\n",
      "epoch:  1   step:  205   train loss:  0.4110553562641144  val loss:  0.40846359729766846\n",
      "epoch:  1   step:  206   train loss:  0.2402913123369217  val loss:  0.4037168323993683\n",
      "min_val_loss_print 0.4037168323993683\n",
      "epoch:  1   step:  207   train loss:  0.29030510783195496  val loss:  0.39896607398986816\n",
      "min_val_loss_print 0.39896607398986816\n",
      "epoch:  1   step:  208   train loss:  0.31938081979751587  val loss:  0.40059712529182434\n",
      "epoch:  1   step:  209   train loss:  0.32706597447395325  val loss:  0.40363118052482605\n",
      "epoch:  1   step:  210   train loss:  0.38064825534820557  val loss:  0.4088151454925537\n",
      "epoch:  1   step:  211   train loss:  0.31830325722694397  val loss:  0.41289153695106506\n",
      "epoch:  1   step:  212   train loss:  0.3697015345096588  val loss:  0.4217349886894226\n",
      "epoch:  1   step:  213   train loss:  0.25014257431030273  val loss:  0.425666481256485\n",
      "epoch:  1   step:  214   train loss:  0.3839623034000397  val loss:  0.4299464225769043\n",
      "epoch:  1   step:  215   train loss:  0.4534491300582886  val loss:  0.4318154454231262\n",
      "epoch:  1   step:  216   train loss:  0.4339161217212677  val loss:  0.43793773651123047\n",
      "epoch:  1   step:  217   train loss:  0.47182369232177734  val loss:  0.4379274249076843\n",
      "epoch:  1   step:  218   train loss:  0.3438386619091034  val loss:  0.43553856015205383\n",
      "epoch:  1   step:  219   train loss:  0.3188644051551819  val loss:  0.43632709980010986\n",
      "epoch:  1   step:  220   train loss:  0.3725525140762329  val loss:  0.4373170733451843\n",
      "epoch:  1   step:  221   train loss:  0.2923385500907898  val loss:  0.43625780940055847\n",
      "epoch:  1   step:  222   train loss:  0.3227437138557434  val loss:  0.4384416341781616\n",
      "epoch:  1   step:  223   train loss:  0.34289368987083435  val loss:  0.43803003430366516\n",
      "epoch:  1   step:  224   train loss:  0.28042319416999817  val loss:  0.4388101398944855\n",
      "epoch:  1   step:  225   train loss:  0.4265369176864624  val loss:  0.43389421701431274\n",
      "epoch:  1   step:  226   train loss:  0.3581676781177521  val loss:  0.4361020028591156\n",
      "epoch:  1   step:  227   train loss:  0.38319867849349976  val loss:  0.43602609634399414\n",
      "epoch:  1   step:  228   train loss:  0.2490491271018982  val loss:  0.4432084262371063\n",
      "epoch:  1   step:  229   train loss:  0.4912986755371094  val loss:  0.4432739019393921\n",
      "epoch:  1   step:  230   train loss:  0.3902430534362793  val loss:  0.4461170732975006\n",
      "epoch:  1   step:  231   train loss:  0.27427932620048523  val loss:  0.44631460309028625\n",
      "epoch:  1   step:  232   train loss:  0.28856074810028076  val loss:  0.441969096660614\n",
      "epoch:  1   step:  233   train loss:  0.33382296562194824  val loss:  0.4430808424949646\n",
      "epoch:  1   step:  234   train loss:  0.3158895969390869  val loss:  0.44266632199287415\n",
      "epoch:  1   step:  235   train loss:  0.4551132321357727  val loss:  0.4477415084838867\n",
      "epoch:  1   step:  236   train loss:  0.5970554351806641  val loss:  0.45238474011421204\n",
      "epoch:  1   step:  237   train loss:  0.2449689656496048  val loss:  0.45758289098739624\n",
      "epoch:  1   step:  238   train loss:  0.42588886618614197  val loss:  0.4653545618057251\n",
      "epoch:  1   step:  239   train loss:  0.3619954586029053  val loss:  0.47581595182418823\n",
      "epoch:  1   step:  240   train loss:  0.3021228015422821  val loss:  0.4817153215408325\n",
      "epoch:  1   step:  241   train loss:  0.3741077184677124  val loss:  0.48718687891960144\n",
      "epoch:  1   step:  242   train loss:  0.31140097975730896  val loss:  0.4844122529029846\n",
      "epoch:  1   step:  243   train loss:  0.33534666895866394  val loss:  0.4791381359100342\n",
      "epoch:  1   step:  244   train loss:  0.3147795498371124  val loss:  0.4767480194568634\n",
      "epoch:  1   step:  245   train loss:  0.6307599544525146  val loss:  0.47860944271087646\n",
      "epoch:  1   step:  246   train loss:  0.34364429116249084  val loss:  0.47933274507522583\n",
      "epoch:  1   step:  247   train loss:  0.3408772051334381  val loss:  0.47904038429260254\n",
      "epoch:  1   step:  248   train loss:  0.34580984711647034  val loss:  0.4721405804157257\n",
      "epoch:  1   step:  249   train loss:  0.3060036301612854  val loss:  0.4618426561355591\n",
      "epoch:  1   step:  250   train loss:  0.3467012941837311  val loss:  0.45219799876213074\n",
      "epoch:  1   step:  251   train loss:  0.3103005886077881  val loss:  0.4473508894443512\n",
      "epoch:  1   step:  252   train loss:  0.429264634847641  val loss:  0.4409680664539337\n",
      "epoch:  1   step:  253   train loss:  0.2753973603248596  val loss:  0.4344906806945801\n",
      "epoch:  1   step:  254   train loss:  0.4253242015838623  val loss:  0.42988479137420654\n",
      "epoch:  1   step:  255   train loss:  0.2981976270675659  val loss:  0.4221120774745941\n",
      "epoch:  1   step:  256   train loss:  0.33606666326522827  val loss:  0.4192110300064087\n",
      "epoch:  1   step:  257   train loss:  0.26765334606170654  val loss:  0.4142706096172333\n",
      "epoch:  1   step:  258   train loss:  0.4097640812397003  val loss:  0.4157713055610657\n",
      "epoch:  1   step:  259   train loss:  0.31924766302108765  val loss:  0.4142772853374481\n",
      "epoch:  1   step:  260   train loss:  0.34417223930358887  val loss:  0.42142394185066223\n",
      "epoch:  1   step:  261   train loss:  0.3824637234210968  val loss:  0.43426239490509033\n",
      "epoch:  1   step:  262   train loss:  0.3387158215045929  val loss:  0.4495707154273987\n",
      "epoch:  1   step:  263   train loss:  0.3404310941696167  val loss:  0.46193161606788635\n",
      "epoch:  1   step:  264   train loss:  0.2778702974319458  val loss:  0.47101885080337524\n",
      "epoch:  1   step:  265   train loss:  0.336824506521225  val loss:  0.4774247705936432\n",
      "epoch:  1   step:  266   train loss:  0.4714837968349457  val loss:  0.5258936882019043\n",
      "epoch:  1   step:  267   train loss:  0.3990388512611389  val loss:  0.5746167302131653\n",
      "epoch:  1   step:  268   train loss:  0.2090846300125122  val loss:  0.6324698328971863\n",
      "epoch:  1   step:  269   train loss:  0.15065814554691315  val loss:  0.674219012260437\n",
      "epoch:  1   step:  270   train loss:  0.3007968068122864  val loss:  0.6911133527755737\n",
      "epoch:  1   step:  271   train loss:  0.3986419141292572  val loss:  0.6781996488571167\n",
      "epoch:  1   step:  272   train loss:  0.5612849593162537  val loss:  0.633287250995636\n",
      "epoch:  1   step:  273   train loss:  0.6149671077728271  val loss:  0.575297474861145\n",
      "epoch:  1   step:  274   train loss:  0.41845858097076416  val loss:  0.5172942876815796\n",
      "epoch:  1   step:  275   train loss:  0.23573629558086395  val loss:  0.48114973306655884\n",
      "epoch:  1   step:  276   train loss:  0.5823251008987427  val loss:  0.45781615376472473\n",
      "epoch:  1   step:  277   train loss:  0.42325612902641296  val loss:  0.4419240355491638\n",
      "epoch:  1   step:  278   train loss:  0.32202234864234924  val loss:  0.4305258095264435\n",
      "epoch:  1   step:  279   train loss:  0.3060145974159241  val loss:  0.4195761978626251\n",
      "epoch:  1   step:  280   train loss:  0.5489687323570251  val loss:  0.4125937521457672\n",
      "epoch:  1   step:  281   train loss:  0.30719518661499023  val loss:  0.40625953674316406\n",
      "epoch:  1   step:  282   train loss:  0.26544174551963806  val loss:  0.39986470341682434\n",
      "epoch:  1   step:  283   train loss:  0.32029494643211365  val loss:  0.39566323161125183\n",
      "min_val_loss_print 0.39566323161125183\n",
      "epoch:  1   step:  284   train loss:  0.326198011636734  val loss:  0.3906000554561615\n",
      "min_val_loss_print 0.3906000554561615\n",
      "epoch:  1   step:  285   train loss:  0.3448716402053833  val loss:  0.38747328519821167\n",
      "min_val_loss_print 0.38747328519821167\n",
      "epoch:  1   step:  286   train loss:  0.4838317334651947  val loss:  0.38972458243370056\n",
      "epoch:  1   step:  297   train loss:  0.2635380029678345  val loss:  0.4023120403289795\n",
      "epoch:  1   step:  298   train loss:  0.3109755516052246  val loss:  0.4040706753730774\n",
      "epoch:  1   step:  299   train loss:  0.3382018804550171  val loss:  0.40684425830841064\n",
      "epoch:  1   step:  300   train loss:  0.4509153962135315  val loss:  0.40841737389564514\n",
      "epoch:  1   step:  301   train loss:  0.3735809326171875  val loss:  0.4077078104019165\n",
      "epoch:  1   step:  302   train loss:  0.28767359256744385  val loss:  0.40578216314315796\n",
      "epoch:  1   step:  303   train loss:  0.19990594685077667  val loss:  0.4111964702606201\n",
      "epoch:  1   step:  304   train loss:  0.3800082504749298  val loss:  0.4223235845565796\n",
      "epoch:  1   step:  305   train loss:  0.22754280269145966  val loss:  0.43774473667144775\n",
      "epoch:  1   step:  306   train loss:  0.2920114994049072  val loss:  0.44730088114738464\n",
      "epoch:  1   step:  307   train loss:  0.2462633103132248  val loss:  0.44447726011276245\n",
      "epoch:  1   step:  308   train loss:  0.29178470373153687  val loss:  0.44525793194770813\n",
      "epoch:  1   step:  309   train loss:  0.36847877502441406  val loss:  0.44101443886756897\n",
      "epoch:  1   step:  310   train loss:  0.3164411187171936  val loss:  0.4398655593395233\n",
      "epoch:  1   step:  311   train loss:  0.23655229806900024  val loss:  0.43691301345825195\n",
      "epoch:  1   step:  312   train loss:  0.3516792953014374  val loss:  0.4335520565509796\n",
      "epoch:  1   step:  313   train loss:  0.3215935230255127  val loss:  0.42950379848480225\n",
      "epoch:  1   step:  314   train loss:  0.49356675148010254  val loss:  0.4196050763130188\n",
      "epoch:  1   step:  315   train loss:  0.43137115240097046  val loss:  0.41392749547958374\n",
      "epoch:  1   step:  316   train loss:  0.337159126996994  val loss:  0.4071809947490692\n",
      "epoch:  1   step:  317   train loss:  0.2931770086288452  val loss:  0.40621501207351685\n",
      "epoch:  1   step:  318   train loss:  0.4133332669734955  val loss:  0.40799611806869507\n",
      "epoch:  1   step:  319   train loss:  0.33419495820999146  val loss:  0.4084698259830475\n",
      "epoch:  1   step:  320   train loss:  0.4228879511356354  val loss:  0.4087502658367157\n",
      "epoch:  1   step:  321   train loss:  0.26481372117996216  val loss:  0.4072284698486328\n",
      "epoch:  1   step:  322   train loss:  0.533935546875  val loss:  0.40576812624931335\n",
      "epoch:  1   step:  323   train loss:  0.2692737877368927  val loss:  0.40335583686828613\n",
      "epoch:  1   step:  324   train loss:  0.17116688191890717  val loss:  0.4007199704647064\n",
      "epoch:  1   step:  325   train loss:  0.48760390281677246  val loss:  0.40312257409095764\n",
      "epoch:  1   step:  326   train loss:  0.26753246784210205  val loss:  0.4116400480270386\n",
      "epoch:  1   step:  327   train loss:  0.2512015402317047  val loss:  0.41834568977355957\n",
      "epoch:  1   step:  328   train loss:  0.4338337481021881  val loss:  0.4226369261741638\n",
      "epoch:  1   step:  329   train loss:  0.25772854685783386  val loss:  0.426220178604126\n",
      "epoch:  1   step:  330   train loss:  0.2853468656539917  val loss:  0.4279480278491974\n",
      "epoch:  1   step:  331   train loss:  0.34642294049263  val loss:  0.4312799274921417\n",
      "epoch:  1   step:  332   train loss:  0.3106330633163452  val loss:  0.4375327527523041\n",
      "epoch:  1   step:  333   train loss:  0.3758028745651245  val loss:  0.4353667199611664\n",
      "epoch:  1   step:  334   train loss:  0.2664228677749634  val loss:  0.4294581711292267\n",
      "epoch:  1   step:  335   train loss:  0.2943437695503235  val loss:  0.4274460971355438\n",
      "epoch:  1   step:  336   train loss:  0.26463374495506287  val loss:  0.41840898990631104\n",
      "epoch:  1   step:  337   train loss:  0.49176478385925293  val loss:  0.415423721075058\n",
      "epoch:  1   step:  338   train loss:  0.38948360085487366  val loss:  0.41460633277893066\n",
      "epoch:  1   step:  339   train loss:  0.21668575704097748  val loss:  0.4146288335323334\n",
      "epoch:  1   step:  427   train loss:  0.2762012779712677  val loss:  0.4603332579135895\n",
      "epoch:  1   step:  428   train loss:  0.3642241358757019  val loss:  0.4756696820259094\n",
      "epoch:  1   step:  429   train loss:  0.3972542881965637  val loss:  0.482452392578125\n",
      "epoch:  1   step:  430   train loss:  0.31172889471054077  val loss:  0.4900868237018585\n",
      "epoch:  1   step:  431   train loss:  0.18903955817222595  val loss:  0.4947696030139923\n",
      "epoch:  1   step:  432   train loss:  0.19337208569049835  val loss:  0.4980507493019104\n",
      "epoch:  1   step:  433   train loss:  0.5767672657966614  val loss:  0.49662911891937256\n",
      "epoch:  1   step:  434   train loss:  0.27749791741371155  val loss:  0.49271443486213684\n",
      "epoch:  1   step:  435   train loss:  0.32887738943099976  val loss:  0.4895614683628082\n",
      "epoch:  1   step:  436   train loss:  0.6095566749572754  val loss:  0.5012161135673523\n",
      "epoch:  1   step:  437   train loss:  0.3886128067970276  val loss:  0.5069950222969055\n",
      "epoch:  1   step:  438   train loss:  0.3057926893234253  val loss:  0.5098794102668762\n",
      "epoch:  1   step:  439   train loss:  0.36811235547065735  val loss:  0.5106440782546997\n",
      "epoch:  1   step:  440   train loss:  0.34982016682624817  val loss:  0.5071948766708374\n",
      "epoch:  1   step:  441   train loss:  0.23225879669189453  val loss:  0.5019118189811707\n",
      "epoch:  1   step:  442   train loss:  0.38763782382011414  val loss:  0.49492642283439636\n",
      "epoch:  1   step:  443   train loss:  0.4713786244392395  val loss:  0.4810413122177124\n",
      "epoch:  1   step:  444   train loss:  0.48392459750175476  val loss:  0.4661010205745697\n",
      "epoch:  1   step:  445   train loss:  0.3228529095649719  val loss:  0.4579264521598816\n",
      "epoch:  1   step:  446   train loss:  0.3243359923362732  val loss:  0.4509624242782593\n",
      "epoch:  1   step:  447   train loss:  0.32972192764282227  val loss:  0.44446635246276855\n",
      "epoch:  1   step:  448   train loss:  0.2676240801811218  val loss:  0.43553826212882996\n",
      "epoch:  1   step:  449   train loss:  0.5170308351516724  val loss:  0.42887428402900696\n",
      "epoch:  1   step:  450   train loss:  0.5268882513046265  val loss:  0.4279642403125763\n",
      "epoch:  1   step:  451   train loss:  0.31372347474098206  val loss:  0.42766255140304565\n",
      "epoch:  1   step:  452   train loss:  0.31990835070610046  val loss:  0.4293793737888336\n",
      "epoch:  1   step:  453   train loss:  0.3483084738254547  val loss:  0.4259500205516815\n",
      "epoch:  1   step:  454   train loss:  0.34856462478637695  val loss:  0.4241265654563904\n",
      "epoch:  1   step:  455   train loss:  0.3634088635444641  val loss:  0.4236825406551361\n",
      "epoch:  1   step:  456   train loss:  0.36172568798065186  val loss:  0.421979695558548\n",
      "epoch:  1   step:  457   train loss:  0.3300181031227112  val loss:  0.41764771938323975\n",
      "epoch:  1   step:  458   train loss:  0.24539893865585327  val loss:  0.41529786586761475\n",
      "epoch:  1   step:  459   train loss:  0.21835607290267944  val loss:  0.41467389464378357\n",
      "epoch:  1   step:  460   train loss:  0.5006921291351318  val loss:  0.4131452739238739\n",
      "epoch:  1   step:  461   train loss:  0.3848266303539276  val loss:  0.4145355522632599\n",
      "epoch:  1   step:  462   train loss:  0.29658037424087524  val loss:  0.41334089636802673\n",
      "epoch:  1   step:  463   train loss:  0.4297274351119995  val loss:  0.41369733214378357\n",
      "epoch:  1   step:  464   train loss:  0.2592659294605255  val loss:  0.41364163160324097\n",
      "epoch:  1   step:  465   train loss:  0.2951202094554901  val loss:  0.40469494462013245\n",
      "epoch:  1   step:  466   train loss:  0.4511134624481201  val loss:  0.3959883749485016\n",
      "epoch:  1   step:  467   train loss:  0.3047095239162445  val loss:  0.3970259428024292\n",
      "epoch:  1   step:  468   train loss:  0.20658259093761444  val loss:  0.3971567451953888\n",
      "epoch:  1   step:  469   train loss:  0.46172034740448  val loss:  0.3956040143966675\n",
      "epoch:  1   step:  470   train loss:  0.1608116179704666  val loss:  0.39556679129600525\n",
      "epoch:  1   step:  471   train loss:  0.358329176902771  val loss:  0.39272573590278625\n",
      "epoch:  1   step:  472   train loss:  0.414055734872818  val loss:  0.3902451694011688\n",
      "epoch:  1   step:  473   train loss:  0.37511417269706726  val loss:  0.39180120825767517\n",
      "epoch:  1   step:  474   train loss:  0.20951572060585022  val loss:  0.393681138753891\n",
      "epoch:  1   step:  475   train loss:  0.4182984232902527  val loss:  0.3914768099784851\n",
      "epoch:  1   step:  476   train loss:  0.361587256193161  val loss:  0.39044496417045593\n",
      "epoch:  1   step:  477   train loss:  0.9160298109054565  val loss:  0.39058172702789307\n",
      "epoch:  1   step:  478   train loss:  0.3437495827674866  val loss:  0.396939754486084\n",
      "epoch:  1   step:  479   train loss:  0.37104105949401855  val loss:  0.4024762809276581\n",
      "epoch:  1   step:  480   train loss:  0.27296340465545654  val loss:  0.4068935513496399\n",
      "epoch:  1   step:  481   train loss:  0.24995575845241547  val loss:  0.4119618535041809\n",
      "epoch:  1   step:  482   train loss:  0.3309369683265686  val loss:  0.41441798210144043\n",
      "epoch:  1   step:  483   train loss:  0.3018916845321655  val loss:  0.4161832332611084\n",
      "epoch:  1   step:  484   train loss:  0.23286761343479156  val loss:  0.4163365960121155\n",
      "epoch:  1   step:  485   train loss:  0.32700568437576294  val loss:  0.4150964915752411\n",
      "epoch:  1   step:  486   train loss:  0.3066985011100769  val loss:  0.41435706615448\n",
      "epoch:  1   step:  487   train loss:  0.4361300766468048  val loss:  0.41753044724464417\n",
      "epoch:  1   step:  488   train loss:  0.4041155278682709  val loss:  0.41006404161453247\n",
      "epoch:  1   step:  489   train loss:  0.41165822744369507  val loss:  0.4016192853450775\n",
      "epoch:  1   step:  490   train loss:  0.2153102606534958  val loss:  0.3969722390174866\n",
      "epoch:  1   step:  491   train loss:  0.3243688941001892  val loss:  0.39240625500679016\n",
      "epoch:  1   step:  492   train loss:  0.5211639404296875  val loss:  0.3911108672618866\n",
      "epoch:  1   step:  493   train loss:  0.2771601974964142  val loss:  0.3854239583015442\n",
      "min_val_loss_print 0.3854239583015442\n",
      "epoch:  1   step:  494   train loss:  0.3382395803928375  val loss:  0.38233181834220886\n",
      "min_val_loss_print 0.38233181834220886\n",
      "epoch:  1   step:  495   train loss:  0.25012701749801636  val loss:  0.38766926527023315\n",
      "epoch:  1   step:  496   train loss:  0.33123353123664856  val loss:  0.38937467336654663\n",
      "epoch:  1   step:  497   train loss:  0.3519485294818878  val loss:  0.39354512095451355\n",
      "epoch:  1   step:  498   train loss:  0.36860761046409607  val loss:  0.3952166736125946\n",
      "epoch:  1   step:  499   train loss:  0.6204845309257507  val loss:  0.39503636956214905\n",
      "epoch:  1   step:  500   train loss:  0.45420193672180176  val loss:  0.3969685137271881\n",
      "epoch:  1   step:  501   train loss:  0.26504015922546387  val loss:  0.393492192029953\n",
      "epoch:  1   step:  502   train loss:  0.24450376629829407  val loss:  0.389767050743103\n",
      "epoch:  1   step:  503   train loss:  0.3720720410346985  val loss:  0.3915049731731415\n",
      "epoch:  1   step:  504   train loss:  0.41628703474998474  val loss:  0.39020705223083496\n",
      "epoch:  1   step:  505   train loss:  0.3800354599952698  val loss:  0.3914475440979004\n",
      "epoch:  1   step:  506   train loss:  0.5388329029083252  val loss:  0.39259353280067444\n",
      "epoch:  1   step:  507   train loss:  0.2482510805130005  val loss:  0.39401566982269287\n",
      "epoch:  1   step:  508   train loss:  0.14198730885982513  val loss:  0.3954295516014099\n",
      "epoch:  1   step:  509   train loss:  0.25996285676956177  val loss:  0.40550264716148376\n",
      "epoch:  1   step:  510   train loss:  0.28693217039108276  val loss:  0.41758763790130615\n",
      "epoch:  1   step:  511   train loss:  0.25583186745643616  val loss:  0.42583176493644714\n",
      "epoch:  1   step:  512   train loss:  0.4100799858570099  val loss:  0.4338201880455017\n",
      "epoch:  1   step:  513   train loss:  0.24910388886928558  val loss:  0.43956154584884644\n",
      "epoch:  1   step:  514   train loss:  0.21123121678829193  val loss:  0.44127190113067627\n",
      "epoch:  1   step:  515   train loss:  0.5267801284790039  val loss:  0.4410853385925293\n",
      "epoch:  1   step:  516   train loss:  0.3232141435146332  val loss:  0.4414391815662384\n",
      "epoch:  1   step:  517   train loss:  0.33372583985328674  val loss:  0.43552365899086\n",
      "epoch:  1   step:  518   train loss:  0.3677833378314972  val loss:  0.43270212411880493\n",
      "epoch:  1   step:  519   train loss:  0.2739638388156891  val loss:  0.42837947607040405\n",
      "epoch:  1   step:  520   train loss:  0.2209063023328781  val loss:  0.4210769534111023\n",
      "epoch:  1   step:  521   train loss:  0.39485007524490356  val loss:  0.41530176997184753\n",
      "epoch:  1   step:  522   train loss:  0.21217411756515503  val loss:  0.4134047031402588\n",
      "epoch:  1   step:  523   train loss:  0.26578566431999207  val loss:  0.4081677794456482\n",
      "epoch:  1   step:  524   train loss:  0.4599267840385437  val loss:  0.4010891318321228\n",
      "epoch:  1   step:  525   train loss:  0.31949782371520996  val loss:  0.3999241590499878\n",
      "epoch:  1   step:  526   train loss:  0.29565683007240295  val loss:  0.40264710783958435\n",
      "epoch:  1   step:  527   train loss:  0.3185290992259979  val loss:  0.4010317325592041\n",
      "epoch:  1   step:  528   train loss:  0.326079398393631  val loss:  0.4025605022907257\n",
      "epoch:  1   step:  529   train loss:  0.2692852318286896  val loss:  0.40272343158721924\n",
      "epoch:  1   step:  530   train loss:  0.2981390058994293  val loss:  0.40422919392585754\n",
      "epoch:  1   step:  531   train loss:  0.4060361385345459  val loss:  0.4078747630119324\n",
      "epoch:  1   step:  532   train loss:  0.45762816071510315  val loss:  0.40827763080596924\n",
      "epoch:  1   step:  533   train loss:  0.2410181760787964  val loss:  0.4067363440990448\n",
      "epoch:  1   step:  534   train loss:  0.33832550048828125  val loss:  0.40616375207901\n",
      "epoch:  1   step:  535   train loss:  0.23028388619422913  val loss:  0.40590429306030273\n",
      "epoch:  1   step:  536   train loss:  0.4051594138145447  val loss:  0.4069122076034546\n",
      "epoch:  1   step:  537   train loss:  0.3115105926990509  val loss:  0.4059506356716156\n",
      "epoch:  1   step:  538   train loss:  0.13425089418888092  val loss:  0.4064282476902008\n",
      "epoch:  1   step:  539   train loss:  0.3721890449523926  val loss:  0.41096869111061096\n",
      "epoch:  1   step:  540   train loss:  0.264495313167572  val loss:  0.41177013516426086\n",
      "epoch:  1   step:  541   train loss:  0.2679522931575775  val loss:  0.4122094511985779\n",
      "epoch:  1   step:  542   train loss:  0.3782205879688263  val loss:  0.408542275428772\n",
      "epoch:  1   step:  543   train loss:  0.34729471802711487  val loss:  0.4083402752876282\n",
      "epoch:  1   step:  544   train loss:  0.21325531601905823  val loss:  0.40522560477256775\n",
      "epoch:  1   step:  545   train loss:  0.4255922734737396  val loss:  0.39955952763557434\n",
      "epoch:  1   step:  546   train loss:  0.28763681650161743  val loss:  0.3918001055717468\n",
      "epoch:  1   step:  547   train loss:  0.36832693219184875  val loss:  0.3892861306667328\n",
      "epoch:  1   step:  548   train loss:  0.33658674359321594  val loss:  0.3909885585308075\n",
      "epoch:  1   step:  549   train loss:  0.3019194006919861  val loss:  0.3913803994655609\n",
      "epoch:  1   step:  550   train loss:  0.22313664853572845  val loss:  0.3947469890117645\n",
      "epoch:  1   step:  551   train loss:  0.27865031361579895  val loss:  0.39702558517456055\n",
      "epoch:  1   step:  552   train loss:  0.31754639744758606  val loss:  0.40018603205680847\n",
      "epoch:  1   step:  553   train loss:  0.18656672537326813  val loss:  0.3985157310962677\n",
      "epoch:  1   step:  554   train loss:  0.22360990941524506  val loss:  0.4002777636051178\n",
      "epoch:  1   step:  555   train loss:  0.3754640519618988  val loss:  0.3974081873893738\n",
      "epoch:  1   step:  556   train loss:  0.33190813660621643  val loss:  0.3908858001232147\n",
      "epoch:  1   step:  557   train loss:  0.43867191672325134  val loss:  0.3878864049911499\n",
      "epoch:  1   step:  558   train loss:  0.22030656039714813  val loss:  0.38414907455444336\n",
      "epoch:  1   step:  559   train loss:  0.42250892519950867  val loss:  0.37510961294174194\n",
      "min_val_loss_print 0.37510961294174194\n",
      "epoch:  1   step:  560   train loss:  0.25286686420440674  val loss:  0.3724922835826874\n",
      "min_val_loss_print 0.3724922835826874\n",
      "epoch:  1   step:  561   train loss:  0.39472562074661255  val loss:  0.37187373638153076\n",
      "min_val_loss_print 0.37187373638153076\n",
      "epoch:  1   step:  562   train loss:  0.28766509890556335  val loss:  0.3657693862915039\n",
      "min_val_loss_print 0.3657693862915039\n",
      "epoch:  1   step:  563   train loss:  0.25984352827072144  val loss:  0.36208871006965637\n",
      "min_val_loss_print 0.36208871006965637\n",
      "epoch:  1   step:  564   train loss:  0.3959670662879944  val loss:  0.3612307012081146\n",
      "min_val_loss_print 0.3612307012081146\n",
      "epoch:  1   step:  565   train loss:  0.4298865795135498  val loss:  0.36097252368927\n",
      "min_val_loss_print 0.36097252368927\n",
      "epoch:  1   step:  566   train loss:  0.4948684871196747  val loss:  0.37387460470199585\n",
      "epoch:  1   step:  567   train loss:  0.24560995399951935  val loss:  0.39107203483581543\n",
      "epoch:  1   step:  568   train loss:  0.3899189531803131  val loss:  0.4048949182033539\n",
      "epoch:  1   step:  569   train loss:  0.22643247246742249  val loss:  0.4189028739929199\n",
      "epoch:  1   step:  570   train loss:  0.3205164074897766  val loss:  0.430700421333313\n",
      "epoch:  1   step:  571   train loss:  0.47989675402641296  val loss:  0.4390653073787689\n",
      "epoch:  1   step:  572   train loss:  0.3440593481063843  val loss:  0.4384058117866516\n"
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.SGD(params = net.parameters(), lr=1e-4, momentum=0.9, weight_decay=2e-5)\n",
    "optimizer = torch.optim.Adam(params = net.parameters(), lr=1e-4)\n",
    "#optimizer = torch.optim.SGD(params = net.parameters(), lr=1e-4)\n",
    "loss_func = torch.nn.CrossEntropyLoss(weight=class_weights).cuda()  # the target label is NOT an one-hotted\n",
    "\n",
    "val_data_input = val_data_input.cuda().float()\n",
    "val_data_label = val_data_label.cuda()\n",
    "\n",
    "min_val_loss_print=float('inf')\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for step, (batch_data, batch_label) in enumerate(loader):\n",
    "        batch_data = batch_data.cuda().float()\n",
    "        batch_label = batch_label.cuda()\n",
    "        net.train()\n",
    "        output  = net(batch_data)\n",
    "        train_loss = loss_func(output,batch_label)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(net.parameters(), max_norm=20, norm_type=2)\n",
    "        optimizer.step()\n",
    "        train_loss_print = train_loss.data.item()\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            val_output  = net(val_data_input)\n",
    "            val_loss = loss_func(val_output,val_data_label)\n",
    "            val_loss_print = val_loss.data.item()\n",
    "            torch.cuda.empty_cache()\n",
    "        #print('epoch: ', epoch, '  step: ', step, '  train loss: ', train_loss_print, ' val loss: ', val_loss_print)\n",
    "        print('epoch: ', epoch, '  step: ', step, '  train loss: ', train_loss_print, ' val loss: ', val_loss_print)\n",
    "        if val_loss_print < min_val_loss_print:\n",
    "            torch.save(net.state_dict(), 'net_params_20210715/net_params_20210715_1/epoch_'+str(epoch)+'.pkl') \n",
    "            min_val_loss_print = val_loss_print\n",
    "            print('min_val_loss_print', min_val_loss_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[114, 196, 182,  ...,  75, 156, 108],\n",
      "         [ 32, 105, 147,  ...,  99,  10, 143],\n",
      "         [ 11, 194, 140,  ..., 126,  23, 108],\n",
      "         ...,\n",
      "         [163, 118,  47,  ..., 166,  67, 167],\n",
      "         [195, 176, 169,  ...,  18,  43, 196],\n",
      "         [114,  50, 182,  ..., 159, 151,  74]],\n",
      "\n",
      "        [[109, 194, 197,  ..., 132,  58,  60],\n",
      "         [157,   1, 118,  ...,   9,  46,  54],\n",
      "         [152, 128, 137,  ...,  79, 196, 145],\n",
      "         ...,\n",
      "         [162,  62, 152,  ...,   7,   9, 147],\n",
      "         [191,  96,  47,  ...,  72, 183, 149],\n",
      "         [110, 186, 141,  ..., 119, 109, 162]],\n",
      "\n",
      "        [[166, 150,  87,  ..., 148, 167,  38],\n",
      "         [ 68,  58, 115,  ...,  66,  38,  73],\n",
      "         [ 39,  44,  89,  ...,   8, 175, 108],\n",
      "         ...,\n",
      "         [110, 153,  64,  ..., 161, 158,  68],\n",
      "         [120, 142,  72,  ..., 161, 186,  80],\n",
      "         [148, 190, 182,  ...,  23,  30,   6]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 91,  52, 122,  ...,  20, 132, 165],\n",
      "         [161,  78, 197,  ...,  10, 100, 103],\n",
      "         [ 32,  43, 126,  ..., 164, 147, 162],\n",
      "         ...,\n",
      "         [160, 166, 122,  ..., 169,  83,  81],\n",
      "         [ 37, 133,  76,  ..., 157,  97,  88],\n",
      "         [149,  56,  15,  ..., 149, 178,  56]],\n",
      "\n",
      "        [[113, 196,  32,  ...,  98, 154,  25],\n",
      "         [ 83, 194, 135,  ..., 165, 125, 166],\n",
      "         [186, 141, 191,  ...,  22,  48,  62],\n",
      "         ...,\n",
      "         [164, 122, 180,  ...,  10, 198, 135],\n",
      "         [ 83, 186, 108,  ...,  87,  81, 170],\n",
      "         [ 67,  14, 148,  ...,  59, 133, 119]],\n",
      "\n",
      "        [[ 87,  72, 160,  ...,   4,  49,  99],\n",
      "         [169, 116,  64,  ...,  58,  92,   4],\n",
      "         [ 79,  16, 197,  ...,  63, 198,   8],\n",
      "         ...,\n",
      "         [ 77, 170,  17,  ...,  25,  65,  89],\n",
      "         [ 59, 171, 171,  ...,  88,  67, 130],\n",
      "         [135, 119,  30,  ..., 164,  40,   3]]], dtype=torch.uint8)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "a=torch.from_numpy((np.random.rand(22074, 200, 200)*200).astype(np.uint8))\n",
    "print(a)\n",
    "np.array([1]).astype(np.uint8)\n",
    "print(np.array([1]).astype(np.uint8).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         ...,\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2]],\n",
      "\n",
      "        [[2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         ...,\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         ...,\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         ...,\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2],\n",
      "         [2, 2, 2,  ..., 2, 2, 2]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
