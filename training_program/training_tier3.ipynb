{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "import scipy.ndimage\n",
    "BATCH_SIZE =  32\n",
    "NUM_EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Construct customized ResNet\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    " \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    " \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    " \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    " \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    " \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    " \n",
    "        return out\n",
    "\n",
    "class Submodule2(nn.Module):\n",
    "        \n",
    "    def __init__(self, pcpt_block, pcpt_layers, scoop_block, scoop_layers, h, w, pcpt_is_upsample=0, scoop_is_upsample=0):\n",
    "        self.inplanes = 64\n",
    "        self.pcpt_is_upsample = pcpt_is_upsample\n",
    "        super(Submodule2, self).__init__()\n",
    "        self.pcpt_conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.pcpt_bn1 = nn.BatchNorm2d(64)\n",
    "        self.pcpt_relu = nn.ReLU(inplace=True)\n",
    "        self.pcpt_maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.pcpt_upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.pcpt_layer1 = self._make_layer(pcpt_block, 128, pcpt_layers[0])\n",
    "        self.pcpt_layer2 = self._make_layer(pcpt_block, 256, pcpt_layers[1])\n",
    "        self.pcpt_layer3 = self._make_layer(pcpt_block, 512, pcpt_layers[2])\n",
    "\n",
    "        self.inplanes = 512\n",
    "        self.scoop_is_upsample = scoop_is_upsample\n",
    "        self.scoop_upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.scoop_layer1 = self._make_layer(scoop_block, 256, scoop_layers[0])\n",
    "        self.scoop_layer2 = self._make_layer(scoop_block, 128, scoop_layers[1])\n",
    "        self.scoop_layer3 = self._make_layer(scoop_block, 64, scoop_layers[2])\n",
    "        self.scoop_conv1 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.scoop_bn1 = nn.BatchNorm2d(1)\n",
    "        self.scoop_conv2 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.scoop_bn2 = nn.BatchNorm2d(3)\n",
    "        self.scoop_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "            \n",
    "            \n",
    "        self.x1_hidden = torch.nn.Linear(3*200*200, 200)\n",
    "        self.x2_hidden = torch.nn.Linear(6, 200)\n",
    "        self.x_hidden1 = torch.nn.Linear(400, 200)\n",
    "        self.x_hidden2 = torch.nn.Linear(200, 50)\n",
    "        self.x_hidden3 = torch.nn.Linear(50, 1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.pcpt_conv1(x1)\n",
    "        x1 = self.pcpt_bn1(x1)\n",
    "        x1 = self.pcpt_relu(x1)\n",
    "        x1 = self.pcpt_maxpool(x1)\n",
    "\n",
    "        x1 = self.pcpt_layer1(x1)\n",
    "        x1 = self.pcpt_maxpool(x1)\n",
    "        x1 = self.pcpt_layer2(x1)\n",
    "        x1 = self.pcpt_layer3(x1)\n",
    "\n",
    "        x1 = self.scoop_layer1(x1)\n",
    "        x1 = self.scoop_layer2(x1)\n",
    "        x1 = self.scoop_upsample(x1)\n",
    "        x1 = self.scoop_layer3(x1)\n",
    "        x1 = self.scoop_upsample(x1)\n",
    "\n",
    "        x1 = self.scoop_conv2(x1)\n",
    "        x1 = self.scoop_bn2(x1)\n",
    "        x1 = self.scoop_relu(x1)       \n",
    "        x1 = x1.reshape(x1.shape[0],-1)\n",
    "        \n",
    "        x1 = F.relu(self.x1_hidden(x1))\n",
    "        \n",
    "        x2 = F.relu(self.x2_hidden(x2))\n",
    "        \n",
    "        x = torch.cat((x1, x2), dim=-1)\n",
    "        \n",
    "        x = F.relu(self.x_hidden1(x))\n",
    "        x = F.relu(self.x_hidden2(x))\n",
    "        x = self.x_hidden3(x)\n",
    "        #x=torch.reshape(x,(-1,3,200))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodule2(\n",
      "  (pcpt_conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (pcpt_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pcpt_relu): ReLU(inplace=True)\n",
      "  (pcpt_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pcpt_upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (pcpt_layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pcpt_layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (pcpt_layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (scoop_layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (scoop_layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_conv1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (scoop_bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (scoop_conv2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (scoop_bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (scoop_relu): ReLU(inplace=True)\n",
      "  (x1_hidden): Linear(in_features=120000, out_features=200, bias=True)\n",
      "  (x2_hidden): Linear(in_features=6, out_features=200, bias=True)\n",
      "  (x_hidden1): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (x_hidden2): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (x_hidden3): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "submodule2_net = Submodule2(pcpt_block=BasicBlock, pcpt_layers=[1,5,1], scoop_block=BasicBlock, scoop_layers=[1,5,1], h=200, w=200).cuda()\n",
    "print(submodule2_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5337, 200, 200, 4)\n",
      "(5337, 6)\n",
      "(5337, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "submodule2_data_input_1 = np.load(\"data_20210814/input_image_channel.npy\")/255.0\n",
    "print(submodule2_data_input_1.shape)\n",
    "\n",
    "temp_index_set = random.sample(range(len(submodule2_data_input_1)), 50)\n",
    "temp_index_set_other = list(set(range(len(submodule2_data_input_1))).difference(set(temp_index_set)))\n",
    "\n",
    "submodule2_data_input_1 = torch.from_numpy(submodule2_data_input_1).permute(0,3,1,2)\n",
    "train_submodule2_data_input_1 = submodule2_data_input_1[temp_index_set_other, :, :, :]\n",
    "val_submodule2_data_input_1 = submodule2_data_input_1[temp_index_set, :, :, :]\n",
    "\n",
    "submodule2_data_input_2 = np.load(\"data_20210814/input_vector_channel.npy\")\n",
    "print(submodule2_data_input_2.shape)\n",
    "submodule2_data_input_2 = torch.from_numpy(submodule2_data_input_2)\n",
    "train_submodule2_data_input_2 = submodule2_data_input_2[temp_index_set_other, :]\n",
    "val_submodule2_data_input_2 = submodule2_data_input_2[temp_index_set, :]\n",
    "\n",
    "#print(train_submodule2_data_input_1)\n",
    "\n",
    "\n",
    "submodule2_data_label = np.load(\"data_20210814/success_array.npy\")\n",
    "print(submodule2_data_label.shape)\n",
    "\n",
    "submodule2_data_label = torch.from_numpy(submodule2_data_label).long()\n",
    "train_submodule2_data_label = submodule2_data_label[temp_index_set_other, :]\n",
    "val_submodule2_data_label = submodule2_data_label[temp_index_set, :]\n",
    "\n",
    "submodule2_train_torch_dataset = Data.TensorDataset(train_submodule2_data_input_1, train_submodule2_data_input_2, train_submodule2_data_label)\n",
    "loader = Data.DataLoader(dataset=submodule2_train_torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "htmap_h = submodule2_data_input_1.shape[2]\n",
    "htmap_w = submodule2_data_input_1.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   step:  0   train loss:  0.5678025484085083  val loss:  0.7392731308937073\n",
      "min_val_loss_print 0.7392731308937073\n",
      "epoch:  0   step:  1   train loss:  0.6046501398086548  val loss:  0.7252206206321716\n",
      "min_val_loss_print 0.7252206206321716\n",
      "epoch:  0   step:  2   train loss:  0.5961199998855591  val loss:  0.7300161719322205\n",
      "epoch:  0   step:  3   train loss:  0.4828310012817383  val loss:  0.7006201148033142\n",
      "min_val_loss_print 0.7006201148033142\n",
      "epoch:  0   step:  4   train loss:  0.669414758682251  val loss:  0.6945967674255371\n",
      "min_val_loss_print 0.6945967674255371\n",
      "epoch:  0   step:  5   train loss:  1.2123053073883057  val loss:  0.6693249344825745\n",
      "min_val_loss_print 0.6693249344825745\n",
      "epoch:  0   step:  6   train loss:  0.5502000451087952  val loss:  0.654883861541748\n",
      "min_val_loss_print 0.654883861541748\n",
      "epoch:  0   step:  7   train loss:  0.8915963768959045  val loss:  0.6621435284614563\n",
      "epoch:  0   step:  8   train loss:  0.5177192687988281  val loss:  0.6863073706626892\n",
      "epoch:  0   step:  9   train loss:  0.6746958494186401  val loss:  0.7055758237838745\n",
      "epoch:  0   step:  10   train loss:  0.6852375268936157  val loss:  0.7237252593040466\n",
      "epoch:  0   step:  11   train loss:  0.6906110644340515  val loss:  0.7276870608329773\n",
      "epoch:  0   step:  12   train loss:  0.6738627552986145  val loss:  0.7034913301467896\n",
      "epoch:  0   step:  13   train loss:  0.6234083771705627  val loss:  0.6673843264579773\n",
      "epoch:  0   step:  14   train loss:  0.5977859497070312  val loss:  0.6381916403770447\n",
      "min_val_loss_print 0.6381916403770447\n",
      "epoch:  0   step:  15   train loss:  0.5905473232269287  val loss:  0.6297802329063416\n",
      "min_val_loss_print 0.6297802329063416\n",
      "epoch:  0   step:  16   train loss:  0.47397705912590027  val loss:  0.6313570141792297\n",
      "epoch:  0   step:  17   train loss:  0.7057437896728516  val loss:  0.6317724585533142\n",
      "epoch:  0   step:  18   train loss:  0.8308015465736389  val loss:  0.6276013851165771\n",
      "min_val_loss_print 0.6276013851165771\n",
      "epoch:  0   step:  19   train loss:  0.4296656548976898  val loss:  0.6226562857627869\n",
      "min_val_loss_print 0.6226562857627869\n",
      "epoch:  0   step:  20   train loss:  0.6225841045379639  val loss:  0.6158546209335327\n",
      "min_val_loss_print 0.6158546209335327\n",
      "epoch:  0   step:  21   train loss:  0.5868191719055176  val loss:  0.6073115468025208\n",
      "min_val_loss_print 0.6073115468025208\n",
      "epoch:  0   step:  22   train loss:  0.8559938669204712  val loss:  0.6033473610877991\n",
      "min_val_loss_print 0.6033473610877991\n",
      "epoch:  0   step:  23   train loss:  0.4583848714828491  val loss:  0.6077165007591248\n",
      "epoch:  0   step:  24   train loss:  0.5462809205055237  val loss:  0.6128842234611511\n",
      "epoch:  0   step:  25   train loss:  0.7181495428085327  val loss:  0.6257829666137695\n",
      "epoch:  0   step:  26   train loss:  0.6169567108154297  val loss:  0.6366968154907227\n",
      "epoch:  0   step:  27   train loss:  0.6326035261154175  val loss:  0.6398754119873047\n",
      "epoch:  0   step:  28   train loss:  0.651044487953186  val loss:  0.6318625211715698\n",
      "epoch:  0   step:  29   train loss:  0.5888622999191284  val loss:  0.6142607927322388\n",
      "epoch:  0   step:  30   train loss:  0.6031812429428101  val loss:  0.5912139415740967\n",
      "min_val_loss_print 0.5912139415740967\n",
      "epoch:  0   step:  31   train loss:  0.5707331299781799  val loss:  0.5711040496826172\n",
      "min_val_loss_print 0.5711040496826172\n",
      "epoch:  0   step:  32   train loss:  0.6130325794219971  val loss:  0.5530929565429688\n",
      "min_val_loss_print 0.5530929565429688\n",
      "epoch:  0   step:  33   train loss:  0.5694835186004639  val loss:  0.5455611348152161\n",
      "min_val_loss_print 0.5455611348152161\n",
      "epoch:  0   step:  34   train loss:  0.5831062197685242  val loss:  0.5448928475379944\n",
      "min_val_loss_print 0.5448928475379944\n",
      "epoch:  0   step:  35   train loss:  0.47430726885795593  val loss:  0.5521772503852844\n",
      "epoch:  0   step:  36   train loss:  0.5090692639350891  val loss:  0.5685499310493469\n",
      "epoch:  0   step:  37   train loss:  0.52311772108078  val loss:  0.5885351300239563\n",
      "epoch:  0   step:  38   train loss:  0.6356509327888489  val loss:  0.5892252922058105\n",
      "epoch:  0   step:  39   train loss:  0.6252121925354004  val loss:  0.5819617509841919\n",
      "epoch:  0   step:  40   train loss:  0.5119818449020386  val loss:  0.5757136940956116\n",
      "epoch:  0   step:  41   train loss:  0.6179190874099731  val loss:  0.5673294067382812\n",
      "epoch:  0   step:  42   train loss:  0.7292391061782837  val loss:  0.5564072132110596\n",
      "epoch:  0   step:  43   train loss:  0.5466272830963135  val loss:  0.555141031742096\n",
      "epoch:  0   step:  44   train loss:  0.6356610059738159  val loss:  0.5608149170875549\n",
      "epoch:  0   step:  45   train loss:  0.7523214817047119  val loss:  0.5687030553817749\n",
      "epoch:  0   step:  46   train loss:  0.5374126434326172  val loss:  0.5767256021499634\n",
      "epoch:  0   step:  47   train loss:  0.5782346725463867  val loss:  0.5796102285385132\n",
      "epoch:  0   step:  48   train loss:  0.7175624370574951  val loss:  0.5812874436378479\n",
      "epoch:  0   step:  49   train loss:  0.6359474062919617  val loss:  0.5839425921440125\n",
      "epoch:  0   step:  50   train loss:  0.6263467669487  val loss:  0.5845971703529358\n",
      "epoch:  0   step:  51   train loss:  0.5790309906005859  val loss:  0.5825634598731995\n",
      "epoch:  0   step:  52   train loss:  0.5810365676879883  val loss:  0.5815213918685913\n",
      "epoch:  0   step:  53   train loss:  0.7496418952941895  val loss:  0.5775588750839233\n",
      "epoch:  0   step:  54   train loss:  0.4954316020011902  val loss:  0.5739097595214844\n",
      "epoch:  0   step:  55   train loss:  0.5322586297988892  val loss:  0.5757858753204346\n",
      "epoch:  0   step:  56   train loss:  0.6358660459518433  val loss:  0.576156735420227\n",
      "epoch:  0   step:  57   train loss:  0.6842852830886841  val loss:  0.5728524923324585\n",
      "epoch:  0   step:  58   train loss:  0.4857766032218933  val loss:  0.5717942714691162\n",
      "epoch:  0   step:  59   train loss:  0.5250161290168762  val loss:  0.5695050954818726\n",
      "epoch:  0   step:  60   train loss:  0.7160127758979797  val loss:  0.5648858547210693\n",
      "epoch:  0   step:  61   train loss:  0.4881410598754883  val loss:  0.5618487596511841\n",
      "epoch:  0   step:  62   train loss:  0.6182688474655151  val loss:  0.5568751692771912\n",
      "epoch:  0   step:  63   train loss:  0.49190962314605713  val loss:  0.5555262565612793\n",
      "epoch:  0   step:  64   train loss:  0.501882791519165  val loss:  0.5550543665885925\n",
      "epoch:  0   step:  65   train loss:  0.5094283223152161  val loss:  0.5547327399253845\n",
      "epoch:  0   step:  66   train loss:  0.683332622051239  val loss:  0.5495434999465942\n",
      "epoch:  0   step:  67   train loss:  0.4962276220321655  val loss:  0.5453607439994812\n",
      "epoch:  0   step:  68   train loss:  0.7051427364349365  val loss:  0.5372198224067688\n",
      "min_val_loss_print 0.5372198224067688\n",
      "epoch:  0   step:  69   train loss:  0.4791603088378906  val loss:  0.5284231901168823\n",
      "min_val_loss_print 0.5284231901168823\n",
      "epoch:  0   step:  70   train loss:  0.5248861908912659  val loss:  0.5228207111358643\n",
      "min_val_loss_print 0.5228207111358643\n",
      "epoch:  0   step:  71   train loss:  0.523166298866272  val loss:  0.5163861513137817\n",
      "min_val_loss_print 0.5163861513137817\n",
      "epoch:  0   step:  72   train loss:  0.8293182253837585  val loss:  0.5145909190177917\n",
      "min_val_loss_print 0.5145909190177917\n",
      "epoch:  0   step:  73   train loss:  0.5429435968399048  val loss:  0.5134262442588806\n",
      "min_val_loss_print 0.5134262442588806\n",
      "epoch:  0   step:  74   train loss:  0.4956859052181244  val loss:  0.5110077261924744\n",
      "min_val_loss_print 0.5110077261924744\n",
      "epoch:  0   step:  75   train loss:  0.5504595041275024  val loss:  0.5118522644042969\n",
      "epoch:  0   step:  76   train loss:  0.584991991519928  val loss:  0.5153928399085999\n",
      "epoch:  0   step:  77   train loss:  0.6882467269897461  val loss:  0.5196644067764282\n",
      "epoch:  0   step:  78   train loss:  0.6756247878074646  val loss:  0.5247329473495483\n",
      "epoch:  0   step:  79   train loss:  0.6000363826751709  val loss:  0.5295827984809875\n",
      "epoch:  0   step:  80   train loss:  0.5307914614677429  val loss:  0.5327829718589783\n",
      "epoch:  0   step:  81   train loss:  0.5338651537895203  val loss:  0.5367429852485657\n",
      "epoch:  0   step:  82   train loss:  0.4537123739719391  val loss:  0.5406553745269775\n",
      "epoch:  0   step:  83   train loss:  0.607337474822998  val loss:  0.5419653058052063\n",
      "epoch:  0   step:  84   train loss:  0.6073856353759766  val loss:  0.5472497940063477\n",
      "epoch:  0   step:  85   train loss:  0.5102078914642334  val loss:  0.5493049025535583\n",
      "epoch:  0   step:  86   train loss:  0.5578279495239258  val loss:  0.5521102547645569\n",
      "epoch:  0   step:  87   train loss:  0.6734768152236938  val loss:  0.5527862906455994\n",
      "epoch:  0   step:  88   train loss:  0.5382764339447021  val loss:  0.5506696701049805\n",
      "epoch:  0   step:  89   train loss:  0.6139225959777832  val loss:  0.5471462607383728\n",
      "epoch:  0   step:  90   train loss:  0.6147759556770325  val loss:  0.5444891452789307\n",
      "epoch:  0   step:  91   train loss:  0.4673466086387634  val loss:  0.5414320826530457\n",
      "epoch:  0   step:  92   train loss:  0.4950700104236603  val loss:  0.5449249744415283\n",
      "epoch:  0   step:  93   train loss:  0.5843273401260376  val loss:  0.5516340732574463\n",
      "epoch:  0   step:  94   train loss:  0.6480315327644348  val loss:  0.5556692481040955\n",
      "epoch:  0   step:  95   train loss:  0.38476234674453735  val loss:  0.5622345209121704\n",
      "epoch:  0   step:  96   train loss:  0.6759369969367981  val loss:  0.5672980546951294\n",
      "epoch:  0   step:  97   train loss:  0.7000126242637634  val loss:  0.5687057375907898\n",
      "epoch:  0   step:  98   train loss:  0.42796874046325684  val loss:  0.5700989365577698\n",
      "epoch:  0   step:  99   train loss:  0.49231958389282227  val loss:  0.5698561072349548\n",
      "epoch:  0   step:  100   train loss:  0.45506757497787476  val loss:  0.5704959034919739\n",
      "epoch:  0   step:  101   train loss:  0.3929959535598755  val loss:  0.5745607018470764\n",
      "epoch:  0   step:  102   train loss:  0.7441651821136475  val loss:  0.578896164894104\n",
      "epoch:  0   step:  103   train loss:  0.5530604720115662  val loss:  0.5808419585227966\n",
      "epoch:  0   step:  104   train loss:  0.6580474376678467  val loss:  0.5881127119064331\n",
      "epoch:  0   step:  105   train loss:  0.49395617842674255  val loss:  0.5910316109657288\n",
      "epoch:  0   step:  106   train loss:  0.596821665763855  val loss:  0.5871309041976929\n",
      "epoch:  0   step:  107   train loss:  0.5912106037139893  val loss:  0.5800570249557495\n",
      "epoch:  0   step:  108   train loss:  0.6449544429779053  val loss:  0.5710812211036682\n",
      "epoch:  0   step:  109   train loss:  0.607089638710022  val loss:  0.5657622814178467\n",
      "epoch:  0   step:  110   train loss:  0.5585134029388428  val loss:  0.5667864680290222\n",
      "epoch:  0   step:  111   train loss:  0.536945641040802  val loss:  0.5703181624412537\n",
      "epoch:  0   step:  112   train loss:  0.575945258140564  val loss:  0.5759636759757996\n",
      "epoch:  0   step:  113   train loss:  0.5668089985847473  val loss:  0.5845606327056885\n",
      "epoch:  0   step:  114   train loss:  0.5533201694488525  val loss:  0.5910627841949463\n",
      "epoch:  0   step:  115   train loss:  0.6671814918518066  val loss:  0.5919337868690491\n",
      "epoch:  0   step:  116   train loss:  0.5707041025161743  val loss:  0.5907396078109741\n",
      "epoch:  0   step:  117   train loss:  0.540305495262146  val loss:  0.5909968018531799\n",
      "epoch:  0   step:  118   train loss:  0.6634329557418823  val loss:  0.5847470760345459\n",
      "epoch:  0   step:  119   train loss:  0.4909253418445587  val loss:  0.5827487707138062\n",
      "epoch:  0   step:  120   train loss:  0.5809874534606934  val loss:  0.579718828201294\n",
      "epoch:  0   step:  121   train loss:  0.651221752166748  val loss:  0.572646975517273\n",
      "epoch:  0   step:  122   train loss:  0.6283578276634216  val loss:  0.5629564523696899\n",
      "epoch:  0   step:  123   train loss:  0.38412970304489136  val loss:  0.5537639260292053\n",
      "epoch:  0   step:  124   train loss:  0.5911886692047119  val loss:  0.5433347225189209\n",
      "epoch:  0   step:  125   train loss:  0.5832588076591492  val loss:  0.5383095741271973\n",
      "epoch:  0   step:  126   train loss:  0.7691326141357422  val loss:  0.5351123213768005\n",
      "epoch:  0   step:  127   train loss:  0.6049846410751343  val loss:  0.5342545509338379\n",
      "epoch:  0   step:  128   train loss:  0.7063021659851074  val loss:  0.5372397303581238\n",
      "epoch:  0   step:  129   train loss:  0.6099833250045776  val loss:  0.5452097058296204\n",
      "epoch:  0   step:  130   train loss:  0.596672773361206  val loss:  0.551514208316803\n",
      "epoch:  0   step:  131   train loss:  0.572384238243103  val loss:  0.5504764914512634\n",
      "epoch:  0   step:  132   train loss:  0.5940991640090942  val loss:  0.5466253757476807\n",
      "epoch:  0   step:  133   train loss:  0.6250631213188171  val loss:  0.5448926687240601\n",
      "epoch:  0   step:  134   train loss:  0.6164760589599609  val loss:  0.5323989987373352\n",
      "epoch:  0   step:  135   train loss:  0.5926240682601929  val loss:  0.5204468369483948\n",
      "epoch:  0   step:  136   train loss:  0.5436133146286011  val loss:  0.516624927520752\n",
      "epoch:  0   step:  137   train loss:  0.4978025555610657  val loss:  0.51882404088974\n",
      "epoch:  0   step:  138   train loss:  0.4339767098426819  val loss:  0.5265279412269592\n",
      "epoch:  0   step:  139   train loss:  0.3808158040046692  val loss:  0.537036120891571\n",
      "epoch:  0   step:  140   train loss:  0.4996301233768463  val loss:  0.5465952754020691\n",
      "epoch:  0   step:  141   train loss:  0.7752889394760132  val loss:  0.5340898036956787\n",
      "epoch:  0   step:  142   train loss:  0.7634772062301636  val loss:  0.517618715763092\n",
      "epoch:  0   step:  143   train loss:  0.6952883005142212  val loss:  0.5030651688575745\n",
      "min_val_loss_print 0.5030651688575745\n",
      "epoch:  0   step:  144   train loss:  0.6909481287002563  val loss:  0.49974265694618225\n",
      "min_val_loss_print 0.49974265694618225\n",
      "epoch:  0   step:  145   train loss:  0.4028171896934509  val loss:  0.5091734528541565\n",
      "epoch:  0   step:  146   train loss:  0.6874097585678101  val loss:  0.5236977338790894\n",
      "epoch:  0   step:  147   train loss:  0.5520130395889282  val loss:  0.5288188457489014\n",
      "epoch:  0   step:  148   train loss:  0.5316538214683533  val loss:  0.5271108746528625\n",
      "epoch:  0   step:  149   train loss:  0.5562867522239685  val loss:  0.5224922299385071\n",
      "epoch:  0   step:  150   train loss:  0.6616840362548828  val loss:  0.5176472067832947\n",
      "epoch:  0   step:  151   train loss:  0.5858744382858276  val loss:  0.5157321691513062\n",
      "epoch:  0   step:  152   train loss:  0.5471299886703491  val loss:  0.5173664689064026\n",
      "epoch:  0   step:  153   train loss:  0.5908085703849792  val loss:  0.5188507437705994\n",
      "epoch:  0   step:  154   train loss:  0.43802541494369507  val loss:  0.517758846282959\n",
      "epoch:  0   step:  155   train loss:  0.5298561453819275  val loss:  0.5140562057495117\n",
      "epoch:  0   step:  156   train loss:  0.6890838742256165  val loss:  0.50927734375\n",
      "epoch:  0   step:  157   train loss:  0.6827738881111145  val loss:  0.5061200857162476\n",
      "epoch:  0   step:  158   train loss:  0.5109714269638062  val loss:  0.5057567358016968\n",
      "epoch:  0   step:  159   train loss:  0.5262148380279541  val loss:  0.5014411211013794\n",
      "epoch:  0   step:  160   train loss:  0.5976604223251343  val loss:  0.5011406540870667\n",
      "epoch:  0   step:  161   train loss:  0.5525261163711548  val loss:  0.5011580586433411\n",
      "epoch:  0   step:  162   train loss:  0.45990559458732605  val loss:  0.49857401847839355\n",
      "min_val_loss_print 0.49857401847839355\n",
      "epoch:  0   step:  163   train loss:  0.4997435212135315  val loss:  0.4941898286342621\n",
      "min_val_loss_print 0.4941898286342621\n",
      "epoch:  0   step:  164   train loss:  0.47337257862091064  val loss:  0.48738065361976624\n",
      "min_val_loss_print 0.48738065361976624\n",
      "epoch:  0   step:  165   train loss:  0.6247260570526123  val loss:  0.487006276845932\n",
      "min_val_loss_print 0.487006276845932\n",
      "epoch:  1   step:  0   train loss:  0.6205365657806396  val loss:  0.4875127673149109\n",
      "epoch:  1   step:  1   train loss:  0.48019152879714966  val loss:  0.48834913969039917\n",
      "epoch:  1   step:  2   train loss:  0.45831483602523804  val loss:  0.4910740554332733\n",
      "epoch:  1   step:  3   train loss:  0.4652990400791168  val loss:  0.4935470521450043\n",
      "epoch:  1   step:  4   train loss:  0.3886532783508301  val loss:  0.49537837505340576\n",
      "epoch:  1   step:  5   train loss:  0.7427663803100586  val loss:  0.49701178073883057\n",
      "epoch:  1   step:  6   train loss:  0.8078697919845581  val loss:  0.49923667311668396\n",
      "epoch:  1   step:  7   train loss:  0.6115251183509827  val loss:  0.5048363208770752\n",
      "epoch:  1   step:  8   train loss:  0.4439333975315094  val loss:  0.510960042476654\n",
      "epoch:  1   step:  9   train loss:  0.6620043516159058  val loss:  0.5196434855461121\n",
      "epoch:  1   step:  10   train loss:  0.625164270401001  val loss:  0.5306770205497742\n",
      "epoch:  1   step:  11   train loss:  0.5215688943862915  val loss:  0.5370926260948181\n",
      "epoch:  1   step:  12   train loss:  0.5568636655807495  val loss:  0.5392868518829346\n",
      "epoch:  1   step:  13   train loss:  0.5953239798545837  val loss:  0.534389853477478\n",
      "epoch:  1   step:  14   train loss:  0.5293952226638794  val loss:  0.5307176113128662\n",
      "epoch:  1   step:  15   train loss:  0.6076747179031372  val loss:  0.5307530164718628\n",
      "epoch:  1   step:  16   train loss:  0.5549370050430298  val loss:  0.533832311630249\n",
      "epoch:  1   step:  17   train loss:  0.5621479749679565  val loss:  0.5360795259475708\n",
      "epoch:  1   step:  18   train loss:  0.5659641027450562  val loss:  0.5372808575630188\n",
      "epoch:  1   step:  19   train loss:  0.6170032024383545  val loss:  0.534032940864563\n",
      "epoch:  1   step:  20   train loss:  0.6554264426231384  val loss:  0.5302522778511047\n",
      "epoch:  1   step:  21   train loss:  0.4748818576335907  val loss:  0.5279473066329956\n",
      "epoch:  1   step:  22   train loss:  0.48286357522010803  val loss:  0.5246305465698242\n",
      "epoch:  1   step:  23   train loss:  0.47220852971076965  val loss:  0.5209193825721741\n",
      "epoch:  1   step:  24   train loss:  0.41301053762435913  val loss:  0.5194530487060547\n",
      "epoch:  1   step:  25   train loss:  0.5240352153778076  val loss:  0.5219869017601013\n",
      "epoch:  1   step:  26   train loss:  0.5698685646057129  val loss:  0.5250241160392761\n",
      "epoch:  1   step:  27   train loss:  0.42777931690216064  val loss:  0.5312933921813965\n",
      "epoch:  1   step:  28   train loss:  0.37865710258483887  val loss:  0.5408739447593689\n",
      "epoch:  1   step:  29   train loss:  0.45964524149894714  val loss:  0.5475735068321228\n",
      "epoch:  1   step:  30   train loss:  0.7192703485488892  val loss:  0.5423004031181335\n",
      "epoch:  1   step:  31   train loss:  0.8360463976860046  val loss:  0.5225403308868408\n",
      "epoch:  1   step:  32   train loss:  0.6093086004257202  val loss:  0.5034995079040527\n",
      "epoch:  1   step:  33   train loss:  0.6036440134048462  val loss:  0.49437567591667175\n",
      "epoch:  1   step:  34   train loss:  0.5367473363876343  val loss:  0.4971342980861664\n",
      "epoch:  1   step:  35   train loss:  0.6360913515090942  val loss:  0.5099875330924988\n",
      "epoch:  1   step:  36   train loss:  0.4933500587940216  val loss:  0.5252386927604675\n",
      "epoch:  1   step:  37   train loss:  0.5058647990226746  val loss:  0.5385538935661316\n",
      "epoch:  1   step:  38   train loss:  0.6716582775115967  val loss:  0.5263696908950806\n",
      "epoch:  1   step:  39   train loss:  0.6605995297431946  val loss:  0.5085784792900085\n",
      "epoch:  1   step:  40   train loss:  0.6757794618606567  val loss:  0.4991973340511322\n",
      "epoch:  1   step:  41   train loss:  0.5678685307502747  val loss:  0.4931075870990753\n",
      "epoch:  1   step:  42   train loss:  0.5609073638916016  val loss:  0.4895285665988922\n",
      "epoch:  1   step:  43   train loss:  0.5026302337646484  val loss:  0.4923945367336273\n",
      "epoch:  1   step:  44   train loss:  0.606818437576294  val loss:  0.4970088005065918\n",
      "epoch:  1   step:  45   train loss:  0.3627628982067108  val loss:  0.5035126805305481\n",
      "epoch:  1   step:  46   train loss:  0.693030834197998  val loss:  0.5102209448814392\n",
      "epoch:  1   step:  47   train loss:  0.6700731515884399  val loss:  0.5121101140975952\n",
      "epoch:  1   step:  48   train loss:  0.5172388553619385  val loss:  0.5153371095657349\n",
      "epoch:  1   step:  49   train loss:  0.5583395957946777  val loss:  0.5157343745231628\n",
      "epoch:  1   step:  50   train loss:  0.3595792055130005  val loss:  0.517525851726532\n",
      "epoch:  1   step:  51   train loss:  0.6275428533554077  val loss:  0.520773708820343\n",
      "epoch:  1   step:  52   train loss:  0.6831508278846741  val loss:  0.5238766074180603\n",
      "epoch:  1   step:  53   train loss:  0.4816277027130127  val loss:  0.5295984745025635\n",
      "epoch:  1   step:  54   train loss:  0.719475269317627  val loss:  0.5385991334915161\n",
      "epoch:  1   step:  55   train loss:  0.5899266600608826  val loss:  0.5495119690895081\n",
      "epoch:  1   step:  56   train loss:  0.612619161605835  val loss:  0.551916241645813\n",
      "epoch:  1   step:  57   train loss:  0.5714274644851685  val loss:  0.5457103848457336\n",
      "epoch:  1   step:  58   train loss:  0.6243481040000916  val loss:  0.5374240875244141\n",
      "epoch:  1   step:  59   train loss:  0.6116564869880676  val loss:  0.5286441445350647\n",
      "epoch:  1   step:  60   train loss:  0.5109724998474121  val loss:  0.5214166641235352\n",
      "epoch:  1   step:  61   train loss:  0.4969128668308258  val loss:  0.5118967294692993\n",
      "epoch:  1   step:  62   train loss:  0.5805000066757202  val loss:  0.5041326284408569\n",
      "epoch:  1   step:  63   train loss:  0.6165201663970947  val loss:  0.5005473494529724\n",
      "epoch:  1   step:  64   train loss:  0.60481858253479  val loss:  0.5014684796333313\n",
      "epoch:  1   step:  65   train loss:  0.7163889408111572  val loss:  0.5014037489891052\n",
      "epoch:  1   step:  66   train loss:  0.6162036657333374  val loss:  0.5068491101264954\n",
      "epoch:  1   step:  67   train loss:  0.5211180448532104  val loss:  0.5106112360954285\n",
      "epoch:  1   step:  68   train loss:  0.5007904767990112  val loss:  0.5117454528808594\n",
      "epoch:  1   step:  69   train loss:  0.5895044207572937  val loss:  0.5108018517494202\n",
      "epoch:  1   step:  70   train loss:  0.46443474292755127  val loss:  0.5089812874794006\n",
      "epoch:  1   step:  71   train loss:  0.6658058166503906  val loss:  0.5116698145866394\n",
      "epoch:  1   step:  72   train loss:  0.5048902034759521  val loss:  0.5134841203689575\n",
      "epoch:  1   step:  73   train loss:  0.5894062519073486  val loss:  0.5192425847053528\n",
      "epoch:  1   step:  74   train loss:  0.49527984857559204  val loss:  0.5206633806228638\n",
      "epoch:  1   step:  75   train loss:  0.5245129466056824  val loss:  0.5237841606140137\n",
      "epoch:  1   step:  76   train loss:  0.5052626132965088  val loss:  0.5269848704338074\n",
      "epoch:  1   step:  77   train loss:  0.6800637245178223  val loss:  0.5318082571029663\n",
      "epoch:  1   step:  78   train loss:  0.5058746933937073  val loss:  0.5307416915893555\n",
      "epoch:  1   step:  79   train loss:  0.5993348360061646  val loss:  0.5282011032104492\n",
      "epoch:  1   step:  80   train loss:  0.4650789499282837  val loss:  0.5217576622962952\n",
      "epoch:  1   step:  81   train loss:  0.5606206655502319  val loss:  0.518467903137207\n",
      "epoch:  1   step:  82   train loss:  0.5729519128799438  val loss:  0.5158665180206299\n",
      "epoch:  1   step:  83   train loss:  0.5693768858909607  val loss:  0.5136346817016602\n",
      "epoch:  1   step:  84   train loss:  0.6285952925682068  val loss:  0.5132316946983337\n",
      "epoch:  1   step:  85   train loss:  0.39205342531204224  val loss:  0.5151496529579163\n",
      "epoch:  1   step:  86   train loss:  0.607182502746582  val loss:  0.5172010064125061\n",
      "epoch:  1   step:  87   train loss:  0.5616068840026855  val loss:  0.5165919065475464\n",
      "epoch:  1   step:  88   train loss:  0.5327337384223938  val loss:  0.5177628397941589\n",
      "epoch:  1   step:  89   train loss:  0.6427062153816223  val loss:  0.5180761218070984\n",
      "epoch:  1   step:  90   train loss:  0.43101441860198975  val loss:  0.5204206705093384\n",
      "epoch:  1   step:  91   train loss:  0.5305639505386353  val loss:  0.5196144580841064\n",
      "epoch:  1   step:  92   train loss:  0.6050065159797668  val loss:  0.5173286199569702\n",
      "epoch:  1   step:  93   train loss:  0.6819875240325928  val loss:  0.5130351185798645\n",
      "epoch:  1   step:  94   train loss:  0.46927618980407715  val loss:  0.5137137770652771\n",
      "epoch:  1   step:  95   train loss:  0.4619765877723694  val loss:  0.5162511467933655\n",
      "epoch:  1   step:  96   train loss:  0.5073611736297607  val loss:  0.5159762501716614\n",
      "epoch:  1   step:  97   train loss:  0.4163038730621338  val loss:  0.519397497177124\n",
      "epoch:  1   step:  98   train loss:  0.6618092060089111  val loss:  0.519122838973999\n",
      "epoch:  1   step:  99   train loss:  0.6186758279800415  val loss:  0.5158522129058838\n",
      "epoch:  1   step:  100   train loss:  0.5819709897041321  val loss:  0.5140514969825745\n",
      "epoch:  1   step:  101   train loss:  0.4804428219795227  val loss:  0.5121616721153259\n",
      "epoch:  1   step:  102   train loss:  0.565928041934967  val loss:  0.513292133808136\n",
      "epoch:  1   step:  103   train loss:  0.5954999923706055  val loss:  0.5155869126319885\n",
      "epoch:  1   step:  104   train loss:  0.5055044889450073  val loss:  0.5208525657653809\n",
      "epoch:  1   step:  105   train loss:  0.4850967824459076  val loss:  0.5256903767585754\n",
      "epoch:  1   step:  106   train loss:  0.6734899282455444  val loss:  0.5220739245414734\n",
      "epoch:  1   step:  107   train loss:  0.4946202337741852  val loss:  0.5200971961021423\n",
      "epoch:  1   step:  108   train loss:  0.4912837743759155  val loss:  0.5141745209693909\n",
      "epoch:  1   step:  109   train loss:  0.7667996883392334  val loss:  0.5054882168769836\n",
      "epoch:  1   step:  110   train loss:  0.5157305002212524  val loss:  0.501670777797699\n",
      "epoch:  1   step:  111   train loss:  0.4196389317512512  val loss:  0.4961548447608948\n",
      "epoch:  1   step:  112   train loss:  0.5254346132278442  val loss:  0.4931727349758148\n",
      "epoch:  1   step:  113   train loss:  0.5059611797332764  val loss:  0.48942407965660095\n",
      "epoch:  1   step:  114   train loss:  0.5946284532546997  val loss:  0.48719316720962524\n",
      "epoch:  1   step:  115   train loss:  0.426806777715683  val loss:  0.4849132299423218\n",
      "min_val_loss_print 0.4849132299423218\n",
      "epoch:  1   step:  116   train loss:  0.5411446690559387  val loss:  0.48080894351005554\n",
      "min_val_loss_print 0.48080894351005554\n",
      "epoch:  1   step:  117   train loss:  0.45123231410980225  val loss:  0.47843995690345764\n",
      "min_val_loss_print 0.47843995690345764\n",
      "epoch:  1   step:  118   train loss:  0.5001204013824463  val loss:  0.4758894741535187\n",
      "min_val_loss_print 0.4758894741535187\n",
      "epoch:  1   step:  119   train loss:  0.3873257637023926  val loss:  0.47601813077926636\n",
      "epoch:  1   step:  120   train loss:  0.6349095702171326  val loss:  0.47441115975379944\n",
      "min_val_loss_print 0.47441115975379944\n",
      "epoch:  1   step:  121   train loss:  0.7563585042953491  val loss:  0.4736807942390442\n",
      "min_val_loss_print 0.4736807942390442\n",
      "epoch:  1   step:  122   train loss:  0.5065064430236816  val loss:  0.4771959185600281\n",
      "epoch:  1   step:  123   train loss:  0.5473917722702026  val loss:  0.4851354956626892\n",
      "epoch:  1   step:  124   train loss:  0.7585877776145935  val loss:  0.5007523894309998\n",
      "epoch:  1   step:  125   train loss:  0.6153776049613953  val loss:  0.515508234500885\n",
      "epoch:  1   step:  126   train loss:  0.6356815099716187  val loss:  0.5292867422103882\n",
      "epoch:  1   step:  127   train loss:  0.5502790212631226  val loss:  0.5358174443244934\n",
      "epoch:  1   step:  128   train loss:  0.5835621356964111  val loss:  0.5432834625244141\n",
      "epoch:  1   step:  129   train loss:  0.5838586091995239  val loss:  0.5463215708732605\n",
      "epoch:  1   step:  130   train loss:  0.6291589140892029  val loss:  0.544968843460083\n",
      "epoch:  1   step:  131   train loss:  0.46581387519836426  val loss:  0.5408104658126831\n",
      "epoch:  1   step:  132   train loss:  0.6026797294616699  val loss:  0.5376591086387634\n",
      "epoch:  1   step:  133   train loss:  0.5895570516586304  val loss:  0.533338725566864\n",
      "epoch:  1   step:  134   train loss:  0.5598301887512207  val loss:  0.5281004905700684\n",
      "epoch:  1   step:  135   train loss:  0.6114169359207153  val loss:  0.5235666632652283\n",
      "epoch:  1   step:  136   train loss:  0.6262855529785156  val loss:  0.5182252526283264\n",
      "epoch:  1   step:  137   train loss:  0.4985200762748718  val loss:  0.5134262442588806\n",
      "epoch:  1   step:  138   train loss:  0.6350109577178955  val loss:  0.5080163478851318\n",
      "epoch:  1   step:  139   train loss:  0.4142531752586365  val loss:  0.5034732222557068\n",
      "epoch:  1   step:  140   train loss:  0.5318846702575684  val loss:  0.5024098753929138\n",
      "epoch:  1   step:  141   train loss:  0.5379162430763245  val loss:  0.5033155083656311\n",
      "epoch:  1   step:  142   train loss:  0.6694438457489014  val loss:  0.49833187460899353\n",
      "epoch:  1   step:  143   train loss:  0.45863673090934753  val loss:  0.4949033260345459\n",
      "epoch:  1   step:  144   train loss:  0.772214949131012  val loss:  0.489895224571228\n",
      "epoch:  1   step:  145   train loss:  0.45563000440597534  val loss:  0.4864200949668884\n",
      "epoch:  1   step:  146   train loss:  0.4942568838596344  val loss:  0.48755109310150146\n",
      "epoch:  1   step:  147   train loss:  0.47159260511398315  val loss:  0.49161863327026367\n",
      "epoch:  1   step:  148   train loss:  0.40389400720596313  val loss:  0.49951696395874023\n",
      "epoch:  1   step:  149   train loss:  0.643916130065918  val loss:  0.5006359815597534\n",
      "epoch:  1   step:  150   train loss:  0.5161241888999939  val loss:  0.49731671810150146\n",
      "epoch:  1   step:  151   train loss:  0.7000783681869507  val loss:  0.49605458974838257\n",
      "epoch:  1   step:  152   train loss:  0.45545563101768494  val loss:  0.49628934264183044\n",
      "epoch:  1   step:  153   train loss:  0.4126891493797302  val loss:  0.49397119879722595\n",
      "epoch:  1   step:  154   train loss:  0.5807464122772217  val loss:  0.48887899518013\n",
      "epoch:  1   step:  155   train loss:  0.5050477981567383  val loss:  0.48401862382888794\n",
      "epoch:  1   step:  156   train loss:  0.49375593662261963  val loss:  0.4824366271495819\n",
      "epoch:  1   step:  157   train loss:  0.4377397894859314  val loss:  0.4821353554725647\n",
      "epoch:  1   step:  158   train loss:  0.4521159529685974  val loss:  0.48330292105674744\n",
      "epoch:  1   step:  159   train loss:  0.504758894443512  val loss:  0.4946189522743225\n",
      "epoch:  1   step:  160   train loss:  0.736386775970459  val loss:  0.5012447237968445\n",
      "epoch:  1   step:  161   train loss:  0.6001282930374146  val loss:  0.504777729511261\n",
      "epoch:  1   step:  162   train loss:  0.6952657103538513  val loss:  0.5010892152786255\n",
      "epoch:  1   step:  163   train loss:  0.5241339206695557  val loss:  0.50107741355896\n",
      "epoch:  1   step:  164   train loss:  0.5861498117446899  val loss:  0.49686166644096375\n",
      "epoch:  1   step:  165   train loss:  0.5453436970710754  val loss:  0.49627822637557983\n",
      "epoch:  2   step:  0   train loss:  0.5384949445724487  val loss:  0.49812957644462585\n",
      "epoch:  2   step:  1   train loss:  0.5664069652557373  val loss:  0.4990886449813843\n",
      "epoch:  2   step:  2   train loss:  0.6029941439628601  val loss:  0.5136375427246094\n",
      "epoch:  2   step:  3   train loss:  0.5498356819152832  val loss:  0.5364084839820862\n",
      "epoch:  2   step:  4   train loss:  0.6602663397789001  val loss:  0.554692804813385\n",
      "epoch:  2   step:  5   train loss:  0.5743366479873657  val loss:  0.5399577021598816\n",
      "epoch:  2   step:  6   train loss:  0.4909799098968506  val loss:  0.5264158844947815\n",
      "epoch:  2   step:  7   train loss:  0.5541747212409973  val loss:  0.5107981562614441\n",
      "epoch:  2   step:  8   train loss:  0.6181272864341736  val loss:  0.5001771450042725\n",
      "epoch:  2   step:  9   train loss:  0.5821200013160706  val loss:  0.4989160895347595\n",
      "epoch:  2   step:  10   train loss:  0.4606243371963501  val loss:  0.511598527431488\n",
      "epoch:  2   step:  11   train loss:  0.47478318214416504  val loss:  0.5118346214294434\n",
      "epoch:  2   step:  12   train loss:  0.8534319996833801  val loss:  0.506303071975708\n",
      "epoch:  2   step:  13   train loss:  0.5573211908340454  val loss:  0.5109217166900635\n",
      "epoch:  2   step:  14   train loss:  0.48182380199432373  val loss:  0.5165266394615173\n",
      "epoch:  2   step:  15   train loss:  0.5917165279388428  val loss:  0.5183318853378296\n",
      "epoch:  2   step:  16   train loss:  0.6967240571975708  val loss:  0.5233015418052673\n",
      "epoch:  2   step:  17   train loss:  0.4869994819164276  val loss:  0.5308970212936401\n",
      "epoch:  2   step:  18   train loss:  0.3929731547832489  val loss:  0.5375046730041504\n",
      "epoch:  2   step:  19   train loss:  0.44840943813323975  val loss:  0.5408875346183777\n",
      "epoch:  2   step:  20   train loss:  0.45151033997535706  val loss:  0.5448416471481323\n",
      "epoch:  2   step:  21   train loss:  0.6019116640090942  val loss:  0.5507649183273315\n",
      "epoch:  2   step:  22   train loss:  0.44552934169769287  val loss:  0.5580021739006042\n",
      "epoch:  2   step:  23   train loss:  0.43643519282341003  val loss:  0.5705413222312927\n",
      "epoch:  2   step:  24   train loss:  0.5541033744812012  val loss:  0.5797250866889954\n",
      "epoch:  2   step:  25   train loss:  0.41808438301086426  val loss:  0.5866937637329102\n",
      "epoch:  2   step:  26   train loss:  0.45078253746032715  val loss:  0.5894483327865601\n",
      "epoch:  2   step:  27   train loss:  0.6114477515220642  val loss:  0.5885828733444214\n",
      "epoch:  2   step:  28   train loss:  0.46912091970443726  val loss:  0.5855991244316101\n",
      "epoch:  2   step:  29   train loss:  0.5466898679733276  val loss:  0.5782854557037354\n",
      "epoch:  2   step:  30   train loss:  0.6704557538032532  val loss:  0.5686811804771423\n",
      "epoch:  2   step:  31   train loss:  0.6551867127418518  val loss:  0.5561439990997314\n",
      "epoch:  2   step:  32   train loss:  0.48054268956184387  val loss:  0.5458846092224121\n",
      "epoch:  2   step:  33   train loss:  0.5730558633804321  val loss:  0.5354975461959839\n",
      "epoch:  2   step:  34   train loss:  0.5339359045028687  val loss:  0.527363121509552\n",
      "epoch:  2   step:  35   train loss:  0.4888528883457184  val loss:  0.5207650661468506\n",
      "epoch:  2   step:  36   train loss:  0.5276898741722107  val loss:  0.5208806991577148\n",
      "epoch:  2   step:  37   train loss:  0.6082884073257446  val loss:  0.5264133214950562\n",
      "epoch:  2   step:  38   train loss:  0.6621420383453369  val loss:  0.5438984036445618\n",
      "epoch:  2   step:  39   train loss:  0.5896082520484924  val loss:  0.5517987608909607\n",
      "epoch:  2   step:  40   train loss:  0.6093581914901733  val loss:  0.5479458570480347\n",
      "epoch:  2   step:  41   train loss:  0.5536066293716431  val loss:  0.52926105260849\n",
      "epoch:  2   step:  42   train loss:  0.634543776512146  val loss:  0.5120806694030762\n",
      "epoch:  2   step:  43   train loss:  0.5878596305847168  val loss:  0.496306449174881\n",
      "epoch:  2   step:  44   train loss:  0.57318115234375  val loss:  0.4876387417316437\n",
      "epoch:  2   step:  45   train loss:  0.5249460339546204  val loss:  0.48223283886909485\n",
      "epoch:  2   step:  46   train loss:  0.4916830062866211  val loss:  0.4776235818862915\n",
      "epoch:  2   step:  47   train loss:  0.5060291290283203  val loss:  0.4759472608566284\n",
      "epoch:  2   step:  48   train loss:  0.5338853597640991  val loss:  0.4758094549179077\n",
      "epoch:  2   step:  49   train loss:  0.462257981300354  val loss:  0.4756028652191162\n",
      "epoch:  2   step:  50   train loss:  0.6192887425422668  val loss:  0.478310763835907\n",
      "epoch:  2   step:  51   train loss:  0.5946224927902222  val loss:  0.48169440031051636\n",
      "epoch:  2   step:  52   train loss:  0.43310046195983887  val loss:  0.4892168641090393\n",
      "epoch:  2   step:  53   train loss:  0.5871715545654297  val loss:  0.4939696490764618\n",
      "epoch:  2   step:  54   train loss:  0.5543856620788574  val loss:  0.496334433555603\n",
      "epoch:  2   step:  55   train loss:  0.6275067329406738  val loss:  0.4985082149505615\n",
      "epoch:  2   step:  56   train loss:  0.5790437459945679  val loss:  0.5009521842002869\n",
      "epoch:  2   step:  57   train loss:  0.580712080001831  val loss:  0.502597987651825\n",
      "epoch:  2   step:  58   train loss:  0.5402233004570007  val loss:  0.502588152885437\n",
      "epoch:  2   step:  59   train loss:  0.5448716878890991  val loss:  0.5050457715988159\n",
      "epoch:  2   step:  60   train loss:  0.45612311363220215  val loss:  0.5075871348381042\n",
      "epoch:  2   step:  61   train loss:  0.42835643887519836  val loss:  0.5090396404266357\n",
      "epoch:  2   step:  62   train loss:  0.5956833958625793  val loss:  0.5100590586662292\n",
      "epoch:  2   step:  63   train loss:  0.4778674244880676  val loss:  0.5112028121948242\n",
      "epoch:  2   step:  64   train loss:  0.5653563737869263  val loss:  0.5128873586654663\n",
      "epoch:  2   step:  65   train loss:  0.5683591365814209  val loss:  0.5134228467941284\n",
      "epoch:  2   step:  66   train loss:  0.5733066201210022  val loss:  0.5145023465156555\n",
      "epoch:  2   step:  67   train loss:  0.6570958495140076  val loss:  0.5133071541786194\n",
      "epoch:  2   step:  68   train loss:  0.466738760471344  val loss:  0.5120013356208801\n",
      "epoch:  2   step:  69   train loss:  0.5184584856033325  val loss:  0.5084915161132812\n",
      "epoch:  2   step:  70   train loss:  0.49213945865631104  val loss:  0.506445050239563\n",
      "epoch:  2   step:  71   train loss:  0.5758304595947266  val loss:  0.5081972479820251\n",
      "epoch:  2   step:  72   train loss:  0.738111674785614  val loss:  0.5087136626243591\n",
      "epoch:  2   step:  73   train loss:  0.6295799016952515  val loss:  0.5145072937011719\n",
      "epoch:  2   step:  74   train loss:  0.4495806097984314  val loss:  0.5212291479110718\n",
      "epoch:  2   step:  75   train loss:  0.5696162581443787  val loss:  0.5256626009941101\n",
      "epoch:  2   step:  76   train loss:  0.5760800242424011  val loss:  0.5259738564491272\n",
      "epoch:  2   step:  77   train loss:  0.5986913442611694  val loss:  0.5241122841835022\n",
      "epoch:  2   step:  78   train loss:  0.4421982169151306  val loss:  0.5181511044502258\n",
      "epoch:  2   step:  79   train loss:  0.6099860668182373  val loss:  0.514674186706543\n",
      "epoch:  2   step:  80   train loss:  0.4330183267593384  val loss:  0.5090270042419434\n",
      "epoch:  2   step:  81   train loss:  0.5259997844696045  val loss:  0.5064164400100708\n",
      "epoch:  2   step:  82   train loss:  0.4660559892654419  val loss:  0.5058626532554626\n",
      "epoch:  2   step:  83   train loss:  0.49199751019477844  val loss:  0.5094829797744751\n",
      "epoch:  2   step:  84   train loss:  0.3922603130340576  val loss:  0.5174316763877869\n",
      "epoch:  2   step:  85   train loss:  0.36711907386779785  val loss:  0.5276227593421936\n",
      "epoch:  2   step:  86   train loss:  0.6237431764602661  val loss:  0.5300760269165039\n",
      "epoch:  2   step:  87   train loss:  0.7318782806396484  val loss:  0.5188577175140381\n",
      "epoch:  2   step:  88   train loss:  0.5015151500701904  val loss:  0.5047096014022827\n",
      "epoch:  2   step:  89   train loss:  0.3716723918914795  val loss:  0.4949266016483307\n",
      "epoch:  2   step:  90   train loss:  0.6451636552810669  val loss:  0.4832766652107239\n",
      "epoch:  2   step:  91   train loss:  0.40321701765060425  val loss:  0.4748225212097168\n",
      "epoch:  2   step:  92   train loss:  0.7924572229385376  val loss:  0.472244530916214\n",
      "min_val_loss_print 0.472244530916214\n",
      "epoch:  2   step:  93   train loss:  0.47616466879844666  val loss:  0.4784969985485077\n",
      "epoch:  2   step:  94   train loss:  0.5968379974365234  val loss:  0.4904904067516327\n",
      "epoch:  2   step:  95   train loss:  0.5393325090408325  val loss:  0.49509871006011963\n",
      "epoch:  2   step:  96   train loss:  0.6503288149833679  val loss:  0.49127334356307983\n",
      "epoch:  2   step:  97   train loss:  0.5455647706985474  val loss:  0.48233604431152344\n",
      "epoch:  2   step:  98   train loss:  0.6030339002609253  val loss:  0.4753229320049286\n",
      "epoch:  2   step:  99   train loss:  0.619429886341095  val loss:  0.4690040051937103\n",
      "min_val_loss_print 0.4690040051937103\n",
      "epoch:  2   step:  100   train loss:  0.5216599702835083  val loss:  0.46475958824157715\n",
      "min_val_loss_print 0.46475958824157715\n",
      "epoch:  2   step:  101   train loss:  0.5537222623825073  val loss:  0.4626960754394531\n",
      "min_val_loss_print 0.4626960754394531\n",
      "epoch:  2   step:  102   train loss:  0.42620301246643066  val loss:  0.463663250207901\n",
      "epoch:  2   step:  103   train loss:  0.5425735712051392  val loss:  0.465918630361557\n",
      "epoch:  2   step:  104   train loss:  0.4891197383403778  val loss:  0.46809181571006775\n",
      "epoch:  2   step:  105   train loss:  0.5497044324874878  val loss:  0.46801093220710754\n",
      "epoch:  2   step:  106   train loss:  0.606849730014801  val loss:  0.4690062701702118\n",
      "epoch:  2   step:  107   train loss:  0.533072292804718  val loss:  0.47028395533561707\n",
      "epoch:  2   step:  108   train loss:  0.5845130681991577  val loss:  0.47378671169281006\n",
      "epoch:  2   step:  109   train loss:  0.5668261051177979  val loss:  0.4766707420349121\n",
      "epoch:  2   step:  110   train loss:  0.5094591379165649  val loss:  0.4781269431114197\n",
      "epoch:  2   step:  111   train loss:  0.5292339324951172  val loss:  0.48206037282943726\n",
      "epoch:  2   step:  112   train loss:  0.6608628034591675  val loss:  0.48983144760131836\n",
      "epoch:  2   step:  113   train loss:  0.5301861763000488  val loss:  0.49768248200416565\n",
      "epoch:  2   step:  114   train loss:  0.5026990175247192  val loss:  0.5022200345993042\n",
      "epoch:  2   step:  115   train loss:  0.49957358837127686  val loss:  0.5021164417266846\n",
      "epoch:  2   step:  116   train loss:  0.5629279613494873  val loss:  0.5022907257080078\n",
      "epoch:  2   step:  117   train loss:  0.6155954599380493  val loss:  0.5038288235664368\n",
      "epoch:  2   step:  118   train loss:  0.6157642006874084  val loss:  0.5070983171463013\n",
      "epoch:  2   step:  119   train loss:  0.4630431532859802  val loss:  0.5063012838363647\n",
      "epoch:  2   step:  120   train loss:  0.615526556968689  val loss:  0.5074529051780701\n",
      "epoch:  2   step:  121   train loss:  0.5564894676208496  val loss:  0.5069018602371216\n",
      "epoch:  2   step:  122   train loss:  0.48746007680892944  val loss:  0.506740927696228\n",
      "epoch:  2   step:  123   train loss:  0.5695633888244629  val loss:  0.5046114921569824\n",
      "epoch:  2   step:  124   train loss:  0.5103243589401245  val loss:  0.5062747001647949\n",
      "epoch:  2   step:  125   train loss:  0.6033728122711182  val loss:  0.505346953868866\n",
      "epoch:  2   step:  126   train loss:  0.6324695348739624  val loss:  0.5043122172355652\n",
      "epoch:  2   step:  127   train loss:  0.4492923617362976  val loss:  0.5050713419914246\n",
      "epoch:  2   step:  128   train loss:  0.5683855414390564  val loss:  0.506362795829773\n",
      "epoch:  2   step:  129   train loss:  0.5776386260986328  val loss:  0.5069161653518677\n",
      "epoch:  2   step:  130   train loss:  0.5377027988433838  val loss:  0.5086056590080261\n",
      "epoch:  2   step:  131   train loss:  0.5019722580909729  val loss:  0.5093221068382263\n",
      "epoch:  2   step:  132   train loss:  0.44171059131622314  val loss:  0.5101699233055115\n",
      "epoch:  2   step:  133   train loss:  0.5120316743850708  val loss:  0.5085242390632629\n",
      "epoch:  2   step:  134   train loss:  0.4560360312461853  val loss:  0.5095175504684448\n",
      "epoch:  2   step:  135   train loss:  0.6287190914154053  val loss:  0.5085899829864502\n",
      "epoch:  2   step:  136   train loss:  0.7056050896644592  val loss:  0.510132908821106\n",
      "epoch:  2   step:  137   train loss:  0.46598362922668457  val loss:  0.5135104060173035\n",
      "epoch:  2   step:  138   train loss:  0.5206698179244995  val loss:  0.5205925107002258\n",
      "epoch:  2   step:  139   train loss:  0.4984491169452667  val loss:  0.5239837765693665\n",
      "epoch:  2   step:  140   train loss:  0.6199592351913452  val loss:  0.5279130935668945\n",
      "epoch:  2   step:  141   train loss:  0.43470287322998047  val loss:  0.5313286781311035\n",
      "epoch:  2   step:  142   train loss:  0.5687108039855957  val loss:  0.532726526260376\n",
      "epoch:  2   step:  143   train loss:  0.4893960952758789  val loss:  0.5337736010551453\n",
      "epoch:  2   step:  144   train loss:  0.4153994917869568  val loss:  0.5342927575111389\n",
      "epoch:  2   step:  145   train loss:  0.4544273912906647  val loss:  0.5344905257225037\n",
      "epoch:  2   step:  146   train loss:  0.5775433778762817  val loss:  0.5364608764648438\n",
      "epoch:  2   step:  147   train loss:  0.532421350479126  val loss:  0.5377744436264038\n",
      "epoch:  2   step:  148   train loss:  0.5646333694458008  val loss:  0.5370125770568848\n",
      "epoch:  2   step:  149   train loss:  0.49103450775146484  val loss:  0.5327956676483154\n",
      "epoch:  2   step:  150   train loss:  0.6441107392311096  val loss:  0.5249122977256775\n",
      "epoch:  2   step:  151   train loss:  0.5043898820877075  val loss:  0.5204913020133972\n",
      "epoch:  2   step:  152   train loss:  0.44214117527008057  val loss:  0.5203609466552734\n",
      "epoch:  2   step:  153   train loss:  0.4635166525840759  val loss:  0.5187490582466125\n",
      "epoch:  2   step:  154   train loss:  0.47478681802749634  val loss:  0.5168176293373108\n",
      "epoch:  2   step:  155   train loss:  0.5506110191345215  val loss:  0.5143954753875732\n",
      "epoch:  2   step:  156   train loss:  0.6247098445892334  val loss:  0.5132231116294861\n",
      "epoch:  2   step:  157   train loss:  0.5478017330169678  val loss:  0.5117272734642029\n",
      "epoch:  2   step:  158   train loss:  0.5936151146888733  val loss:  0.512531042098999\n",
      "epoch:  2   step:  159   train loss:  0.7477063536643982  val loss:  0.5158255696296692\n",
      "epoch:  2   step:  160   train loss:  0.6364613771438599  val loss:  0.5188402533531189\n",
      "epoch:  2   step:  161   train loss:  0.48704957962036133  val loss:  0.5195428729057312\n",
      "epoch:  2   step:  162   train loss:  0.5202588438987732  val loss:  0.5179144740104675\n",
      "epoch:  2   step:  163   train loss:  0.5456994771957397  val loss:  0.5184261202812195\n",
      "epoch:  2   step:  164   train loss:  0.5714102983474731  val loss:  0.5210928320884705\n",
      "epoch:  2   step:  165   train loss:  0.6605621576309204  val loss:  0.5210874080657959\n",
      "epoch:  3   step:  0   train loss:  0.6543916463851929  val loss:  0.5202119946479797\n",
      "epoch:  3   step:  1   train loss:  0.6273970603942871  val loss:  0.5188994407653809\n",
      "epoch:  3   step:  2   train loss:  0.5108345150947571  val loss:  0.5200884938240051\n",
      "epoch:  3   step:  3   train loss:  0.6110188364982605  val loss:  0.5212655663490295\n",
      "epoch:  3   step:  4   train loss:  0.47994351387023926  val loss:  0.5191159844398499\n",
      "epoch:  3   step:  5   train loss:  0.5476956367492676  val loss:  0.5137308835983276\n",
      "epoch:  3   step:  6   train loss:  0.5013523101806641  val loss:  0.5090562105178833\n",
      "epoch:  3   step:  7   train loss:  0.5733621120452881  val loss:  0.5057229399681091\n",
      "epoch:  3   step:  8   train loss:  0.45297157764434814  val loss:  0.5038818120956421\n",
      "epoch:  3   step:  9   train loss:  0.44536685943603516  val loss:  0.5063796639442444\n",
      "epoch:  3   step:  10   train loss:  0.6680368781089783  val loss:  0.5048432946205139\n",
      "epoch:  3   step:  11   train loss:  0.5260095000267029  val loss:  0.5051568746566772\n",
      "epoch:  3   step:  12   train loss:  0.41124922037124634  val loss:  0.5049334764480591\n",
      "epoch:  3   step:  13   train loss:  0.6840541362762451  val loss:  0.5001020431518555\n",
      "epoch:  3   step:  14   train loss:  0.48014581203460693  val loss:  0.5041967630386353\n",
      "epoch:  3   step:  15   train loss:  0.43898439407348633  val loss:  0.497278094291687\n",
      "epoch:  3   step:  16   train loss:  0.5344051122665405  val loss:  0.4944987893104553\n",
      "epoch:  3   step:  17   train loss:  0.49019861221313477  val loss:  0.49113917350769043\n",
      "epoch:  3   step:  18   train loss:  0.4842780530452728  val loss:  0.48597896099090576\n",
      "epoch:  3   step:  19   train loss:  0.5206162333488464  val loss:  0.4843738377094269\n",
      "epoch:  3   step:  20   train loss:  0.40507781505584717  val loss:  0.4831980764865875\n",
      "epoch:  3   step:  21   train loss:  0.7030854821205139  val loss:  0.4827744960784912\n",
      "epoch:  3   step:  22   train loss:  0.6196412444114685  val loss:  0.487729012966156\n",
      "epoch:  3   step:  23   train loss:  0.4949856400489807  val loss:  0.4923388659954071\n",
      "epoch:  3   step:  24   train loss:  0.524073600769043  val loss:  0.49724531173706055\n",
      "epoch:  3   step:  25   train loss:  0.46551403403282166  val loss:  0.49950340390205383\n",
      "epoch:  3   step:  26   train loss:  0.5866393446922302  val loss:  0.4929565191268921\n",
      "epoch:  3   step:  27   train loss:  0.6294310688972473  val loss:  0.48635396361351013\n",
      "epoch:  3   step:  28   train loss:  0.4621393084526062  val loss:  0.48033377528190613\n",
      "epoch:  3   step:  29   train loss:  0.5017641186714172  val loss:  0.4766005277633667\n",
      "epoch:  3   step:  30   train loss:  0.5810363292694092  val loss:  0.4770297110080719\n",
      "epoch:  3   step:  31   train loss:  0.44719651341438293  val loss:  0.47895488142967224\n",
      "epoch:  3   step:  32   train loss:  0.528498113155365  val loss:  0.48225364089012146\n",
      "epoch:  3   step:  33   train loss:  0.5695673227310181  val loss:  0.4853822886943817\n",
      "epoch:  3   step:  34   train loss:  0.6477116346359253  val loss:  0.48787450790405273\n",
      "epoch:  3   step:  35   train loss:  0.5324164628982544  val loss:  0.49082812666893005\n",
      "epoch:  3   step:  36   train loss:  0.5499821901321411  val loss:  0.49497702717781067\n",
      "epoch:  3   step:  37   train loss:  0.5032429695129395  val loss:  0.5025278329849243\n",
      "epoch:  3   step:  38   train loss:  0.47869712114334106  val loss:  0.5114250779151917\n",
      "epoch:  3   step:  39   train loss:  0.5770879983901978  val loss:  0.5201223492622375\n",
      "epoch:  3   step:  40   train loss:  0.5613771677017212  val loss:  0.5278435945510864\n",
      "epoch:  3   step:  41   train loss:  0.5321861505508423  val loss:  0.5273540616035461\n",
      "epoch:  3   step:  42   train loss:  0.4774841070175171  val loss:  0.5262653827667236\n",
      "epoch:  3   step:  43   train loss:  0.4602966010570526  val loss:  0.524309515953064\n",
      "epoch:  3   step:  44   train loss:  0.5302138328552246  val loss:  0.5231021642684937\n",
      "epoch:  3   step:  45   train loss:  0.5289353132247925  val loss:  0.5207247734069824\n",
      "epoch:  3   step:  46   train loss:  0.5408720970153809  val loss:  0.5245783925056458\n",
      "epoch:  3   step:  47   train loss:  0.5036619901657104  val loss:  0.527182400226593\n",
      "epoch:  3   step:  48   train loss:  0.6590597629547119  val loss:  0.5277174115180969\n",
      "epoch:  3   step:  49   train loss:  0.7426190972328186  val loss:  0.5268590450286865\n",
      "epoch:  3   step:  50   train loss:  0.5837763547897339  val loss:  0.5244490504264832\n",
      "epoch:  3   step:  51   train loss:  0.6569739580154419  val loss:  0.5283194780349731\n",
      "epoch:  3   step:  52   train loss:  0.5549419522285461  val loss:  0.5304980278015137\n",
      "epoch:  3   step:  53   train loss:  0.5637662410736084  val loss:  0.5319388508796692\n",
      "epoch:  3   step:  54   train loss:  0.48293888568878174  val loss:  0.527462363243103\n",
      "epoch:  3   step:  55   train loss:  0.5238305926322937  val loss:  0.5178242921829224\n",
      "epoch:  3   step:  56   train loss:  0.5716443061828613  val loss:  0.5085652470588684\n",
      "epoch:  3   step:  57   train loss:  0.5384429693222046  val loss:  0.5044048428535461\n",
      "epoch:  3   step:  58   train loss:  0.5306422710418701  val loss:  0.5022188425064087\n",
      "epoch:  3   step:  59   train loss:  0.37073656916618347  val loss:  0.5034248232841492\n",
      "epoch:  3   step:  60   train loss:  0.5582367777824402  val loss:  0.5022424459457397\n",
      "epoch:  3   step:  61   train loss:  0.3420870304107666  val loss:  0.501250684261322\n",
      "epoch:  3   step:  62   train loss:  0.5107375383377075  val loss:  0.5005504488945007\n",
      "epoch:  3   step:  63   train loss:  0.4518783688545227  val loss:  0.5002166032791138\n",
      "epoch:  3   step:  64   train loss:  0.5087156295776367  val loss:  0.49660253524780273\n",
      "epoch:  3   step:  65   train loss:  0.867584764957428  val loss:  0.47734999656677246\n",
      "epoch:  3   step:  66   train loss:  0.38371142745018005  val loss:  0.4668533504009247\n",
      "epoch:  3   step:  67   train loss:  0.4769340753555298  val loss:  0.46411898732185364\n",
      "epoch:  3   step:  68   train loss:  0.44610369205474854  val loss:  0.46410316228866577\n",
      "epoch:  3   step:  69   train loss:  0.5057783126831055  val loss:  0.4652159810066223\n",
      "epoch:  3   step:  70   train loss:  0.6923035979270935  val loss:  0.4669114649295807\n",
      "epoch:  3   step:  71   train loss:  0.5068017840385437  val loss:  0.46983596682548523\n",
      "epoch:  3   step:  72   train loss:  0.7440549731254578  val loss:  0.4739885926246643\n",
      "epoch:  3   step:  73   train loss:  0.5176675319671631  val loss:  0.4787956774234772\n",
      "epoch:  3   step:  74   train loss:  0.5089647769927979  val loss:  0.48382315039634705\n",
      "epoch:  3   step:  75   train loss:  0.5390390157699585  val loss:  0.4878608286380768\n",
      "epoch:  3   step:  76   train loss:  0.47311142086982727  val loss:  0.4915917217731476\n",
      "epoch:  3   step:  77   train loss:  0.5073169469833374  val loss:  0.49587583541870117\n",
      "epoch:  3   step:  78   train loss:  0.5463269948959351  val loss:  0.4989587962627411\n",
      "epoch:  3   step:  79   train loss:  0.44104355573654175  val loss:  0.5007285475730896\n",
      "epoch:  3   step:  80   train loss:  0.4558076560497284  val loss:  0.49990954995155334\n",
      "epoch:  3   step:  81   train loss:  0.5664883255958557  val loss:  0.49947842955589294\n",
      "epoch:  3   step:  82   train loss:  0.5616570711135864  val loss:  0.49967414140701294\n",
      "epoch:  3   step:  83   train loss:  0.4623992145061493  val loss:  0.4994460940361023\n",
      "epoch:  3   step:  84   train loss:  0.8330789804458618  val loss:  0.49929237365722656\n",
      "epoch:  3   step:  85   train loss:  0.5154163837432861  val loss:  0.5006354451179504\n",
      "epoch:  3   step:  86   train loss:  0.44513434171676636  val loss:  0.5017290115356445\n",
      "epoch:  3   step:  87   train loss:  0.493140310049057  val loss:  0.5002793669700623\n",
      "epoch:  3   step:  88   train loss:  0.4766991436481476  val loss:  0.4978669285774231\n",
      "epoch:  3   step:  89   train loss:  0.49656981229782104  val loss:  0.49403196573257446\n",
      "epoch:  3   step:  90   train loss:  0.46787229180336  val loss:  0.48935264348983765\n",
      "epoch:  3   step:  91   train loss:  0.4371291697025299  val loss:  0.4857553243637085\n",
      "epoch:  3   step:  92   train loss:  0.5192848443984985  val loss:  0.48445937037467957\n",
      "epoch:  3   step:  93   train loss:  0.3707404136657715  val loss:  0.48642218112945557\n",
      "epoch:  3   step:  94   train loss:  0.6168311238288879  val loss:  0.49038100242614746\n",
      "epoch:  3   step:  95   train loss:  0.659098207950592  val loss:  0.49205026030540466\n",
      "epoch:  3   step:  96   train loss:  0.6305199861526489  val loss:  0.48869097232818604\n",
      "epoch:  3   step:  97   train loss:  0.6539710164070129  val loss:  0.48490607738494873\n",
      "epoch:  3   step:  98   train loss:  0.4611058235168457  val loss:  0.47842252254486084\n",
      "epoch:  3   step:  99   train loss:  0.458584189414978  val loss:  0.4733099341392517\n",
      "epoch:  3   step:  100   train loss:  0.5291050672531128  val loss:  0.47034475207328796\n",
      "epoch:  3   step:  101   train loss:  0.5101891756057739  val loss:  0.4724021852016449\n",
      "epoch:  3   step:  102   train loss:  0.5385600328445435  val loss:  0.4741550385951996\n",
      "epoch:  3   step:  103   train loss:  0.6619265079498291  val loss:  0.47896265983581543\n",
      "epoch:  3   step:  104   train loss:  0.4423609972000122  val loss:  0.4834132790565491\n",
      "epoch:  3   step:  105   train loss:  0.559550404548645  val loss:  0.48595675826072693\n",
      "epoch:  3   step:  106   train loss:  0.5259037613868713  val loss:  0.4822314977645874\n",
      "epoch:  3   step:  107   train loss:  0.5835993885993958  val loss:  0.4764953553676605\n",
      "epoch:  3   step:  108   train loss:  0.6207932829856873  val loss:  0.47237640619277954\n",
      "epoch:  3   step:  109   train loss:  0.46862560510635376  val loss:  0.4720487892627716\n",
      "epoch:  3   step:  110   train loss:  0.5031864643096924  val loss:  0.4775666296482086\n",
      "epoch:  3   step:  111   train loss:  0.5484216809272766  val loss:  0.48617270588874817\n",
      "epoch:  3   step:  112   train loss:  0.5453991889953613  val loss:  0.4960273504257202\n",
      "epoch:  3   step:  113   train loss:  0.5789121985435486  val loss:  0.5030115842819214\n",
      "epoch:  3   step:  114   train loss:  0.41202208399772644  val loss:  0.5103201270103455\n",
      "epoch:  3   step:  115   train loss:  0.6623465418815613  val loss:  0.5130428075790405\n",
      "epoch:  3   step:  116   train loss:  0.6882685422897339  val loss:  0.5073619484901428\n",
      "epoch:  3   step:  117   train loss:  0.3788679838180542  val loss:  0.5047182440757751\n",
      "epoch:  3   step:  118   train loss:  0.6326621770858765  val loss:  0.5003161430358887\n",
      "epoch:  3   step:  119   train loss:  0.4726066291332245  val loss:  0.4986647367477417\n",
      "epoch:  3   step:  120   train loss:  0.6643060445785522  val loss:  0.49807965755462646\n",
      "epoch:  3   step:  121   train loss:  0.6254799365997314  val loss:  0.5007603764533997\n",
      "epoch:  3   step:  122   train loss:  0.506409227848053  val loss:  0.5058398842811584\n",
      "epoch:  3   step:  123   train loss:  0.5038851499557495  val loss:  0.5087683200836182\n",
      "epoch:  3   step:  124   train loss:  0.6015745997428894  val loss:  0.5120393633842468\n",
      "epoch:  3   step:  125   train loss:  0.5045604705810547  val loss:  0.5127401351928711\n",
      "epoch:  3   step:  126   train loss:  0.5726581811904907  val loss:  0.5122909545898438\n",
      "epoch:  3   step:  127   train loss:  0.5320895910263062  val loss:  0.5082020163536072\n",
      "epoch:  3   step:  128   train loss:  0.6072746515274048  val loss:  0.5070727467536926\n",
      "epoch:  3   step:  129   train loss:  0.5915403366088867  val loss:  0.5062940120697021\n",
      "epoch:  3   step:  130   train loss:  0.49567365646362305  val loss:  0.5053524374961853\n",
      "epoch:  3   step:  131   train loss:  0.4008421301841736  val loss:  0.5027444958686829\n",
      "epoch:  3   step:  132   train loss:  0.5356335639953613  val loss:  0.5007221698760986\n",
      "epoch:  3   step:  133   train loss:  0.5821755528450012  val loss:  0.49813640117645264\n",
      "epoch:  3   step:  134   train loss:  0.4716595709323883  val loss:  0.49677807092666626\n",
      "epoch:  3   step:  135   train loss:  0.5399614572525024  val loss:  0.49662619829177856\n",
      "epoch:  3   step:  136   train loss:  0.47600600123405457  val loss:  0.49647292494773865\n",
      "epoch:  3   step:  137   train loss:  0.6531660556793213  val loss:  0.4961637854576111\n",
      "epoch:  3   step:  138   train loss:  0.5801295042037964  val loss:  0.49529707431793213\n",
      "epoch:  3   step:  139   train loss:  0.5025115609169006  val loss:  0.4934827387332916\n",
      "epoch:  3   step:  140   train loss:  0.4773100018501282  val loss:  0.4907781779766083\n",
      "epoch:  3   step:  141   train loss:  0.5159552097320557  val loss:  0.48981815576553345\n",
      "epoch:  3   step:  142   train loss:  0.430908203125  val loss:  0.48995712399482727\n",
      "epoch:  3   step:  143   train loss:  0.6079221963882446  val loss:  0.48731204867362976\n",
      "epoch:  3   step:  144   train loss:  0.6163079738616943  val loss:  0.48535627126693726\n",
      "epoch:  3   step:  145   train loss:  0.46432483196258545  val loss:  0.4819468557834625\n",
      "epoch:  3   step:  146   train loss:  0.5161863565444946  val loss:  0.4776657819747925\n",
      "epoch:  3   step:  147   train loss:  0.40216344594955444  val loss:  0.47673964500427246\n",
      "epoch:  3   step:  148   train loss:  0.5151892900466919  val loss:  0.47384634613990784\n",
      "epoch:  3   step:  149   train loss:  0.5109630227088928  val loss:  0.4722110629081726\n",
      "epoch:  3   step:  150   train loss:  0.6500829458236694  val loss:  0.4691309928894043\n",
      "epoch:  3   step:  151   train loss:  0.5582371950149536  val loss:  0.4663503170013428\n",
      "epoch:  3   step:  152   train loss:  0.5296783447265625  val loss:  0.464903324842453\n",
      "epoch:  3   step:  153   train loss:  0.6487336158752441  val loss:  0.4668150246143341\n",
      "epoch:  3   step:  154   train loss:  0.50267493724823  val loss:  0.46986380219459534\n",
      "epoch:  3   step:  155   train loss:  0.5077502131462097  val loss:  0.470768004655838\n",
      "epoch:  3   step:  156   train loss:  0.5457726120948792  val loss:  0.4702681601047516\n",
      "epoch:  3   step:  157   train loss:  0.5847595930099487  val loss:  0.46766194701194763\n",
      "epoch:  3   step:  158   train loss:  0.39485377073287964  val loss:  0.4665950536727905\n",
      "epoch:  3   step:  159   train loss:  0.46135413646698  val loss:  0.4644474983215332\n",
      "epoch:  3   step:  160   train loss:  0.6615086793899536  val loss:  0.4663996696472168\n",
      "epoch:  3   step:  161   train loss:  0.4476986229419708  val loss:  0.4692175090312958\n",
      "epoch:  3   step:  162   train loss:  0.46618837118148804  val loss:  0.47134971618652344\n",
      "epoch:  3   step:  163   train loss:  0.5899866819381714  val loss:  0.47481104731559753\n",
      "epoch:  3   step:  164   train loss:  0.35396867990493774  val loss:  0.47764021158218384\n",
      "epoch:  3   step:  165   train loss:  0.5081146359443665  val loss:  0.4786189794540405\n",
      "epoch:  4   step:  0   train loss:  0.5691204071044922  val loss:  0.47803404927253723\n",
      "epoch:  4   step:  1   train loss:  0.3853245973587036  val loss:  0.4786485433578491\n",
      "epoch:  4   step:  2   train loss:  0.3941912055015564  val loss:  0.47855132818222046\n",
      "epoch:  4   step:  3   train loss:  0.46335744857788086  val loss:  0.4801011085510254\n",
      "epoch:  4   step:  4   train loss:  0.5331030488014221  val loss:  0.47835439443588257\n",
      "epoch:  4   step:  5   train loss:  0.507699728012085  val loss:  0.4768553674221039\n",
      "epoch:  4   step:  6   train loss:  0.45357972383499146  val loss:  0.47442469000816345\n",
      "epoch:  4   step:  7   train loss:  0.5226671099662781  val loss:  0.4721820652484894\n",
      "epoch:  4   step:  8   train loss:  0.4990460276603699  val loss:  0.4698973000049591\n",
      "epoch:  4   step:  9   train loss:  0.5152941942214966  val loss:  0.467800498008728\n",
      "epoch:  4   step:  10   train loss:  0.5194436311721802  val loss:  0.4660283029079437\n",
      "epoch:  4   step:  11   train loss:  0.45719772577285767  val loss:  0.4657539129257202\n",
      "epoch:  4   step:  12   train loss:  0.5690745711326599  val loss:  0.4698147773742676\n",
      "epoch:  4   step:  13   train loss:  0.48048821091651917  val loss:  0.473826140165329\n",
      "epoch:  4   step:  14   train loss:  0.5649115443229675  val loss:  0.48300689458847046\n",
      "epoch:  4   step:  15   train loss:  0.5505207180976868  val loss:  0.4836106300354004\n",
      "epoch:  4   step:  16   train loss:  0.5681173801422119  val loss:  0.48234793543815613\n",
      "epoch:  4   step:  17   train loss:  0.554877758026123  val loss:  0.48003965616226196\n",
      "epoch:  4   step:  18   train loss:  0.4863224923610687  val loss:  0.4710162878036499\n",
      "epoch:  4   step:  19   train loss:  0.5998307466506958  val loss:  0.4637468755245209\n",
      "epoch:  4   step:  20   train loss:  0.44914019107818604  val loss:  0.45651307702064514\n",
      "min_val_loss_print 0.45651307702064514\n",
      "epoch:  4   step:  21   train loss:  0.5305420160293579  val loss:  0.45456019043922424\n",
      "min_val_loss_print 0.45456019043922424\n",
      "epoch:  4   step:  22   train loss:  0.4569430351257324  val loss:  0.45678648352622986\n",
      "epoch:  4   step:  23   train loss:  0.4822605550289154  val loss:  0.45884808897972107\n",
      "epoch:  4   step:  24   train loss:  0.6129556894302368  val loss:  0.4649517834186554\n",
      "epoch:  4   step:  25   train loss:  0.6401365399360657  val loss:  0.46512067317962646\n",
      "epoch:  4   step:  26   train loss:  0.369860976934433  val loss:  0.46843191981315613\n",
      "epoch:  4   step:  27   train loss:  0.3901449143886566  val loss:  0.4728049635887146\n",
      "epoch:  4   step:  28   train loss:  0.45957595109939575  val loss:  0.47585612535476685\n",
      "epoch:  4   step:  29   train loss:  0.4164958596229553  val loss:  0.47893959283828735\n",
      "epoch:  4   step:  30   train loss:  0.6457845568656921  val loss:  0.47992414236068726\n",
      "epoch:  4   step:  31   train loss:  0.45009005069732666  val loss:  0.48047754168510437\n",
      "epoch:  4   step:  32   train loss:  0.561720609664917  val loss:  0.482210636138916\n",
      "epoch:  4   step:  33   train loss:  0.5025877952575684  val loss:  0.4880213439464569\n",
      "epoch:  4   step:  34   train loss:  0.48815295100212097  val loss:  0.49336522817611694\n",
      "epoch:  4   step:  35   train loss:  0.4439384937286377  val loss:  0.49594464898109436\n",
      "epoch:  4   step:  36   train loss:  0.5381003618240356  val loss:  0.49715620279312134\n",
      "epoch:  4   step:  37   train loss:  0.5576448440551758  val loss:  0.4968092739582062\n",
      "epoch:  4   step:  38   train loss:  0.40974196791648865  val loss:  0.49626967310905457\n",
      "epoch:  4   step:  39   train loss:  0.489408940076828  val loss:  0.49514052271842957\n",
      "epoch:  4   step:  40   train loss:  0.46469205617904663  val loss:  0.4952625036239624\n",
      "epoch:  4   step:  41   train loss:  0.4767765998840332  val loss:  0.49513062834739685\n",
      "epoch:  4   step:  42   train loss:  0.4190794825553894  val loss:  0.49529218673706055\n",
      "epoch:  4   step:  43   train loss:  0.5239707231521606  val loss:  0.49273771047592163\n",
      "epoch:  4   step:  44   train loss:  0.6404443383216858  val loss:  0.49017858505249023\n",
      "epoch:  4   step:  45   train loss:  0.40678417682647705  val loss:  0.4910697937011719\n",
      "epoch:  4   step:  46   train loss:  0.45308995246887207  val loss:  0.49138343334198\n",
      "epoch:  4   step:  47   train loss:  0.4480130672454834  val loss:  0.49312326312065125\n",
      "epoch:  4   step:  48   train loss:  0.5344092845916748  val loss:  0.4955297112464905\n",
      "epoch:  4   step:  49   train loss:  0.4578193426132202  val loss:  0.49618297815322876\n",
      "epoch:  4   step:  50   train loss:  0.5038243532180786  val loss:  0.49417760968208313\n",
      "epoch:  4   step:  51   train loss:  0.5874735713005066  val loss:  0.4916263222694397\n",
      "epoch:  4   step:  52   train loss:  0.5773663520812988  val loss:  0.4893270432949066\n",
      "epoch:  4   step:  53   train loss:  0.44431161880493164  val loss:  0.48928916454315186\n",
      "epoch:  4   step:  54   train loss:  0.5567160844802856  val loss:  0.4901541769504547\n",
      "epoch:  4   step:  55   train loss:  0.5177112817764282  val loss:  0.49364614486694336\n",
      "epoch:  4   step:  56   train loss:  0.4146244525909424  val loss:  0.49781230092048645\n",
      "epoch:  4   step:  57   train loss:  0.4924774765968323  val loss:  0.4995133876800537\n",
      "epoch:  4   step:  58   train loss:  0.5353417992591858  val loss:  0.5015801191329956\n",
      "epoch:  4   step:  59   train loss:  0.5128666758537292  val loss:  0.5014496445655823\n",
      "epoch:  4   step:  60   train loss:  0.534241795539856  val loss:  0.5010932087898254\n",
      "epoch:  4   step:  61   train loss:  0.6269679069519043  val loss:  0.5007911920547485\n",
      "epoch:  4   step:  62   train loss:  0.48247581720352173  val loss:  0.5010880827903748\n",
      "epoch:  4   step:  63   train loss:  0.6572363376617432  val loss:  0.5006235837936401\n",
      "epoch:  4   step:  64   train loss:  0.5936522483825684  val loss:  0.5000852942466736\n",
      "epoch:  4   step:  65   train loss:  0.4515105187892914  val loss:  0.5003160238265991\n",
      "epoch:  4   step:  66   train loss:  0.5118420720100403  val loss:  0.4958408772945404\n",
      "epoch:  4   step:  67   train loss:  0.5481430888175964  val loss:  0.49273544549942017\n",
      "epoch:  4   step:  68   train loss:  0.45110124349594116  val loss:  0.48950251936912537\n",
      "epoch:  4   step:  69   train loss:  0.42991650104522705  val loss:  0.48582300543785095\n",
      "epoch:  4   step:  70   train loss:  0.4742257595062256  val loss:  0.4837837815284729\n",
      "epoch:  4   step:  71   train loss:  0.8171936273574829  val loss:  0.479653924703598\n",
      "epoch:  4   step:  72   train loss:  0.5910929441452026  val loss:  0.4775005280971527\n",
      "epoch:  4   step:  73   train loss:  0.4660223126411438  val loss:  0.47604355216026306\n",
      "epoch:  4   step:  74   train loss:  0.5062344074249268  val loss:  0.47614794969558716\n",
      "epoch:  4   step:  75   train loss:  0.5443335771560669  val loss:  0.47833749651908875\n",
      "epoch:  4   step:  76   train loss:  0.5686855316162109  val loss:  0.4832122027873993\n",
      "epoch:  4   step:  77   train loss:  0.4612765312194824  val loss:  0.48830878734588623\n",
      "epoch:  4   step:  78   train loss:  0.41484302282333374  val loss:  0.4928857982158661\n",
      "epoch:  4   step:  79   train loss:  0.5674874782562256  val loss:  0.4959021210670471\n",
      "epoch:  4   step:  80   train loss:  0.47978660464286804  val loss:  0.4981026351451874\n",
      "epoch:  4   step:  81   train loss:  0.596405029296875  val loss:  0.4964129626750946\n",
      "epoch:  4   step:  82   train loss:  0.5129486322402954  val loss:  0.4983876943588257\n",
      "epoch:  4   step:  83   train loss:  0.48943889141082764  val loss:  0.4966541826725006\n",
      "epoch:  4   step:  84   train loss:  0.43236207962036133  val loss:  0.49248892068862915\n",
      "epoch:  4   step:  85   train loss:  0.5273880958557129  val loss:  0.49253028631210327\n",
      "epoch:  4   step:  86   train loss:  0.5481082201004028  val loss:  0.49026817083358765\n",
      "epoch:  4   step:  87   train loss:  0.41410577297210693  val loss:  0.48737409710884094\n",
      "epoch:  4   step:  88   train loss:  0.4507632851600647  val loss:  0.4861897826194763\n",
      "epoch:  4   step:  89   train loss:  0.469160795211792  val loss:  0.4907052516937256\n",
      "epoch:  4   step:  90   train loss:  0.6002404689788818  val loss:  0.5005886554718018\n",
      "epoch:  4   step:  91   train loss:  0.7690026164054871  val loss:  0.505266547203064\n",
      "epoch:  4   step:  92   train loss:  0.6841235160827637  val loss:  0.5026676058769226\n",
      "epoch:  4   step:  93   train loss:  0.795097827911377  val loss:  0.49841275811195374\n",
      "epoch:  4   step:  94   train loss:  0.566138505935669  val loss:  0.5041242241859436\n",
      "epoch:  4   step:  95   train loss:  0.5271227359771729  val loss:  0.5148438215255737\n",
      "epoch:  4   step:  96   train loss:  0.46375077962875366  val loss:  0.524941623210907\n",
      "epoch:  4   step:  97   train loss:  0.6070365905761719  val loss:  0.5354347229003906\n",
      "epoch:  4   step:  98   train loss:  0.5248739719390869  val loss:  0.5460726022720337\n",
      "epoch:  4   step:  99   train loss:  0.5504939556121826  val loss:  0.5492473244667053\n",
      "epoch:  4   step:  100   train loss:  0.6237211227416992  val loss:  0.5426158308982849\n",
      "epoch:  4   step:  101   train loss:  0.49103882908821106  val loss:  0.5332857966423035\n",
      "epoch:  4   step:  102   train loss:  0.481079638004303  val loss:  0.5299975872039795\n",
      "epoch:  4   step:  103   train loss:  0.5459448099136353  val loss:  0.5340700149536133\n",
      "epoch:  4   step:  104   train loss:  0.5372200012207031  val loss:  0.5419428944587708\n",
      "epoch:  4   step:  105   train loss:  0.47237449884414673  val loss:  0.5496978163719177\n",
      "epoch:  4   step:  106   train loss:  0.48778584599494934  val loss:  0.5559062957763672\n",
      "epoch:  4   step:  107   train loss:  0.39690548181533813  val loss:  0.5634885430335999\n",
      "epoch:  4   step:  108   train loss:  0.6930143237113953  val loss:  0.5579407215118408\n",
      "epoch:  4   step:  109   train loss:  0.40283894538879395  val loss:  0.555175244808197\n",
      "epoch:  4   step:  110   train loss:  0.6260295510292053  val loss:  0.5428755283355713\n",
      "epoch:  4   step:  111   train loss:  0.5842524766921997  val loss:  0.5196895003318787\n",
      "epoch:  4   step:  112   train loss:  0.6214631795883179  val loss:  0.4988335371017456\n",
      "epoch:  4   step:  113   train loss:  0.535627007484436  val loss:  0.49016815423965454\n",
      "epoch:  4   step:  114   train loss:  0.43046021461486816  val loss:  0.4880290627479553\n",
      "epoch:  4   step:  115   train loss:  0.44357535243034363  val loss:  0.48744550347328186\n",
      "epoch:  4   step:  116   train loss:  0.5754479169845581  val loss:  0.49061423540115356\n",
      "epoch:  4   step:  117   train loss:  0.5999279022216797  val loss:  0.4931851029396057\n",
      "epoch:  4   step:  118   train loss:  0.4868651032447815  val loss:  0.4898514151573181\n",
      "epoch:  4   step:  119   train loss:  0.548977792263031  val loss:  0.485911101102829\n",
      "epoch:  4   step:  120   train loss:  0.4770757555961609  val loss:  0.4843679964542389\n",
      "epoch:  4   step:  121   train loss:  0.41951990127563477  val loss:  0.485944926738739\n",
      "epoch:  4   step:  122   train loss:  0.6477407217025757  val loss:  0.48972395062446594\n",
      "epoch:  4   step:  123   train loss:  0.5998919010162354  val loss:  0.4904772937297821\n",
      "epoch:  4   step:  124   train loss:  0.6437442302703857  val loss:  0.4905937910079956\n",
      "epoch:  4   step:  125   train loss:  0.4800189435482025  val loss:  0.4895859658718109\n",
      "epoch:  4   step:  126   train loss:  0.4891144335269928  val loss:  0.48901739716529846\n",
      "epoch:  4   step:  127   train loss:  0.5472097992897034  val loss:  0.48879241943359375\n",
      "epoch:  4   step:  128   train loss:  0.48309820890426636  val loss:  0.4901408553123474\n",
      "epoch:  4   step:  129   train loss:  0.6719692349433899  val loss:  0.49511846899986267\n",
      "epoch:  4   step:  130   train loss:  0.5152063965797424  val loss:  0.4999137818813324\n",
      "epoch:  4   step:  131   train loss:  0.49600398540496826  val loss:  0.5016838908195496\n",
      "epoch:  4   step:  132   train loss:  0.5732367038726807  val loss:  0.5052381753921509\n",
      "epoch:  4   step:  133   train loss:  0.532853901386261  val loss:  0.5054025650024414\n",
      "epoch:  4   step:  134   train loss:  0.4894537329673767  val loss:  0.5046606063842773\n",
      "epoch:  4   step:  135   train loss:  0.6081196069717407  val loss:  0.5063395500183105\n",
      "epoch:  4   step:  136   train loss:  0.5003105401992798  val loss:  0.5075335502624512\n",
      "epoch:  4   step:  137   train loss:  0.54713374376297  val loss:  0.5110808610916138\n",
      "epoch:  4   step:  138   train loss:  0.452393114566803  val loss:  0.5135248899459839\n",
      "epoch:  4   step:  139   train loss:  0.4654613137245178  val loss:  0.5135517716407776\n",
      "epoch:  4   step:  140   train loss:  0.5267960429191589  val loss:  0.5145263671875\n",
      "epoch:  4   step:  141   train loss:  0.6057680249214172  val loss:  0.5148561596870422\n",
      "epoch:  4   step:  142   train loss:  0.5527279376983643  val loss:  0.5130037069320679\n",
      "epoch:  4   step:  143   train loss:  0.4837459325790405  val loss:  0.5094924569129944\n",
      "epoch:  4   step:  144   train loss:  0.47909826040267944  val loss:  0.5081338286399841\n",
      "epoch:  4   step:  145   train loss:  0.549789547920227  val loss:  0.5035516023635864\n",
      "epoch:  4   step:  146   train loss:  0.49577707052230835  val loss:  0.4981253445148468\n",
      "epoch:  4   step:  147   train loss:  0.5216220617294312  val loss:  0.49318811297416687\n",
      "epoch:  4   step:  148   train loss:  0.5378267765045166  val loss:  0.49078115820884705\n",
      "epoch:  4   step:  149   train loss:  0.47039127349853516  val loss:  0.4884302020072937\n",
      "epoch:  4   step:  150   train loss:  0.5039498209953308  val loss:  0.4853726029396057\n",
      "epoch:  4   step:  151   train loss:  0.49503034353256226  val loss:  0.4813062846660614\n",
      "epoch:  4   step:  152   train loss:  0.502008855342865  val loss:  0.47639426589012146\n",
      "epoch:  4   step:  153   train loss:  0.5246030688285828  val loss:  0.47503364086151123\n",
      "epoch:  4   step:  154   train loss:  0.4119715094566345  val loss:  0.47589582204818726\n",
      "epoch:  4   step:  155   train loss:  0.4276449680328369  val loss:  0.4825756847858429\n",
      "epoch:  4   step:  156   train loss:  0.463634192943573  val loss:  0.48974281549453735\n",
      "epoch:  4   step:  157   train loss:  0.4976622760295868  val loss:  0.49560049176216125\n",
      "epoch:  4   step:  158   train loss:  0.44958043098449707  val loss:  0.498929500579834\n",
      "epoch:  4   step:  159   train loss:  0.4219163656234741  val loss:  0.5039970874786377\n",
      "epoch:  4   step:  160   train loss:  0.7423185110092163  val loss:  0.4980255663394928\n",
      "epoch:  4   step:  161   train loss:  0.5519640445709229  val loss:  0.4875554144382477\n",
      "epoch:  4   step:  162   train loss:  0.5113973617553711  val loss:  0.4746008813381195\n",
      "epoch:  4   step:  163   train loss:  0.3935084342956543  val loss:  0.4692821800708771\n",
      "epoch:  4   step:  164   train loss:  0.48974716663360596  val loss:  0.46749380230903625\n",
      "epoch:  4   step:  165   train loss:  0.8896763324737549  val loss:  0.4817894697189331\n",
      "epoch:  5   step:  0   train loss:  0.5631731152534485  val loss:  0.5040939450263977\n",
      "epoch:  5   step:  1   train loss:  0.44740384817123413  val loss:  0.5199193358421326\n",
      "epoch:  5   step:  2   train loss:  0.565045952796936  val loss:  0.5250961780548096\n",
      "epoch:  5   step:  3   train loss:  0.6484925746917725  val loss:  0.5261330008506775\n",
      "epoch:  5   step:  4   train loss:  0.7583838701248169  val loss:  0.5051612854003906\n",
      "epoch:  5   step:  5   train loss:  0.6378257274627686  val loss:  0.48664337396621704\n",
      "epoch:  5   step:  6   train loss:  0.6032084226608276  val loss:  0.4727236032485962\n",
      "epoch:  5   step:  7   train loss:  0.4588201940059662  val loss:  0.4641238749027252\n",
      "epoch:  5   step:  8   train loss:  0.4570033550262451  val loss:  0.4718707203865051\n",
      "epoch:  5   step:  9   train loss:  0.4969015121459961  val loss:  0.490122526884079\n",
      "epoch:  5   step:  10   train loss:  0.550870954990387  val loss:  0.5125064849853516\n",
      "epoch:  5   step:  11   train loss:  0.35333386063575745  val loss:  0.5356727242469788\n",
      "epoch:  5   step:  12   train loss:  0.6806173324584961  val loss:  0.5442584753036499\n",
      "epoch:  5   step:  13   train loss:  0.6832175254821777  val loss:  0.5375387668609619\n",
      "epoch:  5   step:  14   train loss:  0.8049958944320679  val loss:  0.5123600363731384\n",
      "epoch:  5   step:  15   train loss:  0.6183209419250488  val loss:  0.4890103042125702\n",
      "epoch:  5   step:  16   train loss:  0.3861216902732849  val loss:  0.4750438630580902\n",
      "epoch:  5   step:  17   train loss:  0.5337432622909546  val loss:  0.46849489212036133\n",
      "epoch:  5   step:  18   train loss:  0.5619816780090332  val loss:  0.47249072790145874\n",
      "epoch:  5   step:  19   train loss:  0.40922069549560547  val loss:  0.48045676946640015\n",
      "epoch:  5   step:  20   train loss:  0.5771598815917969  val loss:  0.4876466691493988\n",
      "epoch:  5   step:  21   train loss:  0.5491448640823364  val loss:  0.48730307817459106\n",
      "epoch:  5   step:  22   train loss:  0.5659350156784058  val loss:  0.48517149686813354\n",
      "epoch:  5   step:  23   train loss:  0.4099634885787964  val loss:  0.4789677560329437\n",
      "epoch:  5   step:  24   train loss:  0.49057427048683167  val loss:  0.4712981581687927\n",
      "epoch:  5   step:  25   train loss:  0.5583682656288147  val loss:  0.45735153555870056\n",
      "epoch:  5   step:  26   train loss:  0.548611044883728  val loss:  0.44633010029792786\n",
      "min_val_loss_print 0.44633010029792786\n",
      "epoch:  5   step:  27   train loss:  0.4446905255317688  val loss:  0.43975910544395447\n",
      "min_val_loss_print 0.43975910544395447\n",
      "epoch:  5   step:  28   train loss:  0.4129825830459595  val loss:  0.4387497305870056\n",
      "min_val_loss_print 0.4387497305870056\n",
      "epoch:  5   step:  29   train loss:  0.8447824120521545  val loss:  0.44542810320854187\n",
      "epoch:  5   step:  30   train loss:  0.4640348553657532  val loss:  0.450457900762558\n",
      "epoch:  5   step:  31   train loss:  0.4664252996444702  val loss:  0.4548443555831909\n",
      "epoch:  5   step:  32   train loss:  0.39673399925231934  val loss:  0.4563401937484741\n",
      "epoch:  5   step:  33   train loss:  0.4657433032989502  val loss:  0.45771703124046326\n",
      "epoch:  5   step:  34   train loss:  0.48743757605552673  val loss:  0.4581339955329895\n",
      "epoch:  5   step:  35   train loss:  0.66045081615448  val loss:  0.4585917592048645\n",
      "epoch:  5   step:  36   train loss:  0.4353342652320862  val loss:  0.4617472290992737\n",
      "epoch:  5   step:  37   train loss:  0.5443663597106934  val loss:  0.4674147665500641\n",
      "epoch:  5   step:  38   train loss:  0.4053286015987396  val loss:  0.47392135858535767\n",
      "epoch:  5   step:  39   train loss:  0.39074552059173584  val loss:  0.4778280556201935\n",
      "epoch:  5   step:  40   train loss:  0.508760929107666  val loss:  0.47956371307373047\n",
      "epoch:  5   step:  41   train loss:  0.5019246935844421  val loss:  0.48089152574539185\n",
      "epoch:  5   step:  42   train loss:  0.44938984513282776  val loss:  0.48187652230262756\n",
      "epoch:  5   step:  43   train loss:  0.40900298953056335  val loss:  0.4803847372531891\n",
      "epoch:  5   step:  44   train loss:  0.5389859676361084  val loss:  0.4802595376968384\n",
      "epoch:  5   step:  45   train loss:  0.6203118562698364  val loss:  0.4816840887069702\n",
      "epoch:  5   step:  46   train loss:  0.44477421045303345  val loss:  0.48254165053367615\n",
      "epoch:  5   step:  47   train loss:  0.42939722537994385  val loss:  0.4828226864337921\n",
      "epoch:  5   step:  48   train loss:  0.4579526484012604  val loss:  0.48293495178222656\n",
      "epoch:  5   step:  49   train loss:  0.4303432106971741  val loss:  0.48031049966812134\n",
      "epoch:  5   step:  50   train loss:  0.49560534954071045  val loss:  0.48078885674476624\n",
      "epoch:  5   step:  51   train loss:  0.6669535636901855  val loss:  0.47952163219451904\n",
      "epoch:  5   step:  52   train loss:  0.439192533493042  val loss:  0.48070228099823\n",
      "epoch:  5   step:  53   train loss:  0.4483349323272705  val loss:  0.48208555579185486\n",
      "epoch:  5   step:  54   train loss:  0.40612709522247314  val loss:  0.48222795128822327\n",
      "epoch:  5   step:  55   train loss:  0.5291412472724915  val loss:  0.48029205203056335\n",
      "epoch:  5   step:  56   train loss:  0.43057531118392944  val loss:  0.4767502546310425\n",
      "epoch:  5   step:  57   train loss:  0.4822949171066284  val loss:  0.47332054376602173\n",
      "epoch:  5   step:  58   train loss:  0.4902662932872772  val loss:  0.4705672860145569\n",
      "epoch:  5   step:  59   train loss:  0.6148587465286255  val loss:  0.46688011288642883\n",
      "epoch:  5   step:  60   train loss:  0.6071268320083618  val loss:  0.4620136022567749\n",
      "epoch:  5   step:  61   train loss:  0.3723352551460266  val loss:  0.4595522880554199\n",
      "epoch:  5   step:  62   train loss:  0.5875562429428101  val loss:  0.45992305874824524\n",
      "epoch:  5   step:  63   train loss:  0.4067062735557556  val loss:  0.45742300152778625\n",
      "epoch:  5   step:  64   train loss:  0.5020946264266968  val loss:  0.45745933055877686\n",
      "epoch:  5   step:  65   train loss:  0.4594617486000061  val loss:  0.46120473742485046\n",
      "epoch:  5   step:  66   train loss:  0.5816516876220703  val loss:  0.46052420139312744\n",
      "epoch:  5   step:  67   train loss:  0.4908716678619385  val loss:  0.4573625326156616\n",
      "epoch:  5   step:  68   train loss:  0.5982793569564819  val loss:  0.4533606469631195\n",
      "epoch:  5   step:  69   train loss:  0.48096585273742676  val loss:  0.45218321681022644\n",
      "epoch:  5   step:  70   train loss:  0.6629458665847778  val loss:  0.4556417465209961\n",
      "epoch:  5   step:  71   train loss:  0.5103216767311096  val loss:  0.4614903926849365\n",
      "epoch:  5   step:  72   train loss:  0.4637863337993622  val loss:  0.4661915600299835\n",
      "epoch:  5   step:  73   train loss:  0.5265591740608215  val loss:  0.4693816900253296\n",
      "epoch:  5   step:  74   train loss:  0.48928993940353394  val loss:  0.4708895683288574\n",
      "epoch:  5   step:  75   train loss:  0.5151427984237671  val loss:  0.4667935073375702\n",
      "epoch:  5   step:  76   train loss:  0.49133995175361633  val loss:  0.4661078453063965\n",
      "epoch:  5   step:  77   train loss:  0.4432549774646759  val loss:  0.4632806181907654\n",
      "epoch:  5   step:  78   train loss:  0.44085368514060974  val loss:  0.4618980288505554\n",
      "epoch:  5   step:  79   train loss:  0.5033572912216187  val loss:  0.4648723602294922\n",
      "epoch:  5   step:  80   train loss:  0.517749011516571  val loss:  0.46821901202201843\n",
      "epoch:  5   step:  81   train loss:  0.6222822666168213  val loss:  0.46647748351097107\n",
      "epoch:  5   step:  82   train loss:  0.6126738786697388  val loss:  0.4619007706642151\n",
      "epoch:  5   step:  83   train loss:  0.842064619064331  val loss:  0.4578743875026703\n",
      "epoch:  5   step:  84   train loss:  0.46080368757247925  val loss:  0.4621356725692749\n",
      "epoch:  5   step:  85   train loss:  0.5921125411987305  val loss:  0.4750751256942749\n",
      "epoch:  5   step:  86   train loss:  0.4013397991657257  val loss:  0.4863736629486084\n",
      "epoch:  5   step:  87   train loss:  0.5461331605911255  val loss:  0.49538886547088623\n",
      "epoch:  5   step:  88   train loss:  0.5218286514282227  val loss:  0.5008749961853027\n",
      "epoch:  5   step:  89   train loss:  0.5441687107086182  val loss:  0.5015561580657959\n",
      "epoch:  5   step:  90   train loss:  0.40553975105285645  val loss:  0.4979632496833801\n",
      "epoch:  5   step:  91   train loss:  0.5156475901603699  val loss:  0.4872438609600067\n",
      "epoch:  5   step:  92   train loss:  0.5296885967254639  val loss:  0.48009246587753296\n",
      "epoch:  5   step:  93   train loss:  0.43720924854278564  val loss:  0.477508544921875\n",
      "epoch:  5   step:  94   train loss:  0.5610214471817017  val loss:  0.47974592447280884\n",
      "epoch:  5   step:  95   train loss:  0.543828547000885  val loss:  0.4859616756439209\n",
      "epoch:  5   step:  96   train loss:  0.48233431577682495  val loss:  0.4917578101158142\n",
      "epoch:  5   step:  97   train loss:  0.48370975255966187  val loss:  0.49520057439804077\n",
      "epoch:  5   step:  98   train loss:  0.5095030665397644  val loss:  0.4886927008628845\n",
      "epoch:  5   step:  99   train loss:  0.5232034921646118  val loss:  0.47941911220550537\n",
      "epoch:  5   step:  100   train loss:  0.4040614366531372  val loss:  0.4734601378440857\n",
      "epoch:  5   step:  101   train loss:  0.7001964449882507  val loss:  0.46756553649902344\n",
      "epoch:  5   step:  102   train loss:  0.4312455654144287  val loss:  0.46467170119285583\n",
      "epoch:  5   step:  103   train loss:  0.44845491647720337  val loss:  0.4619700610637665\n",
      "epoch:  5   step:  104   train loss:  0.4231553077697754  val loss:  0.4598955512046814\n",
      "epoch:  5   step:  105   train loss:  0.519240140914917  val loss:  0.4578576683998108\n",
      "epoch:  5   step:  106   train loss:  0.41794925928115845  val loss:  0.4557436406612396\n",
      "epoch:  5   step:  107   train loss:  0.49188101291656494  val loss:  0.4534868001937866\n",
      "epoch:  5   step:  108   train loss:  0.48242685198783875  val loss:  0.4503112733364105\n",
      "epoch:  5   step:  109   train loss:  0.45997166633605957  val loss:  0.44743216037750244\n",
      "epoch:  5   step:  110   train loss:  0.48566579818725586  val loss:  0.4444611966609955\n",
      "epoch:  5   step:  111   train loss:  0.40280765295028687  val loss:  0.44496238231658936\n",
      "epoch:  5   step:  112   train loss:  0.4562962055206299  val loss:  0.447109580039978\n",
      "epoch:  5   step:  113   train loss:  0.5815198421478271  val loss:  0.4528139531612396\n",
      "epoch:  5   step:  114   train loss:  0.4899018406867981  val loss:  0.45410293340682983\n",
      "epoch:  5   step:  115   train loss:  0.4635929465293884  val loss:  0.45577147603034973\n",
      "epoch:  5   step:  116   train loss:  0.5233280658721924  val loss:  0.4617982506752014\n",
      "epoch:  5   step:  117   train loss:  0.31351613998413086  val loss:  0.469353049993515\n",
      "epoch:  5   step:  118   train loss:  0.34698405861854553  val loss:  0.4750005900859833\n",
      "epoch:  5   step:  119   train loss:  0.42173612117767334  val loss:  0.4839054048061371\n",
      "epoch:  5   step:  120   train loss:  0.44757091999053955  val loss:  0.49541011452674866\n",
      "epoch:  5   step:  121   train loss:  0.5496706962585449  val loss:  0.5008944869041443\n",
      "epoch:  5   step:  122   train loss:  0.366381973028183  val loss:  0.5090689063072205\n",
      "epoch:  5   step:  123   train loss:  0.3198484480381012  val loss:  0.5123838186264038\n",
      "epoch:  5   step:  124   train loss:  0.30757957696914673  val loss:  0.5172140598297119\n",
      "epoch:  5   step:  125   train loss:  0.5718835592269897  val loss:  0.510646641254425\n",
      "epoch:  5   step:  126   train loss:  0.4584401845932007  val loss:  0.5005386471748352\n",
      "epoch:  5   step:  127   train loss:  0.4562569856643677  val loss:  0.4941805601119995\n",
      "epoch:  5   step:  128   train loss:  0.48795777559280396  val loss:  0.4890260696411133\n",
      "epoch:  5   step:  129   train loss:  0.4625464677810669  val loss:  0.4866670072078705\n",
      "epoch:  5   step:  130   train loss:  0.5312753319740295  val loss:  0.4870315492153168\n",
      "epoch:  5   step:  131   train loss:  0.4866704046726227  val loss:  0.4857867956161499\n",
      "epoch:  5   step:  132   train loss:  0.5232990384101868  val loss:  0.48796913027763367\n",
      "epoch:  5   step:  133   train loss:  0.419607937335968  val loss:  0.48982757329940796\n",
      "epoch:  5   step:  134   train loss:  0.5714917778968811  val loss:  0.4891339838504791\n",
      "epoch:  5   step:  135   train loss:  0.5440770387649536  val loss:  0.4876663386821747\n",
      "epoch:  5   step:  136   train loss:  0.5850376486778259  val loss:  0.48722437024116516\n",
      "epoch:  5   step:  137   train loss:  0.4558713436126709  val loss:  0.4821964502334595\n",
      "epoch:  5   step:  138   train loss:  0.5283592939376831  val loss:  0.4749578833580017\n",
      "epoch:  5   step:  139   train loss:  0.6018848419189453  val loss:  0.4702131450176239\n",
      "epoch:  5   step:  140   train loss:  0.47114384174346924  val loss:  0.46507757902145386\n",
      "epoch:  5   step:  141   train loss:  0.420764684677124  val loss:  0.46060025691986084\n",
      "epoch:  5   step:  142   train loss:  0.3747633695602417  val loss:  0.458854615688324\n",
      "epoch:  5   step:  143   train loss:  0.4525454044342041  val loss:  0.4592822194099426\n",
      "epoch:  5   step:  144   train loss:  0.5134851932525635  val loss:  0.46223771572113037\n",
      "epoch:  5   step:  145   train loss:  0.4808746576309204  val loss:  0.4609585106372833\n",
      "epoch:  5   step:  146   train loss:  0.3256520628929138  val loss:  0.46367719769477844\n",
      "epoch:  5   step:  147   train loss:  0.35490524768829346  val loss:  0.4750020503997803\n",
      "epoch:  5   step:  148   train loss:  0.5169099569320679  val loss:  0.4724584221839905\n",
      "epoch:  5   step:  149   train loss:  0.4733084440231323  val loss:  0.46601003408432007\n",
      "epoch:  5   step:  150   train loss:  0.37745192646980286  val loss:  0.45932865142822266\n",
      "epoch:  5   step:  151   train loss:  0.6381123065948486  val loss:  0.44838353991508484\n",
      "epoch:  5   step:  152   train loss:  0.6170809268951416  val loss:  0.4373175799846649\n",
      "min_val_loss_print 0.4373175799846649\n",
      "epoch:  5   step:  153   train loss:  0.5599765777587891  val loss:  0.4334734380245209\n",
      "min_val_loss_print 0.4334734380245209\n",
      "epoch:  5   step:  154   train loss:  0.43258705735206604  val loss:  0.4349049925804138\n",
      "epoch:  5   step:  155   train loss:  0.5173590183258057  val loss:  0.441205769777298\n",
      "epoch:  5   step:  156   train loss:  0.551357626914978  val loss:  0.4493965804576874\n",
      "epoch:  5   step:  157   train loss:  0.5024685859680176  val loss:  0.4499618411064148\n",
      "epoch:  5   step:  158   train loss:  0.49062711000442505  val loss:  0.4481149911880493\n",
      "epoch:  5   step:  159   train loss:  0.4400896430015564  val loss:  0.4485953450202942\n",
      "epoch:  5   step:  160   train loss:  0.4235347509384155  val loss:  0.4528917670249939\n",
      "epoch:  5   step:  161   train loss:  0.460814893245697  val loss:  0.464233934879303\n",
      "epoch:  5   step:  162   train loss:  0.42164963483810425  val loss:  0.4810350835323334\n",
      "epoch:  5   step:  163   train loss:  0.4595988988876343  val loss:  0.49849647283554077\n",
      "epoch:  5   step:  164   train loss:  0.4991031587123871  val loss:  0.5092200636863708\n",
      "epoch:  5   step:  165   train loss:  0.5261536836624146  val loss:  0.5148923993110657\n",
      "epoch:  6   step:  0   train loss:  0.3168611526489258  val loss:  0.5194039940834045\n",
      "epoch:  6   step:  1   train loss:  0.4270874261856079  val loss:  0.5171919465065002\n",
      "epoch:  6   step:  2   train loss:  0.42532360553741455  val loss:  0.5090256333351135\n",
      "epoch:  6   step:  3   train loss:  0.4130258560180664  val loss:  0.4963301122188568\n",
      "epoch:  6   step:  4   train loss:  0.590035617351532  val loss:  0.48183226585388184\n",
      "epoch:  6   step:  5   train loss:  0.7110199928283691  val loss:  0.4672400653362274\n",
      "epoch:  6   step:  6   train loss:  0.4808311462402344  val loss:  0.46666234731674194\n",
      "epoch:  6   step:  7   train loss:  0.4548484683036804  val loss:  0.47465720772743225\n",
      "epoch:  6   step:  8   train loss:  0.558465838432312  val loss:  0.4774462878704071\n",
      "epoch:  6   step:  9   train loss:  0.3979385495185852  val loss:  0.4705325663089752\n",
      "epoch:  6   step:  10   train loss:  0.5074952840805054  val loss:  0.46969932317733765\n",
      "epoch:  6   step:  11   train loss:  0.4605759382247925  val loss:  0.462213933467865\n",
      "epoch:  6   step:  12   train loss:  0.41265276074409485  val loss:  0.45586204528808594\n",
      "epoch:  6   step:  13   train loss:  0.40500396490097046  val loss:  0.4507230222225189\n",
      "epoch:  6   step:  14   train loss:  0.3725440204143524  val loss:  0.4496837258338928\n",
      "epoch:  6   step:  15   train loss:  0.40800410509109497  val loss:  0.4547022581100464\n",
      "epoch:  6   step:  16   train loss:  0.4633829593658447  val loss:  0.4614817798137665\n",
      "epoch:  6   step:  17   train loss:  0.5362178087234497  val loss:  0.46067091822624207\n",
      "epoch:  6   step:  18   train loss:  0.4528951048851013  val loss:  0.45918989181518555\n",
      "epoch:  6   step:  19   train loss:  0.4343527555465698  val loss:  0.45166274905204773\n",
      "epoch:  6   step:  20   train loss:  0.5390783548355103  val loss:  0.4470535218715668\n",
      "epoch:  6   step:  21   train loss:  0.4325166642665863  val loss:  0.45110225677490234\n",
      "epoch:  6   step:  22   train loss:  0.38144034147262573  val loss:  0.45880934596061707\n",
      "epoch:  6   step:  23   train loss:  0.43750184774398804  val loss:  0.4651208519935608\n",
      "epoch:  6   step:  24   train loss:  0.4062032699584961  val loss:  0.4721471965312958\n",
      "epoch:  6   step:  25   train loss:  0.4477245807647705  val loss:  0.4762342870235443\n",
      "epoch:  6   step:  26   train loss:  0.4369705021381378  val loss:  0.4859525263309479\n",
      "epoch:  6   step:  27   train loss:  0.35963183641433716  val loss:  0.5126920938491821\n",
      "epoch:  6   step:  28   train loss:  0.5286875367164612  val loss:  0.48661407828330994\n",
      "epoch:  6   step:  29   train loss:  0.3750890791416168  val loss:  0.46375104784965515\n",
      "epoch:  6   step:  30   train loss:  0.5313107967376709  val loss:  0.46132704615592957\n",
      "epoch:  6   step:  31   train loss:  0.5470204949378967  val loss:  0.45609673857688904\n",
      "epoch:  6   step:  32   train loss:  0.5279673933982849  val loss:  0.45106711983680725\n",
      "epoch:  6   step:  33   train loss:  0.5076828598976135  val loss:  0.45051002502441406\n",
      "epoch:  6   step:  34   train loss:  0.38640737533569336  val loss:  0.4519268572330475\n",
      "epoch:  6   step:  35   train loss:  0.5268138647079468  val loss:  0.45523297786712646\n",
      "epoch:  6   step:  36   train loss:  0.5416690707206726  val loss:  0.45738485455513\n",
      "epoch:  6   step:  37   train loss:  0.43977153301239014  val loss:  0.4562471807003021\n",
      "epoch:  6   step:  38   train loss:  0.4482615292072296  val loss:  0.45026397705078125\n",
      "epoch:  6   step:  39   train loss:  0.4392898976802826  val loss:  0.4452330768108368\n",
      "epoch:  6   step:  40   train loss:  0.42734766006469727  val loss:  0.44434425234794617\n",
      "epoch:  6   step:  41   train loss:  0.3967195749282837  val loss:  0.4493127465248108\n",
      "epoch:  6   step:  42   train loss:  0.4942776560783386  val loss:  0.4623351991176605\n",
      "epoch:  6   step:  43   train loss:  0.576020359992981  val loss:  0.4722401201725006\n",
      "epoch:  6   step:  44   train loss:  0.3834955096244812  val loss:  0.4817562699317932\n",
      "epoch:  6   step:  45   train loss:  0.3772840201854706  val loss:  0.4866439402103424\n",
      "epoch:  6   step:  46   train loss:  0.4675970673561096  val loss:  0.4834989011287689\n",
      "epoch:  6   step:  47   train loss:  0.5225132703781128  val loss:  0.47170835733413696\n",
      "epoch:  6   step:  48   train loss:  0.3880280554294586  val loss:  0.4639330804347992\n",
      "epoch:  6   step:  49   train loss:  0.3703377842903137  val loss:  0.4634418487548828\n",
      "epoch:  6   step:  50   train loss:  0.4529535472393036  val loss:  0.4675865173339844\n",
      "epoch:  6   step:  51   train loss:  0.6103358268737793  val loss:  0.4693361222743988\n",
      "epoch:  6   step:  52   train loss:  0.450481116771698  val loss:  0.4645622968673706\n",
      "epoch:  6   step:  53   train loss:  0.5688865780830383  val loss:  0.4710560441017151\n",
      "epoch:  6   step:  54   train loss:  0.4581422209739685  val loss:  0.47354036569595337\n",
      "epoch:  6   step:  55   train loss:  0.42195773124694824  val loss:  0.47420355677604675\n",
      "epoch:  6   step:  56   train loss:  0.6224926710128784  val loss:  0.4734216630458832\n",
      "epoch:  6   step:  57   train loss:  0.3650723099708557  val loss:  0.47062501311302185\n",
      "epoch:  6   step:  58   train loss:  0.4945757985115051  val loss:  0.46921098232269287\n",
      "epoch:  6   step:  59   train loss:  0.4686797857284546  val loss:  0.46766218543052673\n",
      "epoch:  6   step:  60   train loss:  0.4645206034183502  val loss:  0.46891674399375916\n",
      "epoch:  6   step:  61   train loss:  0.38499313592910767  val loss:  0.473957896232605\n",
      "epoch:  6   step:  62   train loss:  0.4388602077960968  val loss:  0.4872107207775116\n",
      "epoch:  6   step:  63   train loss:  0.4102744162082672  val loss:  0.5002923607826233\n",
      "epoch:  6   step:  64   train loss:  0.5047363042831421  val loss:  0.49972936511039734\n",
      "epoch:  6   step:  65   train loss:  0.3828180134296417  val loss:  0.4968947768211365\n",
      "epoch:  6   step:  66   train loss:  0.4646275043487549  val loss:  0.485471248626709\n",
      "epoch:  6   step:  67   train loss:  0.45888638496398926  val loss:  0.4790278971195221\n",
      "epoch:  6   step:  68   train loss:  0.34859856963157654  val loss:  0.47592893242836\n",
      "epoch:  6   step:  69   train loss:  0.5777658224105835  val loss:  0.47365790605545044\n",
      "epoch:  6   step:  70   train loss:  0.44389238953590393  val loss:  0.46958282589912415\n",
      "epoch:  6   step:  71   train loss:  0.37931931018829346  val loss:  0.4670948386192322\n",
      "epoch:  6   step:  72   train loss:  0.38021641969680786  val loss:  0.4657963514328003\n",
      "epoch:  6   step:  73   train loss:  0.42405539751052856  val loss:  0.46434444189071655\n",
      "epoch:  6   step:  74   train loss:  0.590187668800354  val loss:  0.4593333601951599\n",
      "epoch:  6   step:  75   train loss:  0.48505157232284546  val loss:  0.45623910427093506\n",
      "epoch:  6   step:  76   train loss:  0.42323243618011475  val loss:  0.45122015476226807\n",
      "epoch:  6   step:  77   train loss:  0.4113629460334778  val loss:  0.44045427441596985\n",
      "epoch:  6   step:  78   train loss:  0.41410428285598755  val loss:  0.430327445268631\n",
      "min_val_loss_print 0.430327445268631\n",
      "epoch:  6   step:  79   train loss:  0.42020198702812195  val loss:  0.4252593219280243\n",
      "min_val_loss_print 0.4252593219280243\n",
      "epoch:  6   step:  80   train loss:  0.38598549365997314  val loss:  0.42433610558509827\n",
      "min_val_loss_print 0.42433610558509827\n",
      "epoch:  6   step:  81   train loss:  0.4258030354976654  val loss:  0.42509791254997253\n",
      "epoch:  6   step:  82   train loss:  0.4545457363128662  val loss:  0.4322364032268524\n",
      "epoch:  6   step:  83   train loss:  0.456764280796051  val loss:  0.437588095664978\n",
      "epoch:  6   step:  84   train loss:  0.37244734168052673  val loss:  0.43674564361572266\n",
      "epoch:  6   step:  85   train loss:  0.46185219287872314  val loss:  0.430494099855423\n",
      "epoch:  6   step:  86   train loss:  0.3771805763244629  val loss:  0.4246087074279785\n",
      "epoch:  6   step:  87   train loss:  0.48850440979003906  val loss:  0.41977110505104065\n",
      "min_val_loss_print 0.41977110505104065\n",
      "epoch:  6   step:  88   train loss:  0.3143869638442993  val loss:  0.4191412329673767\n",
      "min_val_loss_print 0.4191412329673767\n",
      "epoch:  6   step:  89   train loss:  0.5229855179786682  val loss:  0.41563642024993896\n",
      "min_val_loss_print 0.41563642024993896\n",
      "epoch:  6   step:  90   train loss:  0.38287094235420227  val loss:  0.41934889554977417\n",
      "epoch:  6   step:  91   train loss:  0.42806655168533325  val loss:  0.42107176780700684\n",
      "epoch:  6   step:  92   train loss:  0.4475521743297577  val loss:  0.42307212948799133\n",
      "epoch:  6   step:  93   train loss:  0.4037517309188843  val loss:  0.4245644211769104\n",
      "epoch:  6   step:  94   train loss:  0.42877882719039917  val loss:  0.4228266775608063\n",
      "epoch:  6   step:  95   train loss:  0.38245463371276855  val loss:  0.4267613887786865\n",
      "epoch:  6   step:  96   train loss:  0.3276287019252777  val loss:  0.4330664873123169\n",
      "epoch:  6   step:  97   train loss:  0.3371645510196686  val loss:  0.44557538628578186\n",
      "epoch:  6   step:  98   train loss:  0.5005946159362793  val loss:  0.45487943291664124\n",
      "epoch:  6   step:  99   train loss:  0.40681391954421997  val loss:  0.46316275000572205\n",
      "epoch:  6   step:  100   train loss:  0.6977375745773315  val loss:  0.44902271032333374\n",
      "epoch:  6   step:  101   train loss:  0.456329882144928  val loss:  0.43373626470565796\n",
      "epoch:  6   step:  102   train loss:  0.5176770091056824  val loss:  0.4201200306415558\n",
      "epoch:  6   step:  103   train loss:  0.39371931552886963  val loss:  0.4198850393295288\n",
      "epoch:  6   step:  104   train loss:  0.43321266770362854  val loss:  0.4305361211299896\n",
      "epoch:  6   step:  105   train loss:  0.4841192960739136  val loss:  0.4410964250564575\n",
      "epoch:  6   step:  106   train loss:  0.46105727553367615  val loss:  0.4479856491088867\n",
      "epoch:  6   step:  107   train loss:  0.48040708899497986  val loss:  0.45541125535964966\n",
      "epoch:  6   step:  108   train loss:  0.436323344707489  val loss:  0.46205493807792664\n",
      "epoch:  6   step:  109   train loss:  0.3784683346748352  val loss:  0.46182215213775635\n",
      "epoch:  6   step:  110   train loss:  0.44357770681381226  val loss:  0.4540816843509674\n",
      "epoch:  6   step:  111   train loss:  0.5335158109664917  val loss:  0.4491335153579712\n",
      "epoch:  6   step:  112   train loss:  0.43416374921798706  val loss:  0.44101783633232117\n",
      "epoch:  6   step:  113   train loss:  0.3825233578681946  val loss:  0.4397915303707123\n",
      "epoch:  6   step:  114   train loss:  0.40892356634140015  val loss:  0.45139822363853455\n",
      "epoch:  6   step:  115   train loss:  0.4957650303840637  val loss:  0.4607470631599426\n",
      "epoch:  6   step:  116   train loss:  0.2997339963912964  val loss:  0.4698585271835327\n",
      "epoch:  6   step:  117   train loss:  0.42002272605895996  val loss:  0.477973997592926\n",
      "epoch:  6   step:  118   train loss:  0.5802474617958069  val loss:  0.46370887756347656\n",
      "epoch:  6   step:  119   train loss:  0.3806349039077759  val loss:  0.4389793276786804\n",
      "epoch:  6   step:  120   train loss:  0.42174649238586426  val loss:  0.4137740135192871\n",
      "min_val_loss_print 0.4137740135192871\n",
      "epoch:  6   step:  121   train loss:  0.42868930101394653  val loss:  0.40206465125083923\n",
      "min_val_loss_print 0.40206465125083923\n",
      "epoch:  6   step:  122   train loss:  0.44358858466148376  val loss:  0.3992712199687958\n",
      "min_val_loss_print 0.3992712199687958\n",
      "epoch:  6   step:  123   train loss:  0.4306509494781494  val loss:  0.40494590997695923\n",
      "epoch:  6   step:  124   train loss:  0.44857528805732727  val loss:  0.41611167788505554\n",
      "epoch:  6   step:  125   train loss:  0.4287125766277313  val loss:  0.4244954288005829\n",
      "epoch:  6   step:  126   train loss:  0.45142728090286255  val loss:  0.4338673949241638\n",
      "epoch:  6   step:  127   train loss:  0.4760575592517853  val loss:  0.43894386291503906\n",
      "epoch:  6   step:  128   train loss:  0.4365121126174927  val loss:  0.4372372329235077\n",
      "epoch:  6   step:  129   train loss:  0.43785300850868225  val loss:  0.4279138743877411\n",
      "epoch:  6   step:  130   train loss:  0.4632661044597626  val loss:  0.4158819615840912\n",
      "epoch:  6   step:  131   train loss:  0.3383824825286865  val loss:  0.4035201966762543\n",
      "epoch:  6   step:  132   train loss:  0.4428778290748596  val loss:  0.39744865894317627\n",
      "min_val_loss_print 0.39744865894317627\n",
      "epoch:  6   step:  133   train loss:  0.41214826703071594  val loss:  0.39465469121932983\n",
      "min_val_loss_print 0.39465469121932983\n",
      "epoch:  6   step:  134   train loss:  0.42102664709091187  val loss:  0.39479535818099976\n",
      "epoch:  6   step:  135   train loss:  0.3954576849937439  val loss:  0.3950088322162628\n",
      "epoch:  6   step:  136   train loss:  0.3426416516304016  val loss:  0.39963066577911377\n",
      "epoch:  6   step:  137   train loss:  0.42780041694641113  val loss:  0.40793150663375854\n",
      "epoch:  6   step:  138   train loss:  0.4542965590953827  val loss:  0.40948453545570374\n",
      "epoch:  6   step:  139   train loss:  0.35661786794662476  val loss:  0.4087091088294983\n",
      "epoch:  6   step:  140   train loss:  0.4513762593269348  val loss:  0.39858579635620117\n",
      "epoch:  6   step:  141   train loss:  0.41052424907684326  val loss:  0.38400644063949585\n",
      "min_val_loss_print 0.38400644063949585\n",
      "epoch:  6   step:  142   train loss:  0.46892744302749634  val loss:  0.3781329393386841\n",
      "min_val_loss_print 0.3781329393386841\n",
      "epoch:  6   step:  143   train loss:  0.49301743507385254  val loss:  0.38425520062446594\n",
      "epoch:  6   step:  144   train loss:  0.5155221223831177  val loss:  0.40582698583602905\n",
      "epoch:  6   step:  145   train loss:  0.41735604405403137  val loss:  0.4303654432296753\n",
      "epoch:  6   step:  146   train loss:  0.4026026427745819  val loss:  0.4448011815547943\n",
      "epoch:  6   step:  147   train loss:  0.474687784910202  val loss:  0.4534200131893158\n",
      "epoch:  6   step:  148   train loss:  0.48999667167663574  val loss:  0.463074266910553\n",
      "epoch:  6   step:  149   train loss:  0.4222908318042755  val loss:  0.46292540431022644\n",
      "epoch:  6   step:  150   train loss:  0.504417896270752  val loss:  0.4470922350883484\n",
      "epoch:  6   step:  151   train loss:  0.40235382318496704  val loss:  0.43216297030448914\n",
      "epoch:  6   step:  152   train loss:  0.3851969540119171  val loss:  0.4170188307762146\n",
      "epoch:  6   step:  153   train loss:  0.36363649368286133  val loss:  0.4016738831996918\n",
      "epoch:  6   step:  154   train loss:  0.373775452375412  val loss:  0.3900105953216553\n",
      "epoch:  6   step:  155   train loss:  0.41843605041503906  val loss:  0.3835891783237457\n",
      "epoch:  6   step:  156   train loss:  0.4911033511161804  val loss:  0.385299950838089\n",
      "epoch:  6   step:  157   train loss:  0.2703478932380676  val loss:  0.39369386434555054\n",
      "epoch:  6   step:  158   train loss:  0.4793856143951416  val loss:  0.3954242765903473\n",
      "epoch:  6   step:  159   train loss:  0.3686375617980957  val loss:  0.3927072584629059\n",
      "epoch:  6   step:  160   train loss:  0.4087062180042267  val loss:  0.3855859637260437\n",
      "epoch:  6   step:  161   train loss:  0.28018897771835327  val loss:  0.37483495473861694\n",
      "min_val_loss_print 0.37483495473861694\n",
      "epoch:  6   step:  162   train loss:  0.5607588291168213  val loss:  0.3660038709640503\n",
      "min_val_loss_print 0.3660038709640503\n",
      "epoch:  6   step:  163   train loss:  0.2602461874485016  val loss:  0.37150654196739197\n",
      "epoch:  6   step:  164   train loss:  0.340719997882843  val loss:  0.3811849057674408\n",
      "epoch:  6   step:  165   train loss:  0.23443472385406494  val loss:  0.4001685380935669\n",
      "epoch:  7   step:  0   train loss:  0.36641210317611694  val loss:  0.4227491617202759\n",
      "epoch:  7   step:  1   train loss:  0.32065868377685547  val loss:  0.43736732006073\n",
      "epoch:  7   step:  2   train loss:  0.4191628098487854  val loss:  0.4405634105205536\n",
      "epoch:  7   step:  3   train loss:  0.42101627588272095  val loss:  0.4335773289203644\n",
      "epoch:  7   step:  4   train loss:  0.4607808589935303  val loss:  0.4214305281639099\n",
      "epoch:  7   step:  5   train loss:  0.32841408252716064  val loss:  0.40577617287635803\n",
      "epoch:  7   step:  6   train loss:  0.4206799864768982  val loss:  0.39079055190086365\n",
      "epoch:  7   step:  7   train loss:  0.35112035274505615  val loss:  0.38181740045547485\n",
      "epoch:  7   step:  8   train loss:  0.3878650963306427  val loss:  0.37856435775756836\n",
      "epoch:  7   step:  9   train loss:  0.2972901165485382  val loss:  0.37385764718055725\n",
      "epoch:  7   step:  10   train loss:  0.3954923748970032  val loss:  0.38167041540145874\n",
      "epoch:  7   step:  11   train loss:  0.3686419129371643  val loss:  0.3851148784160614\n",
      "epoch:  7   step:  12   train loss:  0.3536621332168579  val loss:  0.39279502630233765\n",
      "epoch:  7   step:  13   train loss:  0.44441384077072144  val loss:  0.3967297375202179\n",
      "epoch:  7   step:  14   train loss:  0.3620971441268921  val loss:  0.3931378722190857\n",
      "epoch:  7   step:  15   train loss:  0.43654027581214905  val loss:  0.3845141530036926\n",
      "epoch:  7   step:  16   train loss:  0.36167725920677185  val loss:  0.385366290807724\n",
      "epoch:  7   step:  17   train loss:  0.46907010674476624  val loss:  0.38514867424964905\n",
      "epoch:  7   step:  18   train loss:  0.48425590991973877  val loss:  0.3828427791595459\n",
      "epoch:  7   step:  19   train loss:  0.3769266903400421  val loss:  0.3784011900424957\n",
      "epoch:  7   step:  20   train loss:  0.48484477400779724  val loss:  0.36908966302871704\n",
      "epoch:  7   step:  21   train loss:  0.30777043104171753  val loss:  0.374969482421875\n",
      "epoch:  7   step:  22   train loss:  0.2936996817588806  val loss:  0.38021835684776306\n",
      "epoch:  7   step:  23   train loss:  0.4061412215232849  val loss:  0.3851999342441559\n",
      "epoch:  7   step:  24   train loss:  0.4225728511810303  val loss:  0.38134902715682983\n",
      "epoch:  7   step:  25   train loss:  0.5552845597267151  val loss:  0.3648814558982849\n",
      "min_val_loss_print 0.3648814558982849\n",
      "epoch:  7   step:  26   train loss:  0.36762428283691406  val loss:  0.3553411364555359\n",
      "min_val_loss_print 0.3553411364555359\n",
      "epoch:  7   step:  27   train loss:  0.3566752076148987  val loss:  0.3545321524143219\n",
      "min_val_loss_print 0.3545321524143219\n",
      "epoch:  7   step:  28   train loss:  0.4264153838157654  val loss:  0.3595024645328522\n",
      "epoch:  7   step:  29   train loss:  0.32511454820632935  val loss:  0.3666783571243286\n",
      "epoch:  7   step:  30   train loss:  0.2776066064834595  val loss:  0.3775249123573303\n",
      "epoch:  7   step:  31   train loss:  0.4448609948158264  val loss:  0.37865591049194336\n",
      "epoch:  7   step:  32   train loss:  0.3906117081642151  val loss:  0.3716883659362793\n",
      "epoch:  7   step:  33   train loss:  0.34414032101631165  val loss:  0.36259010434150696\n",
      "epoch:  7   step:  34   train loss:  0.37361979484558105  val loss:  0.3590428829193115\n",
      "epoch:  7   step:  35   train loss:  0.38655251264572144  val loss:  0.3625389337539673\n",
      "epoch:  7   step:  36   train loss:  0.3198756277561188  val loss:  0.3679051101207733\n",
      "epoch:  7   step:  37   train loss:  0.43398767709732056  val loss:  0.3725965917110443\n",
      "epoch:  7   step:  38   train loss:  0.31960541009902954  val loss:  0.3774561882019043\n",
      "epoch:  7   step:  39   train loss:  0.45183050632476807  val loss:  0.3799597918987274\n",
      "epoch:  7   step:  40   train loss:  0.45314598083496094  val loss:  0.3830108642578125\n",
      "epoch:  7   step:  41   train loss:  0.3619181215763092  val loss:  0.3767576515674591\n",
      "epoch:  7   step:  42   train loss:  0.34756171703338623  val loss:  0.3726331889629364\n",
      "epoch:  7   step:  43   train loss:  0.38564378023147583  val loss:  0.3719462454319\n",
      "epoch:  7   step:  44   train loss:  0.4117963910102844  val loss:  0.3714210093021393\n",
      "epoch:  7   step:  45   train loss:  0.40194177627563477  val loss:  0.3711509108543396\n",
      "epoch:  7   step:  46   train loss:  0.34913262724876404  val loss:  0.3717309534549713\n",
      "epoch:  7   step:  47   train loss:  0.3271309435367584  val loss:  0.3730209171772003\n",
      "epoch:  7   step:  48   train loss:  0.3649553060531616  val loss:  0.37240684032440186\n",
      "epoch:  7   step:  49   train loss:  0.20965412259101868  val loss:  0.3775898516178131\n",
      "epoch:  7   step:  50   train loss:  0.3757461905479431  val loss:  0.37334340810775757\n",
      "epoch:  7   step:  51   train loss:  0.5616111755371094  val loss:  0.36711814999580383\n",
      "epoch:  7   step:  52   train loss:  0.285582959651947  val loss:  0.3653026223182678\n",
      "epoch:  7   step:  53   train loss:  0.3196466863155365  val loss:  0.36474958062171936\n",
      "epoch:  7   step:  54   train loss:  0.2789592146873474  val loss:  0.3660403788089752\n",
      "epoch:  7   step:  55   train loss:  0.32124781608581543  val loss:  0.36571747064590454\n",
      "epoch:  7   step:  56   train loss:  0.3126567006111145  val loss:  0.364095002412796\n",
      "epoch:  7   step:  57   train loss:  0.41206690669059753  val loss:  0.3635956346988678\n",
      "epoch:  7   step:  58   train loss:  0.3502449691295624  val loss:  0.36106935143470764\n",
      "epoch:  7   step:  59   train loss:  0.3052390217781067  val loss:  0.3605354130268097\n",
      "epoch:  7   step:  60   train loss:  0.3370974063873291  val loss:  0.35662055015563965\n",
      "epoch:  7   step:  61   train loss:  0.27693164348602295  val loss:  0.3532179594039917\n",
      "min_val_loss_print 0.3532179594039917\n",
      "epoch:  7   step:  62   train loss:  0.3578002154827118  val loss:  0.35057559609413147\n",
      "min_val_loss_print 0.35057559609413147\n",
      "epoch:  7   step:  63   train loss:  0.3616803288459778  val loss:  0.35053253173828125\n",
      "min_val_loss_print 0.35053253173828125\n",
      "epoch:  7   step:  64   train loss:  0.31897616386413574  val loss:  0.35249167680740356\n",
      "epoch:  7   step:  65   train loss:  0.37415367364883423  val loss:  0.3554380238056183\n",
      "epoch:  7   step:  66   train loss:  0.3210274577140808  val loss:  0.35710594058036804\n",
      "epoch:  7   step:  67   train loss:  0.31786906719207764  val loss:  0.3605830669403076\n",
      "epoch:  7   step:  68   train loss:  0.3904907703399658  val loss:  0.3699595034122467\n",
      "epoch:  7   step:  69   train loss:  0.3380618393421173  val loss:  0.3781394064426422\n",
      "epoch:  7   step:  70   train loss:  0.38131147623062134  val loss:  0.37990984320640564\n",
      "epoch:  7   step:  71   train loss:  0.28995323181152344  val loss:  0.37828534841537476\n",
      "epoch:  7   step:  72   train loss:  0.28531593084335327  val loss:  0.376852422952652\n",
      "epoch:  7   step:  73   train loss:  0.25958555936813354  val loss:  0.37386611104011536\n",
      "epoch:  7   step:  74   train loss:  0.35350263118743896  val loss:  0.3704977333545685\n",
      "epoch:  7   step:  75   train loss:  0.3765729069709778  val loss:  0.37071460485458374\n",
      "epoch:  7   step:  76   train loss:  0.41831064224243164  val loss:  0.3711124360561371\n",
      "epoch:  7   step:  77   train loss:  0.34731513261795044  val loss:  0.3715883791446686\n",
      "epoch:  7   step:  78   train loss:  0.3993416130542755  val loss:  0.3661225736141205\n",
      "epoch:  7   step:  79   train loss:  0.3045862317085266  val loss:  0.3667356073856354\n",
      "epoch:  7   step:  80   train loss:  0.3227718770503998  val loss:  0.3718867003917694\n",
      "epoch:  7   step:  81   train loss:  0.2563301920890808  val loss:  0.3775143027305603\n",
      "epoch:  7   step:  82   train loss:  0.35698026418685913  val loss:  0.3756830394268036\n",
      "epoch:  7   step:  83   train loss:  0.2931163012981415  val loss:  0.3682214319705963\n",
      "epoch:  7   step:  84   train loss:  0.27156156301498413  val loss:  0.35255110263824463\n",
      "epoch:  7   step:  85   train loss:  0.29687854647636414  val loss:  0.34718048572540283\n",
      "min_val_loss_print 0.34718048572540283\n",
      "epoch:  7   step:  86   train loss:  0.3786756992340088  val loss:  0.3365391492843628\n",
      "min_val_loss_print 0.3365391492843628\n",
      "epoch:  7   step:  87   train loss:  0.2960021495819092  val loss:  0.3300885558128357\n",
      "min_val_loss_print 0.3300885558128357\n",
      "epoch:  7   step:  88   train loss:  0.307243287563324  val loss:  0.32139429450035095\n",
      "min_val_loss_print 0.32139429450035095\n",
      "epoch:  7   step:  89   train loss:  0.3204152584075928  val loss:  0.3142780363559723\n",
      "min_val_loss_print 0.3142780363559723\n",
      "epoch:  7   step:  90   train loss:  0.23168930411338806  val loss:  0.3096625506877899\n",
      "min_val_loss_print 0.3096625506877899\n",
      "epoch:  7   step:  91   train loss:  0.3199985921382904  val loss:  0.30814674496650696\n",
      "min_val_loss_print 0.30814674496650696\n",
      "epoch:  7   step:  92   train loss:  0.28161701560020447  val loss:  0.3108593821525574\n",
      "epoch:  7   step:  93   train loss:  0.2468450367450714  val loss:  0.3124150037765503\n",
      "epoch:  7   step:  94   train loss:  0.2417873740196228  val loss:  0.31068944931030273\n",
      "epoch:  7   step:  95   train loss:  0.37646013498306274  val loss:  0.31135308742523193\n",
      "epoch:  7   step:  96   train loss:  0.30286094546318054  val loss:  0.3149688243865967\n",
      "epoch:  7   step:  97   train loss:  0.32022392749786377  val loss:  0.31667810678482056\n",
      "epoch:  7   step:  98   train loss:  0.3007766604423523  val loss:  0.31462952494621277\n",
      "epoch:  7   step:  99   train loss:  0.2782335877418518  val loss:  0.3136264681816101\n",
      "epoch:  7   step:  100   train loss:  0.346085786819458  val loss:  0.3143221139907837\n",
      "epoch:  7   step:  101   train loss:  0.3214973211288452  val loss:  0.31505316495895386\n",
      "epoch:  7   step:  102   train loss:  0.29636311531066895  val loss:  0.3176288604736328\n",
      "epoch:  7   step:  103   train loss:  0.16694171726703644  val loss:  0.32114699482917786\n",
      "epoch:  7   step:  104   train loss:  0.299921452999115  val loss:  0.3238522410392761\n",
      "epoch:  7   step:  105   train loss:  0.33164215087890625  val loss:  0.32337692379951477\n",
      "epoch:  7   step:  106   train loss:  0.2013644576072693  val loss:  0.3251016438007355\n",
      "epoch:  7   step:  107   train loss:  0.29693403840065  val loss:  0.3266516923904419\n",
      "epoch:  7   step:  108   train loss:  0.1814914345741272  val loss:  0.3296946585178375\n",
      "epoch:  7   step:  109   train loss:  0.26591166853904724  val loss:  0.3298567235469818\n",
      "epoch:  7   step:  110   train loss:  0.19812053442001343  val loss:  0.3302467167377472\n",
      "epoch:  7   step:  111   train loss:  0.41739243268966675  val loss:  0.33045700192451477\n",
      "epoch:  7   step:  112   train loss:  0.3712199330329895  val loss:  0.3309556543827057\n",
      "epoch:  7   step:  113   train loss:  0.34753847122192383  val loss:  0.33806243538856506\n",
      "epoch:  7   step:  114   train loss:  0.3430371880531311  val loss:  0.3473866581916809\n",
      "epoch:  7   step:  115   train loss:  0.2773151993751526  val loss:  0.349339097738266\n",
      "epoch:  7   step:  116   train loss:  0.38887402415275574  val loss:  0.34388136863708496\n",
      "epoch:  7   step:  117   train loss:  0.27862682938575745  val loss:  0.33511391282081604\n",
      "epoch:  7   step:  118   train loss:  0.2598778009414673  val loss:  0.33319908380508423\n",
      "epoch:  7   step:  119   train loss:  0.3401415944099426  val loss:  0.3394787609577179\n",
      "epoch:  7   step:  120   train loss:  0.19919836521148682  val loss:  0.35678961873054504\n",
      "epoch:  7   step:  121   train loss:  0.3516961932182312  val loss:  0.36476850509643555\n",
      "epoch:  7   step:  122   train loss:  0.29591619968414307  val loss:  0.3565249443054199\n",
      "epoch:  7   step:  123   train loss:  0.32921186089515686  val loss:  0.35136786103248596\n",
      "epoch:  7   step:  124   train loss:  0.35441356897354126  val loss:  0.34573015570640564\n",
      "epoch:  7   step:  125   train loss:  0.3139021396636963  val loss:  0.34731826186180115\n",
      "epoch:  7   step:  126   train loss:  0.35790497064590454  val loss:  0.3534606397151947\n",
      "epoch:  7   step:  127   train loss:  0.32133612036705017  val loss:  0.3565414249897003\n",
      "epoch:  7   step:  128   train loss:  0.4298432469367981  val loss:  0.3580537438392639\n",
      "epoch:  7   step:  129   train loss:  0.34073150157928467  val loss:  0.355142742395401\n",
      "epoch:  7   step:  130   train loss:  0.31737425923347473  val loss:  0.3492905795574188\n",
      "epoch:  7   step:  131   train loss:  0.27041155099868774  val loss:  0.3396598994731903\n",
      "epoch:  7   step:  132   train loss:  0.2528719902038574  val loss:  0.336398720741272\n",
      "epoch:  7   step:  133   train loss:  0.2364029884338379  val loss:  0.3320055305957794\n",
      "epoch:  7   step:  134   train loss:  0.3882611393928528  val loss:  0.32212013006210327\n",
      "epoch:  7   step:  135   train loss:  0.3809901475906372  val loss:  0.315160870552063\n",
      "epoch:  7   step:  136   train loss:  0.33833104372024536  val loss:  0.31281906366348267\n",
      "epoch:  7   step:  137   train loss:  0.2959674298763275  val loss:  0.31556835770606995\n",
      "epoch:  7   step:  138   train loss:  0.3479940891265869  val loss:  0.31308504939079285\n",
      "epoch:  7   step:  139   train loss:  0.31304284930229187  val loss:  0.3124559223651886\n",
      "epoch:  7   step:  140   train loss:  0.29830798506736755  val loss:  0.3139576017856598\n",
      "epoch:  7   step:  141   train loss:  0.24554318189620972  val loss:  0.3193708062171936\n",
      "epoch:  7   step:  142   train loss:  0.233973890542984  val loss:  0.32408857345581055\n",
      "epoch:  7   step:  143   train loss:  0.30635207891464233  val loss:  0.32769128680229187\n",
      "epoch:  7   step:  144   train loss:  0.3176584839820862  val loss:  0.3304491639137268\n",
      "epoch:  7   step:  145   train loss:  0.2657347321510315  val loss:  0.32988211512565613\n",
      "epoch:  7   step:  146   train loss:  0.23619414865970612  val loss:  0.3319036066532135\n",
      "epoch:  7   step:  147   train loss:  0.24067509174346924  val loss:  0.32336506247520447\n",
      "epoch:  7   step:  148   train loss:  0.22888770699501038  val loss:  0.3152521550655365\n",
      "epoch:  7   step:  149   train loss:  0.2632253170013428  val loss:  0.3147170841693878\n",
      "epoch:  7   step:  150   train loss:  0.25327134132385254  val loss:  0.3134619891643524\n",
      "epoch:  7   step:  151   train loss:  0.21585345268249512  val loss:  0.3120080530643463\n",
      "epoch:  7   step:  152   train loss:  0.24476879835128784  val loss:  0.31460437178611755\n",
      "epoch:  7   step:  153   train loss:  0.3832687735557556  val loss:  0.31418123841285706\n",
      "epoch:  7   step:  154   train loss:  0.28737252950668335  val loss:  0.3164364993572235\n",
      "epoch:  7   step:  155   train loss:  0.2558785378932953  val loss:  0.31955546140670776\n",
      "epoch:  7   step:  156   train loss:  0.4148619771003723  val loss:  0.324496328830719\n",
      "epoch:  7   step:  157   train loss:  0.330716073513031  val loss:  0.32650893926620483\n",
      "epoch:  7   step:  158   train loss:  0.22177183628082275  val loss:  0.32922524213790894\n",
      "epoch:  7   step:  159   train loss:  0.29621878266334534  val loss:  0.33169445395469666\n",
      "epoch:  7   step:  160   train loss:  0.2119624763727188  val loss:  0.33712080121040344\n",
      "epoch:  7   step:  161   train loss:  0.26156628131866455  val loss:  0.3422354459762573\n",
      "epoch:  7   step:  162   train loss:  0.2649516761302948  val loss:  0.3412496745586395\n",
      "epoch:  7   step:  163   train loss:  0.3541536033153534  val loss:  0.33184364438056946\n",
      "epoch:  7   step:  164   train loss:  0.20605617761611938  val loss:  0.3230048418045044\n",
      "epoch:  7   step:  165   train loss:  0.2619452476501465  val loss:  0.3217616081237793\n",
      "epoch:  8   step:  0   train loss:  0.2825402021408081  val loss:  0.32830917835235596\n",
      "epoch:  8   step:  1   train loss:  0.24204108119010925  val loss:  0.33003512024879456\n",
      "epoch:  8   step:  2   train loss:  0.2267349809408188  val loss:  0.3324682414531708\n",
      "epoch:  8   step:  3   train loss:  0.3465039134025574  val loss:  0.3236999809741974\n",
      "epoch:  8   step:  4   train loss:  0.2951321005821228  val loss:  0.317798376083374\n",
      "epoch:  8   step:  5   train loss:  0.3633912205696106  val loss:  0.3096999526023865\n",
      "epoch:  8   step:  6   train loss:  0.262626975774765  val loss:  0.30491331219673157\n",
      "min_val_loss_print 0.30491331219673157\n",
      "epoch:  8   step:  7   train loss:  0.31276294589042664  val loss:  0.3049730658531189\n",
      "epoch:  8   step:  8   train loss:  0.2827513813972473  val loss:  0.3046548664569855\n",
      "min_val_loss_print 0.3046548664569855\n",
      "epoch:  8   step:  9   train loss:  0.1833949238061905  val loss:  0.30272555351257324\n",
      "min_val_loss_print 0.30272555351257324\n",
      "epoch:  8   step:  10   train loss:  0.3127644658088684  val loss:  0.29711925983428955\n",
      "min_val_loss_print 0.29711925983428955\n",
      "epoch:  8   step:  11   train loss:  0.2789584994316101  val loss:  0.29096418619155884\n",
      "min_val_loss_print 0.29096418619155884\n",
      "epoch:  8   step:  12   train loss:  0.2827736437320709  val loss:  0.2880391776561737\n",
      "min_val_loss_print 0.2880391776561737\n",
      "epoch:  8   step:  13   train loss:  0.21065673232078552  val loss:  0.28904446959495544\n",
      "epoch:  8   step:  14   train loss:  0.2539463937282562  val loss:  0.29553642868995667\n",
      "epoch:  8   step:  15   train loss:  0.3259168267250061  val loss:  0.30010801553726196\n",
      "epoch:  8   step:  16   train loss:  0.3463924825191498  val loss:  0.2976229786872864\n",
      "epoch:  8   step:  17   train loss:  0.4266377091407776  val loss:  0.29951024055480957\n",
      "epoch:  8   step:  18   train loss:  0.24209262430667877  val loss:  0.3044801354408264\n",
      "epoch:  8   step:  19   train loss:  0.24385684728622437  val loss:  0.3108820915222168\n",
      "epoch:  8   step:  20   train loss:  0.2415040135383606  val loss:  0.3180156350135803\n",
      "epoch:  8   step:  21   train loss:  0.30176469683647156  val loss:  0.32161861658096313\n",
      "epoch:  8   step:  22   train loss:  0.18397322297096252  val loss:  0.32050269842147827\n",
      "epoch:  8   step:  23   train loss:  0.33675098419189453  val loss:  0.3141469359397888\n",
      "epoch:  8   step:  24   train loss:  0.2210896909236908  val loss:  0.3125731348991394\n",
      "epoch:  8   step:  25   train loss:  0.31071698665618896  val loss:  0.30399349331855774\n",
      "epoch:  8   step:  26   train loss:  0.1674989014863968  val loss:  0.3058471381664276\n",
      "epoch:  8   step:  27   train loss:  0.3871678411960602  val loss:  0.29429498314857483\n",
      "epoch:  8   step:  28   train loss:  0.26310956478118896  val loss:  0.29187557101249695\n",
      "epoch:  8   step:  29   train loss:  0.19400638341903687  val loss:  0.29488101601600647\n",
      "epoch:  8   step:  30   train loss:  0.30779916048049927  val loss:  0.2957623302936554\n",
      "epoch:  8   step:  31   train loss:  0.2596370577812195  val loss:  0.3005620837211609\n",
      "epoch:  8   step:  32   train loss:  0.20012050867080688  val loss:  0.3091078996658325\n",
      "epoch:  8   step:  33   train loss:  0.17695757746696472  val loss:  0.31233108043670654\n",
      "epoch:  8   step:  34   train loss:  0.1897406429052353  val loss:  0.3164975047111511\n",
      "epoch:  8   step:  35   train loss:  0.27442169189453125  val loss:  0.3116939663887024\n",
      "epoch:  8   step:  36   train loss:  0.24022753536701202  val loss:  0.30420514941215515\n",
      "epoch:  8   step:  37   train loss:  0.32342320680618286  val loss:  0.29333847761154175\n",
      "epoch:  8   step:  38   train loss:  0.2278304100036621  val loss:  0.2864187955856323\n",
      "min_val_loss_print 0.2864187955856323\n",
      "epoch:  8   step:  39   train loss:  0.23802685737609863  val loss:  0.2849479615688324\n",
      "min_val_loss_print 0.2849479615688324\n",
      "epoch:  8   step:  40   train loss:  0.18627643585205078  val loss:  0.2906457483768463\n",
      "epoch:  8   step:  41   train loss:  0.15482251346111298  val loss:  0.29827624559402466\n",
      "epoch:  8   step:  42   train loss:  0.29100990295410156  val loss:  0.29725196957588196\n",
      "epoch:  8   step:  43   train loss:  0.33326825499534607  val loss:  0.28176477551460266\n",
      "min_val_loss_print 0.28176477551460266\n",
      "epoch:  8   step:  44   train loss:  0.23320633172988892  val loss:  0.27031534910202026\n",
      "min_val_loss_print 0.27031534910202026\n",
      "epoch:  8   step:  45   train loss:  0.2080821692943573  val loss:  0.2642749845981598\n",
      "min_val_loss_print 0.2642749845981598\n",
      "epoch:  8   step:  46   train loss:  0.15078897774219513  val loss:  0.2702765166759491\n",
      "epoch:  8   step:  47   train loss:  0.3482099771499634  val loss:  0.2960125803947449\n",
      "epoch:  8   step:  48   train loss:  0.4215565323829651  val loss:  0.31491386890411377\n",
      "epoch:  8   step:  49   train loss:  0.3056080639362335  val loss:  0.31903108954429626\n",
      "epoch:  8   step:  50   train loss:  0.35653579235076904  val loss:  0.31751182675361633\n",
      "epoch:  8   step:  51   train loss:  0.2819191515445709  val loss:  0.31027719378471375\n",
      "epoch:  8   step:  52   train loss:  0.33729296922683716  val loss:  0.28565365076065063\n",
      "epoch:  8   step:  53   train loss:  0.29180508852005005  val loss:  0.2640760838985443\n",
      "min_val_loss_print 0.2640760838985443\n",
      "epoch:  8   step:  54   train loss:  0.30872562527656555  val loss:  0.26205679774284363\n",
      "min_val_loss_print 0.26205679774284363\n",
      "epoch:  8   step:  55   train loss:  0.2575705051422119  val loss:  0.26844295859336853\n",
      "epoch:  8   step:  56   train loss:  0.18013660609722137  val loss:  0.27444443106651306\n",
      "epoch:  8   step:  57   train loss:  0.32504627108573914  val loss:  0.2757728695869446\n",
      "epoch:  8   step:  58   train loss:  0.23644647002220154  val loss:  0.27637186646461487\n",
      "epoch:  8   step:  59   train loss:  0.2738158702850342  val loss:  0.278426855802536\n",
      "epoch:  8   step:  60   train loss:  0.31674593687057495  val loss:  0.2795223891735077\n",
      "epoch:  8   step:  61   train loss:  0.31088775396347046  val loss:  0.2797316312789917\n",
      "epoch:  8   step:  62   train loss:  0.2433238923549652  val loss:  0.28483983874320984\n",
      "epoch:  8   step:  63   train loss:  0.2657349109649658  val loss:  0.2921954393386841\n",
      "epoch:  8   step:  64   train loss:  0.21113422513008118  val loss:  0.2969359755516052\n",
      "epoch:  8   step:  65   train loss:  0.1762581169605255  val loss:  0.3012089133262634\n",
      "epoch:  8   step:  66   train loss:  0.2789204716682434  val loss:  0.29581645131111145\n",
      "epoch:  8   step:  67   train loss:  0.1696566343307495  val loss:  0.2896045744419098\n",
      "epoch:  8   step:  68   train loss:  0.30964985489845276  val loss:  0.2779828906059265\n",
      "epoch:  8   step:  69   train loss:  0.2899942994117737  val loss:  0.27452918887138367\n",
      "epoch:  8   step:  70   train loss:  0.25958526134490967  val loss:  0.2775736451148987\n",
      "epoch:  8   step:  71   train loss:  0.23633475601673126  val loss:  0.28333780169487\n",
      "epoch:  8   step:  72   train loss:  0.1850423812866211  val loss:  0.2895946502685547\n",
      "epoch:  8   step:  73   train loss:  0.23613838851451874  val loss:  0.2908429801464081\n",
      "epoch:  8   step:  74   train loss:  0.23737013339996338  val loss:  0.2907239496707916\n",
      "epoch:  8   step:  75   train loss:  0.23687618970870972  val loss:  0.29281213879585266\n",
      "epoch:  8   step:  76   train loss:  0.1711725890636444  val loss:  0.2962411344051361\n",
      "epoch:  8   step:  77   train loss:  0.19409748911857605  val loss:  0.2966616451740265\n",
      "epoch:  8   step:  78   train loss:  0.24170741438865662  val loss:  0.29772040247917175\n",
      "epoch:  8   step:  79   train loss:  0.2806781232357025  val loss:  0.30437010526657104\n",
      "epoch:  8   step:  80   train loss:  0.1962760090827942  val loss:  0.3112364411354065\n",
      "epoch:  8   step:  81   train loss:  0.2317626029253006  val loss:  0.314030259847641\n",
      "epoch:  8   step:  82   train loss:  0.24366408586502075  val loss:  0.31725695729255676\n",
      "epoch:  8   step:  83   train loss:  0.16985857486724854  val loss:  0.31523793935775757\n",
      "epoch:  8   step:  84   train loss:  0.15164890885353088  val loss:  0.31218788027763367\n",
      "epoch:  8   step:  85   train loss:  0.13324150443077087  val loss:  0.3139472007751465\n",
      "epoch:  8   step:  86   train loss:  0.23010095953941345  val loss:  0.32220497727394104\n",
      "epoch:  8   step:  87   train loss:  0.3366853594779968  val loss:  0.33427894115448\n",
      "epoch:  8   step:  88   train loss:  0.2707926332950592  val loss:  0.3391532003879547\n",
      "epoch:  8   step:  89   train loss:  0.273270845413208  val loss:  0.33458518981933594\n",
      "epoch:  8   step:  90   train loss:  0.21540269255638123  val loss:  0.3294685184955597\n",
      "epoch:  8   step:  91   train loss:  0.33485379815101624  val loss:  0.3275566101074219\n",
      "epoch:  8   step:  92   train loss:  0.3069639205932617  val loss:  0.3267675042152405\n",
      "epoch:  8   step:  93   train loss:  0.27260738611221313  val loss:  0.32275450229644775\n",
      "epoch:  8   step:  94   train loss:  0.3065551519393921  val loss:  0.3202298581600189\n",
      "epoch:  8   step:  95   train loss:  0.1960470825433731  val loss:  0.31363120675086975\n",
      "epoch:  8   step:  96   train loss:  0.2424047291278839  val loss:  0.31100231409072876\n",
      "epoch:  8   step:  97   train loss:  0.15906667709350586  val loss:  0.3055295944213867\n",
      "epoch:  8   step:  98   train loss:  0.32124513387680054  val loss:  0.3105403184890747\n",
      "epoch:  8   step:  99   train loss:  0.3125312924385071  val loss:  0.3113459050655365\n",
      "epoch:  8   step:  100   train loss:  0.2565293312072754  val loss:  0.31515148282051086\n",
      "epoch:  8   step:  101   train loss:  0.1787308007478714  val loss:  0.31602993607521057\n",
      "epoch:  8   step:  102   train loss:  0.24741168320178986  val loss:  0.3047141134738922\n",
      "epoch:  8   step:  103   train loss:  0.23100358247756958  val loss:  0.2960675060749054\n",
      "epoch:  8   step:  104   train loss:  0.25058799982070923  val loss:  0.28416457772254944\n",
      "epoch:  8   step:  105   train loss:  0.3091256618499756  val loss:  0.274227112531662\n",
      "epoch:  8   step:  106   train loss:  0.29804468154907227  val loss:  0.27382028102874756\n",
      "epoch:  8   step:  107   train loss:  0.19530272483825684  val loss:  0.2782594561576843\n",
      "epoch:  8   step:  108   train loss:  0.24593710899353027  val loss:  0.2796139121055603\n",
      "epoch:  8   step:  109   train loss:  0.23454782366752625  val loss:  0.2736877501010895\n",
      "epoch:  8   step:  110   train loss:  0.204262375831604  val loss:  0.2658882439136505\n",
      "epoch:  8   step:  111   train loss:  0.2180916965007782  val loss:  0.25962644815444946\n",
      "min_val_loss_print 0.25962644815444946\n",
      "epoch:  8   step:  112   train loss:  0.1927158683538437  val loss:  0.2546088695526123\n",
      "min_val_loss_print 0.2546088695526123\n",
      "epoch:  8   step:  113   train loss:  0.21760065853595734  val loss:  0.25694260001182556\n",
      "epoch:  8   step:  114   train loss:  0.14529746770858765  val loss:  0.2641765773296356\n",
      "epoch:  8   step:  115   train loss:  0.17977499961853027  val loss:  0.2716102600097656\n",
      "epoch:  8   step:  116   train loss:  0.2617468237876892  val loss:  0.2775779962539673\n",
      "epoch:  8   step:  117   train loss:  0.2870640754699707  val loss:  0.2711906433105469\n",
      "epoch:  8   step:  118   train loss:  0.22497107088565826  val loss:  0.26568105816841125\n",
      "epoch:  8   step:  119   train loss:  0.34719419479370117  val loss:  0.25510865449905396\n",
      "epoch:  8   step:  120   train loss:  0.21384158730506897  val loss:  0.24470806121826172\n",
      "min_val_loss_print 0.24470806121826172\n",
      "epoch:  8   step:  121   train loss:  0.20946329832077026  val loss:  0.24473237991333008\n",
      "epoch:  8   step:  122   train loss:  0.2289075255393982  val loss:  0.24651753902435303\n",
      "epoch:  8   step:  123   train loss:  0.33934223651885986  val loss:  0.24307027459144592\n",
      "min_val_loss_print 0.24307027459144592\n",
      "epoch:  8   step:  124   train loss:  0.20244894921779633  val loss:  0.241457000374794\n",
      "min_val_loss_print 0.241457000374794\n",
      "epoch:  8   step:  125   train loss:  0.23420976102352142  val loss:  0.24452728033065796\n",
      "epoch:  8   step:  126   train loss:  0.18111377954483032  val loss:  0.2533579170703888\n",
      "epoch:  8   step:  127   train loss:  0.32037973403930664  val loss:  0.2613496780395508\n",
      "epoch:  8   step:  128   train loss:  0.3819144666194916  val loss:  0.2660812735557556\n",
      "epoch:  8   step:  129   train loss:  0.1569736897945404  val loss:  0.2636059522628784\n",
      "epoch:  8   step:  130   train loss:  0.2520633935928345  val loss:  0.26111871004104614\n",
      "epoch:  8   step:  131   train loss:  0.22144219279289246  val loss:  0.26288098096847534\n",
      "epoch:  8   step:  132   train loss:  0.19275684654712677  val loss:  0.26541590690612793\n",
      "epoch:  8   step:  133   train loss:  0.2650395631790161  val loss:  0.26728692650794983\n",
      "epoch:  8   step:  134   train loss:  0.1871844083070755  val loss:  0.2693471312522888\n",
      "epoch:  8   step:  135   train loss:  0.24837802350521088  val loss:  0.27228298783302307\n",
      "epoch:  8   step:  136   train loss:  0.13519449532032013  val loss:  0.27518272399902344\n",
      "epoch:  8   step:  137   train loss:  0.2541634440422058  val loss:  0.2771099805831909\n",
      "epoch:  8   step:  138   train loss:  0.13598325848579407  val loss:  0.28026533126831055\n",
      "epoch:  8   step:  139   train loss:  0.16739633679389954  val loss:  0.28604546189308167\n",
      "epoch:  8   step:  140   train loss:  0.2459823191165924  val loss:  0.28898194432258606\n",
      "epoch:  8   step:  141   train loss:  0.15530063211917877  val loss:  0.29333093762397766\n",
      "epoch:  8   step:  142   train loss:  0.19561122357845306  val loss:  0.29489830136299133\n",
      "epoch:  8   step:  143   train loss:  0.14510515332221985  val loss:  0.2947942912578583\n",
      "epoch:  8   step:  144   train loss:  0.14632581174373627  val loss:  0.2952151298522949\n",
      "epoch:  8   step:  145   train loss:  0.210394024848938  val loss:  0.2942619323730469\n",
      "epoch:  8   step:  146   train loss:  0.17445972561836243  val loss:  0.2950378358364105\n",
      "epoch:  8   step:  147   train loss:  0.16432687640190125  val loss:  0.2977966368198395\n",
      "epoch:  8   step:  148   train loss:  0.1654382050037384  val loss:  0.3075107932090759\n",
      "epoch:  8   step:  149   train loss:  0.19941122829914093  val loss:  0.313449501991272\n",
      "epoch:  8   step:  150   train loss:  0.25503450632095337  val loss:  0.31641554832458496\n",
      "epoch:  8   step:  151   train loss:  0.22550681233406067  val loss:  0.317832887172699\n",
      "epoch:  8   step:  152   train loss:  0.3066560924053192  val loss:  0.31532660126686096\n",
      "epoch:  8   step:  153   train loss:  0.2673288583755493  val loss:  0.31248897314071655\n",
      "epoch:  8   step:  154   train loss:  0.13423608243465424  val loss:  0.3100733458995819\n",
      "epoch:  8   step:  155   train loss:  0.1685103476047516  val loss:  0.30545082688331604\n",
      "epoch:  8   step:  156   train loss:  0.2409454584121704  val loss:  0.303225576877594\n",
      "epoch:  8   step:  157   train loss:  0.26074081659317017  val loss:  0.29977095127105713\n",
      "epoch:  8   step:  158   train loss:  0.23279112577438354  val loss:  0.3003263473510742\n",
      "epoch:  8   step:  159   train loss:  0.14621905982494354  val loss:  0.30405575037002563\n",
      "epoch:  8   step:  160   train loss:  0.5143941640853882  val loss:  0.2930702567100525\n",
      "epoch:  8   step:  161   train loss:  0.2770180404186249  val loss:  0.28375691175460815\n",
      "epoch:  8   step:  162   train loss:  0.17811843752861023  val loss:  0.2805934250354767\n",
      "epoch:  8   step:  163   train loss:  0.21221725642681122  val loss:  0.27606552839279175\n",
      "epoch:  8   step:  164   train loss:  0.33700886368751526  val loss:  0.27158212661743164\n",
      "epoch:  8   step:  165   train loss:  0.37255990505218506  val loss:  0.2685253322124481\n",
      "epoch:  9   step:  0   train loss:  0.32132548093795776  val loss:  0.2642936110496521\n",
      "epoch:  9   step:  1   train loss:  0.25310418009757996  val loss:  0.261142373085022\n",
      "epoch:  9   step:  2   train loss:  0.24334776401519775  val loss:  0.2625044882297516\n",
      "epoch:  9   step:  3   train loss:  0.23954400420188904  val loss:  0.26810649037361145\n",
      "epoch:  9   step:  4   train loss:  0.28281915187835693  val loss:  0.27825504541397095\n",
      "epoch:  9   step:  5   train loss:  0.3068779408931732  val loss:  0.28298497200012207\n",
      "epoch:  9   step:  6   train loss:  0.28121209144592285  val loss:  0.28336119651794434\n",
      "epoch:  9   step:  7   train loss:  0.20746147632598877  val loss:  0.2796284854412079\n",
      "epoch:  9   step:  8   train loss:  0.1662982702255249  val loss:  0.2790743112564087\n",
      "epoch:  9   step:  9   train loss:  0.09969125688076019  val loss:  0.27878978848457336\n",
      "epoch:  9   step:  10   train loss:  0.20051924884319305  val loss:  0.2786906659603119\n",
      "epoch:  9   step:  11   train loss:  0.15819856524467468  val loss:  0.2795027494430542\n",
      "epoch:  9   step:  12   train loss:  0.24869754910469055  val loss:  0.2787570059299469\n",
      "epoch:  9   step:  13   train loss:  0.2408616989850998  val loss:  0.2726048231124878\n",
      "epoch:  9   step:  14   train loss:  0.22526976466178894  val loss:  0.27176350355148315\n",
      "epoch:  9   step:  15   train loss:  0.3992903232574463  val loss:  0.28600645065307617\n",
      "epoch:  9   step:  16   train loss:  0.3135026693344116  val loss:  0.2979412376880646\n",
      "epoch:  9   step:  17   train loss:  0.16870486736297607  val loss:  0.30143290758132935\n",
      "epoch:  9   step:  18   train loss:  0.3393809497356415  val loss:  0.30157271027565\n",
      "epoch:  9   step:  19   train loss:  0.35112860798835754  val loss:  0.30334749817848206\n",
      "epoch:  9   step:  20   train loss:  0.11659957468509674  val loss:  0.31427642703056335\n",
      "epoch:  9   step:  21   train loss:  0.2665138840675354  val loss:  0.30945712327957153\n",
      "epoch:  9   step:  22   train loss:  0.22357389330863953  val loss:  0.2972298562526703\n",
      "epoch:  9   step:  23   train loss:  0.0989474430680275  val loss:  0.29038217663764954\n",
      "epoch:  9   step:  24   train loss:  0.140910342335701  val loss:  0.28800615668296814\n",
      "epoch:  9   step:  25   train loss:  0.44884076714515686  val loss:  0.30147403478622437\n",
      "epoch:  9   step:  26   train loss:  0.315320760011673  val loss:  0.3189820349216461\n",
      "epoch:  9   step:  27   train loss:  0.2571909725666046  val loss:  0.34734630584716797\n",
      "epoch:  9   step:  28   train loss:  0.20988641679286957  val loss:  0.3669029474258423\n",
      "epoch:  9   step:  29   train loss:  0.17189083993434906  val loss:  0.37066221237182617\n",
      "epoch:  9   step:  30   train loss:  0.2579009532928467  val loss:  0.3543371558189392\n",
      "epoch:  9   step:  31   train loss:  0.2723167836666107  val loss:  0.32781845331192017\n",
      "epoch:  9   step:  32   train loss:  0.27545294165611267  val loss:  0.30421650409698486\n",
      "epoch:  9   step:  33   train loss:  0.12049530446529388  val loss:  0.2884175181388855\n",
      "epoch:  9   step:  34   train loss:  0.3199094533920288  val loss:  0.28633368015289307\n",
      "epoch:  9   step:  35   train loss:  0.251648485660553  val loss:  0.28353869915008545\n",
      "epoch:  9   step:  36   train loss:  0.14859667420387268  val loss:  0.2877316474914551\n",
      "epoch:  9   step:  37   train loss:  0.09103228151798248  val loss:  0.2906462848186493\n",
      "epoch:  9   step:  38   train loss:  0.18403582274913788  val loss:  0.28948619961738586\n",
      "epoch:  9   step:  39   train loss:  0.22985465824604034  val loss:  0.2926340103149414\n",
      "epoch:  9   step:  40   train loss:  0.20516148209571838  val loss:  0.28766652941703796\n",
      "epoch:  9   step:  41   train loss:  0.14361408352851868  val loss:  0.2819269001483917\n",
      "epoch:  9   step:  42   train loss:  0.15744799375534058  val loss:  0.2824571430683136\n",
      "epoch:  9   step:  43   train loss:  0.22433409094810486  val loss:  0.28726041316986084\n",
      "epoch:  9   step:  44   train loss:  0.1388445794582367  val loss:  0.2961997091770172\n",
      "epoch:  9   step:  45   train loss:  0.21340176463127136  val loss:  0.2984957695007324\n",
      "epoch:  9   step:  46   train loss:  0.1952236443758011  val loss:  0.29855749011039734\n",
      "epoch:  9   step:  47   train loss:  0.14500877261161804  val loss:  0.2998107075691223\n",
      "epoch:  9   step:  48   train loss:  0.2585250437259674  val loss:  0.3025760054588318\n",
      "epoch:  9   step:  49   train loss:  0.21118518710136414  val loss:  0.3034445643424988\n",
      "epoch:  9   step:  50   train loss:  0.19577442109584808  val loss:  0.3128332495689392\n",
      "epoch:  9   step:  51   train loss:  0.2339152991771698  val loss:  0.31336691975593567\n",
      "epoch:  9   step:  52   train loss:  0.2287050485610962  val loss:  0.3223603665828705\n",
      "epoch:  9   step:  53   train loss:  0.43119922280311584  val loss:  0.33858099579811096\n",
      "epoch:  9   step:  54   train loss:  0.09837606549263  val loss:  0.3378854990005493\n",
      "epoch:  9   step:  55   train loss:  0.3512765169143677  val loss:  0.32331427931785583\n",
      "epoch:  9   step:  56   train loss:  0.31028831005096436  val loss:  0.3128788471221924\n",
      "epoch:  9   step:  57   train loss:  0.10628487914800644  val loss:  0.3055208921432495\n",
      "epoch:  9   step:  58   train loss:  0.20120060443878174  val loss:  0.29601410031318665\n",
      "epoch:  9   step:  59   train loss:  0.2909470796585083  val loss:  0.2887392044067383\n",
      "epoch:  9   step:  60   train loss:  0.2341805100440979  val loss:  0.2886250615119934\n",
      "epoch:  9   step:  61   train loss:  0.12051405012607574  val loss:  0.2987108826637268\n",
      "epoch:  9   step:  62   train loss:  0.19807566702365875  val loss:  0.29849568009376526\n",
      "epoch:  9   step:  63   train loss:  0.1893921196460724  val loss:  0.30311018228530884\n",
      "epoch:  9   step:  64   train loss:  0.1533319354057312  val loss:  0.31047359108924866\n",
      "epoch:  9   step:  65   train loss:  0.320981502532959  val loss:  0.3125776946544647\n",
      "epoch:  9   step:  66   train loss:  0.27028119564056396  val loss:  0.31402724981307983\n",
      "epoch:  9   step:  67   train loss:  0.2774876356124878  val loss:  0.3185715675354004\n",
      "epoch:  9   step:  68   train loss:  0.16627372801303864  val loss:  0.32299599051475525\n",
      "epoch:  9   step:  69   train loss:  0.23820659518241882  val loss:  0.3157224655151367\n",
      "epoch:  9   step:  70   train loss:  0.1958726942539215  val loss:  0.31381210684776306\n",
      "epoch:  9   step:  71   train loss:  0.2911384105682373  val loss:  0.3163096606731415\n",
      "epoch:  9   step:  72   train loss:  0.3789629340171814  val loss:  0.32502907514572144\n",
      "epoch:  9   step:  73   train loss:  0.20853585004806519  val loss:  0.3350328803062439\n",
      "epoch:  9   step:  74   train loss:  0.2594967484474182  val loss:  0.32342368364334106\n",
      "epoch:  9   step:  75   train loss:  0.22752360999584198  val loss:  0.31528323888778687\n",
      "epoch:  9   step:  76   train loss:  0.3268568515777588  val loss:  0.29720091819763184\n",
      "epoch:  9   step:  77   train loss:  0.2501295804977417  val loss:  0.2746010720729828\n",
      "epoch:  9   step:  78   train loss:  0.17602142691612244  val loss:  0.2574852406978607\n",
      "epoch:  9   step:  79   train loss:  0.24196572601795197  val loss:  0.25472167134284973\n",
      "epoch:  9   step:  80   train loss:  0.20602664351463318  val loss:  0.26593920588493347\n",
      "epoch:  9   step:  81   train loss:  0.37253236770629883  val loss:  0.26616328954696655\n",
      "epoch:  9   step:  82   train loss:  0.14554470777511597  val loss:  0.2659931480884552\n",
      "epoch:  9   step:  83   train loss:  0.2564222514629364  val loss:  0.25848186016082764\n",
      "epoch:  9   step:  84   train loss:  0.2839570939540863  val loss:  0.24837175011634827\n",
      "epoch:  9   step:  85   train loss:  0.24943974614143372  val loss:  0.24325767159461975\n",
      "epoch:  9   step:  86   train loss:  0.14386171102523804  val loss:  0.2531126141548157\n",
      "epoch:  9   step:  87   train loss:  0.2742173373699188  val loss:  0.26933881640434265\n",
      "epoch:  9   step:  88   train loss:  0.20879322290420532  val loss:  0.28533056378364563\n",
      "epoch:  9   step:  89   train loss:  0.34895214438438416  val loss:  0.29697859287261963\n",
      "epoch:  9   step:  90   train loss:  0.1479983776807785  val loss:  0.30152830481529236\n",
      "epoch:  9   step:  91   train loss:  0.1889420449733734  val loss:  0.3019021451473236\n",
      "epoch:  9   step:  92   train loss:  0.09542054682970047  val loss:  0.29788559675216675\n",
      "epoch:  9   step:  93   train loss:  0.19687820971012115  val loss:  0.2935805320739746\n",
      "epoch:  9   step:  94   train loss:  0.23702165484428406  val loss:  0.2860910892486572\n",
      "epoch:  9   step:  95   train loss:  0.15998244285583496  val loss:  0.28290125727653503\n",
      "epoch:  9   step:  96   train loss:  0.18184036016464233  val loss:  0.285672664642334\n",
      "epoch:  9   step:  97   train loss:  0.35413026809692383  val loss:  0.30004173517227173\n",
      "epoch:  9   step:  98   train loss:  0.22483119368553162  val loss:  0.3080015778541565\n",
      "epoch:  9   step:  99   train loss:  0.13243970274925232  val loss:  0.31154346466064453\n",
      "epoch:  9   step:  100   train loss:  0.18680144846439362  val loss:  0.32269996404647827\n",
      "epoch:  9   step:  101   train loss:  0.1609574407339096  val loss:  0.3349347710609436\n",
      "epoch:  9   step:  102   train loss:  0.1492856740951538  val loss:  0.34093523025512695\n",
      "epoch:  9   step:  103   train loss:  0.264019250869751  val loss:  0.33857351541519165\n",
      "epoch:  9   step:  104   train loss:  0.21550095081329346  val loss:  0.3400944173336029\n",
      "epoch:  9   step:  105   train loss:  0.2097039818763733  val loss:  0.338992714881897\n",
      "epoch:  9   step:  106   train loss:  0.12823113799095154  val loss:  0.34924936294555664\n",
      "epoch:  9   step:  107   train loss:  0.17656078934669495  val loss:  0.3654559254646301\n",
      "epoch:  9   step:  108   train loss:  0.1633499562740326  val loss:  0.3757200837135315\n",
      "epoch:  9   step:  109   train loss:  0.21467791497707367  val loss:  0.3815070390701294\n",
      "epoch:  9   step:  110   train loss:  0.2673778235912323  val loss:  0.37716442346572876\n",
      "epoch:  9   step:  111   train loss:  0.18746960163116455  val loss:  0.3739509582519531\n",
      "epoch:  9   step:  112   train loss:  0.1419738531112671  val loss:  0.3619909882545471\n",
      "epoch:  9   step:  113   train loss:  0.22947928309440613  val loss:  0.3537438213825226\n",
      "epoch:  9   step:  114   train loss:  0.23919078707695007  val loss:  0.34012484550476074\n",
      "epoch:  9   step:  115   train loss:  0.22711336612701416  val loss:  0.3387707471847534\n",
      "epoch:  9   step:  116   train loss:  0.2122058868408203  val loss:  0.34184059500694275\n",
      "epoch:  9   step:  117   train loss:  0.2006256878376007  val loss:  0.3463372588157654\n",
      "epoch:  9   step:  118   train loss:  0.19672365486621857  val loss:  0.345458984375\n",
      "epoch:  9   step:  119   train loss:  0.19356058537960052  val loss:  0.3443715274333954\n",
      "epoch:  9   step:  120   train loss:  0.22650396823883057  val loss:  0.3360288441181183\n",
      "epoch:  9   step:  121   train loss:  0.22527092695236206  val loss:  0.320724219083786\n",
      "epoch:  9   step:  122   train loss:  0.34715694189071655  val loss:  0.2973424196243286\n",
      "epoch:  9   step:  123   train loss:  0.2916604280471802  val loss:  0.286171555519104\n",
      "epoch:  9   step:  124   train loss:  0.2047833949327469  val loss:  0.2903766930103302\n",
      "epoch:  9   step:  125   train loss:  0.19125248491764069  val loss:  0.2928980886936188\n",
      "epoch:  9   step:  126   train loss:  0.13237883150577545  val loss:  0.2878425419330597\n",
      "epoch:  9   step:  127   train loss:  0.29249322414398193  val loss:  0.2798181176185608\n",
      "epoch:  9   step:  128   train loss:  0.2270631641149521  val loss:  0.27380362153053284\n",
      "epoch:  9   step:  129   train loss:  0.15254946053028107  val loss:  0.2696397006511688\n",
      "epoch:  9   step:  130   train loss:  0.08392056077718735  val loss:  0.26334232091903687\n",
      "epoch:  9   step:  131   train loss:  0.19867774844169617  val loss:  0.2584269344806671\n",
      "epoch:  9   step:  132   train loss:  0.08831033110618591  val loss:  0.2508394122123718\n",
      "epoch:  9   step:  133   train loss:  0.07625816762447357  val loss:  0.24873872101306915\n",
      "epoch:  9   step:  134   train loss:  0.22922858595848083  val loss:  0.24478411674499512\n",
      "epoch:  9   step:  135   train loss:  0.3159334659576416  val loss:  0.2441057413816452\n",
      "epoch:  9   step:  136   train loss:  0.09837529808282852  val loss:  0.24565134942531586\n",
      "epoch:  9   step:  137   train loss:  0.3190711438655853  val loss:  0.25409162044525146\n",
      "epoch:  9   step:  138   train loss:  0.1317254602909088  val loss:  0.264364093542099\n",
      "epoch:  9   step:  139   train loss:  0.1262994110584259  val loss:  0.27050819993019104\n",
      "epoch:  9   step:  140   train loss:  0.21942448616027832  val loss:  0.27109411358833313\n",
      "epoch:  9   step:  141   train loss:  0.13819149136543274  val loss:  0.27423983812332153\n",
      "epoch:  9   step:  142   train loss:  0.2399178296327591  val loss:  0.26628053188323975\n",
      "epoch:  9   step:  143   train loss:  0.17756357789039612  val loss:  0.25563859939575195\n",
      "epoch:  9   step:  144   train loss:  0.2125413715839386  val loss:  0.2510683238506317\n",
      "epoch:  9   step:  145   train loss:  0.21787157654762268  val loss:  0.2533968389034271\n",
      "epoch:  9   step:  146   train loss:  0.15757805109024048  val loss:  0.2581288814544678\n",
      "epoch:  9   step:  147   train loss:  0.22043275833129883  val loss:  0.2586643397808075\n",
      "epoch:  9   step:  148   train loss:  0.20083537697792053  val loss:  0.2635509669780731\n",
      "epoch:  9   step:  149   train loss:  0.15987654030323029  val loss:  0.273690402507782\n",
      "epoch:  9   step:  150   train loss:  0.21826788783073425  val loss:  0.2895645797252655\n",
      "epoch:  9   step:  151   train loss:  0.06119929626584053  val loss:  0.3085032105445862\n",
      "epoch:  9   step:  152   train loss:  0.2489653378725052  val loss:  0.32862424850463867\n",
      "epoch:  9   step:  153   train loss:  0.22584635019302368  val loss:  0.3414005935192108\n",
      "epoch:  9   step:  154   train loss:  0.1884167492389679  val loss:  0.33662834763526917\n",
      "epoch:  9   step:  155   train loss:  0.15308433771133423  val loss:  0.3197029232978821\n",
      "epoch:  9   step:  156   train loss:  0.1373840570449829  val loss:  0.3026954233646393\n",
      "epoch:  9   step:  157   train loss:  0.18857333064079285  val loss:  0.28545063734054565\n",
      "epoch:  9   step:  158   train loss:  0.2105151116847992  val loss:  0.2744746208190918\n",
      "epoch:  9   step:  159   train loss:  0.14279204607009888  val loss:  0.26872313022613525\n",
      "epoch:  9   step:  160   train loss:  0.2293320745229721  val loss:  0.26592203974723816\n",
      "epoch:  9   step:  161   train loss:  0.14028537273406982  val loss:  0.26791298389434814\n",
      "epoch:  9   step:  162   train loss:  0.14176791906356812  val loss:  0.2663716673851013\n",
      "epoch:  9   step:  163   train loss:  0.20614087581634521  val loss:  0.2665678560733795\n",
      "epoch:  9   step:  164   train loss:  0.15317043662071228  val loss:  0.2709449529647827\n",
      "epoch:  9   step:  165   train loss:  0.3117654025554657  val loss:  0.2704782485961914\n",
      "epoch:  10   step:  0   train loss:  0.22582697868347168  val loss:  0.2728412449359894\n",
      "epoch:  10   step:  1   train loss:  0.09890172630548477  val loss:  0.2712583541870117\n",
      "epoch:  10   step:  2   train loss:  0.19314654171466827  val loss:  0.27414244413375854\n",
      "epoch:  10   step:  3   train loss:  0.11701282858848572  val loss:  0.2793678045272827\n",
      "epoch:  10   step:  4   train loss:  0.19892174005508423  val loss:  0.29499709606170654\n",
      "epoch:  10   step:  5   train loss:  0.18082457780838013  val loss:  0.30632922053337097\n",
      "epoch:  10   step:  6   train loss:  0.24036714434623718  val loss:  0.3089140057563782\n",
      "epoch:  10   step:  7   train loss:  0.2078610211610794  val loss:  0.3048967719078064\n",
      "epoch:  10   step:  8   train loss:  0.12147097289562225  val loss:  0.2968745231628418\n",
      "epoch:  10   step:  9   train loss:  0.3131644129753113  val loss:  0.28952330350875854\n",
      "epoch:  10   step:  10   train loss:  0.13509297370910645  val loss:  0.28873392939567566\n",
      "epoch:  10   step:  11   train loss:  0.1711094081401825  val loss:  0.2956651747226715\n",
      "epoch:  10   step:  12   train loss:  0.2011943757534027  val loss:  0.30314475297927856\n",
      "epoch:  10   step:  13   train loss:  0.11799271404743195  val loss:  0.31102266907691956\n",
      "epoch:  10   step:  14   train loss:  0.15304699540138245  val loss:  0.31893137097358704\n",
      "epoch:  10   step:  15   train loss:  0.15849849581718445  val loss:  0.3228961229324341\n",
      "epoch:  10   step:  16   train loss:  0.159900963306427  val loss:  0.3179496228694916\n",
      "epoch:  10   step:  17   train loss:  0.20730248093605042  val loss:  0.31674453616142273\n",
      "epoch:  10   step:  18   train loss:  0.08954373002052307  val loss:  0.3182888627052307\n",
      "epoch:  10   step:  19   train loss:  0.15206244587898254  val loss:  0.3260497450828552\n",
      "epoch:  10   step:  20   train loss:  0.30775970220565796  val loss:  0.33307400345802307\n",
      "epoch:  10   step:  21   train loss:  0.23801091313362122  val loss:  0.33085542917251587\n",
      "epoch:  10   step:  22   train loss:  0.2533970773220062  val loss:  0.33471977710723877\n",
      "epoch:  10   step:  23   train loss:  0.17980945110321045  val loss:  0.3374987244606018\n",
      "epoch:  10   step:  24   train loss:  0.12331117689609528  val loss:  0.3463376462459564\n",
      "epoch:  10   step:  25   train loss:  0.17394255101680756  val loss:  0.3512105941772461\n",
      "epoch:  10   step:  26   train loss:  0.4925764799118042  val loss:  0.3608182370662689\n",
      "epoch:  10   step:  27   train loss:  0.16818319261074066  val loss:  0.3688264787197113\n",
      "epoch:  10   step:  28   train loss:  0.15787707269191742  val loss:  0.371978759765625\n",
      "epoch:  10   step:  29   train loss:  0.11609857529401779  val loss:  0.3730549216270447\n",
      "epoch:  10   step:  30   train loss:  0.24300311505794525  val loss:  0.37272384762763977\n",
      "epoch:  10   step:  31   train loss:  0.18156857788562775  val loss:  0.36468181014060974\n",
      "epoch:  10   step:  32   train loss:  0.20417892932891846  val loss:  0.35400012135505676\n",
      "epoch:  10   step:  33   train loss:  0.10502570122480392  val loss:  0.3473552167415619\n",
      "epoch:  10   step:  34   train loss:  0.23184464871883392  val loss:  0.3428534269332886\n",
      "epoch:  10   step:  35   train loss:  0.1745694875717163  val loss:  0.32126256823539734\n",
      "epoch:  10   step:  36   train loss:  0.1694493591785431  val loss:  0.30011218786239624\n",
      "epoch:  10   step:  37   train loss:  0.16757892072200775  val loss:  0.28554269671440125\n",
      "epoch:  10   step:  38   train loss:  0.19339200854301453  val loss:  0.28287407755851746\n",
      "epoch:  10   step:  39   train loss:  0.11658035963773727  val loss:  0.2971535325050354\n",
      "epoch:  10   step:  40   train loss:  0.103144571185112  val loss:  0.3141075074672699\n",
      "epoch:  10   step:  41   train loss:  0.20850813388824463  val loss:  0.3185359239578247\n",
      "epoch:  10   step:  42   train loss:  0.281863272190094  val loss:  0.29918041825294495\n",
      "epoch:  10   step:  43   train loss:  0.28123703598976135  val loss:  0.2806960344314575\n",
      "epoch:  10   step:  44   train loss:  0.14755704998970032  val loss:  0.28480786085128784\n",
      "epoch:  10   step:  45   train loss:  0.13392318785190582  val loss:  0.30630484223365784\n",
      "epoch:  10   step:  46   train loss:  0.23916733264923096  val loss:  0.3221278786659241\n",
      "epoch:  10   step:  47   train loss:  0.29701173305511475  val loss:  0.32795509696006775\n",
      "epoch:  10   step:  48   train loss:  0.142714262008667  val loss:  0.3289799392223358\n",
      "epoch:  10   step:  49   train loss:  0.19073203206062317  val loss:  0.3181256353855133\n",
      "epoch:  10   step:  50   train loss:  0.1434832513332367  val loss:  0.31468337774276733\n",
      "epoch:  10   step:  51   train loss:  0.13666518032550812  val loss:  0.31304824352264404\n",
      "epoch:  10   step:  52   train loss:  0.20985004305839539  val loss:  0.3139130771160126\n",
      "epoch:  10   step:  53   train loss:  0.13174375891685486  val loss:  0.30667856335639954\n",
      "epoch:  10   step:  54   train loss:  0.3171716630458832  val loss:  0.3042045831680298\n",
      "epoch:  10   step:  55   train loss:  0.17203617095947266  val loss:  0.30505669116973877\n",
      "epoch:  10   step:  56   train loss:  0.25284314155578613  val loss:  0.303510844707489\n",
      "epoch:  10   step:  57   train loss:  0.08008459210395813  val loss:  0.29380348324775696\n",
      "epoch:  10   step:  58   train loss:  0.15309686958789825  val loss:  0.28761860728263855\n",
      "epoch:  10   step:  59   train loss:  0.1619674563407898  val loss:  0.2872425317764282\n",
      "epoch:  10   step:  60   train loss:  0.21788142621517181  val loss:  0.29085105657577515\n",
      "epoch:  10   step:  61   train loss:  0.3042566180229187  val loss:  0.2923588752746582\n",
      "epoch:  10   step:  62   train loss:  0.2104654163122177  val loss:  0.2950034737586975\n",
      "epoch:  10   step:  63   train loss:  0.10039913654327393  val loss:  0.303633451461792\n",
      "epoch:  10   step:  64   train loss:  0.068362757563591  val loss:  0.30635958909988403\n",
      "epoch:  10   step:  65   train loss:  0.18529510498046875  val loss:  0.304007887840271\n",
      "epoch:  10   step:  66   train loss:  0.09719507396221161  val loss:  0.30478635430336\n",
      "epoch:  10   step:  67   train loss:  0.13473351299762726  val loss:  0.2970706820487976\n",
      "epoch:  10   step:  68   train loss:  0.12735271453857422  val loss:  0.29294660687446594\n",
      "epoch:  10   step:  69   train loss:  0.238559752702713  val loss:  0.28393784165382385\n",
      "epoch:  10   step:  70   train loss:  0.0735263004899025  val loss:  0.2808111310005188\n",
      "epoch:  10   step:  71   train loss:  0.1800980120897293  val loss:  0.27873754501342773\n",
      "epoch:  10   step:  72   train loss:  0.2142506241798401  val loss:  0.27764397859573364\n",
      "epoch:  10   step:  73   train loss:  0.14774996042251587  val loss:  0.28404921293258667\n",
      "epoch:  10   step:  74   train loss:  0.04422296956181526  val loss:  0.2945326864719391\n",
      "epoch:  10   step:  75   train loss:  0.17186245322227478  val loss:  0.2971123158931732\n",
      "epoch:  10   step:  76   train loss:  0.15040290355682373  val loss:  0.29327392578125\n",
      "epoch:  10   step:  77   train loss:  0.11750674992799759  val loss:  0.2838171720504761\n",
      "epoch:  10   step:  78   train loss:  0.14737561345100403  val loss:  0.2764166295528412\n",
      "epoch:  10   step:  79   train loss:  0.121783547103405  val loss:  0.26969704031944275\n",
      "epoch:  10   step:  80   train loss:  0.30688267946243286  val loss:  0.2673546075820923\n",
      "epoch:  10   step:  81   train loss:  0.17331373691558838  val loss:  0.2627210319042206\n",
      "epoch:  10   step:  82   train loss:  0.1816437840461731  val loss:  0.2559034824371338\n",
      "epoch:  10   step:  83   train loss:  0.22513873875141144  val loss:  0.25791215896606445\n",
      "epoch:  10   step:  84   train loss:  0.14441141486167908  val loss:  0.2622373104095459\n",
      "epoch:  10   step:  85   train loss:  0.177602618932724  val loss:  0.2620830535888672\n",
      "epoch:  10   step:  86   train loss:  0.21604064106941223  val loss:  0.2626432478427887\n",
      "epoch:  10   step:  87   train loss:  0.2888241410255432  val loss:  0.27184391021728516\n",
      "epoch:  10   step:  88   train loss:  0.3290742337703705  val loss:  0.28199243545532227\n",
      "epoch:  10   step:  89   train loss:  0.10538236051797867  val loss:  0.29077160358428955\n",
      "epoch:  10   step:  90   train loss:  0.11757070571184158  val loss:  0.29273149371147156\n",
      "epoch:  10   step:  91   train loss:  0.2181403636932373  val loss:  0.2859684228897095\n",
      "epoch:  10   step:  92   train loss:  0.30350297689437866  val loss:  0.27204886078834534\n",
      "epoch:  10   step:  93   train loss:  0.11985403299331665  val loss:  0.26172390580177307\n",
      "epoch:  10   step:  94   train loss:  0.25368231534957886  val loss:  0.25308310985565186\n",
      "epoch:  10   step:  95   train loss:  0.27535706758499146  val loss:  0.25327637791633606\n",
      "epoch:  10   step:  96   train loss:  0.13612481951713562  val loss:  0.2554723620414734\n",
      "epoch:  10   step:  97   train loss:  0.08852454274892807  val loss:  0.25822630524635315\n",
      "epoch:  10   step:  98   train loss:  0.18096107244491577  val loss:  0.25020357966423035\n",
      "epoch:  10   step:  99   train loss:  0.3072890639305115  val loss:  0.24488596618175507\n",
      "epoch:  10   step:  100   train loss:  0.114250048995018  val loss:  0.24991624057292938\n",
      "epoch:  10   step:  101   train loss:  0.214950293302536  val loss:  0.25172293186187744\n",
      "epoch:  10   step:  102   train loss:  0.36367878317832947  val loss:  0.24896778166294098\n",
      "epoch:  10   step:  103   train loss:  0.2172325998544693  val loss:  0.24915198981761932\n",
      "epoch:  10   step:  104   train loss:  0.19812671840190887  val loss:  0.24969756603240967\n",
      "epoch:  10   step:  105   train loss:  0.1724277138710022  val loss:  0.25535327196121216\n",
      "epoch:  10   step:  106   train loss:  0.11809291690587997  val loss:  0.26286107301712036\n",
      "epoch:  10   step:  107   train loss:  0.11134383827447891  val loss:  0.2695246636867523\n",
      "epoch:  10   step:  108   train loss:  0.19994071125984192  val loss:  0.26953941583633423\n",
      "epoch:  10   step:  109   train loss:  0.18270555138587952  val loss:  0.268506795167923\n",
      "epoch:  10   step:  110   train loss:  0.18814656138420105  val loss:  0.27272310853004456\n",
      "epoch:  10   step:  111   train loss:  0.20928724110126495  val loss:  0.28092214465141296\n",
      "epoch:  10   step:  112   train loss:  0.2762501537799835  val loss:  0.29000580310821533\n",
      "epoch:  10   step:  113   train loss:  0.17529800534248352  val loss:  0.2984897494316101\n",
      "epoch:  10   step:  114   train loss:  0.22785982489585876  val loss:  0.3119381368160248\n",
      "epoch:  10   step:  115   train loss:  0.12686574459075928  val loss:  0.3202340006828308\n",
      "epoch:  10   step:  116   train loss:  0.32975292205810547  val loss:  0.3199298083782196\n",
      "epoch:  10   step:  117   train loss:  0.14453932642936707  val loss:  0.3164807856082916\n",
      "epoch:  10   step:  118   train loss:  0.2040771245956421  val loss:  0.3034539222717285\n",
      "epoch:  10   step:  119   train loss:  0.1297314465045929  val loss:  0.2897152304649353\n",
      "epoch:  10   step:  120   train loss:  0.18734732270240784  val loss:  0.28538545966148376\n",
      "epoch:  10   step:  121   train loss:  0.12563277781009674  val loss:  0.2853098213672638\n",
      "epoch:  10   step:  122   train loss:  0.12503397464752197  val loss:  0.2855154871940613\n",
      "epoch:  10   step:  123   train loss:  0.05060052126646042  val loss:  0.28687945008277893\n",
      "epoch:  10   step:  124   train loss:  0.11187057197093964  val loss:  0.2892693281173706\n",
      "epoch:  10   step:  125   train loss:  0.18326979875564575  val loss:  0.292364239692688\n",
      "epoch:  10   step:  126   train loss:  0.15893995761871338  val loss:  0.2926790416240692\n",
      "epoch:  10   step:  127   train loss:  0.16916751861572266  val loss:  0.29534396529197693\n",
      "epoch:  10   step:  128   train loss:  0.3411892354488373  val loss:  0.2895573675632477\n",
      "epoch:  10   step:  129   train loss:  0.1200161874294281  val loss:  0.2807912528514862\n",
      "epoch:  10   step:  130   train loss:  0.15133388340473175  val loss:  0.27142196893692017\n",
      "epoch:  10   step:  131   train loss:  0.10430388897657394  val loss:  0.27078375220298767\n",
      "epoch:  10   step:  132   train loss:  0.23606254160404205  val loss:  0.27178141474723816\n",
      "epoch:  10   step:  133   train loss:  0.2236727476119995  val loss:  0.26893535256385803\n",
      "epoch:  10   step:  134   train loss:  0.10565577447414398  val loss:  0.26818376779556274\n",
      "epoch:  10   step:  135   train loss:  0.12832233309745789  val loss:  0.26609230041503906\n",
      "epoch:  10   step:  136   train loss:  0.18136748671531677  val loss:  0.26166027784347534\n",
      "epoch:  10   step:  137   train loss:  0.1789250522851944  val loss:  0.2639051079750061\n",
      "epoch:  10   step:  138   train loss:  0.302946537733078  val loss:  0.2652173638343811\n",
      "epoch:  10   step:  139   train loss:  0.15080824494361877  val loss:  0.27150943875312805\n",
      "epoch:  10   step:  140   train loss:  0.26403069496154785  val loss:  0.2828472852706909\n",
      "epoch:  10   step:  141   train loss:  0.25098761916160583  val loss:  0.2993846535682678\n",
      "epoch:  10   step:  142   train loss:  0.15231743454933167  val loss:  0.3196265995502472\n",
      "epoch:  10   step:  143   train loss:  0.2464505285024643  val loss:  0.34935271739959717\n",
      "epoch:  10   step:  144   train loss:  0.11197377741336823  val loss:  0.3691512644290924\n",
      "epoch:  10   step:  145   train loss:  0.16177362203598022  val loss:  0.37109485268592834\n",
      "epoch:  10   step:  146   train loss:  0.14530551433563232  val loss:  0.36886489391326904\n",
      "epoch:  10   step:  147   train loss:  0.2628830671310425  val loss:  0.36288437247276306\n",
      "epoch:  10   step:  148   train loss:  0.23754379153251648  val loss:  0.3384077847003937\n",
      "epoch:  10   step:  149   train loss:  0.11075228452682495  val loss:  0.3245750069618225\n",
      "epoch:  10   step:  150   train loss:  0.18371394276618958  val loss:  0.31048059463500977\n",
      "epoch:  10   step:  151   train loss:  0.12811020016670227  val loss:  0.30047449469566345\n",
      "epoch:  10   step:  152   train loss:  0.17562991380691528  val loss:  0.29444774985313416\n",
      "epoch:  10   step:  153   train loss:  0.10611152648925781  val loss:  0.28882351517677307\n",
      "epoch:  10   step:  154   train loss:  0.18819421529769897  val loss:  0.2903137803077698\n",
      "epoch:  10   step:  155   train loss:  0.26705706119537354  val loss:  0.2935887277126312\n",
      "epoch:  10   step:  156   train loss:  0.23305176198482513  val loss:  0.29237300157546997\n",
      "epoch:  10   step:  157   train loss:  0.2825281620025635  val loss:  0.29007479548454285\n",
      "epoch:  10   step:  158   train loss:  0.34253549575805664  val loss:  0.29448291659355164\n",
      "epoch:  10   step:  159   train loss:  0.1236603632569313  val loss:  0.2954028844833374\n",
      "epoch:  10   step:  160   train loss:  0.11648250371217728  val loss:  0.2956487238407135\n",
      "epoch:  10   step:  161   train loss:  0.19197139143943787  val loss:  0.2970936596393585\n",
      "epoch:  10   step:  162   train loss:  0.20672965049743652  val loss:  0.3024361729621887\n",
      "epoch:  10   step:  163   train loss:  0.17983782291412354  val loss:  0.31597429513931274\n",
      "epoch:  10   step:  164   train loss:  0.14701446890830994  val loss:  0.32762426137924194\n",
      "epoch:  10   step:  165   train loss:  0.1539614200592041  val loss:  0.3256886601448059\n",
      "epoch:  11   step:  0   train loss:  0.27092844247817993  val loss:  0.3265373110771179\n",
      "epoch:  11   step:  1   train loss:  0.09087742865085602  val loss:  0.3336806893348694\n",
      "epoch:  11   step:  2   train loss:  0.10079781711101532  val loss:  0.34780120849609375\n",
      "epoch:  11   step:  3   train loss:  0.18977302312850952  val loss:  0.3556927740573883\n",
      "epoch:  11   step:  4   train loss:  0.24044758081436157  val loss:  0.36391642689704895\n",
      "epoch:  11   step:  5   train loss:  0.1642312854528427  val loss:  0.3702954649925232\n",
      "epoch:  11   step:  6   train loss:  0.1520698219537735  val loss:  0.3744874894618988\n",
      "epoch:  11   step:  7   train loss:  0.19360239803791046  val loss:  0.37843698263168335\n",
      "epoch:  11   step:  8   train loss:  0.29472410678863525  val loss:  0.37484267354011536\n",
      "epoch:  11   step:  9   train loss:  0.08919784426689148  val loss:  0.3691442310810089\n",
      "epoch:  11   step:  10   train loss:  0.1185389831662178  val loss:  0.3606529235839844\n",
      "epoch:  11   step:  11   train loss:  0.0666370540857315  val loss:  0.35471636056900024\n",
      "epoch:  11   step:  12   train loss:  0.2297423928976059  val loss:  0.33797773718833923\n",
      "epoch:  11   step:  13   train loss:  0.14637106657028198  val loss:  0.31899550557136536\n",
      "epoch:  11   step:  14   train loss:  0.14942702651023865  val loss:  0.31152239441871643\n",
      "epoch:  11   step:  15   train loss:  0.3813076317310333  val loss:  0.3050580620765686\n",
      "epoch:  11   step:  16   train loss:  0.11884545534849167  val loss:  0.3014914393424988\n",
      "epoch:  11   step:  17   train loss:  0.172410786151886  val loss:  0.2974691689014435\n",
      "epoch:  11   step:  18   train loss:  0.15997746586799622  val loss:  0.29875418543815613\n",
      "epoch:  11   step:  19   train loss:  0.2385023981332779  val loss:  0.3028830885887146\n",
      "epoch:  11   step:  20   train loss:  0.048631779849529266  val loss:  0.3092802166938782\n",
      "epoch:  11   step:  21   train loss:  0.2438361495733261  val loss:  0.32606807351112366\n",
      "epoch:  11   step:  22   train loss:  0.15637214481830597  val loss:  0.336283415555954\n",
      "epoch:  11   step:  23   train loss:  0.16618923842906952  val loss:  0.33919814229011536\n",
      "epoch:  11   step:  24   train loss:  0.13058525323867798  val loss:  0.35383158922195435\n",
      "epoch:  11   step:  25   train loss:  0.1947433054447174  val loss:  0.35444238781929016\n",
      "epoch:  11   step:  26   train loss:  0.24531817436218262  val loss:  0.3479973077774048\n",
      "epoch:  11   step:  27   train loss:  0.2912280857563019  val loss:  0.34296688437461853\n",
      "epoch:  11   step:  28   train loss:  0.18711650371551514  val loss:  0.349500447511673\n",
      "epoch:  11   step:  29   train loss:  0.21534724533557892  val loss:  0.347402960062027\n",
      "epoch:  11   step:  30   train loss:  0.10689368844032288  val loss:  0.33745500445365906\n",
      "epoch:  11   step:  31   train loss:  0.17809218168258667  val loss:  0.3329114317893982\n",
      "epoch:  11   step:  32   train loss:  0.12572084367275238  val loss:  0.32298415899276733\n",
      "epoch:  11   step:  33   train loss:  0.09591290354728699  val loss:  0.308120459318161\n",
      "epoch:  11   step:  34   train loss:  0.05846705287694931  val loss:  0.30047357082366943\n",
      "epoch:  11   step:  35   train loss:  0.08925294876098633  val loss:  0.293079674243927\n",
      "epoch:  11   step:  36   train loss:  0.30079707503318787  val loss:  0.2857554852962494\n",
      "epoch:  11   step:  37   train loss:  0.25954264402389526  val loss:  0.2856821119785309\n",
      "epoch:  11   step:  38   train loss:  0.16219614446163177  val loss:  0.29631006717681885\n",
      "epoch:  11   step:  39   train loss:  0.13702738285064697  val loss:  0.310981810092926\n",
      "epoch:  11   step:  40   train loss:  0.12233024090528488  val loss:  0.3233904540538788\n",
      "epoch:  11   step:  41   train loss:  0.18584021925926208  val loss:  0.3262709677219391\n",
      "epoch:  11   step:  42   train loss:  0.08853555470705032  val loss:  0.3202521502971649\n",
      "epoch:  11   step:  43   train loss:  0.09163104742765427  val loss:  0.3053317070007324\n",
      "epoch:  11   step:  44   train loss:  0.23886410892009735  val loss:  0.28665050864219666\n",
      "epoch:  11   step:  45   train loss:  0.08376379311084747  val loss:  0.27120593190193176\n",
      "epoch:  11   step:  46   train loss:  0.12458470463752747  val loss:  0.26751068234443665\n",
      "epoch:  11   step:  47   train loss:  0.1532147228717804  val loss:  0.2704062759876251\n",
      "epoch:  11   step:  48   train loss:  0.11992038041353226  val loss:  0.2709410488605499\n",
      "epoch:  11   step:  49   train loss:  0.17331990599632263  val loss:  0.2704548239707947\n",
      "epoch:  11   step:  50   train loss:  0.06227889657020569  val loss:  0.26832905411720276\n",
      "epoch:  11   step:  51   train loss:  0.17625457048416138  val loss:  0.26502928137779236\n",
      "epoch:  11   step:  52   train loss:  0.28402677178382874  val loss:  0.26727694272994995\n",
      "epoch:  11   step:  53   train loss:  0.13249711692333221  val loss:  0.27515771985054016\n",
      "epoch:  11   step:  54   train loss:  0.06934191286563873  val loss:  0.29173046350479126\n",
      "epoch:  11   step:  55   train loss:  0.08638717234134674  val loss:  0.3143220543861389\n",
      "epoch:  11   step:  56   train loss:  0.14977608621120453  val loss:  0.3409263789653778\n",
      "epoch:  11   step:  57   train loss:  0.17070873081684113  val loss:  0.3512374758720398\n",
      "epoch:  11   step:  58   train loss:  0.2127833068370819  val loss:  0.3514052629470825\n",
      "epoch:  11   step:  59   train loss:  0.13798250257968903  val loss:  0.3528381884098053\n",
      "epoch:  11   step:  60   train loss:  0.059060633182525635  val loss:  0.35658392310142517\n",
      "epoch:  11   step:  61   train loss:  0.10720697045326233  val loss:  0.36581769585609436\n",
      "epoch:  11   step:  62   train loss:  0.11031237989664078  val loss:  0.36837947368621826\n",
      "epoch:  11   step:  63   train loss:  0.060235023498535156  val loss:  0.3644850552082062\n",
      "epoch:  11   step:  64   train loss:  0.1001160740852356  val loss:  0.3604753017425537\n",
      "epoch:  11   step:  65   train loss:  0.16902630031108856  val loss:  0.3595324754714966\n",
      "epoch:  11   step:  66   train loss:  0.15706053376197815  val loss:  0.35376960039138794\n",
      "epoch:  11   step:  67   train loss:  0.06793268024921417  val loss:  0.34981569647789\n",
      "epoch:  11   step:  68   train loss:  0.06933583319187164  val loss:  0.3439289927482605\n",
      "epoch:  11   step:  69   train loss:  0.09514947235584259  val loss:  0.32956674695014954\n",
      "epoch:  11   step:  70   train loss:  0.19140300154685974  val loss:  0.3169392943382263\n",
      "epoch:  11   step:  71   train loss:  0.03134054318070412  val loss:  0.3143616318702698\n",
      "epoch:  11   step:  72   train loss:  0.20708590745925903  val loss:  0.30831411480903625\n",
      "epoch:  11   step:  73   train loss:  0.16675658524036407  val loss:  0.30118295550346375\n",
      "epoch:  11   step:  74   train loss:  0.12270146608352661  val loss:  0.2954954206943512\n",
      "epoch:  11   step:  75   train loss:  0.15801922976970673  val loss:  0.2915891110897064\n",
      "epoch:  11   step:  76   train loss:  0.15679919719696045  val loss:  0.2925092577934265\n",
      "epoch:  11   step:  77   train loss:  0.12960198521614075  val loss:  0.297724187374115\n",
      "epoch:  11   step:  78   train loss:  0.10380961745977402  val loss:  0.3116181194782257\n",
      "epoch:  11   step:  79   train loss:  0.18894483149051666  val loss:  0.3210147023200989\n",
      "epoch:  11   step:  80   train loss:  0.15738525986671448  val loss:  0.32937079668045044\n",
      "epoch:  11   step:  81   train loss:  0.12926732003688812  val loss:  0.32798460125923157\n",
      "epoch:  11   step:  82   train loss:  0.05392523482441902  val loss:  0.3312707841396332\n",
      "epoch:  11   step:  83   train loss:  0.222715824842453  val loss:  0.3205507695674896\n",
      "epoch:  11   step:  84   train loss:  0.11326153576374054  val loss:  0.30717766284942627\n",
      "epoch:  11   step:  85   train loss:  0.0893745943903923  val loss:  0.29921332001686096\n",
      "epoch:  11   step:  86   train loss:  0.17586252093315125  val loss:  0.3014479875564575\n",
      "epoch:  11   step:  87   train loss:  0.12499657273292542  val loss:  0.30255037546157837\n",
      "epoch:  11   step:  88   train loss:  0.1003759577870369  val loss:  0.29573357105255127\n",
      "epoch:  11   step:  89   train loss:  0.22152814269065857  val loss:  0.2884216904640198\n",
      "epoch:  11   step:  90   train loss:  0.06788287311792374  val loss:  0.28174784779548645\n",
      "epoch:  11   step:  91   train loss:  0.061455585062503815  val loss:  0.28657612204551697\n",
      "epoch:  11   step:  92   train loss:  0.15130165219306946  val loss:  0.29589423537254333\n",
      "epoch:  11   step:  93   train loss:  0.1947517991065979  val loss:  0.3088251054286957\n",
      "epoch:  11   step:  94   train loss:  0.18979017436504364  val loss:  0.3136645257472992\n",
      "epoch:  11   step:  95   train loss:  0.20639950037002563  val loss:  0.3023871183395386\n",
      "epoch:  11   step:  96   train loss:  0.3283699154853821  val loss:  0.2876724302768707\n",
      "epoch:  11   step:  97   train loss:  0.2109776735305786  val loss:  0.277611643075943\n",
      "epoch:  11   step:  98   train loss:  0.1689625084400177  val loss:  0.2663469612598419\n",
      "epoch:  11   step:  99   train loss:  0.12819629907608032  val loss:  0.26757314801216125\n",
      "epoch:  11   step:  100   train loss:  0.11863388121128082  val loss:  0.27031925320625305\n",
      "epoch:  11   step:  101   train loss:  0.20428095757961273  val loss:  0.2743520438671112\n",
      "epoch:  11   step:  102   train loss:  0.058167144656181335  val loss:  0.28029686212539673\n",
      "epoch:  11   step:  103   train loss:  0.04550725221633911  val loss:  0.2855375409126282\n",
      "epoch:  11   step:  104   train loss:  0.2069733589887619  val loss:  0.29224449396133423\n",
      "epoch:  11   step:  105   train loss:  0.05596126616001129  val loss:  0.2997407019138336\n",
      "epoch:  11   step:  106   train loss:  0.09244616329669952  val loss:  0.31214025616645813\n",
      "epoch:  11   step:  107   train loss:  0.2194918841123581  val loss:  0.32300901412963867\n",
      "epoch:  11   step:  108   train loss:  0.141886368393898  val loss:  0.3182375729084015\n",
      "epoch:  11   step:  109   train loss:  0.18451370298862457  val loss:  0.3154894709587097\n",
      "epoch:  11   step:  110   train loss:  0.17579051852226257  val loss:  0.3134656250476837\n",
      "epoch:  11   step:  111   train loss:  0.09155899286270142  val loss:  0.3125554323196411\n",
      "epoch:  11   step:  112   train loss:  0.09125369042158127  val loss:  0.3126296103000641\n",
      "epoch:  11   step:  113   train loss:  0.04953788220882416  val loss:  0.3224051594734192\n",
      "epoch:  11   step:  114   train loss:  0.07055284082889557  val loss:  0.3232421278953552\n",
      "epoch:  11   step:  115   train loss:  0.12393820285797119  val loss:  0.3261377513408661\n",
      "epoch:  11   step:  116   train loss:  0.1457335203886032  val loss:  0.32628390192985535\n",
      "epoch:  11   step:  117   train loss:  0.09640762954950333  val loss:  0.3270370364189148\n",
      "epoch:  11   step:  118   train loss:  0.1642201840877533  val loss:  0.3162441551685333\n",
      "epoch:  11   step:  119   train loss:  0.22712761163711548  val loss:  0.3080739676952362\n",
      "epoch:  11   step:  120   train loss:  0.20348091423511505  val loss:  0.29824069142341614\n",
      "epoch:  11   step:  121   train loss:  0.07653245329856873  val loss:  0.2974488437175751\n",
      "epoch:  11   step:  122   train loss:  0.1621522307395935  val loss:  0.2990686893463135\n",
      "epoch:  11   step:  123   train loss:  0.1690451204776764  val loss:  0.29344770312309265\n",
      "epoch:  11   step:  124   train loss:  0.22438056766986847  val loss:  0.28951317071914673\n",
      "epoch:  11   step:  125   train loss:  0.20315003395080566  val loss:  0.27958744764328003\n",
      "epoch:  11   step:  126   train loss:  0.3440626859664917  val loss:  0.25734788179397583\n",
      "epoch:  11   step:  127   train loss:  0.28976911306381226  val loss:  0.22963711619377136\n",
      "min_val_loss_print 0.22963711619377136\n",
      "epoch:  11   step:  128   train loss:  0.34377461671829224  val loss:  0.2250690758228302\n",
      "min_val_loss_print 0.2250690758228302\n",
      "epoch:  11   step:  129   train loss:  0.19117122888565063  val loss:  0.23383605480194092\n",
      "epoch:  11   step:  130   train loss:  0.15213316679000854  val loss:  0.24202190339565277\n",
      "epoch:  11   step:  131   train loss:  0.38750970363616943  val loss:  0.23368705809116364\n",
      "epoch:  11   step:  132   train loss:  0.278870165348053  val loss:  0.22832801938056946\n",
      "epoch:  11   step:  133   train loss:  0.12605509161949158  val loss:  0.2323838621377945\n",
      "epoch:  11   step:  134   train loss:  0.17678165435791016  val loss:  0.2385677546262741\n",
      "epoch:  11   step:  135   train loss:  0.1206059530377388  val loss:  0.24869143962860107\n",
      "epoch:  11   step:  136   train loss:  0.2601642608642578  val loss:  0.2522026598453522\n",
      "epoch:  11   step:  137   train loss:  0.08233030140399933  val loss:  0.25843220949172974\n",
      "epoch:  11   step:  138   train loss:  0.174208402633667  val loss:  0.26447218656539917\n",
      "epoch:  11   step:  139   train loss:  0.1642749011516571  val loss:  0.2641298472881317\n",
      "epoch:  11   step:  140   train loss:  0.2891722023487091  val loss:  0.26065659523010254\n",
      "epoch:  11   step:  141   train loss:  0.07885967940092087  val loss:  0.26011988520622253\n",
      "epoch:  11   step:  142   train loss:  0.18836109340190887  val loss:  0.2560993432998657\n",
      "epoch:  11   step:  143   train loss:  0.16368812322616577  val loss:  0.2459598332643509\n",
      "epoch:  11   step:  144   train loss:  0.144168883562088  val loss:  0.2353251427412033\n",
      "epoch:  11   step:  145   train loss:  0.10659635066986084  val loss:  0.2318822741508484\n",
      "epoch:  11   step:  146   train loss:  0.22971215844154358  val loss:  0.23367002606391907\n",
      "epoch:  11   step:  147   train loss:  0.1327815055847168  val loss:  0.23473475873470306\n",
      "epoch:  11   step:  148   train loss:  0.1179700642824173  val loss:  0.23735247552394867\n",
      "epoch:  11   step:  149   train loss:  0.07049085199832916  val loss:  0.24315674602985382\n",
      "epoch:  11   step:  150   train loss:  0.15973514318466187  val loss:  0.24224084615707397\n",
      "epoch:  11   step:  151   train loss:  0.19976994395256042  val loss:  0.24879039824008942\n",
      "epoch:  11   step:  152   train loss:  0.1478492170572281  val loss:  0.25544804334640503\n",
      "epoch:  11   step:  153   train loss:  0.1183546930551529  val loss:  0.26463115215301514\n",
      "epoch:  11   step:  154   train loss:  0.06070595234632492  val loss:  0.27064600586891174\n",
      "epoch:  11   step:  155   train loss:  0.08631154894828796  val loss:  0.2770199775695801\n",
      "epoch:  11   step:  156   train loss:  0.04873640835285187  val loss:  0.284188836812973\n",
      "epoch:  11   step:  157   train loss:  0.14136344194412231  val loss:  0.29063671827316284\n",
      "epoch:  11   step:  158   train loss:  0.13252347707748413  val loss:  0.29803064465522766\n",
      "epoch:  11   step:  159   train loss:  0.126152902841568  val loss:  0.3017736077308655\n",
      "epoch:  11   step:  160   train loss:  0.14121900498867035  val loss:  0.3116067051887512\n",
      "epoch:  11   step:  161   train loss:  0.18220558762550354  val loss:  0.32261645793914795\n",
      "epoch:  11   step:  162   train loss:  0.1789342761039734  val loss:  0.3274909555912018\n",
      "epoch:  11   step:  163   train loss:  0.10360006988048553  val loss:  0.3326573967933655\n",
      "epoch:  11   step:  164   train loss:  0.08526359498500824  val loss:  0.33669403195381165\n",
      "epoch:  11   step:  165   train loss:  0.5200927257537842  val loss:  0.33329111337661743\n",
      "epoch:  12   step:  0   train loss:  0.14276280999183655  val loss:  0.33176180720329285\n",
      "epoch:  12   step:  1   train loss:  0.15015314519405365  val loss:  0.3235081434249878\n",
      "epoch:  12   step:  2   train loss:  0.06446120142936707  val loss:  0.312242716550827\n",
      "epoch:  12   step:  3   train loss:  0.0963919460773468  val loss:  0.2954341173171997\n",
      "epoch:  12   step:  4   train loss:  0.11982062458992004  val loss:  0.28379151225090027\n",
      "epoch:  12   step:  5   train loss:  0.18424364924430847  val loss:  0.27661341428756714\n",
      "epoch:  12   step:  6   train loss:  0.058011047542095184  val loss:  0.2740950286388397\n",
      "epoch:  12   step:  7   train loss:  0.1089753657579422  val loss:  0.2754189670085907\n",
      "epoch:  12   step:  8   train loss:  0.16473855078220367  val loss:  0.2849981486797333\n",
      "epoch:  12   step:  9   train loss:  0.07624293118715286  val loss:  0.28641465306282043\n",
      "epoch:  12   step:  10   train loss:  0.16672611236572266  val loss:  0.2851063013076782\n",
      "epoch:  12   step:  11   train loss:  0.06297612190246582  val loss:  0.2922441065311432\n",
      "epoch:  12   step:  12   train loss:  0.11241042613983154  val loss:  0.30086463689804077\n",
      "epoch:  12   step:  13   train loss:  0.08177444338798523  val loss:  0.3087654113769531\n",
      "epoch:  12   step:  14   train loss:  0.25190943479537964  val loss:  0.3212302625179291\n",
      "epoch:  12   step:  15   train loss:  0.06603530794382095  val loss:  0.3362608253955841\n",
      "epoch:  12   step:  16   train loss:  0.10905010253190994  val loss:  0.34351906180381775\n",
      "epoch:  12   step:  17   train loss:  0.13567112386226654  val loss:  0.33101630210876465\n",
      "epoch:  12   step:  18   train loss:  0.08303523063659668  val loss:  0.3214438557624817\n",
      "epoch:  12   step:  19   train loss:  0.09144125878810883  val loss:  0.3229250907897949\n",
      "epoch:  12   step:  20   train loss:  0.13451078534126282  val loss:  0.32751548290252686\n",
      "epoch:  12   step:  21   train loss:  0.14148682355880737  val loss:  0.3433840870857239\n",
      "epoch:  12   step:  22   train loss:  0.10656838119029999  val loss:  0.3559000790119171\n",
      "epoch:  12   step:  23   train loss:  0.08171866089105606  val loss:  0.366093248128891\n",
      "epoch:  12   step:  24   train loss:  0.12875425815582275  val loss:  0.3749869167804718\n",
      "epoch:  12   step:  25   train loss:  0.0809912383556366  val loss:  0.3826920688152313\n",
      "epoch:  12   step:  26   train loss:  0.16734132170677185  val loss:  0.38376593589782715\n",
      "epoch:  12   step:  27   train loss:  0.04038568586111069  val loss:  0.38204944133758545\n",
      "epoch:  12   step:  28   train loss:  0.13526129722595215  val loss:  0.39202025532722473\n",
      "epoch:  12   step:  29   train loss:  0.10335603356361389  val loss:  0.3985617756843567\n",
      "epoch:  12   step:  30   train loss:  0.15071630477905273  val loss:  0.39456120133399963\n",
      "epoch:  12   step:  31   train loss:  0.08868800103664398  val loss:  0.3925277590751648\n",
      "epoch:  12   step:  32   train loss:  0.08675873279571533  val loss:  0.3996833264827728\n",
      "epoch:  12   step:  33   train loss:  0.18423452973365784  val loss:  0.39471694827079773\n",
      "epoch:  12   step:  34   train loss:  0.05537412315607071  val loss:  0.4014100432395935\n",
      "epoch:  12   step:  35   train loss:  0.13010939955711365  val loss:  0.3946221172809601\n",
      "epoch:  12   step:  36   train loss:  0.0966309979557991  val loss:  0.3777315318584442\n",
      "epoch:  12   step:  37   train loss:  0.15944966673851013  val loss:  0.37094318866729736\n",
      "epoch:  12   step:  38   train loss:  0.08572965115308762  val loss:  0.362393856048584\n",
      "epoch:  12   step:  39   train loss:  0.14110159873962402  val loss:  0.35444697737693787\n",
      "epoch:  12   step:  40   train loss:  0.12866199016571045  val loss:  0.34731605648994446\n",
      "epoch:  12   step:  41   train loss:  0.17774605751037598  val loss:  0.3385418653488159\n",
      "epoch:  12   step:  42   train loss:  0.05980576202273369  val loss:  0.32923251390457153\n",
      "epoch:  12   step:  43   train loss:  0.19919432699680328  val loss:  0.311090886592865\n",
      "epoch:  12   step:  44   train loss:  0.13218875229358673  val loss:  0.30267471075057983\n",
      "epoch:  12   step:  45   train loss:  0.11028101295232773  val loss:  0.29814809560775757\n",
      "epoch:  12   step:  46   train loss:  0.04398660361766815  val loss:  0.2920078933238983\n",
      "epoch:  12   step:  47   train loss:  0.03701724112033844  val loss:  0.2952807545661926\n",
      "epoch:  12   step:  48   train loss:  0.09183450788259506  val loss:  0.2896817624568939\n",
      "epoch:  12   step:  49   train loss:  0.09418323636054993  val loss:  0.2823811173439026\n",
      "epoch:  12   step:  50   train loss:  0.19216756522655487  val loss:  0.27744972705841064\n",
      "epoch:  12   step:  51   train loss:  0.16738584637641907  val loss:  0.2663571238517761\n",
      "epoch:  12   step:  52   train loss:  0.02355136349797249  val loss:  0.26252490282058716\n",
      "epoch:  12   step:  53   train loss:  0.06509726494550705  val loss:  0.26178058981895447\n",
      "epoch:  12   step:  54   train loss:  0.23848354816436768  val loss:  0.26120632886886597\n",
      "epoch:  12   step:  55   train loss:  0.19782087206840515  val loss:  0.25925886631011963\n",
      "epoch:  12   step:  56   train loss:  0.07805736362934113  val loss:  0.26512467861175537\n",
      "epoch:  12   step:  57   train loss:  0.04500804841518402  val loss:  0.27712324261665344\n",
      "epoch:  12   step:  58   train loss:  0.07499834895133972  val loss:  0.29457157850265503\n",
      "epoch:  12   step:  59   train loss:  0.07519564777612686  val loss:  0.30867767333984375\n",
      "epoch:  12   step:  60   train loss:  0.12906455993652344  val loss:  0.3203362226486206\n",
      "epoch:  12   step:  61   train loss:  0.20482662320137024  val loss:  0.3334106504917145\n",
      "epoch:  12   step:  62   train loss:  0.04919286444783211  val loss:  0.3452569246292114\n",
      "epoch:  12   step:  63   train loss:  0.19475358724594116  val loss:  0.36177170276641846\n",
      "epoch:  12   step:  64   train loss:  0.12271073460578918  val loss:  0.3808596432209015\n",
      "epoch:  12   step:  65   train loss:  0.06474423408508301  val loss:  0.3919655978679657\n",
      "epoch:  12   step:  66   train loss:  0.1596660614013672  val loss:  0.39902251958847046\n",
      "epoch:  12   step:  67   train loss:  0.04349492862820625  val loss:  0.39567840099334717\n",
      "epoch:  12   step:  68   train loss:  0.11396591365337372  val loss:  0.3966146409511566\n",
      "epoch:  12   step:  69   train loss:  0.21258020401000977  val loss:  0.39030542969703674\n",
      "epoch:  12   step:  70   train loss:  0.05849654972553253  val loss:  0.3916238844394684\n",
      "epoch:  12   step:  71   train loss:  0.07308432459831238  val loss:  0.3922191262245178\n",
      "epoch:  12   step:  72   train loss:  0.05862174928188324  val loss:  0.3929044306278229\n",
      "epoch:  12   step:  73   train loss:  0.20195768773555756  val loss:  0.3921045660972595\n",
      "epoch:  12   step:  74   train loss:  0.2575553357601166  val loss:  0.39651724696159363\n",
      "epoch:  12   step:  75   train loss:  0.15604397654533386  val loss:  0.4034893810749054\n",
      "epoch:  12   step:  76   train loss:  0.15387743711471558  val loss:  0.422446608543396\n",
      "epoch:  12   step:  77   train loss:  0.1593606173992157  val loss:  0.41431906819343567\n",
      "epoch:  12   step:  78   train loss:  0.11442627012729645  val loss:  0.4089988172054291\n",
      "epoch:  12   step:  79   train loss:  0.1790209710597992  val loss:  0.42194053530693054\n",
      "epoch:  12   step:  80   train loss:  0.2238386571407318  val loss:  0.4207351505756378\n",
      "epoch:  12   step:  81   train loss:  0.08118931949138641  val loss:  0.42167508602142334\n",
      "epoch:  12   step:  82   train loss:  0.13514308631420135  val loss:  0.4244760274887085\n",
      "epoch:  12   step:  83   train loss:  0.10989280045032501  val loss:  0.4231599271297455\n",
      "epoch:  12   step:  84   train loss:  0.07391627877950668  val loss:  0.42420467734336853\n",
      "epoch:  12   step:  85   train loss:  0.045876920223236084  val loss:  0.4287010133266449\n",
      "epoch:  12   step:  86   train loss:  0.11919553577899933  val loss:  0.4385647773742676\n",
      "epoch:  12   step:  87   train loss:  0.17002859711647034  val loss:  0.4482099711894989\n",
      "epoch:  12   step:  88   train loss:  0.21394282579421997  val loss:  0.44972726702690125\n",
      "epoch:  12   step:  89   train loss:  0.10365407913923264  val loss:  0.45191341638565063\n",
      "epoch:  12   step:  90   train loss:  0.15290600061416626  val loss:  0.45347583293914795\n",
      "epoch:  12   step:  91   train loss:  0.16257712244987488  val loss:  0.4450012743473053\n",
      "epoch:  12   step:  92   train loss:  0.04642077907919884  val loss:  0.43985286355018616\n",
      "epoch:  12   step:  93   train loss:  0.14989617466926575  val loss:  0.4289040267467499\n",
      "epoch:  12   step:  94   train loss:  0.036365002393722534  val loss:  0.4060308039188385\n",
      "epoch:  12   step:  95   train loss:  0.07016855478286743  val loss:  0.39177656173706055\n",
      "epoch:  12   step:  96   train loss:  0.1289520263671875  val loss:  0.38563767075538635\n",
      "epoch:  12   step:  97   train loss:  0.13792933523654938  val loss:  0.3837979733943939\n",
      "epoch:  12   step:  98   train loss:  0.05751427635550499  val loss:  0.3856149911880493\n",
      "epoch:  12   step:  99   train loss:  0.07409742474555969  val loss:  0.38815829157829285\n",
      "epoch:  12   step:  100   train loss:  0.052738040685653687  val loss:  0.3856216371059418\n",
      "epoch:  12   step:  101   train loss:  0.221210777759552  val loss:  0.37518203258514404\n",
      "epoch:  12   step:  102   train loss:  0.045879825949668884  val loss:  0.3742314875125885\n",
      "epoch:  12   step:  103   train loss:  0.04551853984594345  val loss:  0.36950069665908813\n",
      "epoch:  12   step:  104   train loss:  0.08995968103408813  val loss:  0.37439408898353577\n",
      "epoch:  12   step:  105   train loss:  0.16943436861038208  val loss:  0.36696353554725647\n",
      "epoch:  12   step:  106   train loss:  0.12100321054458618  val loss:  0.3643946349620819\n",
      "epoch:  12   step:  107   train loss:  0.17476660013198853  val loss:  0.3603706359863281\n",
      "epoch:  12   step:  108   train loss:  0.09727657586336136  val loss:  0.3727949559688568\n",
      "epoch:  12   step:  109   train loss:  0.0913776084780693  val loss:  0.3798314332962036\n",
      "epoch:  12   step:  110   train loss:  0.10127131640911102  val loss:  0.3850512206554413\n",
      "epoch:  12   step:  111   train loss:  0.09704077243804932  val loss:  0.3865754306316376\n",
      "epoch:  12   step:  112   train loss:  0.08551828563213348  val loss:  0.36636802554130554\n",
      "epoch:  12   step:  113   train loss:  0.03675480931997299  val loss:  0.3612459897994995\n",
      "epoch:  12   step:  114   train loss:  0.1180850937962532  val loss:  0.3514460027217865\n",
      "epoch:  12   step:  115   train loss:  0.16620813310146332  val loss:  0.3410208821296692\n",
      "epoch:  12   step:  116   train loss:  0.16946345567703247  val loss:  0.32840242981910706\n",
      "epoch:  12   step:  117   train loss:  0.1957005262374878  val loss:  0.32027217745780945\n",
      "epoch:  12   step:  118   train loss:  0.05653349310159683  val loss:  0.3152109980583191\n",
      "epoch:  12   step:  119   train loss:  0.09413845837116241  val loss:  0.3132287263870239\n",
      "epoch:  12   step:  120   train loss:  0.08279051631689072  val loss:  0.3132263422012329\n",
      "epoch:  12   step:  121   train loss:  0.21448133885860443  val loss:  0.31072133779525757\n",
      "epoch:  12   step:  122   train loss:  0.13316531479358673  val loss:  0.30062124133110046\n",
      "epoch:  12   step:  123   train loss:  0.1421925276517868  val loss:  0.2909621298313141\n",
      "epoch:  12   step:  124   train loss:  0.09161941707134247  val loss:  0.28586599230766296\n",
      "epoch:  12   step:  125   train loss:  0.12966004014015198  val loss:  0.28591209650039673\n",
      "epoch:  12   step:  126   train loss:  0.06407404690980911  val loss:  0.2941117286682129\n",
      "epoch:  12   step:  127   train loss:  0.0756036639213562  val loss:  0.3055664002895355\n",
      "epoch:  12   step:  128   train loss:  0.09439537674188614  val loss:  0.3079603612422943\n",
      "epoch:  12   step:  129   train loss:  0.25489541888237  val loss:  0.3203187584877014\n",
      "epoch:  12   step:  130   train loss:  0.13562104105949402  val loss:  0.3215559422969818\n",
      "epoch:  12   step:  131   train loss:  0.05357026308774948  val loss:  0.32779133319854736\n",
      "epoch:  12   step:  132   train loss:  0.2692858874797821  val loss:  0.3267558515071869\n",
      "epoch:  12   step:  133   train loss:  0.06599259376525879  val loss:  0.333840548992157\n",
      "epoch:  12   step:  134   train loss:  0.12929090857505798  val loss:  0.34230849146842957\n",
      "epoch:  12   step:  135   train loss:  0.10324959456920624  val loss:  0.3482908606529236\n",
      "epoch:  12   step:  136   train loss:  0.07670137286186218  val loss:  0.34857770800590515\n",
      "epoch:  12   step:  137   train loss:  0.18588978052139282  val loss:  0.3522712290287018\n",
      "epoch:  12   step:  138   train loss:  0.07966159284114838  val loss:  0.3502209782600403\n",
      "epoch:  12   step:  139   train loss:  0.16687288880348206  val loss:  0.346938818693161\n",
      "epoch:  12   step:  140   train loss:  0.10222022980451584  val loss:  0.342014878988266\n",
      "epoch:  12   step:  141   train loss:  0.06147342920303345  val loss:  0.33985817432403564\n",
      "epoch:  12   step:  142   train loss:  0.04165031760931015  val loss:  0.3308331370353699\n",
      "epoch:  12   step:  143   train loss:  0.12308992445468903  val loss:  0.3243328034877777\n",
      "epoch:  12   step:  144   train loss:  0.16888843476772308  val loss:  0.3250976800918579\n",
      "epoch:  12   step:  145   train loss:  0.055328451097011566  val loss:  0.32578951120376587\n",
      "epoch:  12   step:  146   train loss:  0.09097126126289368  val loss:  0.33605289459228516\n",
      "epoch:  12   step:  147   train loss:  0.2198694944381714  val loss:  0.3359556198120117\n",
      "epoch:  12   step:  148   train loss:  0.14789684116840363  val loss:  0.3404516875743866\n",
      "epoch:  12   step:  149   train loss:  0.13285993039608002  val loss:  0.35067057609558105\n",
      "epoch:  12   step:  150   train loss:  0.13286788761615753  val loss:  0.37149283289909363\n",
      "epoch:  12   step:  151   train loss:  0.3485984802246094  val loss:  0.37632444500923157\n",
      "epoch:  12   step:  152   train loss:  0.05424308776855469  val loss:  0.3800398111343384\n",
      "epoch:  12   step:  153   train loss:  0.06838609278202057  val loss:  0.3630829155445099\n",
      "epoch:  12   step:  154   train loss:  0.17460589110851288  val loss:  0.35850363969802856\n",
      "epoch:  12   step:  155   train loss:  0.10618026554584503  val loss:  0.3492434322834015\n",
      "epoch:  12   step:  156   train loss:  0.26344630122184753  val loss:  0.35193774104118347\n",
      "epoch:  12   step:  157   train loss:  0.1545889675617218  val loss:  0.360424667596817\n",
      "epoch:  12   step:  158   train loss:  0.1288684606552124  val loss:  0.36476993560791016\n",
      "epoch:  12   step:  159   train loss:  0.11311358958482742  val loss:  0.3687603771686554\n",
      "epoch:  12   step:  160   train loss:  0.1946200728416443  val loss:  0.374813973903656\n",
      "epoch:  12   step:  161   train loss:  0.16405290365219116  val loss:  0.37719523906707764\n",
      "epoch:  12   step:  162   train loss:  0.12144270539283752  val loss:  0.39216771721839905\n",
      "epoch:  12   step:  163   train loss:  0.05908408761024475  val loss:  0.40086135268211365\n",
      "epoch:  12   step:  164   train loss:  0.10303018987178802  val loss:  0.42068183422088623\n",
      "epoch:  12   step:  165   train loss:  0.014880137518048286  val loss:  0.4167642891407013\n",
      "epoch:  13   step:  0   train loss:  0.09122221171855927  val loss:  0.42919042706489563\n",
      "epoch:  13   step:  1   train loss:  0.07111019641160965  val loss:  0.43779054284095764\n",
      "epoch:  13   step:  2   train loss:  0.07162386924028397  val loss:  0.4550102651119232\n",
      "epoch:  13   step:  3   train loss:  0.05626503378152847  val loss:  0.45767247676849365\n",
      "epoch:  13   step:  4   train loss:  0.04665065556764603  val loss:  0.4585936665534973\n",
      "epoch:  13   step:  5   train loss:  0.09340213239192963  val loss:  0.4562242329120636\n",
      "epoch:  13   step:  6   train loss:  0.0583493746817112  val loss:  0.4451361894607544\n",
      "epoch:  13   step:  7   train loss:  0.11020856350660324  val loss:  0.43301814794540405\n",
      "epoch:  13   step:  8   train loss:  0.034802429378032684  val loss:  0.42311006784439087\n",
      "epoch:  13   step:  9   train loss:  0.04805782437324524  val loss:  0.4154578745365143\n",
      "epoch:  13   step:  10   train loss:  0.22964122891426086  val loss:  0.40318235754966736\n",
      "epoch:  13   step:  11   train loss:  0.09645077586174011  val loss:  0.3932158946990967\n",
      "epoch:  13   step:  12   train loss:  0.0234745591878891  val loss:  0.38258978724479675\n",
      "epoch:  13   step:  13   train loss:  0.16284003853797913  val loss:  0.3892395794391632\n",
      "epoch:  13   step:  14   train loss:  0.19424206018447876  val loss:  0.3892066776752472\n",
      "epoch:  13   step:  15   train loss:  0.02410041354596615  val loss:  0.3882383704185486\n",
      "epoch:  13   step:  16   train loss:  0.04058264195919037  val loss:  0.3911431133747101\n",
      "epoch:  13   step:  17   train loss:  0.05365944653749466  val loss:  0.38949984312057495\n",
      "epoch:  13   step:  18   train loss:  0.04765181988477707  val loss:  0.40049612522125244\n",
      "epoch:  13   step:  19   train loss:  0.04694364592432976  val loss:  0.4001350700855255\n",
      "epoch:  13   step:  20   train loss:  0.05509790778160095  val loss:  0.40102148056030273\n",
      "epoch:  13   step:  21   train loss:  0.04379507899284363  val loss:  0.4000079929828644\n",
      "epoch:  13   step:  22   train loss:  0.09548351168632507  val loss:  0.40549883246421814\n",
      "epoch:  13   step:  23   train loss:  0.045498326420784  val loss:  0.4010288119316101\n",
      "epoch:  13   step:  24   train loss:  0.11769292503595352  val loss:  0.3915340304374695\n",
      "epoch:  13   step:  25   train loss:  0.08108299970626831  val loss:  0.3745805621147156\n",
      "epoch:  13   step:  26   train loss:  0.03459960222244263  val loss:  0.3672093152999878\n",
      "epoch:  13   step:  27   train loss:  0.041267331689596176  val loss:  0.3657356798648834\n",
      "epoch:  13   step:  28   train loss:  0.022190172225236893  val loss:  0.35869133472442627\n",
      "epoch:  13   step:  29   train loss:  0.1062316745519638  val loss:  0.35672086477279663\n",
      "epoch:  13   step:  30   train loss:  0.048683635890483856  val loss:  0.3556344509124756\n",
      "epoch:  13   step:  31   train loss:  0.036188285797834396  val loss:  0.3565540313720703\n",
      "epoch:  13   step:  32   train loss:  0.06225748360157013  val loss:  0.3586199879646301\n",
      "epoch:  13   step:  33   train loss:  0.2232103943824768  val loss:  0.35606881976127625\n",
      "epoch:  13   step:  34   train loss:  0.10994043946266174  val loss:  0.3527586758136749\n",
      "epoch:  13   step:  35   train loss:  0.07227106392383575  val loss:  0.3379741907119751\n",
      "epoch:  13   step:  36   train loss:  0.13905391097068787  val loss:  0.32623979449272156\n",
      "epoch:  13   step:  37   train loss:  0.10974415391683578  val loss:  0.314364492893219\n",
      "epoch:  13   step:  38   train loss:  0.027530644088983536  val loss:  0.3097035586833954\n",
      "epoch:  13   step:  39   train loss:  0.06518477946519852  val loss:  0.30138474702835083\n",
      "epoch:  13   step:  40   train loss:  0.11571994423866272  val loss:  0.2911585867404938\n",
      "epoch:  13   step:  41   train loss:  0.06461883336305618  val loss:  0.2779305577278137\n",
      "epoch:  13   step:  42   train loss:  0.05162364989519119  val loss:  0.2629297077655792\n",
      "epoch:  13   step:  43   train loss:  0.04433412849903107  val loss:  0.25252947211265564\n",
      "epoch:  13   step:  44   train loss:  0.10533785820007324  val loss:  0.2494398057460785\n",
      "epoch:  13   step:  45   train loss:  0.1576794683933258  val loss:  0.24441607296466827\n",
      "epoch:  13   step:  46   train loss:  0.04024365544319153  val loss:  0.24249103665351868\n",
      "epoch:  13   step:  47   train loss:  0.05416649580001831  val loss:  0.24781028926372528\n",
      "epoch:  13   step:  48   train loss:  0.07630733400583267  val loss:  0.25196024775505066\n",
      "epoch:  13   step:  49   train loss:  0.06736265122890472  val loss:  0.25754669308662415\n",
      "epoch:  13   step:  50   train loss:  0.15379951894283295  val loss:  0.26549121737480164\n",
      "epoch:  13   step:  51   train loss:  0.11797139048576355  val loss:  0.2745339572429657\n",
      "epoch:  13   step:  52   train loss:  0.14830061793327332  val loss:  0.2879731357097626\n",
      "epoch:  13   step:  53   train loss:  0.0530124306678772  val loss:  0.3054353594779968\n",
      "epoch:  13   step:  54   train loss:  0.10978692770004272  val loss:  0.3110958933830261\n",
      "epoch:  13   step:  55   train loss:  0.08290994167327881  val loss:  0.30884283781051636\n",
      "epoch:  13   step:  56   train loss:  0.10528923571109772  val loss:  0.304353266954422\n",
      "epoch:  13   step:  57   train loss:  0.05336486175656319  val loss:  0.30593666434288025\n",
      "epoch:  13   step:  58   train loss:  0.08451927453279495  val loss:  0.31499889492988586\n",
      "epoch:  13   step:  59   train loss:  0.11398367583751678  val loss:  0.3412109613418579\n",
      "epoch:  13   step:  60   train loss:  0.07585424929857254  val loss:  0.352752685546875\n",
      "epoch:  13   step:  61   train loss:  0.14501477777957916  val loss:  0.37293747067451477\n",
      "epoch:  13   step:  62   train loss:  0.052604563534259796  val loss:  0.3930860459804535\n",
      "epoch:  13   step:  63   train loss:  0.040375061333179474  val loss:  0.41409581899642944\n",
      "epoch:  13   step:  64   train loss:  0.03186025097966194  val loss:  0.42455634474754333\n",
      "epoch:  13   step:  65   train loss:  0.02129741758108139  val loss:  0.43697813153266907\n",
      "epoch:  13   step:  66   train loss:  0.062130238860845566  val loss:  0.44355544447898865\n",
      "epoch:  13   step:  67   train loss:  0.19817686080932617  val loss:  0.4457288384437561\n",
      "epoch:  13   step:  68   train loss:  0.06314752995967865  val loss:  0.435769647359848\n",
      "epoch:  13   step:  69   train loss:  0.03124038688838482  val loss:  0.4411926865577698\n",
      "epoch:  13   step:  70   train loss:  0.15940837562084198  val loss:  0.44428980350494385\n",
      "epoch:  13   step:  71   train loss:  0.0633232593536377  val loss:  0.4323748052120209\n",
      "epoch:  13   step:  72   train loss:  0.07351722568273544  val loss:  0.41997215151786804\n",
      "epoch:  13   step:  73   train loss:  0.01316402293741703  val loss:  0.4045228064060211\n",
      "epoch:  13   step:  74   train loss:  0.08363206684589386  val loss:  0.39307188987731934\n",
      "epoch:  13   step:  75   train loss:  0.09793327748775482  val loss:  0.38737836480140686\n",
      "epoch:  13   step:  76   train loss:  0.06762588769197464  val loss:  0.3780389726161957\n",
      "epoch:  13   step:  77   train loss:  0.14846916496753693  val loss:  0.36497294902801514\n",
      "epoch:  13   step:  78   train loss:  0.19236159324645996  val loss:  0.362079918384552\n",
      "epoch:  13   step:  79   train loss:  0.03351549804210663  val loss:  0.36348986625671387\n",
      "epoch:  13   step:  80   train loss:  0.05215521901845932  val loss:  0.36540183424949646\n",
      "epoch:  13   step:  81   train loss:  0.06671525537967682  val loss:  0.3686445951461792\n",
      "epoch:  13   step:  82   train loss:  0.0632801279425621  val loss:  0.36782020330429077\n",
      "epoch:  13   step:  83   train loss:  0.030824974179267883  val loss:  0.36681047081947327\n",
      "epoch:  13   step:  84   train loss:  0.08384454250335693  val loss:  0.3639276325702667\n",
      "epoch:  13   step:  85   train loss:  0.25916969776153564  val loss:  0.35802826285362244\n",
      "epoch:  13   step:  86   train loss:  0.04077596589922905  val loss:  0.3521400988101959\n",
      "epoch:  13   step:  87   train loss:  0.08691743016242981  val loss:  0.3420257866382599\n",
      "epoch:  13   step:  88   train loss:  0.09524676948785782  val loss:  0.32964736223220825\n",
      "epoch:  13   step:  89   train loss:  0.05977174639701843  val loss:  0.32465237379074097\n",
      "epoch:  13   step:  90   train loss:  0.11553888767957687  val loss:  0.3167908787727356\n",
      "epoch:  13   step:  91   train loss:  0.09290240705013275  val loss:  0.3190130889415741\n",
      "epoch:  13   step:  92   train loss:  0.11038874089717865  val loss:  0.3236062526702881\n",
      "epoch:  13   step:  93   train loss:  0.12268788367509842  val loss:  0.32699787616729736\n",
      "epoch:  13   step:  94   train loss:  0.15922102332115173  val loss:  0.3295538127422333\n",
      "epoch:  13   step:  95   train loss:  0.11447502672672272  val loss:  0.3304580748081207\n",
      "epoch:  13   step:  96   train loss:  0.0828990638256073  val loss:  0.3296651244163513\n",
      "epoch:  13   step:  97   train loss:  0.06026748940348625  val loss:  0.3347093462944031\n",
      "epoch:  13   step:  98   train loss:  0.05717368423938751  val loss:  0.3422775864601135\n",
      "epoch:  13   step:  99   train loss:  0.05304073542356491  val loss:  0.3447876274585724\n",
      "epoch:  13   step:  100   train loss:  0.16889257729053497  val loss:  0.34003746509552\n",
      "epoch:  13   step:  101   train loss:  0.038352616131305695  val loss:  0.3400563597679138\n",
      "epoch:  13   step:  102   train loss:  0.12489025294780731  val loss:  0.3414231538772583\n",
      "epoch:  13   step:  103   train loss:  0.10759928822517395  val loss:  0.34675332903862\n",
      "epoch:  13   step:  104   train loss:  0.12272277474403381  val loss:  0.3452078104019165\n",
      "epoch:  13   step:  105   train loss:  0.06625940650701523  val loss:  0.3416789174079895\n",
      "epoch:  13   step:  106   train loss:  0.08432882279157639  val loss:  0.3395484387874603\n",
      "epoch:  13   step:  107   train loss:  0.12114115804433823  val loss:  0.3376297652721405\n",
      "epoch:  13   step:  108   train loss:  0.15387199819087982  val loss:  0.3314472436904907\n",
      "epoch:  13   step:  109   train loss:  0.05779440328478813  val loss:  0.3351733386516571\n",
      "epoch:  13   step:  110   train loss:  0.06955386698246002  val loss:  0.3434513509273529\n",
      "epoch:  13   step:  111   train loss:  0.08440907299518585  val loss:  0.35129478573799133\n",
      "epoch:  13   step:  112   train loss:  0.04818670451641083  val loss:  0.36647334694862366\n",
      "epoch:  13   step:  113   train loss:  0.03391295298933983  val loss:  0.37113237380981445\n",
      "epoch:  13   step:  114   train loss:  0.06402712315320969  val loss:  0.38440054655075073\n",
      "epoch:  13   step:  115   train loss:  0.08711495995521545  val loss:  0.397067129611969\n",
      "epoch:  13   step:  116   train loss:  0.06739829480648041  val loss:  0.39970847964286804\n",
      "epoch:  13   step:  117   train loss:  0.06643036007881165  val loss:  0.4042939245700836\n",
      "epoch:  13   step:  118   train loss:  0.03196806460618973  val loss:  0.40152502059936523\n",
      "epoch:  13   step:  119   train loss:  0.06089596077799797  val loss:  0.3948300778865814\n",
      "epoch:  13   step:  120   train loss:  0.04807131737470627  val loss:  0.38813433051109314\n",
      "epoch:  13   step:  121   train loss:  0.05296633392572403  val loss:  0.3797425329685211\n",
      "epoch:  13   step:  122   train loss:  0.04482126235961914  val loss:  0.37058740854263306\n",
      "epoch:  13   step:  123   train loss:  0.11268258094787598  val loss:  0.37065282464027405\n",
      "epoch:  13   step:  124   train loss:  0.021765954792499542  val loss:  0.373248815536499\n",
      "epoch:  13   step:  125   train loss:  0.06645110249519348  val loss:  0.37361183762550354\n",
      "epoch:  13   step:  126   train loss:  0.10196547210216522  val loss:  0.3709526062011719\n",
      "epoch:  13   step:  127   train loss:  0.06656130403280258  val loss:  0.371056467294693\n",
      "epoch:  13   step:  128   train loss:  0.034463874995708466  val loss:  0.37245967984199524\n",
      "epoch:  13   step:  129   train loss:  0.11114038527011871  val loss:  0.37049010396003723\n",
      "epoch:  13   step:  130   train loss:  0.07326348125934601  val loss:  0.37054792046546936\n",
      "epoch:  13   step:  131   train loss:  0.1237490251660347  val loss:  0.3611118495464325\n",
      "epoch:  13   step:  132   train loss:  0.2537487745285034  val loss:  0.3605654835700989\n",
      "epoch:  13   step:  133   train loss:  0.031248124316334724  val loss:  0.35810577869415283\n",
      "epoch:  13   step:  134   train loss:  0.13437488675117493  val loss:  0.35238897800445557\n",
      "epoch:  13   step:  135   train loss:  0.05379251763224602  val loss:  0.3451092541217804\n",
      "epoch:  13   step:  136   train loss:  0.16603747010231018  val loss:  0.33982276916503906\n",
      "epoch:  13   step:  137   train loss:  0.030499985441565514  val loss:  0.3324046730995178\n",
      "epoch:  13   step:  138   train loss:  0.019625762477517128  val loss:  0.3259921073913574\n",
      "epoch:  13   step:  139   train loss:  0.10971835255622864  val loss:  0.31868743896484375\n",
      "epoch:  13   step:  140   train loss:  0.027024248614907265  val loss:  0.32126280665397644\n",
      "epoch:  13   step:  141   train loss:  0.1962626576423645  val loss:  0.32171374559402466\n",
      "epoch:  13   step:  142   train loss:  0.1260511428117752  val loss:  0.3124471604824066\n",
      "epoch:  13   step:  143   train loss:  0.10190508514642715  val loss:  0.29917705059051514\n",
      "epoch:  13   step:  144   train loss:  0.07822449505329132  val loss:  0.2967797517776489\n",
      "epoch:  13   step:  145   train loss:  0.06900916993618011  val loss:  0.3060178756713867\n",
      "epoch:  13   step:  146   train loss:  0.04737522825598717  val loss:  0.3276626169681549\n",
      "epoch:  13   step:  147   train loss:  0.23245510458946228  val loss:  0.33638373017311096\n",
      "epoch:  13   step:  148   train loss:  0.050668131560087204  val loss:  0.3426341116428375\n",
      "epoch:  13   step:  149   train loss:  0.036859504878520966  val loss:  0.342617928981781\n",
      "epoch:  13   step:  150   train loss:  0.15723177790641785  val loss:  0.32358217239379883\n",
      "epoch:  13   step:  151   train loss:  0.18060146272182465  val loss:  0.31537529826164246\n",
      "epoch:  13   step:  152   train loss:  0.1457637995481491  val loss:  0.31467893719673157\n",
      "epoch:  13   step:  153   train loss:  0.24756035208702087  val loss:  0.33230727910995483\n",
      "epoch:  13   step:  154   train loss:  0.21429777145385742  val loss:  0.35112395882606506\n",
      "epoch:  13   step:  155   train loss:  0.15495015680789948  val loss:  0.3584475517272949\n",
      "epoch:  13   step:  156   train loss:  0.1867387294769287  val loss:  0.36435720324516296\n",
      "epoch:  13   step:  157   train loss:  0.08063843846321106  val loss:  0.37749066948890686\n",
      "epoch:  13   step:  158   train loss:  0.11300057172775269  val loss:  0.3872140347957611\n",
      "epoch:  13   step:  159   train loss:  0.27067649364471436  val loss:  0.4055609107017517\n",
      "epoch:  13   step:  160   train loss:  0.11151505261659622  val loss:  0.4189164340496063\n",
      "epoch:  13   step:  161   train loss:  0.11664266884326935  val loss:  0.433999627828598\n",
      "epoch:  13   step:  162   train loss:  0.10130836814641953  val loss:  0.4445544481277466\n",
      "epoch:  13   step:  163   train loss:  0.086468905210495  val loss:  0.46163031458854675\n",
      "epoch:  13   step:  164   train loss:  0.23021501302719116  val loss:  0.484581857919693\n",
      "epoch:  13   step:  165   train loss:  0.17762534320354462  val loss:  0.4737468957901001\n",
      "epoch:  14   step:  0   train loss:  0.11208944767713547  val loss:  0.4653162956237793\n",
      "epoch:  14   step:  1   train loss:  0.041659750044345856  val loss:  0.4630263149738312\n",
      "epoch:  14   step:  2   train loss:  0.08805419504642487  val loss:  0.4506870210170746\n",
      "epoch:  14   step:  3   train loss:  0.024886762723326683  val loss:  0.45260706543922424\n",
      "epoch:  14   step:  4   train loss:  0.08386540412902832  val loss:  0.44640064239501953\n",
      "epoch:  14   step:  5   train loss:  0.037141069769859314  val loss:  0.4351544976234436\n",
      "epoch:  14   step:  6   train loss:  0.1165892705321312  val loss:  0.4340568780899048\n",
      "epoch:  14   step:  7   train loss:  0.09096482396125793  val loss:  0.4232421815395355\n",
      "epoch:  14   step:  8   train loss:  0.24916720390319824  val loss:  0.41491392254829407\n",
      "epoch:  14   step:  9   train loss:  0.15519297122955322  val loss:  0.40411725640296936\n",
      "epoch:  14   step:  10   train loss:  0.23486772179603577  val loss:  0.3974640667438507\n",
      "epoch:  14   step:  11   train loss:  0.09396816045045853  val loss:  0.391101211309433\n",
      "epoch:  14   step:  12   train loss:  0.0772581398487091  val loss:  0.3763222396373749\n",
      "epoch:  14   step:  13   train loss:  0.14946874976158142  val loss:  0.3644126057624817\n",
      "epoch:  14   step:  14   train loss:  0.0997818261384964  val loss:  0.36211633682250977\n",
      "epoch:  14   step:  15   train loss:  0.037436358630657196  val loss:  0.3551570773124695\n",
      "epoch:  14   step:  16   train loss:  0.09367553144693375  val loss:  0.35339564085006714\n",
      "epoch:  14   step:  17   train loss:  0.2564464509487152  val loss:  0.35524797439575195\n",
      "epoch:  14   step:  18   train loss:  0.0389564149081707  val loss:  0.37386247515678406\n",
      "epoch:  14   step:  19   train loss:  0.04808087646961212  val loss:  0.37758708000183105\n",
      "epoch:  14   step:  20   train loss:  0.13041983544826508  val loss:  0.3856119215488434\n",
      "epoch:  14   step:  21   train loss:  0.07884363830089569  val loss:  0.387246310710907\n",
      "epoch:  14   step:  22   train loss:  0.04693775624036789  val loss:  0.39488330483436584\n",
      "epoch:  14   step:  23   train loss:  0.18828678131103516  val loss:  0.38895273208618164\n",
      "epoch:  14   step:  24   train loss:  0.1016693040728569  val loss:  0.37009021639823914\n",
      "epoch:  14   step:  25   train loss:  0.1114407628774643  val loss:  0.34826597571372986\n",
      "epoch:  14   step:  26   train loss:  0.09883280098438263  val loss:  0.3249530792236328\n",
      "epoch:  14   step:  27   train loss:  0.08055223524570465  val loss:  0.322447270154953\n",
      "epoch:  14   step:  28   train loss:  0.06721305847167969  val loss:  0.33700209856033325\n",
      "epoch:  14   step:  29   train loss:  0.058082740753889084  val loss:  0.367710143327713\n",
      "epoch:  14   step:  30   train loss:  0.3986344337463379  val loss:  0.34745046496391296\n",
      "epoch:  14   step:  31   train loss:  0.089503213763237  val loss:  0.33417049050331116\n",
      "epoch:  14   step:  32   train loss:  0.15380950272083282  val loss:  0.3233034014701843\n",
      "epoch:  14   step:  33   train loss:  0.07517381012439728  val loss:  0.3301514983177185\n",
      "epoch:  14   step:  34   train loss:  0.06324004381895065  val loss:  0.3538288474082947\n",
      "epoch:  14   step:  35   train loss:  0.1885940134525299  val loss:  0.37707868218421936\n",
      "epoch:  14   step:  36   train loss:  0.07795499265193939  val loss:  0.38863226771354675\n",
      "epoch:  14   step:  37   train loss:  0.07690763473510742  val loss:  0.4002015292644501\n",
      "epoch:  14   step:  38   train loss:  0.1939966082572937  val loss:  0.39136871695518494\n",
      "epoch:  14   step:  39   train loss:  0.11034101247787476  val loss:  0.37976181507110596\n",
      "epoch:  14   step:  40   train loss:  0.16936834156513214  val loss:  0.369272381067276\n",
      "epoch:  14   step:  41   train loss:  0.033715296536684036  val loss:  0.37364012002944946\n",
      "epoch:  14   step:  42   train loss:  0.10155023634433746  val loss:  0.36128172278404236\n",
      "epoch:  14   step:  43   train loss:  0.06287084519863129  val loss:  0.3573111593723297\n",
      "epoch:  14   step:  44   train loss:  0.052614714950323105  val loss:  0.36556142568588257\n",
      "epoch:  14   step:  45   train loss:  0.04387543350458145  val loss:  0.37350764870643616\n",
      "epoch:  14   step:  46   train loss:  0.09307743608951569  val loss:  0.38388922810554504\n",
      "epoch:  14   step:  47   train loss:  0.1541176736354828  val loss:  0.41054224967956543\n",
      "epoch:  14   step:  48   train loss:  0.06451915204524994  val loss:  0.42194342613220215\n",
      "epoch:  14   step:  49   train loss:  0.07221885770559311  val loss:  0.43237513303756714\n",
      "epoch:  14   step:  50   train loss:  0.024928942322731018  val loss:  0.4315737187862396\n",
      "epoch:  14   step:  51   train loss:  0.12901359796524048  val loss:  0.42785897850990295\n",
      "epoch:  14   step:  52   train loss:  0.07630076259374619  val loss:  0.4101811945438385\n",
      "epoch:  14   step:  53   train loss:  0.07191474735736847  val loss:  0.41418638825416565\n",
      "epoch:  14   step:  54   train loss:  0.0758242979645729  val loss:  0.40920311212539673\n",
      "epoch:  14   step:  55   train loss:  0.08222179114818573  val loss:  0.39844873547554016\n",
      "epoch:  14   step:  56   train loss:  0.08438661694526672  val loss:  0.40599989891052246\n",
      "epoch:  14   step:  57   train loss:  0.06726302206516266  val loss:  0.4183817505836487\n",
      "epoch:  14   step:  58   train loss:  0.018251195549964905  val loss:  0.41819626092910767\n",
      "epoch:  14   step:  59   train loss:  0.147793710231781  val loss:  0.41164466738700867\n",
      "epoch:  14   step:  60   train loss:  0.04746456816792488  val loss:  0.4174724221229553\n",
      "epoch:  14   step:  61   train loss:  0.02430756576359272  val loss:  0.42237356305122375\n",
      "epoch:  14   step:  62   train loss:  0.04562373459339142  val loss:  0.41571810841560364\n",
      "epoch:  14   step:  63   train loss:  0.037951454520225525  val loss:  0.4079037010669708\n",
      "epoch:  14   step:  64   train loss:  0.10130403935909271  val loss:  0.402942419052124\n",
      "epoch:  14   step:  65   train loss:  0.04890533164143562  val loss:  0.38585081696510315\n",
      "epoch:  14   step:  66   train loss:  0.03697790950536728  val loss:  0.3783192038536072\n",
      "epoch:  14   step:  67   train loss:  0.04162196069955826  val loss:  0.37744274735450745\n",
      "epoch:  14   step:  68   train loss:  0.07363221049308777  val loss:  0.37659555673599243\n",
      "epoch:  14   step:  69   train loss:  0.05643676221370697  val loss:  0.3782634139060974\n",
      "epoch:  14   step:  70   train loss:  0.035237498581409454  val loss:  0.3758828938007355\n",
      "epoch:  14   step:  71   train loss:  0.0633847787976265  val loss:  0.37115225195884705\n",
      "epoch:  14   step:  72   train loss:  0.09686785936355591  val loss:  0.3637922704219818\n",
      "epoch:  14   step:  73   train loss:  0.020457390695810318  val loss:  0.3639897406101227\n",
      "epoch:  14   step:  74   train loss:  0.09462008625268936  val loss:  0.36612048745155334\n",
      "epoch:  14   step:  75   train loss:  0.03409218788146973  val loss:  0.3621085286140442\n",
      "epoch:  14   step:  76   train loss:  0.056711290031671524  val loss:  0.36359018087387085\n",
      "epoch:  14   step:  77   train loss:  0.03803107514977455  val loss:  0.36116302013397217\n",
      "epoch:  14   step:  78   train loss:  0.041981883347034454  val loss:  0.36283445358276367\n",
      "epoch:  14   step:  79   train loss:  0.050317175686359406  val loss:  0.3648049533367157\n",
      "epoch:  14   step:  80   train loss:  0.04103734344244003  val loss:  0.3690403699874878\n",
      "epoch:  14   step:  81   train loss:  0.02968505024909973  val loss:  0.3737031817436218\n",
      "epoch:  14   step:  82   train loss:  0.22235199809074402  val loss:  0.3741269111633301\n",
      "epoch:  14   step:  83   train loss:  0.04175802692770958  val loss:  0.37693876028060913\n",
      "epoch:  14   step:  84   train loss:  0.1335904896259308  val loss:  0.38707834482192993\n",
      "epoch:  14   step:  85   train loss:  0.015608935616910458  val loss:  0.39216798543930054\n",
      "epoch:  14   step:  86   train loss:  0.039428748190402985  val loss:  0.3832229673862457\n",
      "epoch:  14   step:  87   train loss:  0.03687246888875961  val loss:  0.37835073471069336\n",
      "epoch:  14   step:  88   train loss:  0.01498458907008171  val loss:  0.3717002272605896\n",
      "epoch:  14   step:  89   train loss:  0.0799635797739029  val loss:  0.3649170696735382\n",
      "epoch:  14   step:  90   train loss:  0.03709923103451729  val loss:  0.3621619641780853\n",
      "epoch:  14   step:  91   train loss:  0.05765121430158615  val loss:  0.3547673225402832\n",
      "epoch:  14   step:  92   train loss:  0.0337006077170372  val loss:  0.35147589445114136\n",
      "epoch:  14   step:  93   train loss:  0.09115076810121536  val loss:  0.34013575315475464\n",
      "epoch:  14   step:  94   train loss:  0.025628844276070595  val loss:  0.32577410340309143\n",
      "epoch:  14   step:  95   train loss:  0.0911550223827362  val loss:  0.3287550210952759\n",
      "epoch:  14   step:  96   train loss:  0.11457706987857819  val loss:  0.3324093222618103\n",
      "epoch:  14   step:  97   train loss:  0.032992880791425705  val loss:  0.33054229617118835\n",
      "epoch:  14   step:  98   train loss:  0.16667453944683075  val loss:  0.33998382091522217\n",
      "epoch:  14   step:  99   train loss:  0.026306137442588806  val loss:  0.35336828231811523\n",
      "epoch:  14   step:  100   train loss:  0.06681983172893524  val loss:  0.37673893570899963\n",
      "epoch:  14   step:  101   train loss:  0.018841926008462906  val loss:  0.39674314856529236\n",
      "epoch:  14   step:  102   train loss:  0.1558288335800171  val loss:  0.40802279114723206\n",
      "epoch:  14   step:  103   train loss:  0.0874372273683548  val loss:  0.41304248571395874\n",
      "epoch:  14   step:  104   train loss:  0.01666303351521492  val loss:  0.41088324785232544\n",
      "epoch:  14   step:  105   train loss:  0.10256385803222656  val loss:  0.40175577998161316\n",
      "epoch:  14   step:  106   train loss:  0.053694240748882294  val loss:  0.3942098915576935\n",
      "epoch:  14   step:  107   train loss:  0.03000093251466751  val loss:  0.3921971917152405\n",
      "epoch:  14   step:  108   train loss:  0.035197220742702484  val loss:  0.39525943994522095\n",
      "epoch:  14   step:  109   train loss:  0.2796002924442291  val loss:  0.3958560824394226\n",
      "epoch:  14   step:  110   train loss:  0.09799643605947495  val loss:  0.3997463583946228\n",
      "epoch:  14   step:  111   train loss:  0.1997058391571045  val loss:  0.38642555475234985\n",
      "epoch:  14   step:  112   train loss:  0.10599688440561295  val loss:  0.3748907744884491\n",
      "epoch:  14   step:  113   train loss:  0.06636513769626617  val loss:  0.3722407817840576\n",
      "epoch:  14   step:  114   train loss:  0.052706584334373474  val loss:  0.37022966146469116\n",
      "epoch:  14   step:  115   train loss:  0.04228819161653519  val loss:  0.37657520174980164\n",
      "epoch:  14   step:  116   train loss:  0.02755705825984478  val loss:  0.3914549648761749\n",
      "epoch:  14   step:  117   train loss:  0.029321014881134033  val loss:  0.40928229689598083\n",
      "epoch:  14   step:  118   train loss:  0.12211796641349792  val loss:  0.4312232732772827\n",
      "epoch:  14   step:  119   train loss:  0.04161106050014496  val loss:  0.4331713914871216\n",
      "epoch:  14   step:  120   train loss:  0.05917888879776001  val loss:  0.43344777822494507\n",
      "epoch:  14   step:  121   train loss:  0.06404479593038559  val loss:  0.4383409023284912\n",
      "epoch:  14   step:  122   train loss:  0.06554174423217773  val loss:  0.43214067816734314\n",
      "epoch:  14   step:  123   train loss:  0.06047048792243004  val loss:  0.4154653549194336\n",
      "epoch:  14   step:  124   train loss:  0.06847779452800751  val loss:  0.4105517566204071\n",
      "epoch:  14   step:  125   train loss:  0.049015242606401443  val loss:  0.4006807208061218\n",
      "epoch:  14   step:  126   train loss:  0.06213967129588127  val loss:  0.3883007764816284\n",
      "epoch:  14   step:  127   train loss:  0.0383562408387661  val loss:  0.38512441515922546\n",
      "epoch:  14   step:  128   train loss:  0.05587897077202797  val loss:  0.3901377022266388\n",
      "epoch:  14   step:  129   train loss:  0.05658189579844475  val loss:  0.3926751911640167\n",
      "epoch:  14   step:  130   train loss:  0.0265819001942873  val loss:  0.4069752097129822\n",
      "epoch:  14   step:  131   train loss:  0.0377851203083992  val loss:  0.4101948142051697\n",
      "epoch:  14   step:  132   train loss:  0.3073193430900574  val loss:  0.40293627977371216\n",
      "epoch:  14   step:  133   train loss:  0.028574500232934952  val loss:  0.3992789089679718\n",
      "epoch:  14   step:  134   train loss:  0.047975845634937286  val loss:  0.39813894033432007\n",
      "epoch:  14   step:  135   train loss:  0.0514395609498024  val loss:  0.40087562799453735\n",
      "epoch:  14   step:  136   train loss:  0.03080335445702076  val loss:  0.40221917629241943\n",
      "epoch:  14   step:  137   train loss:  0.07376714050769806  val loss:  0.39552512764930725\n",
      "epoch:  14   step:  138   train loss:  0.03917236626148224  val loss:  0.38513991236686707\n",
      "epoch:  14   step:  139   train loss:  0.03937685862183571  val loss:  0.3832233250141144\n",
      "epoch:  14   step:  140   train loss:  0.09972557425498962  val loss:  0.3731938898563385\n",
      "epoch:  14   step:  141   train loss:  0.13388711214065552  val loss:  0.37557071447372437\n",
      "epoch:  14   step:  142   train loss:  0.03624992445111275  val loss:  0.3808569610118866\n",
      "epoch:  14   step:  143   train loss:  0.021639671176671982  val loss:  0.3817475140094757\n",
      "epoch:  14   step:  144   train loss:  0.12147368490695953  val loss:  0.37860384583473206\n",
      "epoch:  14   step:  145   train loss:  0.038785532116889954  val loss:  0.3814244270324707\n",
      "epoch:  14   step:  146   train loss:  0.04323006793856621  val loss:  0.3814472556114197\n",
      "epoch:  14   step:  147   train loss:  0.14083194732666016  val loss:  0.38647788763046265\n",
      "epoch:  14   step:  148   train loss:  0.06809025257825851  val loss:  0.3832494914531708\n",
      "epoch:  14   step:  149   train loss:  0.04503188282251358  val loss:  0.3853290379047394\n",
      "epoch:  14   step:  150   train loss:  0.04325138404965401  val loss:  0.38530629873275757\n",
      "epoch:  14   step:  151   train loss:  0.12441091984510422  val loss:  0.38689103722572327\n",
      "epoch:  14   step:  152   train loss:  0.05916266888380051  val loss:  0.38599878549575806\n",
      "epoch:  14   step:  153   train loss:  0.035469137132167816  val loss:  0.38293036818504333\n",
      "epoch:  14   step:  154   train loss:  0.024221889674663544  val loss:  0.3799489438533783\n",
      "epoch:  14   step:  155   train loss:  0.03876662254333496  val loss:  0.38428157567977905\n",
      "epoch:  14   step:  156   train loss:  0.052856191992759705  val loss:  0.39266136288642883\n",
      "epoch:  14   step:  157   train loss:  0.12634751200675964  val loss:  0.39664512872695923\n",
      "epoch:  14   step:  158   train loss:  0.10535825043916702  val loss:  0.3943134844303131\n",
      "epoch:  14   step:  159   train loss:  0.06298145651817322  val loss:  0.4005959630012512\n",
      "epoch:  14   step:  160   train loss:  0.06738458573818207  val loss:  0.3982689082622528\n",
      "epoch:  14   step:  161   train loss:  0.11536815017461777  val loss:  0.38472938537597656\n",
      "epoch:  14   step:  162   train loss:  0.11967002600431442  val loss:  0.3777458369731903\n",
      "epoch:  14   step:  163   train loss:  0.053146760910749435  val loss:  0.3723255395889282\n",
      "epoch:  14   step:  164   train loss:  0.05097537115216255  val loss:  0.36226505041122437\n",
      "epoch:  14   step:  165   train loss:  0.04101188853383064  val loss:  0.3494599461555481\n",
      "epoch:  15   step:  0   train loss:  0.034280598163604736  val loss:  0.3359133005142212\n",
      "epoch:  15   step:  1   train loss:  0.05706220492720604  val loss:  0.3270600140094757\n",
      "epoch:  15   step:  2   train loss:  0.032579679042100906  val loss:  0.3225170373916626\n",
      "epoch:  15   step:  3   train loss:  0.015660110861063004  val loss:  0.3275202810764313\n",
      "epoch:  15   step:  4   train loss:  0.06511329859495163  val loss:  0.33251670002937317\n",
      "epoch:  15   step:  5   train loss:  0.028545983135700226  val loss:  0.34257200360298157\n",
      "epoch:  15   step:  6   train loss:  0.036781758069992065  val loss:  0.3531420826911926\n",
      "epoch:  15   step:  7   train loss:  0.050162382423877716  val loss:  0.36454489827156067\n",
      "epoch:  15   step:  8   train loss:  0.01301516592502594  val loss:  0.37248802185058594\n",
      "epoch:  15   step:  9   train loss:  0.021104518324136734  val loss:  0.38103893399238586\n",
      "epoch:  15   step:  10   train loss:  0.08918978273868561  val loss:  0.39497897028923035\n",
      "epoch:  15   step:  11   train loss:  0.04390232637524605  val loss:  0.39837226271629333\n",
      "epoch:  15   step:  12   train loss:  0.045890819281339645  val loss:  0.4071492552757263\n",
      "epoch:  15   step:  13   train loss:  0.014231255277991295  val loss:  0.4165709614753723\n",
      "epoch:  15   step:  14   train loss:  0.036301858723163605  val loss:  0.4181690216064453\n",
      "epoch:  15   step:  15   train loss:  0.14424823224544525  val loss:  0.41546308994293213\n",
      "epoch:  15   step:  16   train loss:  0.09536100178956985  val loss:  0.4187665581703186\n",
      "epoch:  15   step:  17   train loss:  0.06428448110818863  val loss:  0.42462438344955444\n",
      "epoch:  15   step:  18   train loss:  0.020232759416103363  val loss:  0.43175506591796875\n",
      "epoch:  15   step:  19   train loss:  0.04148022085428238  val loss:  0.435565710067749\n",
      "epoch:  15   step:  20   train loss:  0.016934717074036598  val loss:  0.4402678608894348\n",
      "epoch:  15   step:  21   train loss:  0.026295963674783707  val loss:  0.4438875615596771\n",
      "epoch:  15   step:  22   train loss:  0.022999046370387077  val loss:  0.44498732686042786\n",
      "epoch:  15   step:  23   train loss:  0.033614397048950195  val loss:  0.4463965594768524\n",
      "epoch:  15   step:  24   train loss:  0.012671999633312225  val loss:  0.447753369808197\n",
      "epoch:  15   step:  25   train loss:  0.03151431307196617  val loss:  0.44348323345184326\n",
      "epoch:  15   step:  26   train loss:  0.05469999834895134  val loss:  0.43050551414489746\n",
      "epoch:  15   step:  27   train loss:  0.04030976444482803  val loss:  0.41356685757637024\n",
      "epoch:  15   step:  28   train loss:  0.07654159516096115  val loss:  0.3969976603984833\n",
      "epoch:  15   step:  29   train loss:  0.02618764340877533  val loss:  0.3850908577442169\n",
      "epoch:  15   step:  30   train loss:  0.01231762208044529  val loss:  0.37691769003868103\n",
      "epoch:  15   step:  31   train loss:  0.02248111180961132  val loss:  0.36629050970077515\n",
      "epoch:  15   step:  32   train loss:  0.09952933341264725  val loss:  0.35311710834503174\n",
      "epoch:  15   step:  33   train loss:  0.02549264021217823  val loss:  0.34529951214790344\n",
      "epoch:  15   step:  34   train loss:  0.05177796632051468  val loss:  0.3517468273639679\n",
      "epoch:  15   step:  35   train loss:  0.04541024565696716  val loss:  0.359962522983551\n",
      "epoch:  15   step:  36   train loss:  0.014924490824341774  val loss:  0.36438170075416565\n",
      "epoch:  15   step:  37   train loss:  0.028954144567251205  val loss:  0.3667899966239929\n",
      "epoch:  15   step:  38   train loss:  0.12303110957145691  val loss:  0.3760784864425659\n",
      "epoch:  15   step:  39   train loss:  0.1369435340166092  val loss:  0.3985215723514557\n",
      "epoch:  15   step:  40   train loss:  0.026920586824417114  val loss:  0.41535529494285583\n",
      "epoch:  15   step:  41   train loss:  0.08856035023927689  val loss:  0.42174386978149414\n",
      "epoch:  15   step:  42   train loss:  0.1024271696805954  val loss:  0.415985643863678\n",
      "epoch:  15   step:  43   train loss:  0.03071589767932892  val loss:  0.41125383973121643\n",
      "epoch:  15   step:  44   train loss:  0.0655156597495079  val loss:  0.40885788202285767\n",
      "epoch:  15   step:  45   train loss:  0.017668575048446655  val loss:  0.40931224822998047\n",
      "epoch:  15   step:  46   train loss:  0.02861209586262703  val loss:  0.4064532518386841\n",
      "epoch:  15   step:  47   train loss:  0.015798471868038177  val loss:  0.4031665325164795\n",
      "epoch:  15   step:  48   train loss:  0.07511429488658905  val loss:  0.398878276348114\n",
      "epoch:  15   step:  49   train loss:  0.034262754023075104  val loss:  0.39569300413131714\n",
      "epoch:  15   step:  50   train loss:  0.10352145880460739  val loss:  0.393061101436615\n",
      "epoch:  15   step:  51   train loss:  0.12311018258333206  val loss:  0.37879836559295654\n",
      "epoch:  15   step:  52   train loss:  0.03136631101369858  val loss:  0.37064114212989807\n",
      "epoch:  15   step:  53   train loss:  0.029490383341908455  val loss:  0.3751564621925354\n",
      "epoch:  15   step:  54   train loss:  0.06333722174167633  val loss:  0.3846050202846527\n",
      "epoch:  15   step:  55   train loss:  0.04015878587961197  val loss:  0.3885066509246826\n",
      "epoch:  15   step:  56   train loss:  0.04266740009188652  val loss:  0.3904690146446228\n",
      "epoch:  15   step:  57   train loss:  0.051554154604673386  val loss:  0.3920823931694031\n",
      "epoch:  15   step:  58   train loss:  0.029181843623518944  val loss:  0.39946791529655457\n",
      "epoch:  15   step:  59   train loss:  0.01065940223634243  val loss:  0.39944037795066833\n",
      "epoch:  15   step:  60   train loss:  0.056287094950675964  val loss:  0.40511995553970337\n",
      "epoch:  15   step:  61   train loss:  0.013784905895590782  val loss:  0.40773704648017883\n",
      "epoch:  15   step:  62   train loss:  0.02797868847846985  val loss:  0.4218897521495819\n",
      "epoch:  15   step:  63   train loss:  0.030336584895849228  val loss:  0.42073842883110046\n",
      "epoch:  15   step:  64   train loss:  0.01759776473045349  val loss:  0.4256081283092499\n",
      "epoch:  15   step:  65   train loss:  0.04368726164102554  val loss:  0.4292908310890198\n",
      "epoch:  15   step:  66   train loss:  0.020907852798700333  val loss:  0.4330001771450043\n",
      "epoch:  15   step:  67   train loss:  0.02051418647170067  val loss:  0.4375300109386444\n",
      "epoch:  15   step:  68   train loss:  0.06918998062610626  val loss:  0.4377566874027252\n",
      "epoch:  15   step:  69   train loss:  0.021387502551078796  val loss:  0.43905797600746155\n",
      "epoch:  15   step:  70   train loss:  0.06333231180906296  val loss:  0.4488353133201599\n",
      "epoch:  15   step:  71   train loss:  0.03787016123533249  val loss:  0.4539736211299896\n",
      "epoch:  15   step:  72   train loss:  0.06632066518068314  val loss:  0.46419620513916016\n",
      "epoch:  15   step:  73   train loss:  0.016260642558336258  val loss:  0.46472781896591187\n",
      "epoch:  15   step:  74   train loss:  0.010241938754916191  val loss:  0.4564332067966461\n",
      "epoch:  15   step:  75   train loss:  0.032835375517606735  val loss:  0.4550093114376068\n",
      "epoch:  15   step:  76   train loss:  0.019206304103136063  val loss:  0.46040862798690796\n",
      "epoch:  15   step:  77   train loss:  0.012816781178116798  val loss:  0.46320289373397827\n",
      "epoch:  15   step:  78   train loss:  0.019640754908323288  val loss:  0.4583059847354889\n",
      "epoch:  15   step:  79   train loss:  0.051870718598365784  val loss:  0.4570874273777008\n",
      "epoch:  15   step:  80   train loss:  0.03484227508306503  val loss:  0.4618663787841797\n",
      "epoch:  15   step:  81   train loss:  0.030999118462204933  val loss:  0.4602751135826111\n",
      "epoch:  15   step:  82   train loss:  0.05914445221424103  val loss:  0.45899105072021484\n",
      "epoch:  15   step:  83   train loss:  0.0223392266780138  val loss:  0.44525113701820374\n",
      "epoch:  15   step:  84   train loss:  0.01581917516887188  val loss:  0.4427036643028259\n",
      "epoch:  15   step:  85   train loss:  0.12805107235908508  val loss:  0.4287966787815094\n",
      "epoch:  15   step:  86   train loss:  0.0319431871175766  val loss:  0.4216315448284149\n",
      "epoch:  15   step:  87   train loss:  0.016115043312311172  val loss:  0.41713857650756836\n",
      "epoch:  15   step:  88   train loss:  0.008911289274692535  val loss:  0.41284415125846863\n",
      "epoch:  15   step:  89   train loss:  0.03816499561071396  val loss:  0.40543001890182495\n",
      "epoch:  15   step:  90   train loss:  0.034327130764722824  val loss:  0.39864981174468994\n",
      "epoch:  15   step:  91   train loss:  0.05623389035463333  val loss:  0.3873690366744995\n",
      "epoch:  15   step:  92   train loss:  0.015972139313817024  val loss:  0.38518932461738586\n",
      "epoch:  15   step:  93   train loss:  0.08374886959791183  val loss:  0.3758472502231598\n",
      "epoch:  15   step:  94   train loss:  0.026987675577402115  val loss:  0.3686504364013672\n",
      "epoch:  15   step:  95   train loss:  0.07549828290939331  val loss:  0.36448386311531067\n",
      "epoch:  15   step:  96   train loss:  0.04579887539148331  val loss:  0.3658074140548706\n",
      "epoch:  15   step:  97   train loss:  0.02876359224319458  val loss:  0.3767183721065521\n",
      "epoch:  15   step:  98   train loss:  0.017952345311641693  val loss:  0.3858799338340759\n",
      "epoch:  15   step:  99   train loss:  0.1518859714269638  val loss:  0.40936049818992615\n",
      "epoch:  15   step:  100   train loss:  0.019549982622265816  val loss:  0.4271656572818756\n",
      "epoch:  15   step:  101   train loss:  0.0376763679087162  val loss:  0.44898876547813416\n",
      "epoch:  15   step:  102   train loss:  0.0361441932618618  val loss:  0.4688771367073059\n",
      "epoch:  15   step:  103   train loss:  0.23002628982067108  val loss:  0.45490559935569763\n",
      "epoch:  15   step:  104   train loss:  0.052662406116724014  val loss:  0.4474192261695862\n",
      "epoch:  15   step:  105   train loss:  0.02467048168182373  val loss:  0.4501584768295288\n",
      "epoch:  15   step:  106   train loss:  0.042038388550281525  val loss:  0.4566672146320343\n",
      "epoch:  15   step:  107   train loss:  0.017330573871731758  val loss:  0.4661765992641449\n",
      "epoch:  15   step:  108   train loss:  0.07615890353918076  val loss:  0.4741588234901428\n",
      "epoch:  15   step:  109   train loss:  0.0345289409160614  val loss:  0.4811735451221466\n",
      "epoch:  15   step:  110   train loss:  0.1245393306016922  val loss:  0.4785330891609192\n",
      "epoch:  15   step:  111   train loss:  0.1071351170539856  val loss:  0.4792232811450958\n",
      "epoch:  15   step:  112   train loss:  0.03101305291056633  val loss:  0.48594337701797485\n",
      "epoch:  15   step:  113   train loss:  0.037832409143447876  val loss:  0.4948035776615143\n",
      "epoch:  15   step:  114   train loss:  0.10308312624692917  val loss:  0.504127025604248\n",
      "epoch:  15   step:  115   train loss:  0.03322252631187439  val loss:  0.5063477754592896\n",
      "epoch:  15   step:  116   train loss:  0.10408017784357071  val loss:  0.5040108561515808\n",
      "epoch:  15   step:  117   train loss:  0.00730703491717577  val loss:  0.4987025260925293\n",
      "epoch:  15   step:  118   train loss:  0.04291026294231415  val loss:  0.4931921362876892\n",
      "epoch:  15   step:  119   train loss:  0.02660059928894043  val loss:  0.4878006875514984\n",
      "epoch:  15   step:  120   train loss:  0.16686350107192993  val loss:  0.46528705954551697\n",
      "epoch:  15   step:  121   train loss:  0.024567868560552597  val loss:  0.45268115401268005\n",
      "epoch:  15   step:  122   train loss:  0.02739594504237175  val loss:  0.4411909282207489\n",
      "epoch:  15   step:  123   train loss:  0.03460489213466644  val loss:  0.428737998008728\n",
      "epoch:  15   step:  124   train loss:  0.10392428934574127  val loss:  0.4262999892234802\n",
      "epoch:  15   step:  125   train loss:  0.038267821073532104  val loss:  0.4164198935031891\n",
      "epoch:  15   step:  126   train loss:  0.036889802664518356  val loss:  0.4144464135169983\n",
      "epoch:  15   step:  127   train loss:  0.11892295628786087  val loss:  0.4248194694519043\n",
      "epoch:  15   step:  128   train loss:  0.040637582540512085  val loss:  0.4339836835861206\n",
      "epoch:  15   step:  129   train loss:  0.014583433046936989  val loss:  0.4433484673500061\n",
      "epoch:  15   step:  130   train loss:  0.013485131785273552  val loss:  0.4577176868915558\n",
      "epoch:  15   step:  131   train loss:  0.01958807371556759  val loss:  0.4672984182834625\n",
      "epoch:  15   step:  132   train loss:  0.02048235572874546  val loss:  0.4699154496192932\n",
      "epoch:  15   step:  133   train loss:  0.014967426657676697  val loss:  0.47285354137420654\n",
      "epoch:  15   step:  134   train loss:  0.026790451258420944  val loss:  0.4739461839199066\n",
      "epoch:  15   step:  135   train loss:  0.01220208965241909  val loss:  0.46931788325309753\n",
      "epoch:  15   step:  136   train loss:  0.029945921152830124  val loss:  0.46537962555885315\n",
      "epoch:  15   step:  137   train loss:  0.01536207552999258  val loss:  0.46267279982566833\n",
      "epoch:  15   step:  138   train loss:  0.04334243759512901  val loss:  0.4505149722099304\n",
      "epoch:  15   step:  139   train loss:  0.03463505953550339  val loss:  0.4361073672771454\n",
      "epoch:  15   step:  140   train loss:  0.032744359225034714  val loss:  0.42776361107826233\n",
      "epoch:  15   step:  141   train loss:  0.0351356603205204  val loss:  0.40430524945259094\n",
      "epoch:  15   step:  142   train loss:  0.032736457884311676  val loss:  0.39704492688179016\n",
      "epoch:  15   step:  143   train loss:  0.03099338710308075  val loss:  0.383372038602829\n",
      "epoch:  15   step:  144   train loss:  0.05130080133676529  val loss:  0.3772839903831482\n",
      "epoch:  15   step:  145   train loss:  0.03670690953731537  val loss:  0.38739681243896484\n",
      "epoch:  15   step:  146   train loss:  0.09249396622180939  val loss:  0.39657536149024963\n",
      "epoch:  15   step:  147   train loss:  0.029978225007653236  val loss:  0.39890536665916443\n",
      "epoch:  15   step:  148   train loss:  0.045300085097551346  val loss:  0.40451011061668396\n",
      "epoch:  15   step:  149   train loss:  0.03632783144712448  val loss:  0.406280517578125\n",
      "epoch:  15   step:  150   train loss:  0.009659466333687305  val loss:  0.4122997522354126\n",
      "epoch:  15   step:  151   train loss:  0.15625891089439392  val loss:  0.4191378355026245\n",
      "epoch:  15   step:  152   train loss:  0.045642394572496414  val loss:  0.4168304204940796\n",
      "epoch:  15   step:  153   train loss:  0.04317215457558632  val loss:  0.4240325093269348\n",
      "epoch:  15   step:  154   train loss:  0.01606159657239914  val loss:  0.42511239647865295\n",
      "epoch:  15   step:  155   train loss:  0.04389110207557678  val loss:  0.4292401075363159\n",
      "epoch:  15   step:  156   train loss:  0.05349661782383919  val loss:  0.43051832914352417\n",
      "epoch:  15   step:  157   train loss:  0.021639645099639893  val loss:  0.436292439699173\n",
      "epoch:  15   step:  158   train loss:  0.019693322479724884  val loss:  0.4425736665725708\n",
      "epoch:  15   step:  159   train loss:  0.02985762245953083  val loss:  0.4480946362018585\n",
      "epoch:  15   step:  160   train loss:  0.022618038579821587  val loss:  0.44902297854423523\n",
      "epoch:  15   step:  161   train loss:  0.0372895821928978  val loss:  0.45259079337120056\n",
      "epoch:  15   step:  162   train loss:  0.02126171812415123  val loss:  0.4550175368785858\n",
      "epoch:  15   step:  163   train loss:  0.027904797345399857  val loss:  0.45044997334480286\n",
      "epoch:  15   step:  164   train loss:  0.09870048612356186  val loss:  0.4489392638206482\n",
      "epoch:  15   step:  165   train loss:  0.009674807079136372  val loss:  0.4500606954097748\n",
      "epoch:  16   step:  0   train loss:  0.053484708070755005  val loss:  0.4400140941143036\n",
      "epoch:  16   step:  1   train loss:  0.010643260553479195  val loss:  0.4298500418663025\n",
      "epoch:  16   step:  2   train loss:  0.04940024018287659  val loss:  0.4258064031600952\n",
      "epoch:  16   step:  3   train loss:  0.022737814113497734  val loss:  0.42702582478523254\n",
      "epoch:  16   step:  4   train loss:  0.03371334820985794  val loss:  0.43065255880355835\n",
      "epoch:  16   step:  5   train loss:  0.017912622541189194  val loss:  0.43365776538848877\n",
      "epoch:  16   step:  6   train loss:  0.01234956830739975  val loss:  0.4302513897418976\n",
      "epoch:  16   step:  7   train loss:  0.02166423201560974  val loss:  0.4367149770259857\n",
      "epoch:  16   step:  8   train loss:  0.01266106590628624  val loss:  0.43735626339912415\n",
      "epoch:  16   step:  9   train loss:  0.03168606013059616  val loss:  0.4370327591896057\n",
      "epoch:  16   step:  10   train loss:  0.04607889801263809  val loss:  0.4378214180469513\n",
      "epoch:  16   step:  11   train loss:  0.058596134185791016  val loss:  0.4263630211353302\n",
      "epoch:  16   step:  12   train loss:  0.014394853264093399  val loss:  0.41950300335884094\n",
      "epoch:  16   step:  13   train loss:  0.037924692034721375  val loss:  0.414296418428421\n",
      "epoch:  16   step:  14   train loss:  0.015847956761717796  val loss:  0.4106101393699646\n",
      "epoch:  16   step:  15   train loss:  0.03926093503832817  val loss:  0.4101904630661011\n",
      "epoch:  16   step:  16   train loss:  0.027861744165420532  val loss:  0.4091799557209015\n",
      "epoch:  16   step:  17   train loss:  0.023704368621110916  val loss:  0.4102049171924591\n",
      "epoch:  16   step:  18   train loss:  0.009720047004520893  val loss:  0.4013940393924713\n",
      "epoch:  16   step:  19   train loss:  0.05996144562959671  val loss:  0.40281936526298523\n",
      "epoch:  16   step:  20   train loss:  0.010632500052452087  val loss:  0.40154320001602173\n",
      "epoch:  16   step:  21   train loss:  0.037330612540245056  val loss:  0.3956390917301178\n",
      "epoch:  16   step:  22   train loss:  0.01556183397769928  val loss:  0.3977104127407074\n",
      "epoch:  16   step:  23   train loss:  0.025760386139154434  val loss:  0.3993702530860901\n",
      "epoch:  16   step:  24   train loss:  0.013358278200030327  val loss:  0.3983304500579834\n",
      "epoch:  16   step:  25   train loss:  0.014428948983550072  val loss:  0.3969833254814148\n",
      "epoch:  16   step:  26   train loss:  0.041622623801231384  val loss:  0.4006800651550293\n",
      "epoch:  16   step:  27   train loss:  0.013506462797522545  val loss:  0.40392962098121643\n",
      "epoch:  16   step:  28   train loss:  0.004550449084490538  val loss:  0.4018884301185608\n",
      "epoch:  16   step:  29   train loss:  0.052341755479574203  val loss:  0.3971494734287262\n",
      "epoch:  16   step:  30   train loss:  0.012361833825707436  val loss:  0.38902097940444946\n",
      "epoch:  16   step:  31   train loss:  0.012477589771151543  val loss:  0.38163691759109497\n",
      "epoch:  16   step:  32   train loss:  0.023070719093084335  val loss:  0.3717690110206604\n",
      "epoch:  16   step:  33   train loss:  0.024305444210767746  val loss:  0.36131739616394043\n",
      "epoch:  16   step:  34   train loss:  0.01192083116620779  val loss:  0.35205766558647156\n",
      "epoch:  16   step:  35   train loss:  0.10936558246612549  val loss:  0.3349887728691101\n",
      "epoch:  16   step:  36   train loss:  0.007982228882610798  val loss:  0.32053184509277344\n",
      "epoch:  16   step:  37   train loss:  0.033862095326185226  val loss:  0.31319624185562134\n",
      "epoch:  16   step:  38   train loss:  0.03738069534301758  val loss:  0.31730690598487854\n",
      "epoch:  16   step:  39   train loss:  0.02309679612517357  val loss:  0.32052671909332275\n",
      "epoch:  16   step:  40   train loss:  0.010157423093914986  val loss:  0.32285308837890625\n",
      "epoch:  16   step:  41   train loss:  0.029870521277189255  val loss:  0.32383373379707336\n",
      "epoch:  16   step:  42   train loss:  0.02326689288020134  val loss:  0.32516440749168396\n",
      "epoch:  16   step:  43   train loss:  0.0340435728430748  val loss:  0.32369598746299744\n",
      "epoch:  16   step:  44   train loss:  0.010109694674611092  val loss:  0.32349738478660583\n",
      "epoch:  16   step:  45   train loss:  0.026531005278229713  val loss:  0.3227076530456543\n",
      "epoch:  16   step:  46   train loss:  0.11511160433292389  val loss:  0.32408758997917175\n",
      "epoch:  16   step:  47   train loss:  0.028533851727843285  val loss:  0.325987845659256\n",
      "epoch:  16   step:  48   train loss:  0.02437078393995762  val loss:  0.3320535123348236\n",
      "epoch:  16   step:  49   train loss:  0.023199407383799553  val loss:  0.33747604489326477\n",
      "epoch:  16   step:  50   train loss:  0.006569665856659412  val loss:  0.33707594871520996\n",
      "epoch:  16   step:  51   train loss:  0.015525734052062035  val loss:  0.33716723322868347\n",
      "epoch:  16   step:  52   train loss:  0.011232059448957443  val loss:  0.3391721546649933\n",
      "epoch:  16   step:  53   train loss:  0.09438158571720123  val loss:  0.348048597574234\n",
      "epoch:  16   step:  54   train loss:  0.017963513731956482  val loss:  0.3524351716041565\n",
      "epoch:  16   step:  55   train loss:  0.022068575024604797  val loss:  0.35558825731277466\n",
      "epoch:  16   step:  56   train loss:  0.007298904005438089  val loss:  0.35913607478141785\n",
      "epoch:  16   step:  57   train loss:  0.031050536781549454  val loss:  0.3609254062175751\n",
      "epoch:  16   step:  58   train loss:  0.03137229382991791  val loss:  0.3607563078403473\n",
      "epoch:  16   step:  59   train loss:  0.029412368312478065  val loss:  0.35617122054100037\n",
      "epoch:  16   step:  60   train loss:  0.032882243394851685  val loss:  0.3685881197452545\n",
      "epoch:  16   step:  61   train loss:  0.005797128193080425  val loss:  0.37899982929229736\n",
      "epoch:  16   step:  62   train loss:  0.013732250779867172  val loss:  0.3835681080818176\n",
      "epoch:  16   step:  63   train loss:  0.03975379467010498  val loss:  0.39354804158210754\n",
      "epoch:  16   step:  64   train loss:  0.014283490367233753  val loss:  0.4056553840637207\n",
      "epoch:  16   step:  65   train loss:  0.014884667471051216  val loss:  0.41366690397262573\n",
      "epoch:  16   step:  66   train loss:  0.030254162847995758  val loss:  0.41497260332107544\n",
      "epoch:  16   step:  67   train loss:  0.06770419329404831  val loss:  0.4261132478713989\n",
      "epoch:  16   step:  68   train loss:  0.01518476102501154  val loss:  0.4341544210910797\n",
      "epoch:  16   step:  69   train loss:  0.013617077842354774  val loss:  0.44183626770973206\n",
      "epoch:  16   step:  70   train loss:  0.03643854334950447  val loss:  0.45049652457237244\n",
      "epoch:  16   step:  71   train loss:  0.05516916140913963  val loss:  0.4431265890598297\n",
      "epoch:  16   step:  72   train loss:  0.01635514572262764  val loss:  0.4394044876098633\n",
      "epoch:  16   step:  73   train loss:  0.05007893219590187  val loss:  0.42717280983924866\n",
      "epoch:  16   step:  74   train loss:  0.022302065044641495  val loss:  0.41548940539360046\n",
      "epoch:  16   step:  75   train loss:  0.018009532243013382  val loss:  0.4075604975223541\n",
      "epoch:  16   step:  76   train loss:  0.0483483262360096  val loss:  0.4051298499107361\n",
      "epoch:  16   step:  77   train loss:  0.08426397293806076  val loss:  0.4021342694759369\n",
      "epoch:  16   step:  78   train loss:  0.010950906202197075  val loss:  0.3987308442592621\n",
      "epoch:  16   step:  79   train loss:  0.105553537607193  val loss:  0.4051070809364319\n",
      "epoch:  16   step:  80   train loss:  0.01613508351147175  val loss:  0.41253477334976196\n",
      "epoch:  16   step:  81   train loss:  0.0069846827536821365  val loss:  0.4217168092727661\n",
      "epoch:  16   step:  82   train loss:  0.023301756009459496  val loss:  0.4282330572605133\n",
      "epoch:  16   step:  83   train loss:  0.008636970072984695  val loss:  0.4394032955169678\n",
      "epoch:  16   step:  84   train loss:  0.009311634115874767  val loss:  0.4489055573940277\n",
      "epoch:  16   step:  85   train loss:  0.03925639018416405  val loss:  0.45822417736053467\n",
      "epoch:  16   step:  86   train loss:  0.03012198954820633  val loss:  0.45183631777763367\n",
      "epoch:  16   step:  87   train loss:  0.06836796551942825  val loss:  0.4404670000076294\n",
      "epoch:  16   step:  88   train loss:  0.02779195085167885  val loss:  0.4235421419143677\n",
      "epoch:  16   step:  89   train loss:  0.01217181421816349  val loss:  0.4133695363998413\n",
      "epoch:  16   step:  90   train loss:  0.08125094324350357  val loss:  0.4079260230064392\n",
      "epoch:  16   step:  91   train loss:  0.05513731390237808  val loss:  0.41442543268203735\n",
      "epoch:  16   step:  92   train loss:  0.011646447703242302  val loss:  0.41777122020721436\n",
      "epoch:  16   step:  93   train loss:  0.010576759465038776  val loss:  0.42065703868865967\n",
      "epoch:  16   step:  94   train loss:  0.029300348833203316  val loss:  0.4220004975795746\n",
      "epoch:  16   step:  95   train loss:  0.014456541277468204  val loss:  0.42190441489219666\n",
      "epoch:  16   step:  96   train loss:  0.01587248034775257  val loss:  0.41960692405700684\n",
      "epoch:  16   step:  97   train loss:  0.005120428279042244  val loss:  0.4172574579715729\n",
      "epoch:  16   step:  98   train loss:  0.03905899077653885  val loss:  0.41090303659439087\n",
      "epoch:  16   step:  99   train loss:  0.009021235629916191  val loss:  0.4011619985103607\n",
      "epoch:  16   step:  100   train loss:  0.01338986400514841  val loss:  0.39669379591941833\n",
      "epoch:  16   step:  101   train loss:  0.04093015938997269  val loss:  0.3987545669078827\n",
      "epoch:  16   step:  102   train loss:  0.03809358924627304  val loss:  0.4023541510105133\n",
      "epoch:  16   step:  103   train loss:  0.010852735489606857  val loss:  0.39786097407341003\n",
      "epoch:  16   step:  104   train loss:  0.01116345264017582  val loss:  0.4072268605232239\n",
      "epoch:  16   step:  105   train loss:  0.010723872110247612  val loss:  0.41007688641548157\n",
      "epoch:  16   step:  106   train loss:  0.04654339328408241  val loss:  0.40900954604148865\n",
      "epoch:  16   step:  107   train loss:  0.021730344742536545  val loss:  0.4209328889846802\n",
      "epoch:  16   step:  108   train loss:  0.03542599454522133  val loss:  0.4175473749637604\n",
      "epoch:  16   step:  109   train loss:  0.019542519003152847  val loss:  0.41539037227630615\n",
      "epoch:  16   step:  110   train loss:  0.012742099352180958  val loss:  0.41389331221580505\n",
      "epoch:  16   step:  111   train loss:  0.0200471393764019  val loss:  0.4051188826560974\n",
      "epoch:  16   step:  112   train loss:  0.02002347819507122  val loss:  0.40178829431533813\n",
      "epoch:  16   step:  113   train loss:  0.06143137067556381  val loss:  0.3935510814189911\n",
      "epoch:  16   step:  114   train loss:  0.0318429097533226  val loss:  0.37932249903678894\n",
      "epoch:  16   step:  115   train loss:  0.006970353424549103  val loss:  0.3700307309627533\n",
      "epoch:  16   step:  116   train loss:  0.008855098858475685  val loss:  0.3661389946937561\n",
      "epoch:  16   step:  117   train loss:  0.01700531505048275  val loss:  0.3670526444911957\n",
      "epoch:  16   step:  118   train loss:  0.03810860589146614  val loss:  0.3703303337097168\n",
      "epoch:  16   step:  119   train loss:  0.009549997746944427  val loss:  0.3756886124610901\n",
      "epoch:  16   step:  120   train loss:  0.024038778617978096  val loss:  0.3686526119709015\n",
      "epoch:  16   step:  121   train loss:  0.023840434849262238  val loss:  0.36655744910240173\n",
      "epoch:  16   step:  122   train loss:  0.09976257383823395  val loss:  0.36984989047050476\n",
      "epoch:  16   step:  123   train loss:  0.024092096835374832  val loss:  0.3679949939250946\n",
      "epoch:  16   step:  124   train loss:  0.01081166137009859  val loss:  0.3659158945083618\n",
      "epoch:  16   step:  125   train loss:  0.03464149311184883  val loss:  0.37524378299713135\n",
      "epoch:  16   step:  126   train loss:  0.02495206892490387  val loss:  0.38655057549476624\n",
      "epoch:  16   step:  127   train loss:  0.027027199044823647  val loss:  0.4018092155456543\n",
      "epoch:  16   step:  128   train loss:  0.01722102239727974  val loss:  0.4114377796649933\n",
      "epoch:  16   step:  129   train loss:  0.06393399834632874  val loss:  0.407708078622818\n",
      "epoch:  16   step:  130   train loss:  0.005659313872456551  val loss:  0.40403929352760315\n",
      "epoch:  16   step:  131   train loss:  0.013957524672150612  val loss:  0.40378549695014954\n",
      "epoch:  16   step:  132   train loss:  0.02241583541035652  val loss:  0.41028884053230286\n",
      "epoch:  16   step:  133   train loss:  0.006900461856275797  val loss:  0.4081205725669861\n",
      "epoch:  16   step:  134   train loss:  0.009886812418699265  val loss:  0.40894651412963867\n",
      "epoch:  16   step:  135   train loss:  0.019013382494449615  val loss:  0.4115535616874695\n",
      "epoch:  16   step:  136   train loss:  0.019499745219945908  val loss:  0.41497379541397095\n",
      "epoch:  16   step:  137   train loss:  0.02286812663078308  val loss:  0.4174787700176239\n",
      "epoch:  16   step:  138   train loss:  0.012340793386101723  val loss:  0.4188332259654999\n",
      "epoch:  16   step:  139   train loss:  0.037504892796278  val loss:  0.4246198236942291\n",
      "epoch:  16   step:  140   train loss:  0.013385097496211529  val loss:  0.43159040808677673\n",
      "epoch:  16   step:  141   train loss:  0.03090866282582283  val loss:  0.4334486722946167\n",
      "epoch:  16   step:  142   train loss:  0.013478903099894524  val loss:  0.4312361776828766\n",
      "epoch:  16   step:  143   train loss:  0.011509735137224197  val loss:  0.42845389246940613\n",
      "epoch:  16   step:  144   train loss:  0.00869720708578825  val loss:  0.4245234727859497\n",
      "epoch:  16   step:  145   train loss:  0.01366919931024313  val loss:  0.4242018461227417\n",
      "epoch:  16   step:  146   train loss:  0.02804187685251236  val loss:  0.42919138073921204\n",
      "epoch:  16   step:  147   train loss:  0.0224755946546793  val loss:  0.42639288306236267\n",
      "epoch:  16   step:  148   train loss:  0.006370446179062128  val loss:  0.4238244295120239\n",
      "epoch:  16   step:  149   train loss:  0.004654274322092533  val loss:  0.4182800054550171\n",
      "epoch:  16   step:  150   train loss:  0.015704456716775894  val loss:  0.4209597408771515\n",
      "epoch:  16   step:  151   train loss:  0.03139343485236168  val loss:  0.42856448888778687\n",
      "epoch:  16   step:  152   train loss:  0.02019183151423931  val loss:  0.4328644573688507\n",
      "epoch:  16   step:  153   train loss:  0.017834791913628578  val loss:  0.43622487783432007\n",
      "epoch:  16   step:  154   train loss:  0.020017754286527634  val loss:  0.4434777796268463\n",
      "epoch:  16   step:  155   train loss:  0.0165159460157156  val loss:  0.45350217819213867\n",
      "epoch:  16   step:  156   train loss:  0.00683183129876852  val loss:  0.4662221074104309\n",
      "epoch:  16   step:  157   train loss:  0.011846920475363731  val loss:  0.4720185101032257\n",
      "epoch:  16   step:  158   train loss:  0.00929218903183937  val loss:  0.483791708946228\n",
      "epoch:  16   step:  159   train loss:  0.04420735687017441  val loss:  0.48732122778892517\n",
      "epoch:  16   step:  160   train loss:  0.05502035468816757  val loss:  0.4793214201927185\n",
      "epoch:  16   step:  161   train loss:  0.019022125750780106  val loss:  0.4719058871269226\n",
      "epoch:  16   step:  162   train loss:  0.006201962009072304  val loss:  0.4610677659511566\n",
      "epoch:  16   step:  163   train loss:  0.04842618852853775  val loss:  0.45138993859291077\n",
      "epoch:  16   step:  164   train loss:  0.012646405026316643  val loss:  0.4491952359676361\n",
      "epoch:  16   step:  165   train loss:  0.02401912398636341  val loss:  0.4499033987522125\n",
      "epoch:  17   step:  0   train loss:  0.013752464205026627  val loss:  0.4400576055049896\n",
      "epoch:  17   step:  1   train loss:  0.009168924763798714  val loss:  0.4428127110004425\n",
      "epoch:  17   step:  2   train loss:  0.014012565836310387  val loss:  0.4441189169883728\n",
      "epoch:  17   step:  3   train loss:  0.010185396298766136  val loss:  0.4491320848464966\n",
      "epoch:  17   step:  4   train loss:  0.014518454670906067  val loss:  0.4563973844051361\n",
      "epoch:  17   step:  5   train loss:  0.01467084139585495  val loss:  0.4661255478858948\n",
      "epoch:  17   step:  6   train loss:  0.008912932127714157  val loss:  0.46958640217781067\n",
      "epoch:  17   step:  7   train loss:  0.03284972161054611  val loss:  0.4710145890712738\n",
      "epoch:  17   step:  8   train loss:  0.011546701192855835  val loss:  0.4743323028087616\n",
      "epoch:  17   step:  9   train loss:  0.01382562704384327  val loss:  0.47297507524490356\n",
      "epoch:  17   step:  10   train loss:  0.006879745051264763  val loss:  0.4734361171722412\n",
      "epoch:  17   step:  11   train loss:  0.009167389944195747  val loss:  0.48034054040908813\n",
      "epoch:  17   step:  12   train loss:  0.004194857552647591  val loss:  0.4821711480617523\n",
      "epoch:  17   step:  13   train loss:  0.015183690935373306  val loss:  0.4901656210422516\n",
      "epoch:  17   step:  14   train loss:  0.016036467626690865  val loss:  0.4958442449569702\n",
      "epoch:  17   step:  15   train loss:  0.007836362347006798  val loss:  0.49296945333480835\n",
      "epoch:  17   step:  16   train loss:  0.016643837094306946  val loss:  0.48890480399131775\n",
      "epoch:  17   step:  17   train loss:  0.04734836146235466  val loss:  0.4915270507335663\n",
      "epoch:  17   step:  18   train loss:  0.01553134061396122  val loss:  0.48880210518836975\n",
      "epoch:  17   step:  19   train loss:  0.019266420975327492  val loss:  0.4880141019821167\n",
      "epoch:  17   step:  20   train loss:  0.03323093056678772  val loss:  0.48323652148246765\n",
      "epoch:  17   step:  21   train loss:  0.007351154461503029  val loss:  0.4751136302947998\n",
      "epoch:  17   step:  22   train loss:  0.0076011354103684425  val loss:  0.4700973331928253\n",
      "epoch:  17   step:  23   train loss:  0.025837767869234085  val loss:  0.4542846083641052\n",
      "epoch:  17   step:  24   train loss:  0.005939790513366461  val loss:  0.44146713614463806\n",
      "epoch:  17   step:  25   train loss:  0.010953987017273903  val loss:  0.43514305353164673\n",
      "epoch:  17   step:  26   train loss:  0.00544610945507884  val loss:  0.4300842583179474\n",
      "epoch:  17   step:  27   train loss:  0.008678441867232323  val loss:  0.4313001036643982\n",
      "epoch:  17   step:  28   train loss:  0.014171279966831207  val loss:  0.4258964955806732\n",
      "epoch:  17   step:  29   train loss:  0.005490794777870178  val loss:  0.4201330840587616\n",
      "epoch:  17   step:  30   train loss:  0.01611712947487831  val loss:  0.42043596506118774\n",
      "epoch:  17   step:  31   train loss:  0.009867342188954353  val loss:  0.41338586807250977\n",
      "epoch:  17   step:  32   train loss:  0.0038134236820042133  val loss:  0.4053301513195038\n",
      "epoch:  17   step:  33   train loss:  0.01578679122030735  val loss:  0.40313372015953064\n",
      "epoch:  17   step:  34   train loss:  0.009963286109268665  val loss:  0.3979894518852234\n",
      "epoch:  17   step:  35   train loss:  0.01774234138429165  val loss:  0.3959468901157379\n",
      "epoch:  17   step:  36   train loss:  0.010233771055936813  val loss:  0.39364778995513916\n",
      "epoch:  17   step:  37   train loss:  0.01000523567199707  val loss:  0.3938765227794647\n",
      "epoch:  17   step:  38   train loss:  0.0349542498588562  val loss:  0.3938102722167969\n",
      "epoch:  17   step:  39   train loss:  0.010760900564491749  val loss:  0.39747732877731323\n",
      "epoch:  17   step:  40   train loss:  0.00916331633925438  val loss:  0.3939155042171478\n",
      "epoch:  17   step:  41   train loss:  0.01701536774635315  val loss:  0.39545515179634094\n",
      "epoch:  17   step:  42   train loss:  0.013264747336506844  val loss:  0.39594900608062744\n",
      "epoch:  17   step:  43   train loss:  0.004735250025987625  val loss:  0.39832985401153564\n",
      "epoch:  17   step:  44   train loss:  0.016630936414003372  val loss:  0.39518940448760986\n",
      "epoch:  17   step:  45   train loss:  0.010219409130513668  val loss:  0.39270442724227905\n",
      "epoch:  17   step:  46   train loss:  0.00909433327615261  val loss:  0.39311638474464417\n",
      "epoch:  17   step:  47   train loss:  0.006452756933867931  val loss:  0.3901846706867218\n",
      "epoch:  17   step:  48   train loss:  0.007883409038186073  val loss:  0.38849547505378723\n",
      "epoch:  17   step:  49   train loss:  0.005595189984887838  val loss:  0.3874150514602661\n",
      "epoch:  17   step:  50   train loss:  0.03638766333460808  val loss:  0.38522109389305115\n",
      "epoch:  17   step:  51   train loss:  0.02265463024377823  val loss:  0.3865962624549866\n",
      "epoch:  17   step:  52   train loss:  0.008081556297838688  val loss:  0.3890730142593384\n",
      "epoch:  17   step:  53   train loss:  0.009935199283063412  val loss:  0.39115065336227417\n",
      "epoch:  17   step:  54   train loss:  0.016164224594831467  val loss:  0.3953770101070404\n",
      "epoch:  17   step:  55   train loss:  0.007153328042477369  val loss:  0.40117985010147095\n",
      "epoch:  17   step:  56   train loss:  0.02640894427895546  val loss:  0.40070608258247375\n",
      "epoch:  17   step:  57   train loss:  0.03423419967293739  val loss:  0.40854108333587646\n",
      "epoch:  17   step:  58   train loss:  0.007921070791780949  val loss:  0.4110376536846161\n",
      "epoch:  17   step:  59   train loss:  0.00931454636156559  val loss:  0.4186823070049286\n",
      "epoch:  17   step:  60   train loss:  0.014594797976315022  val loss:  0.41722285747528076\n",
      "epoch:  17   step:  61   train loss:  0.00843818485736847  val loss:  0.4182957410812378\n",
      "epoch:  17   step:  62   train loss:  0.01025954820215702  val loss:  0.417616605758667\n",
      "epoch:  17   step:  63   train loss:  0.006122363265603781  val loss:  0.41715458035469055\n",
      "epoch:  17   step:  64   train loss:  0.00701693631708622  val loss:  0.41406965255737305\n",
      "epoch:  17   step:  65   train loss:  0.006189530715346336  val loss:  0.4101673662662506\n",
      "epoch:  17   step:  66   train loss:  0.008426385000348091  val loss:  0.4082900881767273\n",
      "epoch:  17   step:  67   train loss:  0.018749289214611053  val loss:  0.407598078250885\n",
      "epoch:  17   step:  68   train loss:  0.014449385926127434  val loss:  0.40755152702331543\n",
      "epoch:  17   step:  69   train loss:  0.00662236986681819  val loss:  0.40662023425102234\n",
      "epoch:  17   step:  70   train loss:  0.008262285962700844  val loss:  0.4042973220348358\n",
      "epoch:  17   step:  71   train loss:  0.006521949544548988  val loss:  0.4036446213722229\n",
      "epoch:  17   step:  72   train loss:  0.02218213491141796  val loss:  0.4026939272880554\n",
      "epoch:  17   step:  73   train loss:  0.004745980724692345  val loss:  0.4008232653141022\n",
      "epoch:  17   step:  74   train loss:  0.006938408128917217  val loss:  0.39709123969078064\n",
      "epoch:  17   step:  75   train loss:  0.012953966856002808  val loss:  0.40030959248542786\n",
      "epoch:  17   step:  76   train loss:  0.0068596662022173405  val loss:  0.40097254514694214\n",
      "epoch:  17   step:  77   train loss:  0.012098671868443489  val loss:  0.4028404653072357\n",
      "epoch:  17   step:  78   train loss:  0.008370141498744488  val loss:  0.4068364202976227\n",
      "epoch:  17   step:  79   train loss:  0.01580667495727539  val loss:  0.4075625240802765\n",
      "epoch:  17   step:  80   train loss:  0.007707425393164158  val loss:  0.4093559682369232\n",
      "epoch:  17   step:  81   train loss:  0.00579012930393219  val loss:  0.41634294390678406\n",
      "epoch:  17   step:  82   train loss:  0.006051353178918362  val loss:  0.42047590017318726\n",
      "epoch:  17   step:  83   train loss:  0.0073283943347632885  val loss:  0.4252581298351288\n",
      "epoch:  17   step:  84   train loss:  0.0022633420303463936  val loss:  0.4302614629268646\n",
      "epoch:  17   step:  85   train loss:  0.007278515957295895  val loss:  0.4350447654724121\n",
      "epoch:  17   step:  86   train loss:  0.00910545140504837  val loss:  0.43649977445602417\n",
      "epoch:  17   step:  87   train loss:  0.009605121798813343  val loss:  0.43986964225769043\n",
      "epoch:  17   step:  88   train loss:  0.028731569647789  val loss:  0.4451943039894104\n",
      "epoch:  17   step:  89   train loss:  0.004551901947706938  val loss:  0.453754186630249\n",
      "epoch:  17   step:  90   train loss:  0.003049714956432581  val loss:  0.4579119086265564\n",
      "epoch:  17   step:  91   train loss:  0.004625091329216957  val loss:  0.4592636525630951\n",
      "epoch:  17   step:  92   train loss:  0.01062841061502695  val loss:  0.45621591806411743\n",
      "epoch:  17   step:  93   train loss:  0.0031640189699828625  val loss:  0.45641642808914185\n",
      "epoch:  17   step:  94   train loss:  0.014500558376312256  val loss:  0.46360617876052856\n",
      "epoch:  17   step:  95   train loss:  0.007865042425692081  val loss:  0.46196261048316956\n",
      "epoch:  17   step:  96   train loss:  0.0046872589737176895  val loss:  0.46212321519851685\n",
      "epoch:  17   step:  97   train loss:  0.05760151892900467  val loss:  0.4629836976528168\n",
      "epoch:  17   step:  98   train loss:  0.004428744316101074  val loss:  0.4607972204685211\n",
      "epoch:  17   step:  99   train loss:  0.010164381936192513  val loss:  0.4601435363292694\n",
      "epoch:  17   step:  100   train loss:  0.010268907062709332  val loss:  0.46688777208328247\n",
      "epoch:  17   step:  101   train loss:  0.006009038537740707  val loss:  0.4664545953273773\n",
      "epoch:  17   step:  102   train loss:  0.004125894978642464  val loss:  0.47288668155670166\n",
      "epoch:  17   step:  103   train loss:  0.007266957312822342  val loss:  0.4773845970630646\n",
      "epoch:  17   step:  104   train loss:  0.01879754476249218  val loss:  0.473477840423584\n",
      "epoch:  17   step:  105   train loss:  0.007457711733877659  val loss:  0.46490344405174255\n",
      "epoch:  17   step:  106   train loss:  0.005100114271044731  val loss:  0.46634572744369507\n",
      "epoch:  17   step:  107   train loss:  0.029950914904475212  val loss:  0.4770808219909668\n",
      "epoch:  17   step:  108   train loss:  0.0026815440505743027  val loss:  0.4908550977706909\n",
      "epoch:  17   step:  109   train loss:  0.007570015266537666  val loss:  0.4930301606655121\n",
      "epoch:  17   step:  110   train loss:  0.034387316554784775  val loss:  0.48141971230506897\n",
      "epoch:  17   step:  111   train loss:  0.012501792050898075  val loss:  0.46209996938705444\n",
      "epoch:  17   step:  112   train loss:  0.01746342144906521  val loss:  0.4557352066040039\n",
      "epoch:  17   step:  113   train loss:  0.010249311104416847  val loss:  0.4528399705886841\n",
      "epoch:  17   step:  114   train loss:  0.004016749560832977  val loss:  0.44953426718711853\n",
      "epoch:  17   step:  115   train loss:  0.007498875726014376  val loss:  0.4543072283267975\n",
      "epoch:  17   step:  116   train loss:  0.005593880079686642  val loss:  0.4496239423751831\n",
      "epoch:  17   step:  117   train loss:  0.01938997581601143  val loss:  0.4477682113647461\n",
      "epoch:  17   step:  118   train loss:  0.0054259104654192924  val loss:  0.4526534080505371\n",
      "epoch:  17   step:  119   train loss:  0.008009467273950577  val loss:  0.45676735043525696\n",
      "epoch:  17   step:  120   train loss:  0.019032981246709824  val loss:  0.44431957602500916\n",
      "epoch:  17   step:  121   train loss:  0.009627506136894226  val loss:  0.441854327917099\n",
      "epoch:  17   step:  122   train loss:  0.007563319988548756  val loss:  0.439806193113327\n",
      "epoch:  17   step:  123   train loss:  0.004085024818778038  val loss:  0.43282604217529297\n",
      "epoch:  17   step:  124   train loss:  0.005024990066885948  val loss:  0.4310586750507355\n",
      "epoch:  17   step:  125   train loss:  0.010322995483875275  val loss:  0.42183351516723633\n",
      "epoch:  17   step:  126   train loss:  0.008197026327252388  val loss:  0.4253182113170624\n",
      "epoch:  17   step:  127   train loss:  0.009888559579849243  val loss:  0.42955029010772705\n",
      "epoch:  17   step:  128   train loss:  0.015155036933720112  val loss:  0.42315608263015747\n",
      "epoch:  17   step:  129   train loss:  0.010287782177329063  val loss:  0.42668387293815613\n",
      "epoch:  17   step:  130   train loss:  0.010180504061281681  val loss:  0.4169875383377075\n",
      "epoch:  17   step:  131   train loss:  0.004214709624648094  val loss:  0.41399019956588745\n",
      "epoch:  17   step:  132   train loss:  0.007368849590420723  val loss:  0.406954824924469\n",
      "epoch:  17   step:  133   train loss:  0.008170637302100658  val loss:  0.4070177674293518\n",
      "epoch:  17   step:  134   train loss:  0.01131261233240366  val loss:  0.4140397906303406\n",
      "epoch:  17   step:  135   train loss:  0.0059146760031580925  val loss:  0.4094274938106537\n",
      "epoch:  17   step:  136   train loss:  0.010340578854084015  val loss:  0.4101388156414032\n",
      "epoch:  17   step:  137   train loss:  0.02680600807070732  val loss:  0.4126467704772949\n",
      "epoch:  17   step:  138   train loss:  0.003386938478797674  val loss:  0.41137832403182983\n",
      "epoch:  17   step:  139   train loss:  0.0058259181678295135  val loss:  0.4073095917701721\n",
      "epoch:  17   step:  140   train loss:  0.008035584352910519  val loss:  0.41344374418258667\n",
      "epoch:  17   step:  141   train loss:  0.010514941066503525  val loss:  0.41982242465019226\n",
      "epoch:  17   step:  142   train loss:  0.024181971326470375  val loss:  0.4290045499801636\n",
      "epoch:  17   step:  143   train loss:  0.009050850756466389  val loss:  0.42869043350219727\n",
      "epoch:  17   step:  144   train loss:  0.005257652141153812  val loss:  0.4314986765384674\n",
      "epoch:  17   step:  145   train loss:  0.008503206074237823  val loss:  0.4349285662174225\n",
      "epoch:  17   step:  146   train loss:  0.01034509390592575  val loss:  0.4457988142967224\n",
      "epoch:  17   step:  147   train loss:  0.01714034378528595  val loss:  0.44764983654022217\n",
      "epoch:  17   step:  148   train loss:  0.004444457590579987  val loss:  0.45027241110801697\n",
      "epoch:  17   step:  149   train loss:  0.005924278870224953  val loss:  0.449783056974411\n",
      "epoch:  17   step:  150   train loss:  0.005921521224081516  val loss:  0.4508478045463562\n",
      "epoch:  17   step:  151   train loss:  0.005103267729282379  val loss:  0.45689329504966736\n",
      "epoch:  17   step:  152   train loss:  0.0064039696007966995  val loss:  0.4612959921360016\n",
      "epoch:  17   step:  153   train loss:  0.010649142786860466  val loss:  0.4607696533203125\n",
      "epoch:  17   step:  154   train loss:  0.01824149303138256  val loss:  0.4652724862098694\n",
      "epoch:  17   step:  155   train loss:  0.0072812726721167564  val loss:  0.4721725881099701\n",
      "epoch:  17   step:  156   train loss:  0.0027032396756112576  val loss:  0.481223464012146\n",
      "epoch:  17   step:  157   train loss:  0.01537330262362957  val loss:  0.4818636178970337\n",
      "epoch:  17   step:  158   train loss:  0.007164329756051302  val loss:  0.4809948205947876\n",
      "epoch:  17   step:  159   train loss:  0.009662158787250519  val loss:  0.4827345311641693\n",
      "epoch:  17   step:  160   train loss:  0.009377244859933853  val loss:  0.4867064952850342\n",
      "epoch:  17   step:  161   train loss:  0.008130479604005814  val loss:  0.4896627366542816\n",
      "epoch:  17   step:  162   train loss:  0.007781168445944786  val loss:  0.49216604232788086\n",
      "epoch:  17   step:  163   train loss:  0.013646915555000305  val loss:  0.49033164978027344\n",
      "epoch:  17   step:  164   train loss:  0.01797865703701973  val loss:  0.49111005663871765\n",
      "epoch:  17   step:  165   train loss:  0.025296587496995926  val loss:  0.4836987257003784\n",
      "epoch:  18   step:  0   train loss:  0.0032975326757878065  val loss:  0.4927064776420593\n",
      "epoch:  18   step:  1   train loss:  0.006175042130053043  val loss:  0.51068115234375\n",
      "epoch:  18   step:  2   train loss:  0.012229167856276035  val loss:  0.5186423659324646\n",
      "epoch:  18   step:  3   train loss:  0.019882213324308395  val loss:  0.5223367810249329\n",
      "epoch:  18   step:  4   train loss:  0.004607146605849266  val loss:  0.5330110192298889\n",
      "epoch:  18   step:  5   train loss:  0.0019700326956808567  val loss:  0.5348554253578186\n",
      "epoch:  18   step:  6   train loss:  0.006228568963706493  val loss:  0.5433676838874817\n",
      "epoch:  18   step:  7   train loss:  0.005646697245538235  val loss:  0.5488234758377075\n",
      "epoch:  18   step:  8   train loss:  0.004786223638802767  val loss:  0.5520349144935608\n",
      "epoch:  18   step:  9   train loss:  0.00806982722133398  val loss:  0.5647700428962708\n",
      "epoch:  18   step:  10   train loss:  0.006505243480205536  val loss:  0.5663232207298279\n",
      "epoch:  18   step:  11   train loss:  0.006630291230976582  val loss:  0.5663463473320007\n",
      "epoch:  18   step:  12   train loss:  0.005761757027357817  val loss:  0.5671296715736389\n",
      "epoch:  18   step:  13   train loss:  0.00612662173807621  val loss:  0.5675032138824463\n",
      "epoch:  18   step:  14   train loss:  0.004729857202619314  val loss:  0.5710727572441101\n",
      "epoch:  18   step:  15   train loss:  0.0038247646298259497  val loss:  0.5707665681838989\n",
      "epoch:  18   step:  16   train loss:  0.0028246473520994186  val loss:  0.5692358016967773\n",
      "epoch:  18   step:  17   train loss:  0.010643281042575836  val loss:  0.5757299065589905\n",
      "epoch:  18   step:  18   train loss:  0.006060670129954815  val loss:  0.5812705755233765\n",
      "epoch:  18   step:  19   train loss:  0.006310167722404003  val loss:  0.5776267051696777\n",
      "epoch:  18   step:  20   train loss:  0.007244633510708809  val loss:  0.579470157623291\n",
      "epoch:  18   step:  21   train loss:  0.004412368405610323  val loss:  0.5775448679924011\n",
      "epoch:  18   step:  22   train loss:  0.0032688542269170284  val loss:  0.5806525349617004\n",
      "epoch:  18   step:  23   train loss:  0.0031210915185511112  val loss:  0.5753014087677002\n",
      "epoch:  18   step:  24   train loss:  0.004934300202876329  val loss:  0.5799951553344727\n",
      "epoch:  18   step:  25   train loss:  0.010688175447285175  val loss:  0.5731261968612671\n",
      "epoch:  18   step:  26   train loss:  0.004134639166295528  val loss:  0.5724289417266846\n",
      "epoch:  18   step:  27   train loss:  0.005112307146191597  val loss:  0.5720697045326233\n",
      "epoch:  18   step:  28   train loss:  0.004144166596233845  val loss:  0.5780619978904724\n",
      "epoch:  18   step:  29   train loss:  0.005550383124500513  val loss:  0.5786911249160767\n",
      "epoch:  18   step:  30   train loss:  0.00336075434461236  val loss:  0.5775330066680908\n",
      "epoch:  18   step:  31   train loss:  0.01505481917411089  val loss:  0.5859658122062683\n",
      "epoch:  18   step:  32   train loss:  0.0033259836491197348  val loss:  0.5774020552635193\n",
      "epoch:  18   step:  33   train loss:  0.0031592375598847866  val loss:  0.5689537525177002\n",
      "epoch:  18   step:  34   train loss:  0.012495484203100204  val loss:  0.5616286993026733\n",
      "epoch:  18   step:  35   train loss:  0.004166335798799992  val loss:  0.5641224980354309\n",
      "epoch:  18   step:  36   train loss:  0.003412031102925539  val loss:  0.5639670491218567\n",
      "epoch:  18   step:  37   train loss:  0.006764107849448919  val loss:  0.5701339244842529\n",
      "epoch:  18   step:  38   train loss:  0.004818500019609928  val loss:  0.5737972259521484\n",
      "epoch:  18   step:  39   train loss:  0.004668280482292175  val loss:  0.5702148079872131\n",
      "epoch:  18   step:  40   train loss:  0.004004190675914288  val loss:  0.5711367130279541\n",
      "epoch:  18   step:  41   train loss:  0.0032436782494187355  val loss:  0.5649721622467041\n",
      "epoch:  18   step:  42   train loss:  0.0022139011416584253  val loss:  0.5605409741401672\n",
      "epoch:  18   step:  43   train loss:  0.003991849720478058  val loss:  0.5584816932678223\n",
      "epoch:  18   step:  44   train loss:  0.0067654866725206375  val loss:  0.5510359406471252\n",
      "epoch:  18   step:  45   train loss:  0.001763459062203765  val loss:  0.5484828352928162\n",
      "epoch:  18   step:  46   train loss:  0.006417514756321907  val loss:  0.5456907153129578\n",
      "epoch:  18   step:  47   train loss:  0.00832512229681015  val loss:  0.546002984046936\n",
      "epoch:  18   step:  48   train loss:  0.01832004263997078  val loss:  0.5353587865829468\n",
      "epoch:  18   step:  49   train loss:  0.0047309561632573605  val loss:  0.5304645895957947\n",
      "epoch:  18   step:  50   train loss:  0.004886888898909092  val loss:  0.5279111862182617\n",
      "epoch:  18   step:  51   train loss:  0.010343596339225769  val loss:  0.5229160785675049\n",
      "epoch:  18   step:  52   train loss:  0.008047323673963547  val loss:  0.5232414603233337\n",
      "epoch:  18   step:  53   train loss:  0.005826936103403568  val loss:  0.5109835863113403\n",
      "epoch:  18   step:  54   train loss:  0.003326105885207653  val loss:  0.49872058629989624\n",
      "epoch:  18   step:  55   train loss:  0.0021928143687546253  val loss:  0.49304547905921936\n",
      "epoch:  18   step:  56   train loss:  0.0038926484994590282  val loss:  0.49092158675193787\n",
      "epoch:  18   step:  57   train loss:  0.0032396495807915926  val loss:  0.48919302225112915\n",
      "epoch:  18   step:  58   train loss:  0.0076074739918112755  val loss:  0.48797452449798584\n",
      "epoch:  18   step:  59   train loss:  0.008466017432510853  val loss:  0.4836236834526062\n",
      "epoch:  18   step:  60   train loss:  0.006037433631718159  val loss:  0.48489314317703247\n",
      "epoch:  18   step:  61   train loss:  0.0036153066903352737  val loss:  0.4861389100551605\n",
      "epoch:  18   step:  62   train loss:  0.0037298675160855055  val loss:  0.48342111706733704\n",
      "epoch:  18   step:  63   train loss:  0.00890311598777771  val loss:  0.48416343331336975\n",
      "epoch:  18   step:  64   train loss:  0.0033759335055947304  val loss:  0.48947951197624207\n",
      "epoch:  18   step:  65   train loss:  0.0059862080961465836  val loss:  0.4906879961490631\n",
      "epoch:  18   step:  66   train loss:  0.005477871745824814  val loss:  0.4876593053340912\n",
      "epoch:  18   step:  67   train loss:  0.0026980782859027386  val loss:  0.49128517508506775\n",
      "epoch:  18   step:  68   train loss:  0.0031770667992532253  val loss:  0.4957853853702545\n",
      "epoch:  18   step:  69   train loss:  0.003853635396808386  val loss:  0.49452516436576843\n",
      "epoch:  18   step:  70   train loss:  0.00404616491869092  val loss:  0.4964202046394348\n",
      "epoch:  18   step:  71   train loss:  0.004252301063388586  val loss:  0.49419814348220825\n",
      "epoch:  18   step:  72   train loss:  0.009855128824710846  val loss:  0.4964863061904907\n",
      "epoch:  18   step:  73   train loss:  0.004085774999111891  val loss:  0.49199599027633667\n",
      "epoch:  18   step:  74   train loss:  0.0029719797894358635  val loss:  0.4928826093673706\n",
      "epoch:  18   step:  75   train loss:  0.005589855369180441  val loss:  0.4938385784626007\n",
      "epoch:  18   step:  76   train loss:  0.002221522154286504  val loss:  0.49694082140922546\n",
      "epoch:  18   step:  77   train loss:  0.00410858541727066  val loss:  0.49529552459716797\n",
      "epoch:  18   step:  78   train loss:  0.006843586452305317  val loss:  0.4876419007778168\n",
      "epoch:  18   step:  79   train loss:  0.0038565632421523333  val loss:  0.49393224716186523\n",
      "epoch:  18   step:  80   train loss:  0.0013936501927673817  val loss:  0.4977097511291504\n",
      "epoch:  18   step:  81   train loss:  0.0022365632466971874  val loss:  0.502757728099823\n",
      "epoch:  18   step:  82   train loss:  0.01077483780682087  val loss:  0.509118914604187\n",
      "epoch:  18   step:  83   train loss:  0.006976628210395575  val loss:  0.5127131938934326\n",
      "epoch:  18   step:  84   train loss:  0.00354826170951128  val loss:  0.5140780806541443\n",
      "epoch:  18   step:  85   train loss:  0.008728602901101112  val loss:  0.5191413164138794\n",
      "epoch:  18   step:  86   train loss:  0.003794991411268711  val loss:  0.5233238339424133\n",
      "epoch:  18   step:  87   train loss:  0.004375919699668884  val loss:  0.5203949809074402\n",
      "epoch:  18   step:  88   train loss:  0.004672088660299778  val loss:  0.519501805305481\n",
      "epoch:  18   step:  89   train loss:  0.006033233366906643  val loss:  0.5183049440383911\n",
      "epoch:  18   step:  90   train loss:  0.0028796198312193155  val loss:  0.5136187672615051\n",
      "epoch:  18   step:  91   train loss:  0.007886519655585289  val loss:  0.5143338441848755\n",
      "epoch:  18   step:  92   train loss:  0.002936751116067171  val loss:  0.5112138986587524\n",
      "epoch:  18   step:  93   train loss:  0.005481589585542679  val loss:  0.5091648101806641\n",
      "epoch:  18   step:  94   train loss:  0.004242497496306896  val loss:  0.5041333436965942\n",
      "epoch:  18   step:  95   train loss:  0.006093473639339209  val loss:  0.49738773703575134\n",
      "epoch:  18   step:  96   train loss:  0.0053176842629909515  val loss:  0.4939633011817932\n",
      "epoch:  18   step:  97   train loss:  0.003480717074126005  val loss:  0.4914100468158722\n",
      "epoch:  18   step:  98   train loss:  0.006736232899129391  val loss:  0.492319792509079\n",
      "epoch:  18   step:  99   train loss:  0.007993175648152828  val loss:  0.4941144585609436\n",
      "epoch:  18   step:  100   train loss:  0.0058098891749978065  val loss:  0.4914795160293579\n",
      "epoch:  18   step:  101   train loss:  0.0035425485111773014  val loss:  0.4920647442340851\n",
      "epoch:  18   step:  102   train loss:  0.006583927199244499  val loss:  0.49531638622283936\n",
      "epoch:  18   step:  103   train loss:  0.0029243184253573418  val loss:  0.49256423115730286\n",
      "epoch:  18   step:  104   train loss:  0.004579754546284676  val loss:  0.5002260804176331\n",
      "epoch:  18   step:  105   train loss:  0.006672171875834465  val loss:  0.505253791809082\n",
      "epoch:  18   step:  106   train loss:  0.0047953249886631966  val loss:  0.507167398929596\n",
      "epoch:  18   step:  107   train loss:  0.004740213043987751  val loss:  0.510128915309906\n",
      "epoch:  18   step:  108   train loss:  0.0018443249864503741  val loss:  0.5110490918159485\n",
      "epoch:  18   step:  109   train loss:  0.0059910910204052925  val loss:  0.5132240056991577\n",
      "epoch:  18   step:  110   train loss:  0.004837725777179003  val loss:  0.5129820108413696\n",
      "epoch:  18   step:  111   train loss:  0.0030934058595448732  val loss:  0.5120230913162231\n",
      "epoch:  18   step:  112   train loss:  0.002654526149854064  val loss:  0.5079538226127625\n",
      "epoch:  18   step:  113   train loss:  0.0043257297948002815  val loss:  0.5123338103294373\n",
      "epoch:  18   step:  114   train loss:  0.018899325281381607  val loss:  0.5017460584640503\n",
      "epoch:  18   step:  115   train loss:  0.013702644035220146  val loss:  0.49715057015419006\n",
      "epoch:  18   step:  116   train loss:  0.013252051547169685  val loss:  0.49747732281684875\n",
      "epoch:  18   step:  117   train loss:  0.005020213313400745  val loss:  0.49190840125083923\n",
      "epoch:  18   step:  118   train loss:  0.0038331272080540657  val loss:  0.4867061376571655\n",
      "epoch:  18   step:  119   train loss:  0.002622038358822465  val loss:  0.48756083846092224\n",
      "epoch:  18   step:  120   train loss:  0.0031991174910217524  val loss:  0.49297216534614563\n",
      "epoch:  18   step:  121   train loss:  0.01211665291339159  val loss:  0.4967536926269531\n",
      "epoch:  18   step:  122   train loss:  0.004690603353083134  val loss:  0.4923389256000519\n",
      "epoch:  18   step:  123   train loss:  0.0042406050488352776  val loss:  0.4912634789943695\n",
      "epoch:  18   step:  124   train loss:  0.004843573551625013  val loss:  0.4922087490558624\n",
      "epoch:  18   step:  125   train loss:  0.007550983224064112  val loss:  0.4968947172164917\n",
      "epoch:  18   step:  126   train loss:  0.006200334522873163  val loss:  0.4917934834957123\n",
      "epoch:  18   step:  127   train loss:  0.009725963696837425  val loss:  0.4915410578250885\n",
      "epoch:  18   step:  128   train loss:  0.00776634132489562  val loss:  0.4920702874660492\n",
      "epoch:  18   step:  129   train loss:  0.010969152674078941  val loss:  0.49676379561424255\n",
      "epoch:  18   step:  130   train loss:  0.005325389094650745  val loss:  0.4958540201187134\n",
      "epoch:  18   step:  131   train loss:  0.004981980659067631  val loss:  0.49399933218955994\n",
      "epoch:  18   step:  132   train loss:  0.004239419940859079  val loss:  0.49713465571403503\n",
      "epoch:  18   step:  133   train loss:  0.003963071387261152  val loss:  0.4949578642845154\n",
      "epoch:  18   step:  134   train loss:  0.0020534817595034838  val loss:  0.49175959825515747\n",
      "epoch:  18   step:  135   train loss:  0.0038235930260270834  val loss:  0.4953319728374481\n",
      "epoch:  18   step:  136   train loss:  0.006981814745813608  val loss:  0.4970875382423401\n",
      "epoch:  18   step:  137   train loss:  0.002944373991340399  val loss:  0.4996586740016937\n",
      "epoch:  18   step:  138   train loss:  0.004503423348069191  val loss:  0.5007844567298889\n",
      "epoch:  18   step:  139   train loss:  0.003247068962082267  val loss:  0.4974067509174347\n",
      "epoch:  18   step:  140   train loss:  0.019995739683508873  val loss:  0.49457916617393494\n",
      "epoch:  18   step:  141   train loss:  0.00554173719137907  val loss:  0.4899754524230957\n",
      "epoch:  18   step:  142   train loss:  0.004839368164539337  val loss:  0.48651906847953796\n",
      "epoch:  18   step:  143   train loss:  0.00418801698833704  val loss:  0.4856344163417816\n",
      "epoch:  18   step:  144   train loss:  0.004791556857526302  val loss:  0.4807724952697754\n",
      "epoch:  18   step:  145   train loss:  0.007330617867410183  val loss:  0.47786638140678406\n",
      "epoch:  18   step:  146   train loss:  0.005867371801286936  val loss:  0.47693321108818054\n",
      "epoch:  18   step:  147   train loss:  0.005042584612965584  val loss:  0.4815404415130615\n",
      "epoch:  18   step:  148   train loss:  0.004278799518942833  val loss:  0.4809153378009796\n",
      "epoch:  18   step:  149   train loss:  0.0042863585986196995  val loss:  0.4773189425468445\n",
      "epoch:  18   step:  150   train loss:  0.003844211343675852  val loss:  0.47615671157836914\n",
      "epoch:  18   step:  151   train loss:  0.010243101045489311  val loss:  0.4784124195575714\n",
      "epoch:  18   step:  152   train loss:  0.004341343883424997  val loss:  0.4853510558605194\n",
      "epoch:  18   step:  153   train loss:  0.014004496857523918  val loss:  0.4842901825904846\n",
      "epoch:  18   step:  154   train loss:  0.00471175042912364  val loss:  0.485797256231308\n",
      "epoch:  18   step:  155   train loss:  0.008639580570161343  val loss:  0.48149093985557556\n",
      "epoch:  18   step:  156   train loss:  0.005906600505113602  val loss:  0.476228803396225\n",
      "epoch:  18   step:  157   train loss:  0.004945193417370319  val loss:  0.47150835394859314\n",
      "epoch:  18   step:  158   train loss:  0.003901038784533739  val loss:  0.4711441695690155\n",
      "epoch:  18   step:  159   train loss:  0.005107599310576916  val loss:  0.47355833649635315\n",
      "epoch:  18   step:  160   train loss:  0.00722707062959671  val loss:  0.4743228852748871\n",
      "epoch:  18   step:  161   train loss:  0.007401930168271065  val loss:  0.47817614674568176\n",
      "epoch:  18   step:  162   train loss:  0.010728836990892887  val loss:  0.48289334774017334\n",
      "epoch:  18   step:  163   train loss:  0.004194633103907108  val loss:  0.4870443642139435\n",
      "epoch:  18   step:  164   train loss:  0.005406832788139582  val loss:  0.490811288356781\n",
      "epoch:  18   step:  165   train loss:  0.0012002167059108615  val loss:  0.5000150799751282\n",
      "epoch:  19   step:  0   train loss:  0.0018366860458627343  val loss:  0.5037273168563843\n",
      "epoch:  19   step:  1   train loss:  0.00476796692237258  val loss:  0.5094234347343445\n",
      "epoch:  19   step:  2   train loss:  0.004221080802381039  val loss:  0.5094999670982361\n",
      "epoch:  19   step:  3   train loss:  0.003125901799649  val loss:  0.5182790756225586\n",
      "epoch:  19   step:  4   train loss:  0.0022642575204372406  val loss:  0.5184357762336731\n",
      "epoch:  19   step:  5   train loss:  0.001997139072045684  val loss:  0.5182405710220337\n",
      "epoch:  19   step:  6   train loss:  0.002873303834348917  val loss:  0.5170828700065613\n",
      "epoch:  19   step:  7   train loss:  0.0020433657336980104  val loss:  0.5209059715270996\n",
      "epoch:  19   step:  8   train loss:  0.0029828883707523346  val loss:  0.5178056359291077\n",
      "epoch:  19   step:  9   train loss:  0.0016274997033178806  val loss:  0.5248372554779053\n",
      "epoch:  19   step:  10   train loss:  0.0038780835457146168  val loss:  0.5175421237945557\n",
      "epoch:  19   step:  11   train loss:  0.0015235606115311384  val loss:  0.5180501937866211\n",
      "epoch:  19   step:  12   train loss:  0.0031613330356776714  val loss:  0.5181035399436951\n",
      "epoch:  19   step:  13   train loss:  0.002361918333917856  val loss:  0.5199270248413086\n",
      "epoch:  19   step:  14   train loss:  0.002100727753713727  val loss:  0.5162543654441833\n",
      "epoch:  19   step:  15   train loss:  0.002444697078317404  val loss:  0.5160879492759705\n",
      "epoch:  19   step:  16   train loss:  0.0028626848943531513  val loss:  0.5166259407997131\n",
      "epoch:  19   step:  17   train loss:  0.0014834358589723706  val loss:  0.5191662907600403\n",
      "epoch:  19   step:  18   train loss:  0.0025580935180187225  val loss:  0.5199435353279114\n",
      "epoch:  19   step:  19   train loss:  0.004084219690412283  val loss:  0.5211745500564575\n",
      "epoch:  19   step:  20   train loss:  0.006485274992883205  val loss:  0.5155420303344727\n",
      "epoch:  19   step:  21   train loss:  0.005484779365360737  val loss:  0.5150113105773926\n",
      "epoch:  19   step:  22   train loss:  0.00487960409373045  val loss:  0.5192364454269409\n",
      "epoch:  19   step:  23   train loss:  0.0023603010922670364  val loss:  0.5154457688331604\n",
      "epoch:  19   step:  24   train loss:  0.008533789776265621  val loss:  0.5095556378364563\n",
      "epoch:  19   step:  25   train loss:  0.00799317192286253  val loss:  0.5030889511108398\n",
      "epoch:  19   step:  26   train loss:  0.0017540411790832877  val loss:  0.5038523077964783\n",
      "epoch:  19   step:  27   train loss:  0.001094346633180976  val loss:  0.499468058347702\n",
      "epoch:  19   step:  28   train loss:  0.0016815467970445752  val loss:  0.49541229009628296\n",
      "epoch:  19   step:  29   train loss:  0.0050401845946908  val loss:  0.49849769473075867\n",
      "epoch:  19   step:  30   train loss:  0.005617784336209297  val loss:  0.49738314747810364\n",
      "epoch:  19   step:  31   train loss:  0.0034971721470355988  val loss:  0.49743619561195374\n",
      "epoch:  19   step:  32   train loss:  0.005217648111283779  val loss:  0.5014551281929016\n",
      "epoch:  19   step:  33   train loss:  0.0027655865997076035  val loss:  0.5053542256355286\n",
      "epoch:  19   step:  34   train loss:  0.0061316885985434055  val loss:  0.5125846266746521\n",
      "epoch:  19   step:  35   train loss:  0.003481493564322591  val loss:  0.5100289583206177\n",
      "epoch:  19   step:  36   train loss:  0.004762954078614712  val loss:  0.5175301432609558\n",
      "epoch:  19   step:  37   train loss:  0.00487740570679307  val loss:  0.5217236876487732\n",
      "epoch:  19   step:  38   train loss:  0.003653709776699543  val loss:  0.5304621458053589\n",
      "epoch:  19   step:  39   train loss:  0.004042983986437321  val loss:  0.5304758548736572\n",
      "epoch:  19   step:  40   train loss:  0.0025964942760765553  val loss:  0.5349217057228088\n",
      "epoch:  19   step:  41   train loss:  0.004287169314920902  val loss:  0.5304874777793884\n",
      "epoch:  19   step:  42   train loss:  0.003544148523360491  val loss:  0.5240392088890076\n",
      "epoch:  19   step:  43   train loss:  0.0030453328508883715  val loss:  0.5309502482414246\n",
      "epoch:  19   step:  44   train loss:  0.003315047360956669  val loss:  0.5313791036605835\n",
      "epoch:  19   step:  45   train loss:  0.003597796894609928  val loss:  0.5331966876983643\n",
      "epoch:  19   step:  46   train loss:  0.0018974015256389976  val loss:  0.5329426527023315\n",
      "epoch:  19   step:  47   train loss:  0.003258500713855028  val loss:  0.5382329225540161\n",
      "epoch:  19   step:  48   train loss:  0.009784776717424393  val loss:  0.5454356074333191\n",
      "epoch:  19   step:  49   train loss:  0.0032623030710965395  val loss:  0.5383455157279968\n",
      "epoch:  19   step:  50   train loss:  0.00440051918849349  val loss:  0.5367155075073242\n",
      "epoch:  19   step:  51   train loss:  0.0018375430954620242  val loss:  0.530488908290863\n",
      "epoch:  19   step:  52   train loss:  0.00460909865796566  val loss:  0.5270853638648987\n",
      "epoch:  19   step:  53   train loss:  0.005488877184689045  val loss:  0.5226654410362244\n",
      "epoch:  19   step:  54   train loss:  0.001600183779373765  val loss:  0.5155823230743408\n",
      "epoch:  19   step:  55   train loss:  0.002826908603310585  val loss:  0.5183694958686829\n",
      "epoch:  19   step:  56   train loss:  0.003206471912562847  val loss:  0.5256038308143616\n",
      "epoch:  19   step:  57   train loss:  0.004164692014455795  val loss:  0.5267401933670044\n",
      "epoch:  19   step:  58   train loss:  0.0023320838809013367  val loss:  0.5302804708480835\n",
      "epoch:  19   step:  59   train loss:  0.0024374821223318577  val loss:  0.5273838043212891\n",
      "epoch:  19   step:  60   train loss:  0.0038016601465642452  val loss:  0.5293227434158325\n",
      "epoch:  19   step:  61   train loss:  0.002880724146962166  val loss:  0.5351884365081787\n",
      "epoch:  19   step:  62   train loss:  0.002220599912106991  val loss:  0.5318374037742615\n",
      "epoch:  19   step:  63   train loss:  0.003814056748524308  val loss:  0.5302835702896118\n",
      "epoch:  19   step:  64   train loss:  0.003082671668380499  val loss:  0.5303534865379333\n",
      "epoch:  19   step:  65   train loss:  0.0022963506635278463  val loss:  0.5265588164329529\n",
      "epoch:  19   step:  66   train loss:  0.0030404170975089073  val loss:  0.5243529677391052\n",
      "epoch:  19   step:  67   train loss:  0.0018823286518454552  val loss:  0.5231530070304871\n",
      "epoch:  19   step:  68   train loss:  0.0012114847777411342  val loss:  0.522599458694458\n",
      "epoch:  19   step:  69   train loss:  0.0026320237666368484  val loss:  0.5205727815628052\n",
      "epoch:  19   step:  70   train loss:  0.002552008256316185  val loss:  0.5230733752250671\n",
      "epoch:  19   step:  71   train loss:  0.006107148714363575  val loss:  0.5240755677223206\n",
      "epoch:  19   step:  72   train loss:  0.0028245921712368727  val loss:  0.5258662104606628\n",
      "epoch:  19   step:  73   train loss:  0.0020745161455124617  val loss:  0.5275535583496094\n",
      "epoch:  19   step:  74   train loss:  0.001410299097187817  val loss:  0.5233983397483826\n",
      "epoch:  19   step:  75   train loss:  0.0035399068146944046  val loss:  0.5233689546585083\n",
      "epoch:  19   step:  76   train loss:  0.0027639311738312244  val loss:  0.5214394927024841\n",
      "epoch:  19   step:  77   train loss:  0.002688242122530937  val loss:  0.5213521718978882\n",
      "epoch:  19   step:  78   train loss:  0.0035552545450627804  val loss:  0.5188364386558533\n",
      "epoch:  19   step:  79   train loss:  0.0016609430313110352  val loss:  0.5136962532997131\n",
      "epoch:  19   step:  80   train loss:  0.002732332330197096  val loss:  0.5161596536636353\n",
      "epoch:  19   step:  81   train loss:  0.004237025510519743  val loss:  0.518545925617218\n",
      "epoch:  19   step:  82   train loss:  0.0016415840946137905  val loss:  0.5156868696212769\n",
      "epoch:  19   step:  83   train loss:  0.0018665464594960213  val loss:  0.5166467428207397\n",
      "epoch:  19   step:  84   train loss:  0.0036689569242298603  val loss:  0.5209789872169495\n",
      "epoch:  19   step:  85   train loss:  0.0048470208421349525  val loss:  0.5190421938896179\n",
      "epoch:  19   step:  86   train loss:  0.001585477963089943  val loss:  0.5173710584640503\n",
      "epoch:  19   step:  87   train loss:  0.0034210607409477234  val loss:  0.5145828127861023\n",
      "epoch:  19   step:  88   train loss:  0.0022826248314231634  val loss:  0.5108993053436279\n",
      "epoch:  19   step:  89   train loss:  0.0028989140409976244  val loss:  0.5039326548576355\n",
      "epoch:  19   step:  90   train loss:  0.002587202936410904  val loss:  0.5060352087020874\n",
      "epoch:  19   step:  91   train loss:  0.0027687859255820513  val loss:  0.5027251839637756\n",
      "epoch:  19   step:  92   train loss:  0.004436523653566837  val loss:  0.5038283467292786\n",
      "epoch:  19   step:  93   train loss:  0.0028707897290587425  val loss:  0.5107961297035217\n",
      "epoch:  19   step:  94   train loss:  0.002201368799433112  val loss:  0.5092539191246033\n",
      "epoch:  19   step:  95   train loss:  0.005211453884840012  val loss:  0.5121278166770935\n",
      "epoch:  19   step:  96   train loss:  0.002853545593097806  val loss:  0.5098880529403687\n",
      "epoch:  19   step:  97   train loss:  0.0024533590767532587  val loss:  0.5136887431144714\n",
      "epoch:  19   step:  98   train loss:  0.002672700909897685  val loss:  0.5134932398796082\n",
      "epoch:  19   step:  99   train loss:  0.003826478961855173  val loss:  0.5108184814453125\n",
      "epoch:  19   step:  100   train loss:  0.004844220355153084  val loss:  0.5160617828369141\n",
      "epoch:  19   step:  101   train loss:  0.008009631186723709  val loss:  0.5218517780303955\n",
      "epoch:  19   step:  102   train loss:  0.004425361752510071  val loss:  0.5212805867195129\n",
      "epoch:  19   step:  103   train loss:  0.0032372912392020226  val loss:  0.5216194987297058\n",
      "epoch:  19   step:  104   train loss:  0.00665645208209753  val loss:  0.5262582898139954\n",
      "epoch:  19   step:  105   train loss:  0.008661316707730293  val loss:  0.5262590050697327\n",
      "epoch:  19   step:  106   train loss:  0.002129988046362996  val loss:  0.526464581489563\n",
      "epoch:  19   step:  107   train loss:  0.0022420210298150778  val loss:  0.5209904909133911\n",
      "epoch:  19   step:  108   train loss:  0.019275285303592682  val loss:  0.5360803008079529\n",
      "epoch:  19   step:  109   train loss:  0.0023331791162490845  val loss:  0.5447909235954285\n",
      "epoch:  19   step:  110   train loss:  0.0019782409071922302  val loss:  0.555860161781311\n",
      "epoch:  19   step:  111   train loss:  0.0032623070292174816  val loss:  0.5705900192260742\n",
      "epoch:  19   step:  112   train loss:  0.003517019096761942  val loss:  0.5761070251464844\n",
      "epoch:  19   step:  113   train loss:  0.0018148386152461171  val loss:  0.5823903679847717\n",
      "epoch:  19   step:  114   train loss:  0.0025515223387628794  val loss:  0.5884177088737488\n",
      "epoch:  19   step:  115   train loss:  0.0038296268321573734  val loss:  0.5906299352645874\n",
      "epoch:  19   step:  116   train loss:  0.0026614340022206306  val loss:  0.5931108593940735\n",
      "epoch:  19   step:  117   train loss:  0.004448444116860628  val loss:  0.5962836146354675\n",
      "epoch:  19   step:  118   train loss:  0.0028563786763697863  val loss:  0.5993154048919678\n",
      "epoch:  19   step:  119   train loss:  0.0126614049077034  val loss:  0.5908164978027344\n",
      "epoch:  19   step:  120   train loss:  0.004830934572964907  val loss:  0.5812907218933105\n",
      "epoch:  19   step:  121   train loss:  0.004517357796430588  val loss:  0.5745759010314941\n",
      "epoch:  19   step:  122   train loss:  0.018042515963315964  val loss:  0.5702267289161682\n",
      "epoch:  19   step:  123   train loss:  0.00489009777083993  val loss:  0.5680294036865234\n",
      "epoch:  19   step:  124   train loss:  0.0013765394687652588  val loss:  0.562576413154602\n",
      "epoch:  19   step:  125   train loss:  0.0027639667969197035  val loss:  0.5552295446395874\n",
      "epoch:  19   step:  126   train loss:  0.006820693612098694  val loss:  0.5542318820953369\n",
      "epoch:  19   step:  127   train loss:  0.003957271575927734  val loss:  0.5511125326156616\n",
      "epoch:  19   step:  128   train loss:  0.007797009311616421  val loss:  0.5440942645072937\n",
      "epoch:  19   step:  129   train loss:  0.004206329584121704  val loss:  0.5536560416221619\n",
      "epoch:  19   step:  130   train loss:  0.002612660638988018  val loss:  0.5572963356971741\n",
      "epoch:  19   step:  131   train loss:  0.004200478084385395  val loss:  0.5501776933670044\n",
      "epoch:  19   step:  132   train loss:  0.004407908767461777  val loss:  0.5461481213569641\n",
      "epoch:  19   step:  133   train loss:  0.008152544498443604  val loss:  0.5359668135643005\n",
      "epoch:  19   step:  134   train loss:  0.004689306020736694  val loss:  0.5393876433372498\n",
      "epoch:  19   step:  135   train loss:  0.00764935277402401  val loss:  0.536985456943512\n",
      "epoch:  19   step:  136   train loss:  0.0034974829759448767  val loss:  0.5374467968940735\n",
      "epoch:  19   step:  137   train loss:  0.0074295420199632645  val loss:  0.5389638543128967\n",
      "epoch:  19   step:  138   train loss:  0.0027138874866068363  val loss:  0.5322885513305664\n",
      "epoch:  19   step:  139   train loss:  0.003378864610567689  val loss:  0.5322096347808838\n",
      "epoch:  19   step:  140   train loss:  0.004060567356646061  val loss:  0.5349980592727661\n",
      "epoch:  19   step:  141   train loss:  0.0023276861757040024  val loss:  0.5315549969673157\n",
      "epoch:  19   step:  142   train loss:  0.0033420331310480833  val loss:  0.5227693319320679\n",
      "epoch:  19   step:  143   train loss:  0.002497997134923935  val loss:  0.5280985236167908\n",
      "epoch:  19   step:  144   train loss:  0.0025675450451672077  val loss:  0.529486894607544\n",
      "epoch:  19   step:  145   train loss:  0.003533810842782259  val loss:  0.5317798852920532\n",
      "epoch:  19   step:  146   train loss:  0.0025696742814034224  val loss:  0.5326350331306458\n",
      "epoch:  19   step:  147   train loss:  0.002169206039980054  val loss:  0.5307775139808655\n",
      "epoch:  19   step:  148   train loss:  0.0028775264509022236  val loss:  0.5383715033531189\n",
      "epoch:  19   step:  149   train loss:  0.0034673193003982306  val loss:  0.5397042632102966\n",
      "epoch:  19   step:  150   train loss:  0.003413137514144182  val loss:  0.5496472120285034\n",
      "epoch:  19   step:  151   train loss:  0.0023993123322725296  val loss:  0.5531812310218811\n",
      "epoch:  19   step:  152   train loss:  0.02259404957294464  val loss:  0.5788783431053162\n",
      "epoch:  19   step:  153   train loss:  0.0034132031723856926  val loss:  0.595730185508728\n",
      "epoch:  19   step:  154   train loss:  0.00257244985550642  val loss:  0.6059553623199463\n",
      "epoch:  19   step:  155   train loss:  0.00210357503965497  val loss:  0.6210760474205017\n",
      "epoch:  19   step:  156   train loss:  0.0031052238773554564  val loss:  0.6339550614356995\n",
      "epoch:  19   step:  157   train loss:  0.0037046396173536777  val loss:  0.6492820382118225\n",
      "epoch:  19   step:  158   train loss:  0.002625614171847701  val loss:  0.6645728945732117\n",
      "epoch:  19   step:  159   train loss:  0.0015174702275544405  val loss:  0.6708093285560608\n",
      "epoch:  19   step:  160   train loss:  0.0042364103719592094  val loss:  0.6763567924499512\n",
      "epoch:  19   step:  161   train loss:  0.0028083291836082935  val loss:  0.6911927461624146\n",
      "epoch:  19   step:  162   train loss:  0.01875067688524723  val loss:  0.6842940449714661\n",
      "epoch:  19   step:  163   train loss:  0.011262231506407261  val loss:  0.6784881949424744\n",
      "epoch:  19   step:  164   train loss:  0.005304650403559208  val loss:  0.684041440486908\n",
      "epoch:  19   step:  165   train loss:  0.0012682543601840734  val loss:  0.6920865774154663\n",
      "epoch:  20   step:  0   train loss:  0.003967110067605972  val loss:  0.691834568977356\n",
      "epoch:  20   step:  1   train loss:  0.007341047283262014  val loss:  0.6934346556663513\n",
      "epoch:  20   step:  2   train loss:  0.001981946872547269  val loss:  0.6856539249420166\n",
      "epoch:  20   step:  3   train loss:  0.0023507659789174795  val loss:  0.6859626770019531\n",
      "epoch:  20   step:  4   train loss:  0.010317496955394745  val loss:  0.6895270347595215\n",
      "epoch:  20   step:  5   train loss:  0.004071027971804142  val loss:  0.7013285160064697\n",
      "epoch:  20   step:  6   train loss:  0.0049180141650140285  val loss:  0.6989917755126953\n",
      "epoch:  20   step:  7   train loss:  0.0023805024102330208  val loss:  0.6842901706695557\n",
      "epoch:  20   step:  8   train loss:  0.0017482269322499633  val loss:  0.676947295665741\n",
      "epoch:  20   step:  9   train loss:  0.004030268173664808  val loss:  0.6769973635673523\n",
      "epoch:  20   step:  10   train loss:  0.0023025539703667164  val loss:  0.678260087966919\n",
      "epoch:  20   step:  11   train loss:  0.0027293378952890635  val loss:  0.678492546081543\n",
      "epoch:  20   step:  12   train loss:  0.005215659737586975  val loss:  0.6719464063644409\n",
      "epoch:  20   step:  13   train loss:  0.0034217224456369877  val loss:  0.6784815788269043\n",
      "epoch:  20   step:  14   train loss:  0.003836007323116064  val loss:  0.6768458485603333\n",
      "epoch:  20   step:  15   train loss:  0.002027339767664671  val loss:  0.6697635650634766\n",
      "epoch:  20   step:  16   train loss:  0.007117828354239464  val loss:  0.6709232330322266\n",
      "epoch:  20   step:  17   train loss:  0.0046502393670380116  val loss:  0.6627377271652222\n",
      "epoch:  20   step:  18   train loss:  0.0035602205898612738  val loss:  0.65738844871521\n",
      "epoch:  20   step:  19   train loss:  0.0025797795969992876  val loss:  0.6491999626159668\n",
      "epoch:  20   step:  20   train loss:  0.0013606806751340628  val loss:  0.6554841995239258\n",
      "epoch:  20   step:  21   train loss:  0.0014811222208663821  val loss:  0.6538366675376892\n",
      "epoch:  20   step:  22   train loss:  0.0016254348447546363  val loss:  0.6484588384628296\n",
      "epoch:  20   step:  23   train loss:  0.007104184478521347  val loss:  0.6396872401237488\n",
      "epoch:  20   step:  24   train loss:  0.0022813943214714527  val loss:  0.6295853853225708\n",
      "epoch:  20   step:  25   train loss:  0.007683303207159042  val loss:  0.6173515915870667\n",
      "epoch:  20   step:  26   train loss:  0.0026675607077777386  val loss:  0.6208435297012329\n",
      "epoch:  20   step:  27   train loss:  0.0011335319140926003  val loss:  0.6173250675201416\n",
      "epoch:  20   step:  28   train loss:  0.0024687862023711205  val loss:  0.6159917116165161\n",
      "epoch:  20   step:  29   train loss:  0.0021265000104904175  val loss:  0.6144599914550781\n",
      "epoch:  20   step:  30   train loss:  0.001863681711256504  val loss:  0.6139124631881714\n",
      "epoch:  20   step:  31   train loss:  0.0024547323118895292  val loss:  0.610867977142334\n",
      "epoch:  20   step:  32   train loss:  0.004521272145211697  val loss:  0.6191982626914978\n",
      "epoch:  20   step:  33   train loss:  0.0028599186334758997  val loss:  0.6169067621231079\n",
      "epoch:  20   step:  34   train loss:  0.0018129688687622547  val loss:  0.6128312349319458\n",
      "epoch:  20   step:  35   train loss:  0.0029741176404058933  val loss:  0.6191580891609192\n",
      "epoch:  20   step:  36   train loss:  0.0021224776282906532  val loss:  0.6283689141273499\n",
      "epoch:  20   step:  37   train loss:  0.0025965373497456312  val loss:  0.6231408715248108\n",
      "epoch:  20   step:  38   train loss:  0.0025221852120012045  val loss:  0.6192644238471985\n",
      "epoch:  20   step:  39   train loss:  0.0016705022426322103  val loss:  0.6079839468002319\n",
      "epoch:  20   step:  40   train loss:  0.0016218498349189758  val loss:  0.6014189124107361\n",
      "epoch:  20   step:  41   train loss:  0.002266300842165947  val loss:  0.5977548956871033\n",
      "epoch:  20   step:  42   train loss:  0.001980516593903303  val loss:  0.5967825651168823\n",
      "epoch:  20   step:  43   train loss:  0.002266800496727228  val loss:  0.6019904017448425\n",
      "epoch:  20   step:  44   train loss:  0.004225732292979956  val loss:  0.6020182967185974\n",
      "epoch:  20   step:  45   train loss:  0.001756872981786728  val loss:  0.6063885688781738\n",
      "epoch:  20   step:  46   train loss:  0.0015886796172708273  val loss:  0.5995361804962158\n",
      "epoch:  20   step:  47   train loss:  0.0010050435084849596  val loss:  0.5930460095405579\n",
      "epoch:  20   step:  48   train loss:  0.0016653644852340221  val loss:  0.5908775925636292\n",
      "epoch:  20   step:  49   train loss:  0.0019406710052862763  val loss:  0.5903473496437073\n",
      "epoch:  20   step:  50   train loss:  0.001825913554057479  val loss:  0.5841271281242371\n",
      "epoch:  20   step:  51   train loss:  0.007053778972476721  val loss:  0.5765008926391602\n",
      "epoch:  20   step:  52   train loss:  0.005172623321413994  val loss:  0.5796043872833252\n",
      "epoch:  20   step:  53   train loss:  0.006185535807162523  val loss:  0.5746964812278748\n",
      "epoch:  20   step:  54   train loss:  0.004665086977183819  val loss:  0.571720540523529\n",
      "epoch:  20   step:  55   train loss:  0.0030797296203672886  val loss:  0.5664047002792358\n",
      "epoch:  20   step:  56   train loss:  0.003231724025681615  val loss:  0.5664845705032349\n",
      "epoch:  20   step:  57   train loss:  0.005405593663454056  val loss:  0.5616794228553772\n",
      "epoch:  20   step:  58   train loss:  0.0025666425935924053  val loss:  0.5635208487510681\n",
      "epoch:  20   step:  59   train loss:  0.0006069109076634049  val loss:  0.5626495480537415\n",
      "epoch:  20   step:  60   train loss:  0.002879776293411851  val loss:  0.5611757636070251\n",
      "epoch:  20   step:  61   train loss:  0.0018184606451541185  val loss:  0.5597801208496094\n",
      "epoch:  20   step:  62   train loss:  0.0022362584713846445  val loss:  0.5555141568183899\n",
      "epoch:  20   step:  63   train loss:  0.0018659253837540746  val loss:  0.5564313530921936\n",
      "epoch:  20   step:  64   train loss:  0.00304599292576313  val loss:  0.5564494729042053\n",
      "epoch:  20   step:  65   train loss:  0.0014787990367040038  val loss:  0.55381840467453\n",
      "epoch:  20   step:  66   train loss:  0.003231493756175041  val loss:  0.5600298047065735\n",
      "epoch:  20   step:  67   train loss:  0.001253635622560978  val loss:  0.5570546388626099\n",
      "epoch:  20   step:  68   train loss:  0.003530908841639757  val loss:  0.5541746616363525\n",
      "epoch:  20   step:  69   train loss:  0.0011834441684186459  val loss:  0.5571485161781311\n",
      "epoch:  20   step:  70   train loss:  0.003007249440997839  val loss:  0.5498649477958679\n",
      "epoch:  20   step:  71   train loss:  0.0031082858331501484  val loss:  0.553741991519928\n",
      "epoch:  20   step:  72   train loss:  0.0023139724507927895  val loss:  0.5536735653877258\n",
      "epoch:  20   step:  73   train loss:  0.002791488077491522  val loss:  0.5605576038360596\n",
      "epoch:  20   step:  74   train loss:  0.0012637218460440636  val loss:  0.5658695101737976\n",
      "epoch:  20   step:  75   train loss:  0.0015513466205447912  val loss:  0.567998468875885\n",
      "epoch:  20   step:  76   train loss:  0.0016261647688224912  val loss:  0.569195032119751\n",
      "epoch:  20   step:  77   train loss:  0.002634336007758975  val loss:  0.5685043334960938\n",
      "epoch:  20   step:  78   train loss:  0.0021458729170262814  val loss:  0.5758509635925293\n",
      "epoch:  20   step:  79   train loss:  0.0012403065338730812  val loss:  0.5692272186279297\n",
      "epoch:  20   step:  80   train loss:  0.0021993708796799183  val loss:  0.5645689368247986\n",
      "epoch:  20   step:  81   train loss:  0.0012262581149116158  val loss:  0.5607475638389587\n",
      "epoch:  20   step:  82   train loss:  0.00201970967464149  val loss:  0.5640531182289124\n",
      "epoch:  20   step:  83   train loss:  0.0019608631264418364  val loss:  0.5645793676376343\n",
      "epoch:  20   step:  84   train loss:  0.0027380071114748716  val loss:  0.5613470077514648\n",
      "epoch:  20   step:  85   train loss:  0.0018606355879455805  val loss:  0.5629777908325195\n",
      "epoch:  20   step:  86   train loss:  0.0013392537366598845  val loss:  0.56389319896698\n",
      "epoch:  20   step:  87   train loss:  0.00274474173784256  val loss:  0.5663833618164062\n",
      "epoch:  20   step:  88   train loss:  0.005092747043818235  val loss:  0.5632658004760742\n",
      "epoch:  20   step:  89   train loss:  0.001768781105056405  val loss:  0.5656332969665527\n",
      "epoch:  20   step:  90   train loss:  0.0029547419399023056  val loss:  0.5598950386047363\n",
      "epoch:  20   step:  91   train loss:  0.002512561157345772  val loss:  0.5611070394515991\n",
      "epoch:  20   step:  92   train loss:  0.002379343379288912  val loss:  0.560794472694397\n",
      "epoch:  20   step:  93   train loss:  0.004043255466967821  val loss:  0.5655458569526672\n",
      "epoch:  20   step:  94   train loss:  0.0020838933996856213  val loss:  0.5726861953735352\n",
      "epoch:  20   step:  95   train loss:  0.0011392569867894053  val loss:  0.5684561729431152\n",
      "epoch:  20   step:  96   train loss:  0.002227058634161949  val loss:  0.5688597559928894\n",
      "epoch:  20   step:  97   train loss:  0.0016759666614234447  val loss:  0.5695767998695374\n",
      "epoch:  20   step:  98   train loss:  0.0019202916882932186  val loss:  0.5615947842597961\n",
      "epoch:  20   step:  99   train loss:  0.0024314061738550663  val loss:  0.5679025650024414\n",
      "epoch:  20   step:  100   train loss:  0.001889356761239469  val loss:  0.5662431120872498\n",
      "epoch:  20   step:  101   train loss:  0.0022883201017975807  val loss:  0.5675038695335388\n",
      "epoch:  20   step:  102   train loss:  0.002060619182884693  val loss:  0.5625280737876892\n",
      "epoch:  20   step:  103   train loss:  0.0011227282229810953  val loss:  0.5574601292610168\n",
      "epoch:  20   step:  104   train loss:  0.006455121096223593  val loss:  0.565971314907074\n",
      "epoch:  20   step:  105   train loss:  0.002394048497080803  val loss:  0.5649774074554443\n",
      "epoch:  20   step:  106   train loss:  0.002255625557154417  val loss:  0.5823841691017151\n",
      "epoch:  20   step:  107   train loss:  0.007692565210163593  val loss:  0.5761421322822571\n",
      "epoch:  20   step:  108   train loss:  0.000995796057395637  val loss:  0.5796276926994324\n",
      "epoch:  20   step:  109   train loss:  0.002687839325517416  val loss:  0.5771801471710205\n",
      "epoch:  20   step:  110   train loss:  0.0030282943043857813  val loss:  0.5732988119125366\n",
      "epoch:  20   step:  111   train loss:  0.00324312224984169  val loss:  0.5668796300888062\n",
      "epoch:  20   step:  112   train loss:  0.0018452618969604373  val loss:  0.5641575455665588\n",
      "epoch:  20   step:  113   train loss:  0.002073906362056732  val loss:  0.5563461780548096\n",
      "epoch:  20   step:  114   train loss:  0.0016548768617212772  val loss:  0.5662574172019958\n",
      "epoch:  20   step:  115   train loss:  0.001202194020152092  val loss:  0.5734480023384094\n",
      "epoch:  20   step:  116   train loss:  0.0030240893829613924  val loss:  0.5759238600730896\n",
      "epoch:  20   step:  117   train loss:  0.0012376675149425864  val loss:  0.5782508850097656\n",
      "epoch:  20   step:  118   train loss:  0.0010438438039273024  val loss:  0.5835553407669067\n",
      "epoch:  20   step:  119   train loss:  0.0033843149431049824  val loss:  0.5874946713447571\n",
      "epoch:  20   step:  120   train loss:  0.003518674522638321  val loss:  0.583712100982666\n",
      "epoch:  20   step:  121   train loss:  0.0024812081828713417  val loss:  0.5708556771278381\n",
      "epoch:  20   step:  122   train loss:  0.00150204636156559  val loss:  0.5610785484313965\n",
      "epoch:  20   step:  123   train loss:  0.001834512804634869  val loss:  0.5548698306083679\n",
      "epoch:  20   step:  124   train loss:  0.0018855829257518053  val loss:  0.5650392174720764\n",
      "epoch:  20   step:  125   train loss:  0.0008934164652600884  val loss:  0.5665656924247742\n",
      "epoch:  20   step:  126   train loss:  0.002884524641558528  val loss:  0.5639575719833374\n",
      "epoch:  20   step:  127   train loss:  0.002706082072108984  val loss:  0.5716114044189453\n",
      "epoch:  20   step:  128   train loss:  0.0020951873157173395  val loss:  0.5760908722877502\n",
      "epoch:  20   step:  129   train loss:  0.0010560285300016403  val loss:  0.5850544571876526\n",
      "epoch:  20   step:  130   train loss:  0.0019361674785614014  val loss:  0.5834247469902039\n",
      "epoch:  20   step:  131   train loss:  0.002792691346257925  val loss:  0.5901073217391968\n",
      "epoch:  20   step:  132   train loss:  0.0016747767804190516  val loss:  0.5903593301773071\n",
      "epoch:  20   step:  133   train loss:  0.002077787183225155  val loss:  0.5879234671592712\n",
      "epoch:  20   step:  134   train loss:  0.003519466845318675  val loss:  0.5966202616691589\n",
      "epoch:  20   step:  135   train loss:  0.0014147519832476974  val loss:  0.5903896689414978\n",
      "epoch:  20   step:  136   train loss:  0.0014127881731837988  val loss:  0.5901479721069336\n",
      "epoch:  20   step:  137   train loss:  0.001118574757128954  val loss:  0.5937181115150452\n",
      "epoch:  20   step:  138   train loss:  0.0026317047886550426  val loss:  0.5838993191719055\n",
      "epoch:  20   step:  139   train loss:  0.0030322717502713203  val loss:  0.5920095443725586\n",
      "epoch:  20   step:  140   train loss:  0.0018742935499176383  val loss:  0.5865463018417358\n",
      "epoch:  20   step:  141   train loss:  0.0017119291005656123  val loss:  0.5900132656097412\n",
      "epoch:  20   step:  142   train loss:  0.0029700780287384987  val loss:  0.5954702496528625\n",
      "epoch:  20   step:  143   train loss:  0.003062944393604994  val loss:  0.6025592684745789\n",
      "epoch:  20   step:  144   train loss:  0.002468045335263014  val loss:  0.5968217849731445\n",
      "epoch:  20   step:  145   train loss:  0.0012281483504921198  val loss:  0.5980993509292603\n",
      "epoch:  20   step:  146   train loss:  0.0033885687589645386  val loss:  0.5967567563056946\n",
      "epoch:  20   step:  147   train loss:  0.002487852005288005  val loss:  0.5975182056427002\n",
      "epoch:  20   step:  148   train loss:  0.002393093192949891  val loss:  0.5997907519340515\n",
      "epoch:  20   step:  149   train loss:  0.0015355044743046165  val loss:  0.6056132316589355\n",
      "epoch:  20   step:  150   train loss:  0.00174612901173532  val loss:  0.6004009246826172\n",
      "epoch:  20   step:  151   train loss:  0.0017256556311622262  val loss:  0.6018655896186829\n",
      "epoch:  20   step:  152   train loss:  0.00141819566488266  val loss:  0.6072286367416382\n",
      "epoch:  20   step:  153   train loss:  0.0018631964921951294  val loss:  0.6002304553985596\n",
      "epoch:  20   step:  154   train loss:  0.001356876571662724  val loss:  0.5997714996337891\n",
      "epoch:  20   step:  155   train loss:  0.0019618687219917774  val loss:  0.6088283658027649\n",
      "epoch:  20   step:  156   train loss:  0.002191992709413171  val loss:  0.6052797436714172\n",
      "epoch:  20   step:  157   train loss:  0.002603144384920597  val loss:  0.6081985831260681\n",
      "epoch:  20   step:  158   train loss:  0.0013696083333343267  val loss:  0.6035943627357483\n",
      "epoch:  20   step:  159   train loss:  0.0023268694058060646  val loss:  0.6048540472984314\n",
      "epoch:  20   step:  160   train loss:  0.0030047083273530006  val loss:  0.6043152809143066\n",
      "epoch:  20   step:  161   train loss:  0.001279992051422596  val loss:  0.6026554107666016\n",
      "epoch:  20   step:  162   train loss:  0.001476307399570942  val loss:  0.6063753962516785\n",
      "epoch:  20   step:  163   train loss:  0.001797324395738542  val loss:  0.609542965888977\n",
      "epoch:  20   step:  164   train loss:  0.0029091383330523968  val loss:  0.6140885949134827\n",
      "epoch:  20   step:  165   train loss:  0.0008000499219633639  val loss:  0.6226772665977478\n",
      "epoch:  21   step:  0   train loss:  0.0010660495609045029  val loss:  0.6261183619499207\n",
      "epoch:  21   step:  1   train loss:  0.0021359308157116175  val loss:  0.6257185935974121\n",
      "epoch:  21   step:  2   train loss:  0.00105794589035213  val loss:  0.618870198726654\n",
      "epoch:  21   step:  3   train loss:  0.0012573667336255312  val loss:  0.6184095144271851\n",
      "epoch:  21   step:  4   train loss:  0.0013988259015604854  val loss:  0.6179364919662476\n",
      "epoch:  21   step:  5   train loss:  0.0014141196152195334  val loss:  0.6204429268836975\n",
      "epoch:  21   step:  6   train loss:  0.0014730968978255987  val loss:  0.617668867111206\n",
      "epoch:  21   step:  7   train loss:  0.0010539449285715818  val loss:  0.610673725605011\n",
      "epoch:  21   step:  8   train loss:  0.0012732121394947171  val loss:  0.6092880964279175\n",
      "epoch:  21   step:  9   train loss:  0.0013358083087950945  val loss:  0.6115317344665527\n",
      "epoch:  21   step:  10   train loss:  0.0016179087106138468  val loss:  0.605526864528656\n",
      "epoch:  21   step:  11   train loss:  0.0012064715847373009  val loss:  0.6060084104537964\n",
      "epoch:  21   step:  12   train loss:  0.0012848202604800463  val loss:  0.6152370572090149\n",
      "epoch:  21   step:  13   train loss:  0.001352484803646803  val loss:  0.6117647886276245\n",
      "epoch:  21   step:  14   train loss:  0.001150537864305079  val loss:  0.6123450994491577\n",
      "epoch:  21   step:  15   train loss:  0.0015326037537306547  val loss:  0.6215192079544067\n",
      "epoch:  21   step:  16   train loss:  0.0023078040685504675  val loss:  0.6216403245925903\n",
      "epoch:  21   step:  17   train loss:  0.00229869456961751  val loss:  0.6174268126487732\n",
      "epoch:  21   step:  18   train loss:  0.0013811071403324604  val loss:  0.6145143508911133\n",
      "epoch:  21   step:  19   train loss:  0.004987784195691347  val loss:  0.6145868301391602\n",
      "epoch:  21   step:  20   train loss:  0.0019789193756878376  val loss:  0.6074458360671997\n",
      "epoch:  21   step:  21   train loss:  0.003601579926908016  val loss:  0.617290735244751\n",
      "epoch:  21   step:  22   train loss:  0.0008081090054474771  val loss:  0.6109813451766968\n",
      "epoch:  21   step:  23   train loss:  0.001148332143202424  val loss:  0.607844352722168\n",
      "epoch:  21   step:  24   train loss:  0.002200168091803789  val loss:  0.6016455888748169\n",
      "epoch:  21   step:  25   train loss:  0.001707941060885787  val loss:  0.6027989983558655\n",
      "epoch:  21   step:  26   train loss:  0.0024107336066663265  val loss:  0.6166645884513855\n",
      "epoch:  21   step:  27   train loss:  0.002277391729876399  val loss:  0.6226603984832764\n",
      "epoch:  21   step:  28   train loss:  0.0011650408850982785  val loss:  0.6190615892410278\n",
      "epoch:  21   step:  29   train loss:  0.001578624825924635  val loss:  0.6190673112869263\n",
      "epoch:  21   step:  30   train loss:  0.0011576807592064142  val loss:  0.6134267449378967\n",
      "epoch:  21   step:  31   train loss:  0.0013054916635155678  val loss:  0.6161323189735413\n",
      "epoch:  21   step:  32   train loss:  0.0012906685005873442  val loss:  0.6140272617340088\n",
      "epoch:  21   step:  33   train loss:  0.0010562024544924498  val loss:  0.6114396452903748\n",
      "epoch:  21   step:  34   train loss:  0.001438745530322194  val loss:  0.6092592477798462\n",
      "epoch:  21   step:  35   train loss:  0.0010045527014881372  val loss:  0.6137959361076355\n",
      "epoch:  21   step:  36   train loss:  0.0014505647122859955  val loss:  0.6169630885124207\n",
      "epoch:  21   step:  37   train loss:  0.0012962515465915203  val loss:  0.6219656467437744\n",
      "epoch:  21   step:  38   train loss:  0.001621719915419817  val loss:  0.6268841624259949\n",
      "epoch:  21   step:  39   train loss:  0.0037256069481372833  val loss:  0.6298531293869019\n",
      "epoch:  21   step:  40   train loss:  0.0013558040373027325  val loss:  0.6350093483924866\n",
      "epoch:  21   step:  41   train loss:  0.001691451296210289  val loss:  0.6298958659172058\n",
      "epoch:  21   step:  42   train loss:  0.0009209994459524751  val loss:  0.6316134333610535\n",
      "epoch:  21   step:  43   train loss:  0.0014186090556904674  val loss:  0.6301155090332031\n",
      "epoch:  21   step:  44   train loss:  0.0015663285739719868  val loss:  0.6275337338447571\n",
      "epoch:  21   step:  45   train loss:  0.0011050840839743614  val loss:  0.6246793866157532\n",
      "epoch:  21   step:  46   train loss:  0.0014900716487318277  val loss:  0.6267878413200378\n",
      "epoch:  21   step:  47   train loss:  0.001107816118746996  val loss:  0.6244356632232666\n",
      "epoch:  21   step:  48   train loss:  0.0011911392211914062  val loss:  0.6274690628051758\n",
      "epoch:  21   step:  49   train loss:  0.002380203688517213  val loss:  0.638851523399353\n",
      "epoch:  21   step:  50   train loss:  0.0011862852843478322  val loss:  0.6367779970169067\n",
      "epoch:  21   step:  51   train loss:  0.001055391738191247  val loss:  0.6323683261871338\n",
      "epoch:  21   step:  52   train loss:  0.0026933590415865183  val loss:  0.6401212215423584\n",
      "epoch:  21   step:  53   train loss:  0.001780274324119091  val loss:  0.6386212110519409\n",
      "epoch:  21   step:  54   train loss:  0.001180088147521019  val loss:  0.6340458393096924\n",
      "epoch:  21   step:  55   train loss:  0.0009147592936642468  val loss:  0.632716715335846\n",
      "epoch:  21   step:  56   train loss:  0.00475764786824584  val loss:  0.6393135786056519\n",
      "epoch:  21   step:  57   train loss:  0.0009773599449545145  val loss:  0.6385064125061035\n",
      "epoch:  21   step:  58   train loss:  0.001477968180552125  val loss:  0.6397998929023743\n",
      "epoch:  21   step:  59   train loss:  0.0024871539790183306  val loss:  0.6355367302894592\n",
      "epoch:  21   step:  60   train loss:  0.002089279005303979  val loss:  0.6281599998474121\n",
      "epoch:  21   step:  61   train loss:  0.003433298086747527  val loss:  0.635302722454071\n",
      "epoch:  21   step:  62   train loss:  0.0014587444020435214  val loss:  0.633866012096405\n",
      "epoch:  21   step:  63   train loss:  0.0025443986523896456  val loss:  0.6267603039741516\n",
      "epoch:  21   step:  64   train loss:  0.0025296383537352085  val loss:  0.6261967420578003\n",
      "epoch:  21   step:  65   train loss:  0.0010430020047351718  val loss:  0.6221618056297302\n",
      "epoch:  21   step:  66   train loss:  0.001583276316523552  val loss:  0.6283705234527588\n",
      "epoch:  21   step:  67   train loss:  0.002569530624896288  val loss:  0.6170181632041931\n",
      "epoch:  21   step:  68   train loss:  0.0012339514214545488  val loss:  0.6210524439811707\n",
      "epoch:  21   step:  69   train loss:  0.0012934824917465448  val loss:  0.6197981834411621\n",
      "epoch:  21   step:  70   train loss:  0.001858094590716064  val loss:  0.6141901016235352\n",
      "epoch:  21   step:  71   train loss:  0.002552575897425413  val loss:  0.613422691822052\n",
      "epoch:  21   step:  72   train loss:  0.0011788451811298728  val loss:  0.6101104021072388\n",
      "epoch:  21   step:  73   train loss:  0.001073304796591401  val loss:  0.6059185266494751\n",
      "epoch:  21   step:  74   train loss:  0.001891263760626316  val loss:  0.6057804226875305\n",
      "epoch:  21   step:  75   train loss:  0.001742529566399753  val loss:  0.603210985660553\n",
      "epoch:  21   step:  76   train loss:  0.001790069043636322  val loss:  0.6077784895896912\n",
      "epoch:  21   step:  77   train loss:  0.0013298190897330642  val loss:  0.6051426529884338\n",
      "epoch:  21   step:  78   train loss:  0.0012920207809656858  val loss:  0.608799159526825\n",
      "epoch:  21   step:  79   train loss:  0.0010369637748226523  val loss:  0.5967135429382324\n",
      "epoch:  21   step:  80   train loss:  0.0023770255502313375  val loss:  0.5924734473228455\n",
      "epoch:  21   step:  81   train loss:  0.0009043262107297778  val loss:  0.5925731658935547\n",
      "epoch:  21   step:  82   train loss:  0.0017659305594861507  val loss:  0.6003418564796448\n",
      "epoch:  21   step:  83   train loss:  0.0025780722498893738  val loss:  0.5982701182365417\n",
      "epoch:  21   step:  84   train loss:  0.002105665858834982  val loss:  0.5949957370758057\n",
      "epoch:  21   step:  85   train loss:  0.0016469825059175491  val loss:  0.5902604460716248\n",
      "epoch:  21   step:  86   train loss:  0.0012907078489661217  val loss:  0.5944803357124329\n",
      "epoch:  21   step:  87   train loss:  0.0012423406587913632  val loss:  0.5918256044387817\n",
      "epoch:  21   step:  88   train loss:  0.002103614853695035  val loss:  0.5997055768966675\n",
      "epoch:  21   step:  89   train loss:  0.0009481068700551987  val loss:  0.5996133089065552\n",
      "epoch:  21   step:  90   train loss:  0.0008550034835934639  val loss:  0.591769278049469\n",
      "epoch:  21   step:  91   train loss:  0.0011821620864793658  val loss:  0.5953874588012695\n",
      "epoch:  21   step:  92   train loss:  0.0011389366118237376  val loss:  0.5963611602783203\n",
      "epoch:  21   step:  93   train loss:  0.0016812649555504322  val loss:  0.6125627160072327\n",
      "epoch:  21   step:  94   train loss:  0.001119090709835291  val loss:  0.6072725653648376\n",
      "epoch:  21   step:  95   train loss:  0.0015334971249103546  val loss:  0.6055030822753906\n",
      "epoch:  21   step:  96   train loss:  0.0013477818574756384  val loss:  0.600703239440918\n",
      "epoch:  21   step:  97   train loss:  0.0012922913301736116  val loss:  0.5982748866081238\n",
      "epoch:  21   step:  98   train loss:  0.00099341687746346  val loss:  0.6004197001457214\n",
      "epoch:  21   step:  99   train loss:  0.0012100529856979847  val loss:  0.6017727255821228\n",
      "epoch:  21   step:  100   train loss:  0.0019197782967239618  val loss:  0.6044939756393433\n",
      "epoch:  21   step:  101   train loss:  0.0022438322193920612  val loss:  0.5958438515663147\n",
      "epoch:  21   step:  102   train loss:  0.0012410221388563514  val loss:  0.5922950506210327\n",
      "epoch:  21   step:  103   train loss:  0.0033433109056204557  val loss:  0.600564181804657\n",
      "epoch:  21   step:  104   train loss:  0.001158095896244049  val loss:  0.6036993861198425\n",
      "epoch:  21   step:  105   train loss:  0.0028420784510672092  val loss:  0.6085841655731201\n",
      "epoch:  21   step:  106   train loss:  0.002051334595307708  val loss:  0.6141015291213989\n",
      "epoch:  21   step:  107   train loss:  0.002601877087727189  val loss:  0.6174169182777405\n",
      "epoch:  21   step:  108   train loss:  0.0006880993023514748  val loss:  0.6115366816520691\n",
      "epoch:  21   step:  109   train loss:  0.0006808170583099127  val loss:  0.6111946702003479\n",
      "epoch:  21   step:  110   train loss:  0.0019540046341717243  val loss:  0.6108048558235168\n",
      "epoch:  21   step:  111   train loss:  0.0007698010886088014  val loss:  0.6138026714324951\n",
      "epoch:  21   step:  112   train loss:  0.0009823031723499298  val loss:  0.6180496215820312\n",
      "epoch:  21   step:  113   train loss:  0.0022447076626122  val loss:  0.6235378384590149\n",
      "epoch:  21   step:  114   train loss:  0.0010316416155546904  val loss:  0.6187020540237427\n",
      "epoch:  21   step:  115   train loss:  0.001356985536403954  val loss:  0.6165692210197449\n",
      "epoch:  21   step:  116   train loss:  0.0022511668503284454  val loss:  0.6201347708702087\n",
      "epoch:  21   step:  117   train loss:  0.0012642345391213894  val loss:  0.623027503490448\n",
      "epoch:  21   step:  118   train loss:  0.0014064436545595527  val loss:  0.6292752623558044\n",
      "epoch:  21   step:  119   train loss:  0.0004957419587299228  val loss:  0.6284267902374268\n",
      "epoch:  21   step:  120   train loss:  0.0027411626651883125  val loss:  0.6197986602783203\n",
      "epoch:  21   step:  121   train loss:  0.0024159951135516167  val loss:  0.6251702904701233\n",
      "epoch:  21   step:  122   train loss:  0.0013503064401447773  val loss:  0.6263132095336914\n",
      "epoch:  21   step:  123   train loss:  0.000950781861320138  val loss:  0.6274086236953735\n",
      "epoch:  21   step:  124   train loss:  0.0019355970434844494  val loss:  0.6386038064956665\n",
      "epoch:  21   step:  125   train loss:  0.002300749998539686  val loss:  0.6396019458770752\n",
      "epoch:  21   step:  126   train loss:  0.001366296550258994  val loss:  0.6387578248977661\n",
      "epoch:  21   step:  127   train loss:  0.001125822076573968  val loss:  0.6406543850898743\n",
      "epoch:  21   step:  128   train loss:  0.0015888100024312735  val loss:  0.6287079453468323\n",
      "epoch:  21   step:  129   train loss:  0.0018544442718848586  val loss:  0.6300945281982422\n",
      "epoch:  21   step:  130   train loss:  0.0008676693541929126  val loss:  0.6225877404212952\n",
      "epoch:  21   step:  131   train loss:  0.0015679888892918825  val loss:  0.6142363548278809\n",
      "epoch:  21   step:  132   train loss:  0.0006232837913557887  val loss:  0.6198779344558716\n",
      "epoch:  21   step:  133   train loss:  0.0007905651582404971  val loss:  0.6208447217941284\n",
      "epoch:  21   step:  134   train loss:  0.0017524814466014504  val loss:  0.6308906674385071\n",
      "epoch:  21   step:  135   train loss:  0.001370653510093689  val loss:  0.6199427843093872\n",
      "epoch:  21   step:  136   train loss:  0.0008681745384819806  val loss:  0.6152240037918091\n",
      "epoch:  21   step:  137   train loss:  0.001548593514598906  val loss:  0.6170960664749146\n",
      "epoch:  21   step:  138   train loss:  0.0014866252895444632  val loss:  0.6239643096923828\n",
      "epoch:  21   step:  139   train loss:  0.001403850968927145  val loss:  0.6275725960731506\n",
      "epoch:  21   step:  140   train loss:  0.002276789164170623  val loss:  0.6220494508743286\n",
      "epoch:  21   step:  141   train loss:  0.001121570123359561  val loss:  0.6172803640365601\n",
      "epoch:  21   step:  142   train loss:  0.0007883271900936961  val loss:  0.6171790957450867\n",
      "epoch:  21   step:  143   train loss:  0.0013963248347863555  val loss:  0.6249615550041199\n",
      "epoch:  21   step:  144   train loss:  0.0014104500878602266  val loss:  0.6299656629562378\n",
      "epoch:  21   step:  145   train loss:  0.0012495846021920443  val loss:  0.6268235445022583\n",
      "epoch:  21   step:  146   train loss:  0.0015767945442348719  val loss:  0.632370114326477\n",
      "epoch:  21   step:  147   train loss:  0.00125886348541826  val loss:  0.6265443563461304\n",
      "epoch:  21   step:  148   train loss:  0.002362857572734356  val loss:  0.613280713558197\n",
      "epoch:  21   step:  149   train loss:  0.0013494095765054226  val loss:  0.6116113066673279\n",
      "epoch:  21   step:  150   train loss:  0.0020873290486633778  val loss:  0.6178629398345947\n",
      "epoch:  21   step:  151   train loss:  0.0011922238627448678  val loss:  0.6174665689468384\n",
      "epoch:  21   step:  152   train loss:  0.0017600880237296224  val loss:  0.6259480118751526\n",
      "epoch:  21   step:  153   train loss:  0.001660082722082734  val loss:  0.6249679327011108\n",
      "epoch:  21   step:  154   train loss:  0.0019933325238525867  val loss:  0.6179086565971375\n",
      "epoch:  21   step:  155   train loss:  0.0023807696998119354  val loss:  0.6200096011161804\n",
      "epoch:  21   step:  156   train loss:  0.0018800131510943174  val loss:  0.6230434775352478\n",
      "epoch:  21   step:  157   train loss:  0.001096080639399588  val loss:  0.6256804466247559\n",
      "epoch:  21   step:  158   train loss:  0.0011095054214820266  val loss:  0.6238381266593933\n",
      "epoch:  21   step:  159   train loss:  0.0017680275486782193  val loss:  0.6206849813461304\n",
      "epoch:  21   step:  160   train loss:  0.0011720651527866721  val loss:  0.6145960092544556\n",
      "epoch:  21   step:  161   train loss:  0.0019708017352968454  val loss:  0.6163541674613953\n",
      "epoch:  21   step:  162   train loss:  0.0007293532835319638  val loss:  0.6188074946403503\n",
      "epoch:  21   step:  163   train loss:  0.0016221031546592712  val loss:  0.6241166591644287\n",
      "epoch:  21   step:  164   train loss:  0.0011860481463372707  val loss:  0.6154755353927612\n",
      "epoch:  21   step:  165   train loss:  0.0021288010757416487  val loss:  0.609035074710846\n",
      "epoch:  22   step:  0   train loss:  0.001417377032339573  val loss:  0.6033251285552979\n",
      "epoch:  22   step:  1   train loss:  0.0010759326396510005  val loss:  0.6031778454780579\n",
      "epoch:  22   step:  2   train loss:  0.000710794935002923  val loss:  0.6077123284339905\n",
      "epoch:  22   step:  3   train loss:  0.0012467894703149796  val loss:  0.6221796870231628\n",
      "epoch:  22   step:  4   train loss:  0.0006591666024178267  val loss:  0.6173633337020874\n",
      "epoch:  22   step:  5   train loss:  0.0008927545277401805  val loss:  0.6219311952590942\n",
      "epoch:  22   step:  6   train loss:  0.0012870905920863152  val loss:  0.6233210563659668\n",
      "epoch:  22   step:  7   train loss:  0.0023260735906660557  val loss:  0.6240438222885132\n",
      "epoch:  22   step:  8   train loss:  0.001068335259333253  val loss:  0.6240934133529663\n",
      "epoch:  22   step:  9   train loss:  0.0059242937713861465  val loss:  0.625472903251648\n",
      "epoch:  22   step:  10   train loss:  0.0010178671218454838  val loss:  0.6126457452774048\n",
      "epoch:  22   step:  11   train loss:  0.0010099159553647041  val loss:  0.6134275197982788\n",
      "epoch:  22   step:  12   train loss:  0.00038282311288639903  val loss:  0.6193199157714844\n",
      "epoch:  22   step:  13   train loss:  0.001595483161509037  val loss:  0.6287680864334106\n",
      "epoch:  22   step:  14   train loss:  0.0014775735326111317  val loss:  0.6266489624977112\n",
      "epoch:  22   step:  15   train loss:  0.001659677131101489  val loss:  0.6393980383872986\n",
      "epoch:  22   step:  16   train loss:  0.001377146109007299  val loss:  0.6322486400604248\n",
      "epoch:  22   step:  17   train loss:  0.00117116526234895  val loss:  0.6260062456130981\n",
      "epoch:  22   step:  18   train loss:  0.0010558213107287884  val loss:  0.6292629837989807\n",
      "epoch:  22   step:  19   train loss:  0.001092638005502522  val loss:  0.6244784593582153\n",
      "epoch:  22   step:  20   train loss:  0.000864717410877347  val loss:  0.6173889636993408\n",
      "epoch:  22   step:  21   train loss:  0.0014984954614192247  val loss:  0.6188579797744751\n",
      "epoch:  22   step:  22   train loss:  0.0013238165993243456  val loss:  0.626736581325531\n",
      "epoch:  22   step:  23   train loss:  0.0007857860182411969  val loss:  0.6357479095458984\n",
      "epoch:  22   step:  24   train loss:  0.0006906076450832188  val loss:  0.6346811056137085\n",
      "epoch:  22   step:  25   train loss:  0.0011369094718247652  val loss:  0.6405934691429138\n",
      "epoch:  22   step:  26   train loss:  0.0011983230942860246  val loss:  0.6373838186264038\n",
      "epoch:  22   step:  27   train loss:  0.0007402859046123922  val loss:  0.6346950531005859\n",
      "epoch:  22   step:  28   train loss:  0.001485847751609981  val loss:  0.6322646141052246\n",
      "epoch:  22   step:  29   train loss:  0.0007580001838505268  val loss:  0.6239599585533142\n",
      "epoch:  22   step:  30   train loss:  0.004668829962611198  val loss:  0.6299804449081421\n",
      "epoch:  22   step:  31   train loss:  0.0016873504500836134  val loss:  0.6209667325019836\n",
      "epoch:  22   step:  32   train loss:  0.0010651552584022284  val loss:  0.6237370371818542\n",
      "epoch:  22   step:  33   train loss:  0.0018875447567552328  val loss:  0.6351648569107056\n",
      "epoch:  22   step:  34   train loss:  0.0011794278398156166  val loss:  0.638285756111145\n",
      "epoch:  22   step:  35   train loss:  0.0007848301320336759  val loss:  0.6393078565597534\n",
      "epoch:  22   step:  36   train loss:  0.0006461709272116423  val loss:  0.6358845233917236\n",
      "epoch:  22   step:  37   train loss:  0.001206599990837276  val loss:  0.6351627111434937\n",
      "epoch:  22   step:  38   train loss:  0.0011966484598815441  val loss:  0.6389761567115784\n",
      "epoch:  22   step:  39   train loss:  0.0019275369122624397  val loss:  0.6388962268829346\n",
      "epoch:  22   step:  40   train loss:  0.0009294742485508323  val loss:  0.6458314657211304\n",
      "epoch:  22   step:  41   train loss:  0.0010093417949974537  val loss:  0.6507694721221924\n",
      "epoch:  22   step:  42   train loss:  0.0017075573559850454  val loss:  0.6468141078948975\n",
      "epoch:  22   step:  43   train loss:  0.0017963640857487917  val loss:  0.650084912776947\n",
      "epoch:  22   step:  44   train loss:  0.0011321306228637695  val loss:  0.6519399285316467\n",
      "epoch:  22   step:  45   train loss:  0.0009094033157452941  val loss:  0.646282970905304\n",
      "epoch:  22   step:  46   train loss:  0.0006026712362654507  val loss:  0.6419795751571655\n",
      "epoch:  22   step:  47   train loss:  0.000714137393515557  val loss:  0.6427427530288696\n",
      "epoch:  22   step:  48   train loss:  0.001787957618944347  val loss:  0.6387664079666138\n",
      "epoch:  22   step:  49   train loss:  0.0006359204417094588  val loss:  0.6356697082519531\n",
      "epoch:  22   step:  50   train loss:  0.0020154835656285286  val loss:  0.63817298412323\n",
      "epoch:  22   step:  51   train loss:  0.0014379490166902542  val loss:  0.6389681696891785\n",
      "epoch:  22   step:  52   train loss:  0.0009548170492053032  val loss:  0.6425866484642029\n",
      "epoch:  22   step:  53   train loss:  0.0013030522968620062  val loss:  0.6499465107917786\n",
      "epoch:  22   step:  54   train loss:  0.00209594052284956  val loss:  0.6390420198440552\n",
      "epoch:  22   step:  55   train loss:  0.0009697627974674106  val loss:  0.6398866176605225\n",
      "epoch:  22   step:  56   train loss:  0.0008082560962066054  val loss:  0.638182520866394\n",
      "epoch:  22   step:  57   train loss:  0.0006680767983198166  val loss:  0.6437352895736694\n",
      "epoch:  22   step:  58   train loss:  0.005444106180220842  val loss:  0.6447903513908386\n",
      "epoch:  22   step:  59   train loss:  0.001751609263010323  val loss:  0.6527799963951111\n",
      "epoch:  22   step:  60   train loss:  0.002766781020909548  val loss:  0.657043993473053\n",
      "epoch:  22   step:  61   train loss:  0.000733091845177114  val loss:  0.667386531829834\n",
      "epoch:  22   step:  62   train loss:  0.0011757039465010166  val loss:  0.6685371994972229\n",
      "epoch:  22   step:  63   train loss:  0.0017341162310913205  val loss:  0.6741042137145996\n",
      "epoch:  22   step:  64   train loss:  0.0012231548316776752  val loss:  0.6756124496459961\n",
      "epoch:  22   step:  65   train loss:  0.0014558397233486176  val loss:  0.6752944588661194\n",
      "epoch:  22   step:  66   train loss:  0.0008788704872131348  val loss:  0.668403148651123\n",
      "epoch:  22   step:  67   train loss:  0.0008565312018617988  val loss:  0.6626783013343811\n",
      "epoch:  22   step:  68   train loss:  0.000702190212905407  val loss:  0.6632134318351746\n",
      "epoch:  22   step:  69   train loss:  0.0007367408834397793  val loss:  0.6635616421699524\n",
      "epoch:  22   step:  70   train loss:  0.0007600547978654504  val loss:  0.6642932891845703\n",
      "epoch:  22   step:  71   train loss:  0.0008826008997857571  val loss:  0.6651453971862793\n",
      "epoch:  22   step:  72   train loss:  0.001041944371536374  val loss:  0.66496342420578\n",
      "epoch:  22   step:  73   train loss:  0.0016531107248738408  val loss:  0.6681947112083435\n",
      "epoch:  22   step:  74   train loss:  0.0017286371439695358  val loss:  0.6730345487594604\n",
      "epoch:  22   step:  75   train loss:  0.0011716935550794005  val loss:  0.6748930215835571\n",
      "epoch:  22   step:  76   train loss:  0.0013881942722946405  val loss:  0.6783855557441711\n",
      "epoch:  22   step:  77   train loss:  0.0008453081245534122  val loss:  0.6791734099388123\n",
      "epoch:  22   step:  78   train loss:  0.0009263270767405629  val loss:  0.6792657971382141\n",
      "epoch:  22   step:  79   train loss:  0.0012148991227149963  val loss:  0.6794121265411377\n",
      "epoch:  22   step:  80   train loss:  0.0012093319091945887  val loss:  0.6848046779632568\n",
      "epoch:  22   step:  81   train loss:  0.0009269794682040811  val loss:  0.6894316077232361\n",
      "epoch:  22   step:  82   train loss:  0.0013636681251227856  val loss:  0.6836639046669006\n",
      "epoch:  22   step:  83   train loss:  0.0012397649697959423  val loss:  0.6845368146896362\n",
      "epoch:  22   step:  84   train loss:  0.0009894210379570723  val loss:  0.6863330602645874\n",
      "epoch:  22   step:  85   train loss:  0.0009993037674576044  val loss:  0.6889499425888062\n",
      "epoch:  22   step:  86   train loss:  0.0011493652127683163  val loss:  0.6866129636764526\n",
      "epoch:  22   step:  87   train loss:  0.0011419106740504503  val loss:  0.6899102926254272\n",
      "epoch:  22   step:  88   train loss:  0.0008739363984204829  val loss:  0.6865382790565491\n",
      "epoch:  22   step:  89   train loss:  0.0008243207703344524  val loss:  0.6829609274864197\n",
      "epoch:  22   step:  90   train loss:  0.0010657170787453651  val loss:  0.6875494122505188\n",
      "epoch:  22   step:  91   train loss:  0.0008309695986099541  val loss:  0.6852151155471802\n",
      "epoch:  22   step:  92   train loss:  0.0013623593840748072  val loss:  0.6890134215354919\n",
      "epoch:  22   step:  93   train loss:  0.0011943541467189789  val loss:  0.6839817762374878\n",
      "epoch:  22   step:  94   train loss:  0.0008476327639073133  val loss:  0.6848254799842834\n",
      "epoch:  22   step:  95   train loss:  0.000765912642236799  val loss:  0.6854379177093506\n",
      "epoch:  22   step:  96   train loss:  0.001105351373553276  val loss:  0.691886305809021\n",
      "epoch:  22   step:  97   train loss:  0.0021469080820679665  val loss:  0.6946702599525452\n",
      "epoch:  22   step:  98   train loss:  0.0011200588196516037  val loss:  0.6962147951126099\n",
      "epoch:  22   step:  99   train loss:  0.0007098973728716373  val loss:  0.6935405731201172\n",
      "epoch:  22   step:  100   train loss:  0.0012411896605044603  val loss:  0.697018563747406\n",
      "epoch:  22   step:  101   train loss:  0.0013977654743939638  val loss:  0.6934147477149963\n",
      "epoch:  22   step:  102   train loss:  0.0013318819692358375  val loss:  0.695083498954773\n",
      "epoch:  22   step:  103   train loss:  0.001795545220375061  val loss:  0.6830840110778809\n",
      "epoch:  22   step:  104   train loss:  0.0007047379040159285  val loss:  0.6810854077339172\n",
      "epoch:  22   step:  105   train loss:  0.0013391252141445875  val loss:  0.6790731549263\n",
      "epoch:  22   step:  106   train loss:  0.001098006498068571  val loss:  0.68904709815979\n",
      "epoch:  22   step:  107   train loss:  0.0007779634324833751  val loss:  0.6788437366485596\n",
      "epoch:  22   step:  108   train loss:  0.0006923862383700907  val loss:  0.6800014972686768\n",
      "epoch:  22   step:  109   train loss:  0.0007320790318772197  val loss:  0.6809008121490479\n",
      "epoch:  22   step:  110   train loss:  0.0009388293838128448  val loss:  0.6794978380203247\n",
      "epoch:  22   step:  111   train loss:  0.001189646078273654  val loss:  0.6826186776161194\n",
      "epoch:  22   step:  112   train loss:  0.001006864826194942  val loss:  0.6774084568023682\n",
      "epoch:  22   step:  113   train loss:  0.0018027832265943289  val loss:  0.6780592203140259\n",
      "epoch:  22   step:  114   train loss:  0.0015968489460647106  val loss:  0.6871049404144287\n",
      "epoch:  22   step:  115   train loss:  0.0010717306286096573  val loss:  0.6913837790489197\n",
      "epoch:  22   step:  116   train loss:  0.0011509315809234977  val loss:  0.6936474442481995\n",
      "epoch:  22   step:  117   train loss:  0.0011903721606358886  val loss:  0.6896605491638184\n",
      "epoch:  22   step:  118   train loss:  0.001341652823612094  val loss:  0.6954673528671265\n",
      "epoch:  22   step:  119   train loss:  0.0015799241373315454  val loss:  0.6964340209960938\n",
      "epoch:  22   step:  120   train loss:  0.0009181672940030694  val loss:  0.699116051197052\n",
      "epoch:  22   step:  121   train loss:  0.0009033739333972335  val loss:  0.6938117146492004\n",
      "epoch:  22   step:  122   train loss:  0.0018057465786114335  val loss:  0.6833043694496155\n",
      "epoch:  22   step:  123   train loss:  0.0014238004805520177  val loss:  0.6861643195152283\n",
      "epoch:  22   step:  124   train loss:  0.0007955733453854918  val loss:  0.6925304532051086\n",
      "epoch:  22   step:  125   train loss:  0.001328261336311698  val loss:  0.7057915925979614\n",
      "epoch:  22   step:  126   train loss:  0.0017900752136483788  val loss:  0.6898632049560547\n",
      "epoch:  22   step:  127   train loss:  0.0024502365849912167  val loss:  0.6974024772644043\n",
      "epoch:  22   step:  128   train loss:  0.0012198899639770389  val loss:  0.6957563757896423\n",
      "epoch:  22   step:  129   train loss:  0.001209564390592277  val loss:  0.6965115070343018\n",
      "epoch:  22   step:  130   train loss:  0.0007417573942802846  val loss:  0.6969651579856873\n",
      "epoch:  22   step:  131   train loss:  0.001297859475016594  val loss:  0.6959761381149292\n",
      "epoch:  22   step:  132   train loss:  0.0008072406053543091  val loss:  0.6926370859146118\n",
      "epoch:  22   step:  133   train loss:  0.0010014423169195652  val loss:  0.698045551776886\n",
      "epoch:  22   step:  134   train loss:  0.0007480062777176499  val loss:  0.7027096152305603\n",
      "epoch:  22   step:  135   train loss:  0.0011694850400090218  val loss:  0.7094836831092834\n",
      "epoch:  22   step:  136   train loss:  0.0008289539255201817  val loss:  0.7112078070640564\n",
      "epoch:  22   step:  137   train loss:  0.0009175663581117988  val loss:  0.6992942094802856\n",
      "epoch:  22   step:  138   train loss:  0.0012329225428402424  val loss:  0.7001879811286926\n",
      "epoch:  22   step:  139   train loss:  0.00048236933071166277  val loss:  0.6897038221359253\n",
      "epoch:  22   step:  140   train loss:  0.000819938606582582  val loss:  0.6877897381782532\n",
      "epoch:  22   step:  141   train loss:  0.002195593435317278  val loss:  0.6919386982917786\n",
      "epoch:  22   step:  142   train loss:  0.0010062216315418482  val loss:  0.6896265149116516\n",
      "epoch:  22   step:  143   train loss:  0.0011842946987599134  val loss:  0.6917356252670288\n",
      "epoch:  22   step:  144   train loss:  0.001428612507879734  val loss:  0.6814879179000854\n",
      "epoch:  22   step:  145   train loss:  0.0010566258570179343  val loss:  0.6860009431838989\n",
      "epoch:  22   step:  146   train loss:  0.0014594127424061298  val loss:  0.680965781211853\n",
      "epoch:  22   step:  147   train loss:  0.00039389601442962885  val loss:  0.6777331233024597\n",
      "epoch:  22   step:  148   train loss:  0.0017356302123516798  val loss:  0.6828760504722595\n",
      "epoch:  22   step:  149   train loss:  0.000849125673994422  val loss:  0.6867359280586243\n",
      "epoch:  22   step:  150   train loss:  0.0015125629724934697  val loss:  0.6896923184394836\n",
      "epoch:  22   step:  151   train loss:  0.0007203015265986323  val loss:  0.6865273118019104\n",
      "epoch:  22   step:  152   train loss:  0.0008281772024929523  val loss:  0.6778966188430786\n",
      "epoch:  22   step:  153   train loss:  0.0009168506949208677  val loss:  0.668539822101593\n",
      "epoch:  22   step:  154   train loss:  0.0008469310123473406  val loss:  0.6632299423217773\n",
      "epoch:  22   step:  155   train loss:  0.0005794123280793428  val loss:  0.6569241881370544\n",
      "epoch:  22   step:  156   train loss:  0.002525578485801816  val loss:  0.6581313014030457\n",
      "epoch:  22   step:  157   train loss:  0.001256056479178369  val loss:  0.6616933941841125\n",
      "epoch:  22   step:  158   train loss:  0.003362058661878109  val loss:  0.6585385799407959\n",
      "epoch:  22   step:  159   train loss:  0.0012019583955407143  val loss:  0.6665421724319458\n",
      "epoch:  22   step:  160   train loss:  0.0017184574389830232  val loss:  0.6721873879432678\n",
      "epoch:  22   step:  161   train loss:  0.004565674811601639  val loss:  0.6696659922599792\n",
      "epoch:  22   step:  162   train loss:  0.0008414773619733751  val loss:  0.6760159730911255\n",
      "epoch:  22   step:  163   train loss:  0.0016982448287308216  val loss:  0.6607812643051147\n",
      "epoch:  22   step:  164   train loss:  0.0009023762540891767  val loss:  0.6678684949874878\n",
      "epoch:  22   step:  165   train loss:  0.0011245699133723974  val loss:  0.6561195850372314\n",
      "epoch:  23   step:  0   train loss:  0.0008666838984936476  val loss:  0.66379314661026\n",
      "epoch:  23   step:  1   train loss:  0.001055753557011485  val loss:  0.6601260304450989\n",
      "epoch:  23   step:  2   train loss:  0.0012180751655250788  val loss:  0.6657582521438599\n",
      "epoch:  23   step:  3   train loss:  0.001005691708996892  val loss:  0.672646164894104\n",
      "epoch:  23   step:  4   train loss:  0.000840046675875783  val loss:  0.6718830466270447\n",
      "epoch:  23   step:  5   train loss:  0.0012976808939129114  val loss:  0.6739054918289185\n",
      "epoch:  23   step:  6   train loss:  0.0006234698230400681  val loss:  0.6738004684448242\n",
      "epoch:  23   step:  7   train loss:  0.001279891119338572  val loss:  0.6751464605331421\n",
      "epoch:  23   step:  8   train loss:  0.0010244655422866344  val loss:  0.6798669099807739\n",
      "epoch:  23   step:  9   train loss:  0.0006699585355818272  val loss:  0.6747239828109741\n",
      "epoch:  23   step:  10   train loss:  0.0014770794659852982  val loss:  0.6788084506988525\n",
      "epoch:  23   step:  11   train loss:  0.0013008855748921633  val loss:  0.6782915592193604\n",
      "epoch:  23   step:  12   train loss:  0.0011284742504358292  val loss:  0.6794835329055786\n",
      "epoch:  23   step:  13   train loss:  0.0008015757775865495  val loss:  0.6834046840667725\n",
      "epoch:  23   step:  14   train loss:  0.0009833453223109245  val loss:  0.6837207674980164\n",
      "epoch:  23   step:  15   train loss:  0.0009112204425036907  val loss:  0.6793496012687683\n",
      "epoch:  23   step:  16   train loss:  0.0013638039818033576  val loss:  0.6690641641616821\n",
      "epoch:  23   step:  17   train loss:  0.0008991819340735674  val loss:  0.672382652759552\n",
      "epoch:  23   step:  18   train loss:  0.0008618031279183924  val loss:  0.6818017363548279\n",
      "epoch:  23   step:  19   train loss:  0.00101082818582654  val loss:  0.6840513944625854\n",
      "epoch:  23   step:  20   train loss:  0.0006263531977310777  val loss:  0.6821286678314209\n",
      "epoch:  23   step:  21   train loss:  0.0007849127287045121  val loss:  0.6799169778823853\n",
      "epoch:  23   step:  22   train loss:  0.0009527062065899372  val loss:  0.6864529252052307\n",
      "epoch:  23   step:  23   train loss:  0.0015350300818681717  val loss:  0.689691424369812\n",
      "epoch:  23   step:  24   train loss:  0.0005677193403244019  val loss:  0.6826981902122498\n",
      "epoch:  23   step:  25   train loss:  0.0005858835647813976  val loss:  0.6803452968597412\n",
      "epoch:  23   step:  26   train loss:  0.0006510503008030355  val loss:  0.6776046752929688\n",
      "epoch:  23   step:  27   train loss:  0.0023041542153805494  val loss:  0.6744527816772461\n",
      "epoch:  23   step:  28   train loss:  0.0010459512704983354  val loss:  0.6705767512321472\n",
      "epoch:  23   step:  29   train loss:  0.0005669513484463096  val loss:  0.6744050979614258\n",
      "epoch:  23   step:  30   train loss:  0.0008032688056118786  val loss:  0.6597709655761719\n",
      "epoch:  23   step:  31   train loss:  0.0011369789717718959  val loss:  0.6666407585144043\n",
      "epoch:  23   step:  32   train loss:  0.0008906059665605426  val loss:  0.6562365293502808\n",
      "epoch:  23   step:  33   train loss:  0.0006838524714112282  val loss:  0.6638387441635132\n",
      "epoch:  23   step:  34   train loss:  0.001005985657684505  val loss:  0.6645824909210205\n",
      "epoch:  23   step:  35   train loss:  0.0015274039469659328  val loss:  0.6681925654411316\n",
      "epoch:  23   step:  36   train loss:  0.0016032801941037178  val loss:  0.6733440160751343\n",
      "epoch:  23   step:  37   train loss:  0.0008935742662288249  val loss:  0.6711509823799133\n",
      "epoch:  23   step:  38   train loss:  0.0007343055913224816  val loss:  0.675675094127655\n",
      "epoch:  23   step:  39   train loss:  0.0008509497274644673  val loss:  0.6864741444587708\n",
      "epoch:  23   step:  40   train loss:  0.0006852477672509849  val loss:  0.6795523762702942\n",
      "epoch:  23   step:  41   train loss:  0.0015494830440729856  val loss:  0.682730495929718\n",
      "epoch:  23   step:  42   train loss:  0.0010317853884771466  val loss:  0.6865540742874146\n",
      "epoch:  23   step:  43   train loss:  0.001015144633129239  val loss:  0.6799151301383972\n",
      "epoch:  23   step:  44   train loss:  0.0008351060096174479  val loss:  0.6720744967460632\n",
      "epoch:  23   step:  45   train loss:  0.0008779721101745963  val loss:  0.6747798919677734\n",
      "epoch:  23   step:  46   train loss:  0.0009343641577288508  val loss:  0.6752640604972839\n",
      "epoch:  23   step:  47   train loss:  0.002014367375522852  val loss:  0.6803812384605408\n",
      "epoch:  23   step:  48   train loss:  0.0007100982475094497  val loss:  0.6772534251213074\n",
      "epoch:  23   step:  49   train loss:  0.0006209269631654024  val loss:  0.668494701385498\n",
      "epoch:  23   step:  50   train loss:  0.000692130415700376  val loss:  0.6747173070907593\n",
      "epoch:  23   step:  51   train loss:  0.0014817104674875736  val loss:  0.6756513118743896\n",
      "epoch:  23   step:  52   train loss:  0.0013074856251478195  val loss:  0.6668952107429504\n",
      "epoch:  23   step:  53   train loss:  0.0008344606030732393  val loss:  0.6708829998970032\n",
      "epoch:  23   step:  54   train loss:  0.0009346403530798852  val loss:  0.6735060214996338\n",
      "epoch:  23   step:  55   train loss:  0.0008467327570542693  val loss:  0.6716669797897339\n",
      "epoch:  23   step:  56   train loss:  0.0009093161206692457  val loss:  0.6767814755439758\n",
      "epoch:  23   step:  57   train loss:  0.0006965792272239923  val loss:  0.6681262254714966\n",
      "epoch:  23   step:  58   train loss:  0.0008083778666332364  val loss:  0.6701945066452026\n",
      "epoch:  23   step:  59   train loss:  0.0006705719279125333  val loss:  0.6782338619232178\n",
      "epoch:  23   step:  60   train loss:  0.002198710571974516  val loss:  0.6814358234405518\n",
      "epoch:  23   step:  61   train loss:  0.0007043872028589249  val loss:  0.6761718392372131\n",
      "epoch:  23   step:  62   train loss:  0.0011963231954723597  val loss:  0.6839603781700134\n",
      "epoch:  23   step:  63   train loss:  0.0008930258336476982  val loss:  0.6752932667732239\n",
      "epoch:  23   step:  64   train loss:  0.0007020253106020391  val loss:  0.6735048294067383\n",
      "epoch:  23   step:  65   train loss:  0.000697729061357677  val loss:  0.6753493547439575\n",
      "epoch:  23   step:  66   train loss:  0.0011125316377729177  val loss:  0.6696729063987732\n",
      "epoch:  23   step:  67   train loss:  0.0007243025465868413  val loss:  0.6736845970153809\n",
      "epoch:  23   step:  68   train loss:  0.000653116381727159  val loss:  0.676959753036499\n",
      "epoch:  23   step:  69   train loss:  0.001042675692588091  val loss:  0.6737289428710938\n",
      "epoch:  23   step:  70   train loss:  0.0011560607235878706  val loss:  0.6798200011253357\n",
      "epoch:  23   step:  71   train loss:  0.0012236721813678741  val loss:  0.6752831339836121\n",
      "epoch:  23   step:  72   train loss:  0.0007433036807924509  val loss:  0.6870208382606506\n",
      "epoch:  23   step:  73   train loss:  0.0008555438835173845  val loss:  0.6858150959014893\n",
      "epoch:  23   step:  74   train loss:  0.0011452613398432732  val loss:  0.6864628195762634\n",
      "epoch:  23   step:  75   train loss:  0.0014211030211299658  val loss:  0.6900622248649597\n",
      "epoch:  23   step:  76   train loss:  0.0008059098618105054  val loss:  0.6887146830558777\n",
      "epoch:  23   step:  77   train loss:  0.0008770626736804843  val loss:  0.6856371760368347\n",
      "epoch:  23   step:  78   train loss:  0.0007998149958439171  val loss:  0.6821436285972595\n",
      "epoch:  23   step:  79   train loss:  0.0009011347428895533  val loss:  0.6727011203765869\n",
      "epoch:  23   step:  80   train loss:  0.0007493873126804829  val loss:  0.6772654056549072\n",
      "epoch:  23   step:  81   train loss:  0.0011891182512044907  val loss:  0.6736159920692444\n",
      "epoch:  23   step:  82   train loss:  0.0009312315960414708  val loss:  0.6784728169441223\n",
      "epoch:  23   step:  83   train loss:  0.0012744665145874023  val loss:  0.6815297603607178\n",
      "epoch:  23   step:  84   train loss:  0.0007048442494124174  val loss:  0.6847763061523438\n",
      "epoch:  23   step:  85   train loss:  0.000991861685179174  val loss:  0.6908792853355408\n",
      "epoch:  23   step:  86   train loss:  0.001112961326725781  val loss:  0.6865261793136597\n",
      "epoch:  23   step:  87   train loss:  0.0008310710545629263  val loss:  0.6902435421943665\n",
      "epoch:  23   step:  88   train loss:  0.0006115089636296034  val loss:  0.690222442150116\n",
      "epoch:  23   step:  89   train loss:  0.0011065613944083452  val loss:  0.6944559812545776\n",
      "epoch:  23   step:  90   train loss:  0.0007001228514127433  val loss:  0.6842154264450073\n",
      "epoch:  23   step:  91   train loss:  0.000490697450004518  val loss:  0.6705402135848999\n",
      "epoch:  23   step:  92   train loss:  0.0008706945227459073  val loss:  0.6713126301765442\n",
      "epoch:  23   step:  93   train loss:  0.0008126468164846301  val loss:  0.6727082133293152\n",
      "epoch:  23   step:  94   train loss:  0.0004082891973666847  val loss:  0.6628228425979614\n",
      "epoch:  23   step:  95   train loss:  0.001177382655441761  val loss:  0.6611491441726685\n",
      "epoch:  23   step:  96   train loss:  0.001817221287637949  val loss:  0.6590935587882996\n",
      "epoch:  23   step:  97   train loss:  0.0006056687561795115  val loss:  0.6572686433792114\n",
      "epoch:  23   step:  98   train loss:  0.0010287252953276038  val loss:  0.6563230752944946\n",
      "epoch:  23   step:  99   train loss:  0.0006495635025203228  val loss:  0.6445220112800598\n",
      "epoch:  23   step:  100   train loss:  0.0008224357152357697  val loss:  0.6406258344650269\n",
      "epoch:  23   step:  101   train loss:  0.0014389290008693933  val loss:  0.6411525011062622\n",
      "epoch:  23   step:  102   train loss:  0.0010724086314439774  val loss:  0.6373141407966614\n",
      "epoch:  23   step:  103   train loss:  0.001046583754941821  val loss:  0.6337548494338989\n",
      "epoch:  23   step:  104   train loss:  0.0010300751309841871  val loss:  0.6371872425079346\n",
      "epoch:  23   step:  105   train loss:  0.0006089187227189541  val loss:  0.6418849229812622\n",
      "epoch:  23   step:  106   train loss:  0.001650148769840598  val loss:  0.6560271382331848\n",
      "epoch:  23   step:  107   train loss:  0.0005717375315725803  val loss:  0.6564742922782898\n",
      "epoch:  23   step:  108   train loss:  0.00041588733438402414  val loss:  0.6581721901893616\n",
      "epoch:  23   step:  109   train loss:  0.001119663706049323  val loss:  0.6606055498123169\n",
      "epoch:  23   step:  110   train loss:  0.000978236785158515  val loss:  0.6531822085380554\n",
      "epoch:  23   step:  111   train loss:  0.0008023882401175797  val loss:  0.6516419649124146\n",
      "epoch:  23   step:  112   train loss:  0.000886567635461688  val loss:  0.6555715203285217\n",
      "epoch:  23   step:  113   train loss:  0.00079227180685848  val loss:  0.6486432552337646\n",
      "epoch:  23   step:  114   train loss:  0.0007718619308434427  val loss:  0.6554725766181946\n",
      "epoch:  23   step:  115   train loss:  0.0014315142761915922  val loss:  0.6634660959243774\n",
      "epoch:  23   step:  116   train loss:  0.0010543465614318848  val loss:  0.658405065536499\n",
      "epoch:  23   step:  117   train loss:  0.0007882012287154794  val loss:  0.661291241645813\n",
      "epoch:  23   step:  118   train loss:  0.0009847632609307766  val loss:  0.6625937223434448\n",
      "epoch:  23   step:  119   train loss:  0.0004329975345171988  val loss:  0.6597656607627869\n",
      "epoch:  23   step:  120   train loss:  0.0005123588489368558  val loss:  0.6682687997817993\n",
      "epoch:  23   step:  121   train loss:  0.00106897356454283  val loss:  0.6646353602409363\n",
      "epoch:  23   step:  122   train loss:  0.0006875521503388882  val loss:  0.6569609045982361\n",
      "epoch:  23   step:  123   train loss:  0.0011497049126774073  val loss:  0.6514331102371216\n",
      "epoch:  23   step:  124   train loss:  0.0008633679244667292  val loss:  0.6602478623390198\n",
      "epoch:  23   step:  125   train loss:  0.0007149308221414685  val loss:  0.6566287875175476\n",
      "epoch:  23   step:  126   train loss:  0.0019726569298654795  val loss:  0.6624494194984436\n",
      "epoch:  23   step:  127   train loss:  0.0008946465095505118  val loss:  0.6656764149665833\n",
      "epoch:  23   step:  128   train loss:  0.0021018795669078827  val loss:  0.6761662364006042\n",
      "epoch:  23   step:  129   train loss:  0.0002614297845866531  val loss:  0.6848572492599487\n",
      "epoch:  23   step:  130   train loss:  0.0012073783436790109  val loss:  0.6857779622077942\n",
      "epoch:  23   step:  131   train loss:  0.0009996516164392233  val loss:  0.6834391355514526\n",
      "epoch:  23   step:  132   train loss:  0.0019421942997723818  val loss:  0.6939123272895813\n",
      "epoch:  23   step:  133   train loss:  0.0007029499392956495  val loss:  0.6878446340560913\n",
      "epoch:  23   step:  134   train loss:  0.0005328513216227293  val loss:  0.681065022945404\n",
      "epoch:  23   step:  135   train loss:  0.0009863818995654583  val loss:  0.6768500208854675\n",
      "epoch:  23   step:  136   train loss:  0.0008507354650646448  val loss:  0.6813622117042542\n",
      "epoch:  23   step:  137   train loss:  0.0007880653138272464  val loss:  0.6840622425079346\n",
      "epoch:  23   step:  138   train loss:  0.0006883739843033254  val loss:  0.6819859147071838\n",
      "epoch:  23   step:  139   train loss:  0.0012304096017032862  val loss:  0.6700059175491333\n",
      "epoch:  23   step:  140   train loss:  0.0009726383723318577  val loss:  0.6699309945106506\n",
      "epoch:  23   step:  141   train loss:  0.0006544151110574603  val loss:  0.6646631956100464\n",
      "epoch:  23   step:  142   train loss:  0.001043995376676321  val loss:  0.6545489430427551\n",
      "epoch:  23   step:  143   train loss:  0.0014710014220327139  val loss:  0.6698500514030457\n",
      "epoch:  23   step:  144   train loss:  0.0008702795603312552  val loss:  0.6708329916000366\n",
      "epoch:  23   step:  145   train loss:  0.0005669749225489795  val loss:  0.6691622734069824\n",
      "epoch:  23   step:  146   train loss:  0.0011460244422778487  val loss:  0.6752514243125916\n",
      "epoch:  23   step:  147   train loss:  0.0019002491608262062  val loss:  0.6710590124130249\n",
      "epoch:  23   step:  148   train loss:  0.0008127167820930481  val loss:  0.6685707569122314\n",
      "epoch:  23   step:  149   train loss:  0.0003638578928075731  val loss:  0.6679911613464355\n",
      "epoch:  23   step:  150   train loss:  0.00043421488953754306  val loss:  0.664819598197937\n",
      "epoch:  23   step:  151   train loss:  0.0008691782131791115  val loss:  0.6740514039993286\n",
      "epoch:  23   step:  152   train loss:  0.001163110719062388  val loss:  0.6687159538269043\n",
      "epoch:  23   step:  153   train loss:  0.0010855096625164151  val loss:  0.66887366771698\n",
      "epoch:  23   step:  154   train loss:  0.001161039574071765  val loss:  0.6711690425872803\n",
      "epoch:  23   step:  155   train loss:  0.0013591677416116  val loss:  0.6723976135253906\n",
      "epoch:  23   step:  156   train loss:  0.0012324878480285406  val loss:  0.6832809448242188\n",
      "epoch:  23   step:  157   train loss:  0.0008416865020990372  val loss:  0.6736361384391785\n",
      "epoch:  23   step:  158   train loss:  0.0007203792920336127  val loss:  0.6713048219680786\n",
      "epoch:  23   step:  159   train loss:  0.00044730474473908544  val loss:  0.6715216040611267\n",
      "epoch:  23   step:  160   train loss:  0.0010031904093921185  val loss:  0.6657293438911438\n",
      "epoch:  23   step:  161   train loss:  0.000450821069534868  val loss:  0.6607666015625\n",
      "epoch:  23   step:  162   train loss:  0.0006554918945766985  val loss:  0.6630597710609436\n",
      "epoch:  23   step:  163   train loss:  0.0008541736751794815  val loss:  0.6591051816940308\n",
      "epoch:  23   step:  164   train loss:  0.0005050886538811028  val loss:  0.6613062620162964\n",
      "epoch:  23   step:  165   train loss:  0.0008869936573319137  val loss:  0.6591811180114746\n",
      "epoch:  24   step:  0   train loss:  0.0005575361428782344  val loss:  0.6583088040351868\n",
      "epoch:  24   step:  1   train loss:  0.0006942678592167795  val loss:  0.6634239554405212\n",
      "epoch:  24   step:  2   train loss:  0.0009728846489451826  val loss:  0.672379732131958\n",
      "epoch:  24   step:  3   train loss:  0.0007113945321179926  val loss:  0.6725669503211975\n",
      "epoch:  24   step:  4   train loss:  0.0005402701208367944  val loss:  0.6753556132316589\n",
      "epoch:  24   step:  5   train loss:  0.0011029202723875642  val loss:  0.6675847768783569\n",
      "epoch:  24   step:  6   train loss:  0.0007997122593224049  val loss:  0.6650876402854919\n",
      "epoch:  24   step:  7   train loss:  0.0011097085662186146  val loss:  0.6657721400260925\n",
      "epoch:  24   step:  8   train loss:  0.0007880933117121458  val loss:  0.6710890531539917\n",
      "epoch:  24   step:  9   train loss:  0.0006290564779192209  val loss:  0.6719633340835571\n",
      "epoch:  24   step:  10   train loss:  0.0006529965903609991  val loss:  0.6743725538253784\n",
      "epoch:  24   step:  11   train loss:  0.0007493646116927266  val loss:  0.6621479392051697\n",
      "epoch:  24   step:  12   train loss:  0.0006355951190926135  val loss:  0.6581928730010986\n",
      "epoch:  24   step:  13   train loss:  0.0009373356588184834  val loss:  0.6606947779655457\n",
      "epoch:  24   step:  14   train loss:  0.0004489399725571275  val loss:  0.6658689975738525\n",
      "epoch:  24   step:  15   train loss:  0.0012651110300794244  val loss:  0.672157883644104\n",
      "epoch:  24   step:  16   train loss:  0.0008015930652618408  val loss:  0.6795780658721924\n",
      "epoch:  24   step:  17   train loss:  0.0009869539644569159  val loss:  0.6725599765777588\n",
      "epoch:  24   step:  18   train loss:  0.0006117743905633688  val loss:  0.6699113249778748\n",
      "epoch:  24   step:  19   train loss:  0.0008062486886046827  val loss:  0.6701139807701111\n",
      "epoch:  24   step:  20   train loss:  0.0005421214154921472  val loss:  0.6592997312545776\n",
      "epoch:  24   step:  21   train loss:  0.0008700921898707747  val loss:  0.6519231200218201\n",
      "epoch:  24   step:  22   train loss:  0.0006376167293637991  val loss:  0.6588979959487915\n",
      "epoch:  24   step:  23   train loss:  0.0010158393997699022  val loss:  0.657139241695404\n",
      "epoch:  24   step:  24   train loss:  0.0004758767317980528  val loss:  0.663368284702301\n",
      "epoch:  24   step:  25   train loss:  0.0006084113847464323  val loss:  0.6694311499595642\n",
      "epoch:  24   step:  26   train loss:  0.0008245347999036312  val loss:  0.686341404914856\n",
      "epoch:  24   step:  27   train loss:  0.00045257920282892883  val loss:  0.6887127757072449\n",
      "epoch:  24   step:  28   train loss:  0.0007662473362870514  val loss:  0.6838987469673157\n",
      "epoch:  24   step:  29   train loss:  0.0014222245663404465  val loss:  0.6991936564445496\n",
      "epoch:  24   step:  30   train loss:  0.0005531730130314827  val loss:  0.6962372660636902\n",
      "epoch:  24   step:  31   train loss:  0.0007762279128655791  val loss:  0.7013547420501709\n",
      "epoch:  24   step:  32   train loss:  0.0005204059416428208  val loss:  0.7032732367515564\n",
      "epoch:  24   step:  33   train loss:  0.0006956030847504735  val loss:  0.6994460225105286\n",
      "epoch:  24   step:  34   train loss:  0.0007046976825222373  val loss:  0.7002624869346619\n",
      "epoch:  24   step:  35   train loss:  0.0008702627383172512  val loss:  0.7042743563652039\n",
      "epoch:  24   step:  36   train loss:  0.0015608267858624458  val loss:  0.6913299560546875\n",
      "epoch:  24   step:  37   train loss:  0.0009354046196676791  val loss:  0.7102579474449158\n",
      "epoch:  24   step:  38   train loss:  0.0009954421548172832  val loss:  0.7037886381149292\n",
      "epoch:  24   step:  39   train loss:  0.0009268171852454543  val loss:  0.705426037311554\n",
      "epoch:  24   step:  40   train loss:  0.0012377225793898106  val loss:  0.7099726796150208\n",
      "epoch:  24   step:  41   train loss:  0.0004747582133859396  val loss:  0.714481770992279\n",
      "epoch:  24   step:  42   train loss:  0.0004141485260333866  val loss:  0.7124506831169128\n",
      "epoch:  24   step:  43   train loss:  0.0005117445252835751  val loss:  0.7056072354316711\n",
      "epoch:  24   step:  44   train loss:  0.00034134171437472105  val loss:  0.7120316624641418\n",
      "epoch:  24   step:  45   train loss:  0.0007281993748620152  val loss:  0.7162348031997681\n",
      "epoch:  24   step:  46   train loss:  0.00036851392360404134  val loss:  0.7139272093772888\n",
      "epoch:  24   step:  47   train loss:  0.0009003004524856806  val loss:  0.7182437181472778\n",
      "epoch:  24   step:  48   train loss:  0.0011431574821472168  val loss:  0.7208961844444275\n",
      "epoch:  24   step:  49   train loss:  0.0007306315819732845  val loss:  0.7278744578361511\n",
      "epoch:  24   step:  50   train loss:  0.0006441351724788547  val loss:  0.7242605686187744\n",
      "epoch:  24   step:  51   train loss:  0.0006313748890534043  val loss:  0.7295664548873901\n",
      "epoch:  24   step:  52   train loss:  0.0008075324585661292  val loss:  0.7264853715896606\n",
      "epoch:  24   step:  53   train loss:  0.001227579079568386  val loss:  0.7201423645019531\n",
      "epoch:  24   step:  54   train loss:  0.0005866503342986107  val loss:  0.723487913608551\n",
      "epoch:  24   step:  55   train loss:  0.000768890546169132  val loss:  0.7220606803894043\n",
      "epoch:  24   step:  56   train loss:  0.000436639238614589  val loss:  0.7150490880012512\n",
      "epoch:  24   step:  57   train loss:  0.00046561326598748565  val loss:  0.7237585783004761\n",
      "epoch:  24   step:  58   train loss:  0.0005089017213322222  val loss:  0.729490339756012\n",
      "epoch:  24   step:  59   train loss:  0.0005083935102447867  val loss:  0.7320049405097961\n",
      "epoch:  24   step:  60   train loss:  0.0011564175365492702  val loss:  0.7294116616249084\n",
      "epoch:  24   step:  61   train loss:  0.0011231995886191726  val loss:  0.7392886281013489\n",
      "epoch:  24   step:  62   train loss:  0.0009784819558262825  val loss:  0.7327142357826233\n",
      "epoch:  24   step:  63   train loss:  0.001259730546735227  val loss:  0.7386219501495361\n",
      "epoch:  24   step:  64   train loss:  0.002224134048447013  val loss:  0.747477114200592\n",
      "epoch:  24   step:  65   train loss:  0.0008095318917185068  val loss:  0.7435417771339417\n",
      "epoch:  24   step:  66   train loss:  0.00058393320068717  val loss:  0.7444232106208801\n",
      "epoch:  24   step:  67   train loss:  0.0008282792987301946  val loss:  0.7372221350669861\n",
      "epoch:  24   step:  68   train loss:  0.0008744095684960485  val loss:  0.747606635093689\n",
      "epoch:  24   step:  69   train loss:  0.0006673549069091678  val loss:  0.7500527501106262\n",
      "epoch:  24   step:  70   train loss:  0.0006716534262523055  val loss:  0.7356770038604736\n",
      "epoch:  24   step:  71   train loss:  0.0006153619615361094  val loss:  0.737170934677124\n",
      "epoch:  24   step:  72   train loss:  0.0008612180827185512  val loss:  0.7341928482055664\n",
      "epoch:  24   step:  73   train loss:  0.0009921342134475708  val loss:  0.7426205277442932\n",
      "epoch:  24   step:  74   train loss:  0.0006123867351561785  val loss:  0.7378853559494019\n",
      "epoch:  24   step:  75   train loss:  0.0012858400586992502  val loss:  0.7405068278312683\n",
      "epoch:  24   step:  76   train loss:  0.0006463577155955136  val loss:  0.742695689201355\n",
      "epoch:  24   step:  77   train loss:  0.0006011192454025149  val loss:  0.7396345138549805\n",
      "epoch:  24   step:  78   train loss:  0.001063776551745832  val loss:  0.734475314617157\n",
      "epoch:  24   step:  79   train loss:  0.0007086710538715124  val loss:  0.7297988533973694\n",
      "epoch:  24   step:  80   train loss:  0.0012732002651318908  val loss:  0.727026641368866\n",
      "epoch:  24   step:  81   train loss:  0.001355728949420154  val loss:  0.7338695526123047\n",
      "epoch:  24   step:  82   train loss:  0.0006750505999661982  val loss:  0.7289549112319946\n",
      "epoch:  24   step:  83   train loss:  0.0006067102076485753  val loss:  0.7185824513435364\n",
      "epoch:  24   step:  84   train loss:  0.0004488887498155236  val loss:  0.7202733159065247\n",
      "epoch:  24   step:  85   train loss:  0.0005700978799723089  val loss:  0.7190918326377869\n",
      "epoch:  24   step:  86   train loss:  0.001360480790026486  val loss:  0.7088095545768738\n",
      "epoch:  24   step:  87   train loss:  0.0008307276293635368  val loss:  0.7113730311393738\n",
      "epoch:  24   step:  88   train loss:  0.0006835825042799115  val loss:  0.7107057571411133\n",
      "epoch:  24   step:  89   train loss:  0.0008829694706946611  val loss:  0.7138320207595825\n",
      "epoch:  24   step:  90   train loss:  0.0008795108878985047  val loss:  0.7091772556304932\n",
      "epoch:  24   step:  91   train loss:  0.0010733221424743533  val loss:  0.7138733267784119\n",
      "epoch:  24   step:  92   train loss:  0.0005958389956504107  val loss:  0.7127702236175537\n",
      "epoch:  24   step:  93   train loss:  0.0008379251230508089  val loss:  0.7156121730804443\n",
      "epoch:  24   step:  94   train loss:  0.0006679052021354437  val loss:  0.7172130346298218\n",
      "epoch:  24   step:  95   train loss:  0.0017213618848472834  val loss:  0.7240630984306335\n",
      "epoch:  24   step:  96   train loss:  0.0007898292969912291  val loss:  0.7297208309173584\n",
      "epoch:  24   step:  97   train loss:  0.0008698158198967576  val loss:  0.7290785312652588\n",
      "epoch:  24   step:  98   train loss:  0.0008037376101128757  val loss:  0.7263271808624268\n",
      "epoch:  24   step:  99   train loss:  0.00043203929089941084  val loss:  0.7224522829055786\n",
      "epoch:  24   step:  100   train loss:  0.00032512182951904833  val loss:  0.7178195714950562\n",
      "epoch:  24   step:  101   train loss:  0.0011614442337304354  val loss:  0.7216140627861023\n",
      "epoch:  24   step:  102   train loss:  0.0008300361805595458  val loss:  0.709039568901062\n",
      "epoch:  24   step:  103   train loss:  0.00044881022768095136  val loss:  0.6979748606681824\n",
      "epoch:  24   step:  104   train loss:  0.00031714580836705863  val loss:  0.7000899910926819\n",
      "epoch:  24   step:  105   train loss:  0.0009870358044281602  val loss:  0.6784196496009827\n",
      "epoch:  24   step:  106   train loss:  0.0009624108788557351  val loss:  0.6667407751083374\n",
      "epoch:  24   step:  107   train loss:  0.000984670128673315  val loss:  0.675898551940918\n",
      "epoch:  24   step:  108   train loss:  0.0011131439823657274  val loss:  0.6692850589752197\n",
      "epoch:  24   step:  109   train loss:  0.0008876260253600776  val loss:  0.6710052490234375\n",
      "epoch:  24   step:  110   train loss:  0.0011136166285723448  val loss:  0.672054648399353\n",
      "epoch:  24   step:  111   train loss:  0.0004885309608653188  val loss:  0.675160825252533\n",
      "epoch:  24   step:  112   train loss:  0.0005917646922171116  val loss:  0.681007444858551\n",
      "epoch:  24   step:  113   train loss:  0.0006345139117911458  val loss:  0.6768513321876526\n",
      "epoch:  24   step:  114   train loss:  0.0006542584160342813  val loss:  0.6612718105316162\n",
      "epoch:  24   step:  115   train loss:  0.0011724841315299273  val loss:  0.6580761075019836\n",
      "epoch:  24   step:  116   train loss:  0.00048527197213843465  val loss:  0.660835325717926\n",
      "epoch:  24   step:  117   train loss:  0.0011017750948667526  val loss:  0.6706988215446472\n",
      "epoch:  24   step:  118   train loss:  0.0005636851419694722  val loss:  0.6805320382118225\n",
      "epoch:  24   step:  119   train loss:  0.000917473342269659  val loss:  0.6861140131950378\n",
      "epoch:  24   step:  120   train loss:  0.0005577458650805056  val loss:  0.6753494739532471\n",
      "epoch:  24   step:  121   train loss:  0.0006692020688205957  val loss:  0.6803593039512634\n",
      "epoch:  24   step:  122   train loss:  0.0008859478402882814  val loss:  0.6740210652351379\n",
      "epoch:  24   step:  123   train loss:  0.0037441537715494633  val loss:  0.687454342842102\n",
      "epoch:  24   step:  124   train loss:  0.0012420620769262314  val loss:  0.7014898657798767\n",
      "epoch:  24   step:  125   train loss:  0.0007064296514727175  val loss:  0.7002524137496948\n",
      "epoch:  24   step:  126   train loss:  0.001709293806925416  val loss:  0.6869637370109558\n",
      "epoch:  24   step:  127   train loss:  0.0005396050401031971  val loss:  0.6828606128692627\n",
      "epoch:  24   step:  128   train loss:  0.0004883324145339429  val loss:  0.6856030821800232\n",
      "epoch:  24   step:  129   train loss:  0.0005867937579751015  val loss:  0.6853763461112976\n",
      "epoch:  24   step:  130   train loss:  0.0004943559761159122  val loss:  0.6838567852973938\n",
      "epoch:  24   step:  131   train loss:  0.0007519584032706916  val loss:  0.69342041015625\n",
      "epoch:  24   step:  132   train loss:  0.0006335751386359334  val loss:  0.7027440667152405\n",
      "epoch:  24   step:  133   train loss:  0.0007609173189848661  val loss:  0.6992705464363098\n",
      "epoch:  24   step:  134   train loss:  0.0007970917504280806  val loss:  0.7000133991241455\n",
      "epoch:  24   step:  135   train loss:  0.001290884567424655  val loss:  0.6976407170295715\n",
      "epoch:  24   step:  136   train loss:  0.0010536429472267628  val loss:  0.7012625932693481\n",
      "epoch:  24   step:  137   train loss:  0.0005031342152506113  val loss:  0.6984899044036865\n",
      "epoch:  24   step:  138   train loss:  0.0005338873597793281  val loss:  0.7027643918991089\n",
      "epoch:  24   step:  139   train loss:  0.0009036526316776872  val loss:  0.6986520886421204\n",
      "epoch:  24   step:  140   train loss:  0.0006994887953624129  val loss:  0.6914355158805847\n",
      "epoch:  24   step:  141   train loss:  0.0006240999209694564  val loss:  0.6861618161201477\n",
      "epoch:  24   step:  142   train loss:  0.0005756886093877256  val loss:  0.6912907361984253\n",
      "epoch:  24   step:  143   train loss:  0.0006428140914067626  val loss:  0.6913655996322632\n",
      "epoch:  24   step:  144   train loss:  0.0005659038433805108  val loss:  0.6879960298538208\n",
      "epoch:  24   step:  145   train loss:  0.0013756738044321537  val loss:  0.6760385036468506\n",
      "epoch:  24   step:  146   train loss:  0.000486989738419652  val loss:  0.6761860251426697\n",
      "epoch:  24   step:  147   train loss:  0.0005673913983628154  val loss:  0.679221510887146\n",
      "epoch:  24   step:  148   train loss:  0.000604793312959373  val loss:  0.6758546829223633\n",
      "epoch:  24   step:  149   train loss:  0.0009133084677159786  val loss:  0.683803915977478\n",
      "epoch:  24   step:  150   train loss:  0.0006862618611194193  val loss:  0.6709104776382446\n",
      "epoch:  24   step:  151   train loss:  0.00046781584387645125  val loss:  0.6799744963645935\n",
      "epoch:  24   step:  152   train loss:  0.0008028934244066477  val loss:  0.6812761425971985\n",
      "epoch:  24   step:  153   train loss:  0.0008853907347656786  val loss:  0.683020830154419\n",
      "epoch:  24   step:  154   train loss:  0.0005410942249000072  val loss:  0.6731785535812378\n",
      "epoch:  24   step:  155   train loss:  0.00070839689578861  val loss:  0.6732425689697266\n",
      "epoch:  24   step:  156   train loss:  0.0012109788367524743  val loss:  0.6707728505134583\n",
      "epoch:  24   step:  157   train loss:  0.0007217106176540256  val loss:  0.6782916188240051\n",
      "epoch:  24   step:  158   train loss:  0.0005305359954945743  val loss:  0.6832290291786194\n",
      "epoch:  24   step:  159   train loss:  0.0007698790868744254  val loss:  0.6852816343307495\n",
      "epoch:  24   step:  160   train loss:  0.0006183405639603734  val loss:  0.6843182444572449\n",
      "epoch:  24   step:  161   train loss:  0.001692259800620377  val loss:  0.6789231896400452\n",
      "epoch:  24   step:  162   train loss:  0.0006813554209657013  val loss:  0.6805077791213989\n",
      "epoch:  24   step:  163   train loss:  0.0010188095038756728  val loss:  0.6923468112945557\n",
      "epoch:  24   step:  164   train loss:  0.001017604023218155  val loss:  0.6940869092941284\n",
      "epoch:  24   step:  165   train loss:  0.0006556825246661901  val loss:  0.6918231844902039\n",
      "epoch:  25   step:  0   train loss:  0.0007845510262995958  val loss:  0.6957241892814636\n",
      "epoch:  25   step:  1   train loss:  0.0007925513782538474  val loss:  0.700129508972168\n",
      "epoch:  25   step:  2   train loss:  0.0003325113211758435  val loss:  0.7004242539405823\n",
      "epoch:  25   step:  3   train loss:  0.000654209463391453  val loss:  0.7106127738952637\n",
      "epoch:  25   step:  4   train loss:  0.0011118568945676088  val loss:  0.6995081305503845\n",
      "epoch:  25   step:  5   train loss:  0.0007383351912721992  val loss:  0.6976639628410339\n",
      "epoch:  25   step:  6   train loss:  0.000449597486294806  val loss:  0.6970240473747253\n",
      "epoch:  25   step:  7   train loss:  0.0007835348369553685  val loss:  0.7040672302246094\n",
      "epoch:  25   step:  8   train loss:  0.0009130261605605483  val loss:  0.7102800607681274\n",
      "epoch:  25   step:  9   train loss:  0.0004886388778686523  val loss:  0.7061272859573364\n",
      "epoch:  25   step:  10   train loss:  0.0007346055936068296  val loss:  0.7045525908470154\n",
      "epoch:  25   step:  11   train loss:  0.000540701556019485  val loss:  0.7049455046653748\n",
      "epoch:  25   step:  12   train loss:  0.0007754673133604228  val loss:  0.6988723874092102\n",
      "epoch:  25   step:  13   train loss:  0.0007689538178965449  val loss:  0.6937694549560547\n",
      "epoch:  25   step:  14   train loss:  0.0006390328053385019  val loss:  0.6925890445709229\n",
      "epoch:  25   step:  15   train loss:  0.0006681554950773716  val loss:  0.6905083060264587\n",
      "epoch:  25   step:  16   train loss:  0.00022313366935122758  val loss:  0.6922769546508789\n",
      "epoch:  25   step:  17   train loss:  0.0014957867097109556  val loss:  0.6818300485610962\n",
      "epoch:  25   step:  18   train loss:  0.001311528729274869  val loss:  0.687524676322937\n",
      "epoch:  25   step:  19   train loss:  0.0006513164844363928  val loss:  0.685282826423645\n",
      "epoch:  25   step:  20   train loss:  0.0004476256435737014  val loss:  0.6859854459762573\n",
      "epoch:  25   step:  21   train loss:  0.00041977083310484886  val loss:  0.6877259612083435\n",
      "epoch:  25   step:  22   train loss:  0.0006438330747187138  val loss:  0.6931596994400024\n",
      "epoch:  25   step:  23   train loss:  0.0012494829716160893  val loss:  0.7023456692695618\n",
      "epoch:  25   step:  24   train loss:  0.0004385940555948764  val loss:  0.7057307362556458\n",
      "epoch:  25   step:  25   train loss:  0.0005053308559581637  val loss:  0.6978335380554199\n",
      "epoch:  25   step:  26   train loss:  0.0008134572417475283  val loss:  0.70274418592453\n",
      "epoch:  25   step:  27   train loss:  0.000818499072920531  val loss:  0.7062149047851562\n",
      "epoch:  25   step:  28   train loss:  0.0004706403415184468  val loss:  0.6956988573074341\n",
      "epoch:  25   step:  29   train loss:  0.0005420942907221615  val loss:  0.6896201372146606\n",
      "epoch:  25   step:  30   train loss:  0.0005957759567536414  val loss:  0.6928687691688538\n",
      "epoch:  25   step:  31   train loss:  0.0005985343595966697  val loss:  0.6883456110954285\n",
      "epoch:  25   step:  32   train loss:  0.0003827009932138026  val loss:  0.6765896677970886\n",
      "epoch:  25   step:  33   train loss:  0.0003747661248780787  val loss:  0.6824278831481934\n",
      "epoch:  25   step:  34   train loss:  0.000748600868973881  val loss:  0.6810882687568665\n",
      "epoch:  25   step:  35   train loss:  0.0013241039123386145  val loss:  0.6729347705841064\n",
      "epoch:  25   step:  36   train loss:  0.0004506800905801356  val loss:  0.6789107322692871\n",
      "epoch:  25   step:  37   train loss:  0.000644541229121387  val loss:  0.6834889054298401\n",
      "epoch:  25   step:  38   train loss:  0.0009757382213138044  val loss:  0.683102011680603\n",
      "epoch:  25   step:  39   train loss:  0.00032886903500184417  val loss:  0.6789997220039368\n",
      "epoch:  25   step:  40   train loss:  0.00042131225927732885  val loss:  0.6748774647712708\n",
      "epoch:  25   step:  41   train loss:  0.0006116798613220453  val loss:  0.6800169944763184\n",
      "epoch:  25   step:  42   train loss:  0.00038917677011340857  val loss:  0.6803997755050659\n",
      "epoch:  25   step:  43   train loss:  0.0004906728863716125  val loss:  0.6857513785362244\n",
      "epoch:  25   step:  44   train loss:  0.0007268954068422318  val loss:  0.6812191605567932\n",
      "epoch:  25   step:  45   train loss:  0.0007149460725486279  val loss:  0.6788235902786255\n",
      "epoch:  25   step:  46   train loss:  0.000641178572550416  val loss:  0.6874299645423889\n",
      "epoch:  25   step:  47   train loss:  0.0008509122999384999  val loss:  0.6853353977203369\n",
      "epoch:  25   step:  48   train loss:  0.0004869774274993688  val loss:  0.6822643876075745\n",
      "epoch:  25   step:  49   train loss:  0.0002992167428601533  val loss:  0.6855700612068176\n",
      "epoch:  25   step:  50   train loss:  0.0005678714369423687  val loss:  0.6842424869537354\n",
      "epoch:  25   step:  51   train loss:  0.0006729952292516828  val loss:  0.6897867918014526\n",
      "epoch:  25   step:  52   train loss:  0.00047770258970558643  val loss:  0.6899044513702393\n",
      "epoch:  25   step:  53   train loss:  0.0005364784155972302  val loss:  0.6904504895210266\n",
      "epoch:  25   step:  54   train loss:  0.0006034807302057743  val loss:  0.6920884847640991\n",
      "epoch:  25   step:  55   train loss:  0.0004912010626867414  val loss:  0.6892510652542114\n",
      "epoch:  25   step:  56   train loss:  0.0005805558757856488  val loss:  0.6808181405067444\n",
      "epoch:  25   step:  57   train loss:  0.0007757126586511731  val loss:  0.6743839979171753\n",
      "epoch:  25   step:  58   train loss:  0.000332640134729445  val loss:  0.6720576286315918\n",
      "epoch:  25   step:  59   train loss:  0.00047560271923430264  val loss:  0.6725287437438965\n",
      "epoch:  25   step:  60   train loss:  0.00041732130921445787  val loss:  0.6640979051589966\n",
      "epoch:  25   step:  61   train loss:  0.00077646947465837  val loss:  0.6581548452377319\n",
      "epoch:  25   step:  62   train loss:  0.0005915574147365987  val loss:  0.6635487079620361\n",
      "epoch:  25   step:  63   train loss:  0.0005151219666004181  val loss:  0.6642162799835205\n",
      "epoch:  25   step:  64   train loss:  0.0006585099617950618  val loss:  0.6726742386817932\n",
      "epoch:  25   step:  65   train loss:  0.0006268327706493437  val loss:  0.6826766133308411\n",
      "epoch:  25   step:  66   train loss:  0.0007515009492635727  val loss:  0.6804042458534241\n",
      "epoch:  25   step:  67   train loss:  0.00047961517702788115  val loss:  0.675773024559021\n",
      "epoch:  25   step:  68   train loss:  0.0008561566937714815  val loss:  0.6805828213691711\n",
      "epoch:  25   step:  69   train loss:  0.0005008316366001964  val loss:  0.6848427653312683\n",
      "epoch:  25   step:  70   train loss:  0.0008122219005599618  val loss:  0.6849242448806763\n",
      "epoch:  25   step:  71   train loss:  0.0004705364699475467  val loss:  0.6950051784515381\n",
      "epoch:  25   step:  72   train loss:  0.0006165802478790283  val loss:  0.6979571580886841\n",
      "epoch:  25   step:  73   train loss:  0.0006548314122483134  val loss:  0.7049185037612915\n",
      "epoch:  25   step:  74   train loss:  0.000660120858810842  val loss:  0.705381453037262\n",
      "epoch:  25   step:  75   train loss:  0.0005613129469566047  val loss:  0.6988955736160278\n",
      "epoch:  25   step:  76   train loss:  0.0007000647601671517  val loss:  0.6939956545829773\n",
      "epoch:  25   step:  77   train loss:  0.0004526166303548962  val loss:  0.7056705355644226\n",
      "epoch:  25   step:  78   train loss:  0.0006552988197654486  val loss:  0.7059537172317505\n",
      "epoch:  25   step:  79   train loss:  0.00047651707427576184  val loss:  0.7014462351799011\n",
      "epoch:  25   step:  80   train loss:  0.0005463470588438213  val loss:  0.7077911496162415\n",
      "epoch:  25   step:  81   train loss:  0.0012580056209117174  val loss:  0.7051535844802856\n",
      "epoch:  25   step:  82   train loss:  0.0007328036008402705  val loss:  0.7013825178146362\n",
      "epoch:  25   step:  83   train loss:  0.0004038509214296937  val loss:  0.7089555859565735\n",
      "epoch:  25   step:  84   train loss:  0.0005611319793388247  val loss:  0.7130333781242371\n",
      "epoch:  25   step:  85   train loss:  0.00047837890451774  val loss:  0.7209821343421936\n",
      "epoch:  25   step:  86   train loss:  0.0005058349925093353  val loss:  0.7225884795188904\n",
      "epoch:  25   step:  87   train loss:  0.00048702265485189855  val loss:  0.7232745289802551\n",
      "epoch:  25   step:  88   train loss:  0.0008946314337663352  val loss:  0.7292724251747131\n",
      "epoch:  25   step:  89   train loss:  0.0006082839099690318  val loss:  0.7257956266403198\n",
      "epoch:  25   step:  90   train loss:  0.000443165103206411  val loss:  0.7191227078437805\n",
      "epoch:  25   step:  91   train loss:  0.0002261670888401568  val loss:  0.7162577509880066\n",
      "epoch:  25   step:  92   train loss:  0.000697641633450985  val loss:  0.7136708498001099\n",
      "epoch:  25   step:  93   train loss:  0.00045681995106860995  val loss:  0.7098590731620789\n",
      "epoch:  25   step:  94   train loss:  0.0017989021725952625  val loss:  0.714299201965332\n",
      "epoch:  25   step:  95   train loss:  0.0004939462523907423  val loss:  0.7132299542427063\n",
      "epoch:  25   step:  96   train loss:  0.00054643937619403  val loss:  0.714324414730072\n",
      "epoch:  25   step:  97   train loss:  0.0007559510413557291  val loss:  0.7117214798927307\n",
      "epoch:  25   step:  98   train loss:  0.0003825983148999512  val loss:  0.7105610966682434\n",
      "epoch:  25   step:  99   train loss:  0.0008632165845483541  val loss:  0.7116332650184631\n",
      "epoch:  25   step:  100   train loss:  0.00046162924263626337  val loss:  0.7187739610671997\n",
      "epoch:  25   step:  101   train loss:  0.0004082900704815984  val loss:  0.7110464572906494\n",
      "epoch:  25   step:  102   train loss:  0.0016576258931308985  val loss:  0.7067400217056274\n",
      "epoch:  25   step:  103   train loss:  0.0006635376485064626  val loss:  0.7057915329933167\n",
      "epoch:  25   step:  104   train loss:  0.0004195842775516212  val loss:  0.7068871259689331\n",
      "epoch:  25   step:  105   train loss:  0.000576632097363472  val loss:  0.7161418199539185\n",
      "epoch:  25   step:  106   train loss:  0.0005596160772256553  val loss:  0.7129116654396057\n",
      "epoch:  25   step:  107   train loss:  0.00038535651401616633  val loss:  0.709645688533783\n",
      "epoch:  25   step:  108   train loss:  0.0008319264743477106  val loss:  0.7048202157020569\n",
      "epoch:  25   step:  109   train loss:  0.0008089592447504401  val loss:  0.6962481737136841\n",
      "epoch:  25   step:  110   train loss:  0.00042023661080747843  val loss:  0.700103223323822\n",
      "epoch:  25   step:  111   train loss:  0.00053602852858603  val loss:  0.7048103213310242\n",
      "epoch:  25   step:  112   train loss:  0.0006937546422705054  val loss:  0.706732451915741\n",
      "epoch:  25   step:  113   train loss:  0.0007537137717008591  val loss:  0.7079876661300659\n",
      "epoch:  25   step:  114   train loss:  0.000561010732781142  val loss:  0.7037002444267273\n",
      "epoch:  25   step:  115   train loss:  0.0005490582552738488  val loss:  0.6939427256584167\n",
      "epoch:  25   step:  116   train loss:  0.0006666725385002792  val loss:  0.7032861709594727\n",
      "epoch:  25   step:  117   train loss:  0.000515289546456188  val loss:  0.6975400447845459\n",
      "epoch:  25   step:  118   train loss:  0.0009618264157325029  val loss:  0.7079617381095886\n",
      "epoch:  25   step:  119   train loss:  0.0007478395709767938  val loss:  0.7099648714065552\n",
      "epoch:  25   step:  120   train loss:  0.000571971177123487  val loss:  0.7043851017951965\n",
      "epoch:  25   step:  121   train loss:  0.0003091157996095717  val loss:  0.7093757390975952\n",
      "epoch:  25   step:  122   train loss:  0.0004701924044638872  val loss:  0.7157790064811707\n",
      "epoch:  25   step:  123   train loss:  0.0008119444246403873  val loss:  0.7165471315383911\n",
      "epoch:  25   step:  124   train loss:  0.0007571704918518662  val loss:  0.7212871313095093\n",
      "epoch:  25   step:  125   train loss:  0.0002902451960835606  val loss:  0.7275114059448242\n",
      "epoch:  25   step:  126   train loss:  0.0003908601065631956  val loss:  0.7246856689453125\n",
      "epoch:  25   step:  127   train loss:  0.0008478314848616719  val loss:  0.7264597415924072\n",
      "epoch:  25   step:  128   train loss:  0.0005317343748174608  val loss:  0.7252480387687683\n",
      "epoch:  25   step:  129   train loss:  0.0006567966775037348  val loss:  0.7265853881835938\n",
      "epoch:  25   step:  130   train loss:  0.0009836920071393251  val loss:  0.7279801964759827\n",
      "epoch:  25   step:  131   train loss:  0.00032376658055000007  val loss:  0.7229531407356262\n",
      "epoch:  25   step:  132   train loss:  0.0008785951649770141  val loss:  0.720032274723053\n",
      "epoch:  25   step:  133   train loss:  0.0005580775905400515  val loss:  0.7274044156074524\n",
      "epoch:  25   step:  134   train loss:  0.00042063897126354277  val loss:  0.7268036007881165\n",
      "epoch:  25   step:  135   train loss:  0.00031197466887533665  val loss:  0.7283892631530762\n",
      "epoch:  25   step:  136   train loss:  0.0006385036394931376  val loss:  0.7319263815879822\n",
      "epoch:  25   step:  137   train loss:  0.0006171701243147254  val loss:  0.7307883501052856\n",
      "epoch:  25   step:  138   train loss:  0.00029461283702403307  val loss:  0.736634373664856\n",
      "epoch:  25   step:  139   train loss:  0.0005061872070655227  val loss:  0.7395787835121155\n",
      "epoch:  25   step:  140   train loss:  0.0006427150219678879  val loss:  0.7466821074485779\n",
      "epoch:  25   step:  141   train loss:  0.0011277955491095781  val loss:  0.744684100151062\n",
      "epoch:  25   step:  142   train loss:  0.00043634153553284705  val loss:  0.7607806324958801\n",
      "epoch:  25   step:  143   train loss:  0.0005153067759238183  val loss:  0.7613852024078369\n",
      "epoch:  25   step:  144   train loss:  0.0003962238843087107  val loss:  0.7627078890800476\n",
      "epoch:  25   step:  145   train loss:  0.0008023512782528996  val loss:  0.7502414584159851\n",
      "epoch:  25   step:  146   train loss:  0.000858306244481355  val loss:  0.7483774423599243\n",
      "epoch:  25   step:  147   train loss:  0.0006776225054636598  val loss:  0.7510245442390442\n",
      "epoch:  25   step:  148   train loss:  0.0010126917622983456  val loss:  0.7495877146720886\n",
      "epoch:  25   step:  149   train loss:  0.0005597970448434353  val loss:  0.7592419385910034\n",
      "epoch:  25   step:  150   train loss:  0.0007471177959814668  val loss:  0.7548844218254089\n",
      "epoch:  25   step:  151   train loss:  0.00047000625636428595  val loss:  0.7572227120399475\n",
      "epoch:  25   step:  152   train loss:  0.0007342607714235783  val loss:  0.7687760591506958\n",
      "epoch:  25   step:  153   train loss:  0.0005492624477483332  val loss:  0.7609338164329529\n",
      "epoch:  25   step:  154   train loss:  0.0013857611920684576  val loss:  0.756305992603302\n",
      "epoch:  25   step:  155   train loss:  0.00035785933141596615  val loss:  0.7579857707023621\n",
      "epoch:  25   step:  156   train loss:  0.0003476919373497367  val loss:  0.7511056065559387\n",
      "epoch:  25   step:  157   train loss:  0.0004313972021918744  val loss:  0.752895176410675\n",
      "epoch:  25   step:  158   train loss:  0.0005159536376595497  val loss:  0.7500432133674622\n",
      "epoch:  25   step:  159   train loss:  0.0005265079089440405  val loss:  0.7522004246711731\n",
      "epoch:  25   step:  160   train loss:  0.0007677134126424789  val loss:  0.7516931891441345\n",
      "epoch:  25   step:  161   train loss:  0.00024438928812742233  val loss:  0.7394393682479858\n",
      "epoch:  25   step:  162   train loss:  0.0007884461083449423  val loss:  0.7375862002372742\n",
      "epoch:  25   step:  163   train loss:  0.0007774675032123923  val loss:  0.7451485395431519\n",
      "epoch:  25   step:  164   train loss:  0.0003779861726798117  val loss:  0.7563753128051758\n",
      "epoch:  25   step:  165   train loss:  0.0005094562657177448  val loss:  0.7402944564819336\n",
      "epoch:  26   step:  0   train loss:  0.0005401333910413086  val loss:  0.7532783150672913\n",
      "epoch:  26   step:  1   train loss:  0.0007840056205168366  val loss:  0.744002640247345\n",
      "epoch:  26   step:  2   train loss:  0.00042343756649643183  val loss:  0.7423815131187439\n",
      "epoch:  26   step:  3   train loss:  0.0006703007384203374  val loss:  0.7448758482933044\n",
      "epoch:  26   step:  4   train loss:  0.00034222210524603724  val loss:  0.7485494613647461\n",
      "epoch:  26   step:  5   train loss:  0.0005317154573276639  val loss:  0.7411579489707947\n",
      "epoch:  26   step:  6   train loss:  0.000471859413664788  val loss:  0.7467787265777588\n",
      "epoch:  26   step:  7   train loss:  0.000543591973837465  val loss:  0.7513863444328308\n",
      "epoch:  26   step:  8   train loss:  0.0006917306454852223  val loss:  0.7514844536781311\n",
      "epoch:  26   step:  9   train loss:  0.0005223294720053673  val loss:  0.746105432510376\n",
      "epoch:  26   step:  10   train loss:  0.0003563352220226079  val loss:  0.7453374266624451\n",
      "epoch:  26   step:  11   train loss:  0.0008233501575887203  val loss:  0.7579448819160461\n",
      "epoch:  26   step:  12   train loss:  0.0010259744012728333  val loss:  0.7618085145950317\n",
      "epoch:  26   step:  13   train loss:  0.0005595024558715522  val loss:  0.7673909068107605\n",
      "epoch:  26   step:  14   train loss:  0.0003929410013370216  val loss:  0.7642411589622498\n",
      "epoch:  26   step:  15   train loss:  0.0002906517474912107  val loss:  0.7615523934364319\n",
      "epoch:  26   step:  16   train loss:  0.0004026125534437597  val loss:  0.7561914920806885\n",
      "epoch:  26   step:  17   train loss:  0.0004655281372833997  val loss:  0.7430301308631897\n",
      "epoch:  26   step:  18   train loss:  0.0008035486098378897  val loss:  0.7429508566856384\n",
      "epoch:  26   step:  19   train loss:  0.00045809298171661794  val loss:  0.7578117847442627\n",
      "epoch:  26   step:  20   train loss:  0.00033137216814793646  val loss:  0.7552561163902283\n",
      "epoch:  26   step:  21   train loss:  0.0004539683577604592  val loss:  0.749195396900177\n",
      "epoch:  26   step:  22   train loss:  0.00028609795845113695  val loss:  0.7525363564491272\n",
      "epoch:  26   step:  23   train loss:  0.00028504463261924684  val loss:  0.7587518095970154\n",
      "epoch:  26   step:  24   train loss:  0.00028843770269304514  val loss:  0.7595385313034058\n",
      "epoch:  26   step:  25   train loss:  0.0007932387525215745  val loss:  0.7630038261413574\n",
      "epoch:  26   step:  26   train loss:  0.00031655849306844175  val loss:  0.7598232626914978\n",
      "epoch:  26   step:  27   train loss:  0.00022159158834256232  val loss:  0.7535803318023682\n",
      "epoch:  26   step:  28   train loss:  0.0006873788079246879  val loss:  0.7541820406913757\n",
      "epoch:  26   step:  29   train loss:  0.0004695880925282836  val loss:  0.7494090795516968\n",
      "epoch:  26   step:  30   train loss:  0.0003606582176871598  val loss:  0.7407503128051758\n",
      "epoch:  26   step:  31   train loss:  0.00048354361206293106  val loss:  0.7428823113441467\n",
      "epoch:  26   step:  32   train loss:  0.00035959717934019864  val loss:  0.7416219711303711\n",
      "epoch:  26   step:  33   train loss:  0.0006863899761810899  val loss:  0.7484212517738342\n",
      "epoch:  26   step:  34   train loss:  0.00020669920195359737  val loss:  0.7494900226593018\n",
      "epoch:  26   step:  35   train loss:  0.0002958682889584452  val loss:  0.7379635572433472\n",
      "epoch:  26   step:  36   train loss:  0.0004783350450452417  val loss:  0.7356857061386108\n",
      "epoch:  26   step:  37   train loss:  0.0003505122731439769  val loss:  0.7457265853881836\n",
      "epoch:  26   step:  38   train loss:  0.0004820779722649604  val loss:  0.7497254610061646\n",
      "epoch:  26   step:  39   train loss:  0.0008577255066484213  val loss:  0.7542704343795776\n",
      "epoch:  26   step:  40   train loss:  0.0004771406820509583  val loss:  0.7510010600090027\n",
      "epoch:  26   step:  41   train loss:  0.00035741133615374565  val loss:  0.7544830441474915\n",
      "epoch:  26   step:  42   train loss:  0.0003959914029110223  val loss:  0.7554038166999817\n",
      "epoch:  26   step:  43   train loss:  0.0005967140896245837  val loss:  0.7521278858184814\n",
      "epoch:  26   step:  44   train loss:  0.00046915560960769653  val loss:  0.7516835331916809\n",
      "epoch:  26   step:  45   train loss:  0.0007241673883982003  val loss:  0.7544401288032532\n",
      "epoch:  26   step:  46   train loss:  0.00041654196684248745  val loss:  0.762500524520874\n",
      "epoch:  26   step:  47   train loss:  0.000560486747417599  val loss:  0.7590751647949219\n",
      "epoch:  26   step:  48   train loss:  0.00042445561848580837  val loss:  0.7547727823257446\n",
      "epoch:  26   step:  49   train loss:  0.0008593086968176067  val loss:  0.7604737877845764\n",
      "epoch:  26   step:  50   train loss:  0.0006576922605745494  val loss:  0.7557389140129089\n",
      "epoch:  26   step:  51   train loss:  0.00021779895178042352  val loss:  0.7639572620391846\n",
      "epoch:  26   step:  52   train loss:  0.00048167776549234986  val loss:  0.7712487578392029\n",
      "epoch:  26   step:  53   train loss:  0.0004611996409948915  val loss:  0.7709917426109314\n",
      "epoch:  26   step:  54   train loss:  0.0006336680380627513  val loss:  0.777256429195404\n",
      "epoch:  26   step:  55   train loss:  0.0006184016820043325  val loss:  0.7662364840507507\n",
      "epoch:  26   step:  56   train loss:  0.0004553194157779217  val loss:  0.7576397657394409\n",
      "epoch:  26   step:  57   train loss:  0.0005035513313487172  val loss:  0.7605575919151306\n",
      "epoch:  26   step:  58   train loss:  0.00035302777541801333  val loss:  0.7669872045516968\n",
      "epoch:  26   step:  59   train loss:  0.0005233706906437874  val loss:  0.7611700296401978\n",
      "epoch:  26   step:  60   train loss:  0.0005420153611339629  val loss:  0.7579905390739441\n",
      "epoch:  26   step:  61   train loss:  0.0006908518262207508  val loss:  0.762008011341095\n",
      "epoch:  26   step:  62   train loss:  0.0008708726963959634  val loss:  0.7704235911369324\n",
      "epoch:  26   step:  63   train loss:  0.00029595656087622046  val loss:  0.7683936953544617\n",
      "epoch:  26   step:  64   train loss:  0.0004933536401949823  val loss:  0.7712239623069763\n",
      "epoch:  26   step:  65   train loss:  0.0008902004919946194  val loss:  0.7673473358154297\n",
      "epoch:  26   step:  66   train loss:  0.0005131180514581501  val loss:  0.7671998143196106\n",
      "epoch:  26   step:  67   train loss:  0.00035316587309353054  val loss:  0.7596426010131836\n",
      "epoch:  26   step:  68   train loss:  0.0004773582040797919  val loss:  0.7625344395637512\n",
      "epoch:  26   step:  69   train loss:  0.000390559493098408  val loss:  0.7589877247810364\n",
      "epoch:  26   step:  70   train loss:  0.00029849971178919077  val loss:  0.7587293982505798\n",
      "epoch:  26   step:  71   train loss:  0.0004906821995973587  val loss:  0.7496343851089478\n",
      "epoch:  26   step:  72   train loss:  0.00041426578536629677  val loss:  0.7405096888542175\n",
      "epoch:  26   step:  73   train loss:  0.0004562419489957392  val loss:  0.7398050427436829\n",
      "epoch:  26   step:  74   train loss:  0.0004846847732551396  val loss:  0.7401081919670105\n",
      "epoch:  26   step:  75   train loss:  0.0005199944134801626  val loss:  0.7294788360595703\n",
      "epoch:  26   step:  76   train loss:  0.0004842866910621524  val loss:  0.7251944541931152\n",
      "epoch:  26   step:  77   train loss:  0.0011781621724367142  val loss:  0.7406739592552185\n",
      "epoch:  26   step:  78   train loss:  0.0003742725239135325  val loss:  0.7365946173667908\n",
      "epoch:  26   step:  79   train loss:  0.0005408109864220023  val loss:  0.7412333488464355\n",
      "epoch:  26   step:  80   train loss:  0.0005278617609292269  val loss:  0.7271231412887573\n",
      "epoch:  26   step:  81   train loss:  0.0006408085464499891  val loss:  0.7266464829444885\n",
      "epoch:  26   step:  82   train loss:  0.0005803263047710061  val loss:  0.7339777946472168\n",
      "epoch:  26   step:  83   train loss:  0.0004210157203488052  val loss:  0.7328750491142273\n",
      "epoch:  26   step:  84   train loss:  0.0004679865960497409  val loss:  0.7281385660171509\n",
      "epoch:  26   step:  85   train loss:  0.00044737663120031357  val loss:  0.723074734210968\n",
      "epoch:  26   step:  86   train loss:  0.0006231791921891272  val loss:  0.7260743379592896\n",
      "epoch:  26   step:  87   train loss:  0.0005960265407338738  val loss:  0.7149236798286438\n",
      "epoch:  26   step:  88   train loss:  0.0007372009567916393  val loss:  0.7200650572776794\n",
      "epoch:  26   step:  89   train loss:  0.0004235468222759664  val loss:  0.7284720540046692\n",
      "epoch:  26   step:  90   train loss:  0.0006709530716761947  val loss:  0.7194870710372925\n",
      "epoch:  26   step:  91   train loss:  0.00029870716389268637  val loss:  0.7224035263061523\n",
      "epoch:  26   step:  92   train loss:  0.00046224385732784867  val loss:  0.7292850613594055\n",
      "epoch:  26   step:  93   train loss:  0.0005009303567931056  val loss:  0.7374069094657898\n",
      "epoch:  26   step:  94   train loss:  0.0004425588413141668  val loss:  0.7348598837852478\n",
      "epoch:  26   step:  95   train loss:  0.00046815257519483566  val loss:  0.7452499866485596\n",
      "epoch:  26   step:  96   train loss:  0.0005635297857224941  val loss:  0.7542687058448792\n",
      "epoch:  26   step:  97   train loss:  0.00033022239222191274  val loss:  0.7451629638671875\n",
      "epoch:  26   step:  98   train loss:  0.00041413542930968106  val loss:  0.7401867508888245\n",
      "epoch:  26   step:  99   train loss:  0.0004945086548104882  val loss:  0.7464099526405334\n",
      "epoch:  26   step:  100   train loss:  0.00034349982161074877  val loss:  0.7468952536582947\n",
      "epoch:  26   step:  101   train loss:  0.00040654849726706743  val loss:  0.7498462796211243\n",
      "epoch:  26   step:  102   train loss:  0.0007103001698851585  val loss:  0.7512311339378357\n",
      "epoch:  26   step:  103   train loss:  0.00046734651550650597  val loss:  0.7495407462120056\n",
      "epoch:  26   step:  104   train loss:  0.0003770292387343943  val loss:  0.7481812834739685\n",
      "epoch:  26   step:  105   train loss:  0.0006915155099704862  val loss:  0.7418943643569946\n",
      "epoch:  26   step:  106   train loss:  0.0006541047478094697  val loss:  0.7297382354736328\n",
      "epoch:  26   step:  107   train loss:  0.00038125828723423183  val loss:  0.7364952564239502\n",
      "epoch:  26   step:  108   train loss:  0.0009685777476988733  val loss:  0.7405966520309448\n",
      "epoch:  26   step:  109   train loss:  0.000504303548950702  val loss:  0.7408663034439087\n",
      "epoch:  26   step:  110   train loss:  0.00029624233138747513  val loss:  0.7377448678016663\n",
      "epoch:  26   step:  111   train loss:  0.00047685817116871476  val loss:  0.7575122117996216\n",
      "epoch:  26   step:  112   train loss:  0.00037864266778342426  val loss:  0.7572720050811768\n",
      "epoch:  26   step:  113   train loss:  0.000333924574078992  val loss:  0.762860119342804\n",
      "epoch:  26   step:  114   train loss:  0.0011968652252107859  val loss:  0.7694546580314636\n",
      "epoch:  26   step:  115   train loss:  0.0009499596199020743  val loss:  0.7740002870559692\n",
      "epoch:  26   step:  116   train loss:  0.0005662311450578272  val loss:  0.7755522131919861\n",
      "epoch:  26   step:  117   train loss:  0.0008795169997029006  val loss:  0.7734607458114624\n",
      "epoch:  26   step:  118   train loss:  0.00033197604352608323  val loss:  0.770309329032898\n",
      "epoch:  26   step:  119   train loss:  0.0003096676664426923  val loss:  0.7689270377159119\n",
      "epoch:  26   step:  120   train loss:  0.00030729814898222685  val loss:  0.775249719619751\n",
      "epoch:  26   step:  121   train loss:  0.0005181797314435244  val loss:  0.7663918733596802\n",
      "epoch:  26   step:  122   train loss:  0.00024383557320106775  val loss:  0.7654850482940674\n",
      "epoch:  26   step:  123   train loss:  0.0004630019247997552  val loss:  0.7708801627159119\n",
      "epoch:  26   step:  124   train loss:  0.00046888994984328747  val loss:  0.7767707705497742\n",
      "epoch:  26   step:  125   train loss:  0.0006930897943675518  val loss:  0.7720299363136292\n",
      "epoch:  26   step:  126   train loss:  0.0010361155727878213  val loss:  0.770051121711731\n",
      "epoch:  26   step:  127   train loss:  0.00042567061609588563  val loss:  0.7701923847198486\n",
      "epoch:  26   step:  128   train loss:  0.0006414067465811968  val loss:  0.7652575373649597\n",
      "epoch:  26   step:  129   train loss:  0.0003910523955710232  val loss:  0.7658297419548035\n",
      "epoch:  26   step:  130   train loss:  0.0006734702037647367  val loss:  0.7569172382354736\n",
      "epoch:  26   step:  131   train loss:  0.000619900762103498  val loss:  0.7618441581726074\n",
      "epoch:  26   step:  132   train loss:  0.0005538071854971349  val loss:  0.7703902125358582\n",
      "epoch:  26   step:  133   train loss:  0.0006103695486672223  val loss:  0.772943377494812\n",
      "epoch:  26   step:  134   train loss:  0.0017947901505976915  val loss:  0.7723814845085144\n",
      "epoch:  26   step:  135   train loss:  0.0005667972145602107  val loss:  0.7856361865997314\n",
      "epoch:  26   step:  136   train loss:  0.0004986043204553425  val loss:  0.7848008275032043\n",
      "epoch:  26   step:  137   train loss:  0.0007101133815012872  val loss:  0.7933974266052246\n",
      "epoch:  26   step:  138   train loss:  0.0006401423597708344  val loss:  0.7968239188194275\n",
      "epoch:  26   step:  139   train loss:  0.0003764628490898758  val loss:  0.7957636117935181\n",
      "epoch:  26   step:  140   train loss:  0.0008062133565545082  val loss:  0.793635368347168\n",
      "epoch:  26   step:  141   train loss:  0.0003349166363477707  val loss:  0.7965778112411499\n",
      "epoch:  26   step:  142   train loss:  0.0006836316315457225  val loss:  0.7970221042633057\n",
      "epoch:  26   step:  143   train loss:  0.0005221895407885313  val loss:  0.7973878979682922\n",
      "epoch:  26   step:  144   train loss:  0.000410168751841411  val loss:  0.8055868148803711\n",
      "epoch:  26   step:  145   train loss:  0.0003663347742985934  val loss:  0.8115799427032471\n",
      "epoch:  26   step:  146   train loss:  0.0002811293234117329  val loss:  0.8146793246269226\n",
      "epoch:  26   step:  147   train loss:  0.0005243066698312759  val loss:  0.8094163537025452\n",
      "epoch:  26   step:  148   train loss:  0.000457212416222319  val loss:  0.8079497218132019\n",
      "epoch:  26   step:  149   train loss:  0.00055570004042238  val loss:  0.8048673868179321\n",
      "epoch:  26   step:  150   train loss:  0.0006524124182760715  val loss:  0.8016548156738281\n",
      "epoch:  26   step:  151   train loss:  0.0009422251023352146  val loss:  0.799243152141571\n",
      "epoch:  26   step:  152   train loss:  0.0005674272542819381  val loss:  0.8071322441101074\n",
      "epoch:  26   step:  153   train loss:  0.00042901208507828414  val loss:  0.7987737059593201\n",
      "epoch:  26   step:  154   train loss:  0.0006628772243857384  val loss:  0.7903273105621338\n",
      "epoch:  26   step:  155   train loss:  0.0005082057323306799  val loss:  0.7841724753379822\n",
      "epoch:  26   step:  156   train loss:  0.0006401096470654011  val loss:  0.7961967587471008\n",
      "epoch:  26   step:  157   train loss:  0.0004623098357114941  val loss:  0.7940092086791992\n",
      "epoch:  26   step:  158   train loss:  0.0004164381534792483  val loss:  0.7990055680274963\n",
      "epoch:  26   step:  159   train loss:  0.0004202702548354864  val loss:  0.8082629442214966\n",
      "epoch:  26   step:  160   train loss:  0.0005736872553825378  val loss:  0.8048284649848938\n",
      "epoch:  26   step:  161   train loss:  0.0005974597297608852  val loss:  0.8107374310493469\n",
      "epoch:  26   step:  162   train loss:  0.0003316365764476359  val loss:  0.8130238652229309\n",
      "epoch:  26   step:  163   train loss:  0.00027861050330102444  val loss:  0.8158394694328308\n",
      "epoch:  26   step:  164   train loss:  0.0006677911733277142  val loss:  0.8295798897743225\n",
      "epoch:  26   step:  165   train loss:  0.0005571447545662522  val loss:  0.8158867359161377\n",
      "epoch:  27   step:  0   train loss:  0.00033831220935098827  val loss:  0.8130193948745728\n",
      "epoch:  27   step:  1   train loss:  0.0005026167491450906  val loss:  0.8095183968544006\n",
      "epoch:  27   step:  2   train loss:  0.0003830085042864084  val loss:  0.8062747716903687\n",
      "epoch:  27   step:  3   train loss:  0.00036837844527326524  val loss:  0.8059628009796143\n",
      "epoch:  27   step:  4   train loss:  0.00027618356398306787  val loss:  0.808804452419281\n",
      "epoch:  27   step:  5   train loss:  0.0003942924668081105  val loss:  0.8046200275421143\n",
      "epoch:  27   step:  6   train loss:  0.000305938592646271  val loss:  0.7992150783538818\n",
      "epoch:  27   step:  7   train loss:  0.0005031697219237685  val loss:  0.7963253855705261\n",
      "epoch:  27   step:  8   train loss:  0.00023076223442330956  val loss:  0.786810576915741\n",
      "epoch:  27   step:  9   train loss:  0.0004858255560975522  val loss:  0.7888980507850647\n",
      "epoch:  27   step:  10   train loss:  0.00019944243831560016  val loss:  0.7893542051315308\n",
      "epoch:  27   step:  11   train loss:  0.0003327777376398444  val loss:  0.7865538597106934\n",
      "epoch:  27   step:  12   train loss:  0.0003811079077422619  val loss:  0.7880598306655884\n",
      "epoch:  27   step:  13   train loss:  0.00048640830209478736  val loss:  0.7871592044830322\n",
      "epoch:  27   step:  14   train loss:  0.00036297625047154725  val loss:  0.7968667149543762\n",
      "epoch:  27   step:  15   train loss:  0.0006152850110083818  val loss:  0.7991753220558167\n",
      "epoch:  27   step:  16   train loss:  0.0003126765077468008  val loss:  0.7978212833404541\n",
      "epoch:  27   step:  17   train loss:  0.00035517627838999033  val loss:  0.8039036989212036\n",
      "epoch:  27   step:  18   train loss:  0.00045933888759464025  val loss:  0.8059138059616089\n",
      "epoch:  27   step:  19   train loss:  0.000343653024174273  val loss:  0.8058867454528809\n",
      "epoch:  27   step:  20   train loss:  0.0009055614937096834  val loss:  0.8077641129493713\n",
      "epoch:  27   step:  21   train loss:  0.00038974222843535244  val loss:  0.8014277219772339\n",
      "epoch:  27   step:  22   train loss:  0.00025503404322080314  val loss:  0.7960992455482483\n",
      "epoch:  27   step:  23   train loss:  0.0003673788742162287  val loss:  0.7958613634109497\n",
      "epoch:  27   step:  24   train loss:  0.0003918212023563683  val loss:  0.7897847294807434\n",
      "epoch:  27   step:  25   train loss:  0.0003013555833604187  val loss:  0.7950158715248108\n",
      "epoch:  27   step:  26   train loss:  0.0009592429269105196  val loss:  0.8062793016433716\n",
      "epoch:  27   step:  27   train loss:  0.0005136350519023836  val loss:  0.8042423129081726\n",
      "epoch:  27   step:  28   train loss:  0.00041635349043644965  val loss:  0.7946249842643738\n",
      "epoch:  27   step:  29   train loss:  0.000612502102740109  val loss:  0.7932592034339905\n",
      "epoch:  27   step:  30   train loss:  0.0004539188521448523  val loss:  0.7841446995735168\n",
      "epoch:  27   step:  31   train loss:  0.00044853915460407734  val loss:  0.7868683338165283\n",
      "epoch:  27   step:  32   train loss:  0.0003154987352900207  val loss:  0.7841852903366089\n",
      "epoch:  27   step:  33   train loss:  0.0003072888939641416  val loss:  0.7813032269477844\n",
      "epoch:  27   step:  34   train loss:  0.0009191081044264138  val loss:  0.7795401811599731\n",
      "epoch:  27   step:  35   train loss:  0.0002657837758306414  val loss:  0.7836002707481384\n",
      "epoch:  27   step:  36   train loss:  0.0006457265699282289  val loss:  0.7762644290924072\n",
      "epoch:  27   step:  37   train loss:  0.0006018754793331027  val loss:  0.7806623578071594\n",
      "epoch:  27   step:  38   train loss:  0.00039257854223251343  val loss:  0.7875189781188965\n",
      "epoch:  27   step:  39   train loss:  0.0010718144476413727  val loss:  0.7750173807144165\n",
      "epoch:  27   step:  40   train loss:  0.0006211597938090563  val loss:  0.7795397043228149\n",
      "epoch:  27   step:  41   train loss:  0.0005111116333864629  val loss:  0.7719056010246277\n",
      "epoch:  27   step:  42   train loss:  0.00030903471633791924  val loss:  0.7770138382911682\n",
      "epoch:  27   step:  43   train loss:  0.00043312375782988966  val loss:  0.776568591594696\n",
      "epoch:  27   step:  44   train loss:  0.0003383254515938461  val loss:  0.768238365650177\n",
      "epoch:  27   step:  45   train loss:  0.00021714536705985665  val loss:  0.7630906701087952\n",
      "epoch:  27   step:  46   train loss:  0.0007682203431613743  val loss:  0.7810094952583313\n",
      "epoch:  27   step:  47   train loss:  0.0003982123453170061  val loss:  0.7720759510993958\n",
      "epoch:  27   step:  48   train loss:  0.00020315696019679308  val loss:  0.7754408717155457\n",
      "epoch:  27   step:  49   train loss:  0.00046686356654390693  val loss:  0.7802878618240356\n",
      "epoch:  27   step:  50   train loss:  0.0004222247516736388  val loss:  0.7739484906196594\n",
      "epoch:  27   step:  51   train loss:  0.0006937577272765338  val loss:  0.7676160931587219\n",
      "epoch:  27   step:  52   train loss:  0.0004910287098027766  val loss:  0.7707065343856812\n",
      "epoch:  27   step:  53   train loss:  0.00033656624145805836  val loss:  0.7712997198104858\n",
      "epoch:  27   step:  54   train loss:  0.0003222748637199402  val loss:  0.7715234756469727\n",
      "epoch:  27   step:  55   train loss:  0.0003655916079878807  val loss:  0.768621563911438\n",
      "epoch:  27   step:  56   train loss:  0.00035546597791835666  val loss:  0.7720233798027039\n",
      "epoch:  27   step:  57   train loss:  0.0003081958566326648  val loss:  0.7706378102302551\n",
      "epoch:  27   step:  58   train loss:  0.0002447629813104868  val loss:  0.7724631428718567\n",
      "epoch:  27   step:  59   train loss:  0.0003016727278009057  val loss:  0.7727591395378113\n",
      "epoch:  27   step:  60   train loss:  0.000803006871137768  val loss:  0.7645695805549622\n",
      "epoch:  27   step:  61   train loss:  0.0002695615403354168  val loss:  0.7652751207351685\n",
      "epoch:  27   step:  62   train loss:  0.0004029849369544536  val loss:  0.7703731656074524\n",
      "epoch:  27   step:  63   train loss:  0.000382770667783916  val loss:  0.7807376980781555\n",
      "epoch:  27   step:  64   train loss:  0.00021565286442637444  val loss:  0.7746947407722473\n",
      "epoch:  27   step:  65   train loss:  0.00037584753590635955  val loss:  0.7698401212692261\n",
      "epoch:  27   step:  66   train loss:  0.000314523174893111  val loss:  0.7716177105903625\n",
      "epoch:  27   step:  67   train loss:  0.00035763756022788584  val loss:  0.7765380144119263\n",
      "epoch:  27   step:  68   train loss:  0.00045335537288337946  val loss:  0.7773013114929199\n",
      "epoch:  27   step:  69   train loss:  0.000543816015124321  val loss:  0.7704631686210632\n",
      "epoch:  27   step:  70   train loss:  0.00029857931076548994  val loss:  0.7802844047546387\n",
      "epoch:  27   step:  71   train loss:  0.0003300076350569725  val loss:  0.7817374467849731\n",
      "epoch:  27   step:  72   train loss:  0.0005360093782655895  val loss:  0.7833174467086792\n",
      "epoch:  27   step:  73   train loss:  0.000324389518937096  val loss:  0.7853042483329773\n",
      "epoch:  27   step:  74   train loss:  0.000443146622274071  val loss:  0.7757226228713989\n",
      "epoch:  27   step:  75   train loss:  0.0005797748453915119  val loss:  0.781450629234314\n",
      "epoch:  27   step:  76   train loss:  0.00031624024268239737  val loss:  0.7821835875511169\n",
      "epoch:  27   step:  77   train loss:  0.00037592012085951865  val loss:  0.7795851826667786\n",
      "epoch:  27   step:  78   train loss:  0.00023147251340560615  val loss:  0.7836724519729614\n",
      "epoch:  27   step:  79   train loss:  0.00030393031192943454  val loss:  0.7831652164459229\n",
      "epoch:  27   step:  80   train loss:  0.00037767854519188404  val loss:  0.7794126868247986\n",
      "epoch:  27   step:  81   train loss:  0.0004086591361556202  val loss:  0.7773773074150085\n",
      "epoch:  27   step:  82   train loss:  0.002206453587859869  val loss:  0.7778674364089966\n",
      "epoch:  27   step:  83   train loss:  0.0006569101824425161  val loss:  0.7806421518325806\n",
      "epoch:  27   step:  84   train loss:  0.00031957594910636544  val loss:  0.7745475769042969\n",
      "epoch:  27   step:  85   train loss:  0.00030257407343015075  val loss:  0.7726023197174072\n",
      "epoch:  27   step:  86   train loss:  0.0005859059747308493  val loss:  0.7741294503211975\n",
      "epoch:  27   step:  87   train loss:  0.00047755270497873425  val loss:  0.7627758383750916\n",
      "epoch:  27   step:  88   train loss:  0.0003224412212148309  val loss:  0.7520450949668884\n",
      "epoch:  27   step:  89   train loss:  0.0004922616062685847  val loss:  0.748552143573761\n",
      "epoch:  27   step:  90   train loss:  0.0003891697560902685  val loss:  0.7604421973228455\n",
      "epoch:  27   step:  91   train loss:  0.0003826599568128586  val loss:  0.7649192810058594\n",
      "epoch:  27   step:  92   train loss:  0.000636851997114718  val loss:  0.7680719494819641\n",
      "epoch:  27   step:  93   train loss:  0.0004099934012629092  val loss:  0.7790207266807556\n",
      "epoch:  27   step:  94   train loss:  0.00034761495771817863  val loss:  0.7810394167900085\n",
      "epoch:  27   step:  95   train loss:  0.0004684553714469075  val loss:  0.7764402031898499\n",
      "epoch:  27   step:  96   train loss:  0.0005713312420994043  val loss:  0.7769027352333069\n",
      "epoch:  27   step:  97   train loss:  0.0005567744374275208  val loss:  0.7666465044021606\n",
      "epoch:  27   step:  98   train loss:  0.0006773152854293585  val loss:  0.7669197916984558\n",
      "epoch:  27   step:  99   train loss:  0.00036516672116704285  val loss:  0.7605502009391785\n",
      "epoch:  27   step:  100   train loss:  0.0003936957800760865  val loss:  0.7597153782844543\n",
      "epoch:  27   step:  101   train loss:  0.0004320128937251866  val loss:  0.7684193253517151\n",
      "epoch:  27   step:  102   train loss:  0.00021819944959133863  val loss:  0.7619777321815491\n",
      "epoch:  27   step:  103   train loss:  0.0004561088280752301  val loss:  0.7665735483169556\n",
      "epoch:  27   step:  104   train loss:  0.0005847449647262692  val loss:  0.7714552879333496\n",
      "epoch:  27   step:  105   train loss:  0.0005980631103739142  val loss:  0.7729342579841614\n",
      "epoch:  27   step:  106   train loss:  0.00031907815719023347  val loss:  0.7718813419342041\n",
      "epoch:  27   step:  107   train loss:  0.0005725349183194339  val loss:  0.7769668698310852\n",
      "epoch:  27   step:  108   train loss:  0.000666330277454108  val loss:  0.7672392129898071\n",
      "epoch:  27   step:  109   train loss:  0.0004344130284152925  val loss:  0.7758575081825256\n",
      "epoch:  27   step:  110   train loss:  0.00046097993617877364  val loss:  0.7590038776397705\n",
      "epoch:  27   step:  111   train loss:  0.0006083848420530558  val loss:  0.7596924304962158\n",
      "epoch:  27   step:  112   train loss:  0.00047348858788609505  val loss:  0.7578404545783997\n",
      "epoch:  27   step:  113   train loss:  0.0008496049558743834  val loss:  0.7476047277450562\n",
      "epoch:  27   step:  114   train loss:  0.0003228758287150413  val loss:  0.7473095655441284\n",
      "epoch:  27   step:  115   train loss:  0.0009377784444950521  val loss:  0.7582125663757324\n",
      "epoch:  27   step:  116   train loss:  0.0007343975594267249  val loss:  0.7565566301345825\n",
      "epoch:  27   step:  117   train loss:  0.0002549854980316013  val loss:  0.7492137551307678\n",
      "epoch:  27   step:  118   train loss:  0.00032248010393232107  val loss:  0.7373431324958801\n",
      "epoch:  27   step:  119   train loss:  0.00045517561375163496  val loss:  0.7362294793128967\n",
      "epoch:  27   step:  120   train loss:  0.0003793592914007604  val loss:  0.732735276222229\n",
      "epoch:  27   step:  121   train loss:  0.0005376898916438222  val loss:  0.7199051380157471\n",
      "epoch:  27   step:  122   train loss:  0.0003128795651718974  val loss:  0.7257896065711975\n",
      "epoch:  27   step:  123   train loss:  0.0005388084100559354  val loss:  0.7098358869552612\n",
      "epoch:  27   step:  124   train loss:  0.0003570373810362071  val loss:  0.7207136154174805\n",
      "epoch:  27   step:  125   train loss:  0.0004848162643611431  val loss:  0.7318872809410095\n",
      "epoch:  27   step:  126   train loss:  0.00035019079223275185  val loss:  0.732948899269104\n",
      "epoch:  27   step:  127   train loss:  0.0003427595947869122  val loss:  0.7357501983642578\n",
      "epoch:  27   step:  128   train loss:  0.0004877215251326561  val loss:  0.7465728521347046\n",
      "epoch:  27   step:  129   train loss:  0.0002842408721335232  val loss:  0.751126229763031\n",
      "epoch:  27   step:  130   train loss:  0.00035646045580506325  val loss:  0.7511404156684875\n",
      "epoch:  27   step:  131   train loss:  0.0007365539786405861  val loss:  0.7415469288825989\n",
      "epoch:  27   step:  132   train loss:  0.0007890109554864466  val loss:  0.7260392308235168\n",
      "epoch:  27   step:  133   train loss:  0.0003676020132843405  val loss:  0.7337213158607483\n",
      "epoch:  27   step:  134   train loss:  0.0005555151728913188  val loss:  0.7190176248550415\n",
      "epoch:  27   step:  135   train loss:  0.0004939104546792805  val loss:  0.7347534894943237\n",
      "epoch:  27   step:  136   train loss:  0.0005762254586443305  val loss:  0.7416151165962219\n",
      "epoch:  27   step:  137   train loss:  0.0003371502389200032  val loss:  0.7355384826660156\n",
      "epoch:  27   step:  138   train loss:  0.0006550369434989989  val loss:  0.7489818334579468\n",
      "epoch:  27   step:  139   train loss:  0.00036335241748020053  val loss:  0.7362135648727417\n",
      "epoch:  27   step:  140   train loss:  0.0005830968730151653  val loss:  0.7464550137519836\n",
      "epoch:  27   step:  141   train loss:  0.00030430121114477515  val loss:  0.7406196594238281\n",
      "epoch:  27   step:  142   train loss:  0.00028151192236691713  val loss:  0.7572878003120422\n",
      "epoch:  27   step:  143   train loss:  0.00039067951729521155  val loss:  0.7524482011795044\n",
      "epoch:  27   step:  144   train loss:  0.00047367947991006076  val loss:  0.757603108882904\n",
      "epoch:  27   step:  145   train loss:  0.00039744560490362346  val loss:  0.7562445402145386\n",
      "epoch:  27   step:  146   train loss:  0.00032129220198839903  val loss:  0.764090359210968\n",
      "epoch:  27   step:  147   train loss:  0.0003897764836438  val loss:  0.7601218223571777\n",
      "epoch:  27   step:  148   train loss:  0.0002182414464186877  val loss:  0.7544781565666199\n",
      "epoch:  27   step:  149   train loss:  0.0007208216120488942  val loss:  0.7462458610534668\n",
      "epoch:  27   step:  150   train loss:  0.00040549470577389  val loss:  0.7454439997673035\n",
      "epoch:  27   step:  151   train loss:  0.00040695961797609925  val loss:  0.7519456148147583\n",
      "epoch:  27   step:  152   train loss:  0.0002824388211593032  val loss:  0.754901111125946\n",
      "epoch:  27   step:  153   train loss:  0.00043232773896306753  val loss:  0.7523060441017151\n",
      "epoch:  27   step:  154   train loss:  0.0004415942239575088  val loss:  0.7426193952560425\n",
      "epoch:  27   step:  155   train loss:  0.00027366395806893706  val loss:  0.753466784954071\n",
      "epoch:  27   step:  156   train loss:  0.0005238262820057571  val loss:  0.7610487341880798\n",
      "epoch:  27   step:  157   train loss:  0.00036346641718409956  val loss:  0.7683223485946655\n",
      "epoch:  27   step:  158   train loss:  0.0005295841256156564  val loss:  0.7707660794258118\n",
      "epoch:  27   step:  159   train loss:  0.0004304134054109454  val loss:  0.7692462801933289\n",
      "epoch:  27   step:  160   train loss:  0.0003133312857244164  val loss:  0.75322026014328\n",
      "epoch:  27   step:  161   train loss:  0.00029583973810076714  val loss:  0.752360999584198\n",
      "epoch:  27   step:  162   train loss:  0.0006078837905079126  val loss:  0.7608610391616821\n",
      "epoch:  27   step:  163   train loss:  0.00032576630474068224  val loss:  0.766567587852478\n",
      "epoch:  27   step:  164   train loss:  0.0006680029910057783  val loss:  0.7673846483230591\n",
      "epoch:  27   step:  165   train loss:  6.08421687502414e-05  val loss:  0.7681008577346802\n",
      "epoch:  28   step:  0   train loss:  0.00024934945395216346  val loss:  0.7732804417610168\n",
      "epoch:  28   step:  1   train loss:  0.00033590965904295444  val loss:  0.7595853209495544\n",
      "epoch:  28   step:  2   train loss:  0.00046691435272805393  val loss:  0.7654950022697449\n",
      "epoch:  28   step:  3   train loss:  0.0002351253933738917  val loss:  0.7663496136665344\n",
      "epoch:  28   step:  4   train loss:  0.00037342874566093087  val loss:  0.7721856236457825\n",
      "epoch:  28   step:  5   train loss:  0.00028152999584563076  val loss:  0.7622770667076111\n",
      "epoch:  28   step:  6   train loss:  0.0003789634210988879  val loss:  0.7683048248291016\n",
      "epoch:  28   step:  7   train loss:  0.00031821534503251314  val loss:  0.7717562913894653\n",
      "epoch:  28   step:  8   train loss:  0.0002616136916913092  val loss:  0.766965389251709\n",
      "epoch:  28   step:  9   train loss:  0.00021174194989725947  val loss:  0.7702850103378296\n",
      "epoch:  28   step:  10   train loss:  0.00046888148062862456  val loss:  0.7782599329948425\n",
      "epoch:  28   step:  11   train loss:  0.00024077598936855793  val loss:  0.781004011631012\n",
      "epoch:  28   step:  12   train loss:  0.0007421145564876497  val loss:  0.7682424783706665\n",
      "epoch:  28   step:  13   train loss:  0.0008436406496912241  val loss:  0.7473756074905396\n",
      "epoch:  28   step:  14   train loss:  0.00024888550979085267  val loss:  0.7408989071846008\n",
      "epoch:  28   step:  15   train loss:  0.0005491345655173063  val loss:  0.7369267344474792\n",
      "epoch:  28   step:  16   train loss:  0.00037514721043407917  val loss:  0.735463559627533\n",
      "epoch:  28   step:  17   train loss:  0.00023902964312583208  val loss:  0.7355701923370361\n",
      "epoch:  28   step:  18   train loss:  0.00032537110382691026  val loss:  0.7440189123153687\n",
      "epoch:  28   step:  19   train loss:  0.0004240659764036536  val loss:  0.7440423965454102\n",
      "epoch:  28   step:  20   train loss:  0.0003182751825079322  val loss:  0.7514215707778931\n",
      "epoch:  28   step:  21   train loss:  0.0005528234178200364  val loss:  0.7496938109397888\n",
      "epoch:  28   step:  22   train loss:  0.00047985304263420403  val loss:  0.7538984417915344\n",
      "epoch:  28   step:  23   train loss:  0.00029348573298193514  val loss:  0.7463208436965942\n",
      "epoch:  28   step:  24   train loss:  0.0003209295100532472  val loss:  0.7509365677833557\n",
      "epoch:  28   step:  25   train loss:  0.00030316549236886203  val loss:  0.7499240040779114\n",
      "epoch:  28   step:  26   train loss:  0.0002797846682369709  val loss:  0.7461575865745544\n",
      "epoch:  28   step:  27   train loss:  0.0008959054248407483  val loss:  0.7329633235931396\n",
      "epoch:  28   step:  28   train loss:  0.0004733394307550043  val loss:  0.7357757687568665\n",
      "epoch:  28   step:  29   train loss:  0.0005771513096988201  val loss:  0.7450360655784607\n",
      "epoch:  28   step:  30   train loss:  0.0003183680528309196  val loss:  0.7469561696052551\n",
      "epoch:  28   step:  31   train loss:  0.000434418732766062  val loss:  0.757451593875885\n",
      "epoch:  28   step:  32   train loss:  0.00030493922531604767  val loss:  0.7490641474723816\n",
      "epoch:  28   step:  33   train loss:  0.000310699047986418  val loss:  0.7494535446166992\n",
      "epoch:  28   step:  34   train loss:  0.000327639514580369  val loss:  0.7551054358482361\n",
      "epoch:  28   step:  35   train loss:  0.0002873539924621582  val loss:  0.7533769607543945\n",
      "epoch:  28   step:  36   train loss:  0.0002598571009002626  val loss:  0.7639206647872925\n",
      "epoch:  28   step:  37   train loss:  0.00017367114196531475  val loss:  0.7580772042274475\n",
      "epoch:  28   step:  38   train loss:  0.000615765224210918  val loss:  0.7590515613555908\n",
      "epoch:  28   step:  39   train loss:  0.00035907665733247995  val loss:  0.7595568895339966\n",
      "epoch:  28   step:  40   train loss:  0.0002757467154879123  val loss:  0.7619261741638184\n",
      "epoch:  28   step:  41   train loss:  0.00028800786822102964  val loss:  0.7525662183761597\n",
      "epoch:  28   step:  42   train loss:  0.0002059309190372005  val loss:  0.7510733604431152\n",
      "epoch:  28   step:  43   train loss:  0.0004461950156837702  val loss:  0.7636575698852539\n",
      "epoch:  28   step:  44   train loss:  0.0003378254477865994  val loss:  0.7604485154151917\n",
      "epoch:  28   step:  45   train loss:  0.000335624412400648  val loss:  0.7527967095375061\n",
      "epoch:  28   step:  46   train loss:  0.0004146595310885459  val loss:  0.7649821043014526\n",
      "epoch:  28   step:  47   train loss:  0.00040308432653546333  val loss:  0.7514257431030273\n",
      "epoch:  28   step:  48   train loss:  0.00023942769621498883  val loss:  0.7527245879173279\n",
      "epoch:  28   step:  49   train loss:  0.0005678244051523507  val loss:  0.7638473510742188\n",
      "epoch:  28   step:  50   train loss:  0.0005826740525662899  val loss:  0.7633097767829895\n",
      "epoch:  28   step:  51   train loss:  0.00042174625559709966  val loss:  0.7634645104408264\n",
      "epoch:  28   step:  52   train loss:  0.0006098682642914355  val loss:  0.758030354976654\n",
      "epoch:  28   step:  53   train loss:  0.00032261654268950224  val loss:  0.7589970827102661\n",
      "epoch:  28   step:  54   train loss:  0.0002542168367654085  val loss:  0.7660367488861084\n",
      "epoch:  28   step:  55   train loss:  0.00034012083779089153  val loss:  0.7800582051277161\n",
      "epoch:  28   step:  56   train loss:  0.00034884660271927714  val loss:  0.7870811820030212\n",
      "epoch:  28   step:  57   train loss:  0.0005738910404033959  val loss:  0.7892380952835083\n",
      "epoch:  28   step:  58   train loss:  0.0004587469156831503  val loss:  0.7862410545349121\n",
      "epoch:  28   step:  59   train loss:  0.00031942661735229194  val loss:  0.7882113456726074\n",
      "epoch:  28   step:  60   train loss:  0.00026488580624572933  val loss:  0.7805002927780151\n",
      "epoch:  28   step:  61   train loss:  0.0004511444130912423  val loss:  0.7881432771682739\n",
      "epoch:  28   step:  62   train loss:  0.00029179465491324663  val loss:  0.7909848093986511\n",
      "epoch:  28   step:  63   train loss:  0.00026434112805873156  val loss:  0.7784535884857178\n",
      "epoch:  28   step:  64   train loss:  0.0001484048698330298  val loss:  0.7841448783874512\n",
      "epoch:  28   step:  65   train loss:  0.00037183708627708256  val loss:  0.7902213335037231\n",
      "epoch:  28   step:  66   train loss:  0.00023916628560982645  val loss:  0.7865068912506104\n",
      "epoch:  28   step:  67   train loss:  0.0003058621659874916  val loss:  0.7841276526451111\n",
      "epoch:  28   step:  68   train loss:  0.00029392156284302473  val loss:  0.7864820957183838\n",
      "epoch:  28   step:  69   train loss:  0.00029486193670891225  val loss:  0.777920663356781\n",
      "epoch:  28   step:  70   train loss:  0.00034895128919743  val loss:  0.7644063234329224\n",
      "epoch:  28   step:  71   train loss:  0.0005274891154840589  val loss:  0.7678854465484619\n",
      "epoch:  28   step:  72   train loss:  0.00041071255691349506  val loss:  0.7652856111526489\n",
      "epoch:  28   step:  73   train loss:  0.0003313928609713912  val loss:  0.7660809755325317\n",
      "epoch:  28   step:  74   train loss:  0.0003635035827755928  val loss:  0.758182942867279\n",
      "epoch:  28   step:  75   train loss:  0.00032482348615303636  val loss:  0.764904260635376\n",
      "epoch:  28   step:  76   train loss:  0.00018645454838406295  val loss:  0.7733772993087769\n",
      "epoch:  28   step:  77   train loss:  0.00027189659886062145  val loss:  0.7644304633140564\n",
      "epoch:  28   step:  78   train loss:  0.00033352465834468603  val loss:  0.7727290391921997\n",
      "epoch:  28   step:  79   train loss:  0.0003440496511757374  val loss:  0.7753446698188782\n",
      "epoch:  28   step:  80   train loss:  0.00023282237816601992  val loss:  0.7784772515296936\n",
      "epoch:  28   step:  81   train loss:  0.00037914153654128313  val loss:  0.7775340676307678\n",
      "epoch:  28   step:  82   train loss:  0.00035818887408822775  val loss:  0.7871935367584229\n",
      "epoch:  28   step:  83   train loss:  0.00033027416793629527  val loss:  0.7920261025428772\n",
      "epoch:  28   step:  84   train loss:  0.0005007496220059693  val loss:  0.7935019731521606\n",
      "epoch:  28   step:  85   train loss:  0.0005641099414788187  val loss:  0.8016197681427002\n",
      "epoch:  28   step:  86   train loss:  0.0002522449067328125  val loss:  0.7966936826705933\n",
      "epoch:  28   step:  87   train loss:  0.00019658137171063572  val loss:  0.7922606468200684\n",
      "epoch:  28   step:  88   train loss:  0.00039134250255301595  val loss:  0.7979440689086914\n",
      "epoch:  28   step:  89   train loss:  0.0007133879698812962  val loss:  0.8076552152633667\n",
      "epoch:  28   step:  90   train loss:  0.00040750420885160565  val loss:  0.8043175339698792\n",
      "epoch:  28   step:  91   train loss:  0.0005299766780808568  val loss:  0.8030595779418945\n",
      "epoch:  28   step:  92   train loss:  0.0004124203114770353  val loss:  0.7858341336250305\n",
      "epoch:  28   step:  93   train loss:  0.00018427291070111096  val loss:  0.7834833860397339\n",
      "epoch:  28   step:  94   train loss:  0.0003076972789131105  val loss:  0.7881180644035339\n",
      "epoch:  28   step:  95   train loss:  0.0003925938217435032  val loss:  0.7828930020332336\n",
      "epoch:  28   step:  96   train loss:  0.0002173298562411219  val loss:  0.7837448716163635\n",
      "epoch:  28   step:  97   train loss:  0.0005746129318140447  val loss:  0.7962276935577393\n",
      "epoch:  28   step:  98   train loss:  0.00042960012797266245  val loss:  0.7974211573600769\n",
      "epoch:  28   step:  99   train loss:  0.0009192334255203605  val loss:  0.8050948977470398\n",
      "epoch:  28   step:  100   train loss:  0.00044354356941767037  val loss:  0.8096023201942444\n",
      "epoch:  28   step:  101   train loss:  0.00029303214978426695  val loss:  0.8020228147506714\n",
      "epoch:  28   step:  102   train loss:  0.00026692444225773215  val loss:  0.8027781248092651\n",
      "epoch:  28   step:  103   train loss:  0.0002810297883115709  val loss:  0.8023703098297119\n",
      "epoch:  28   step:  104   train loss:  0.00023478566436097026  val loss:  0.7968712449073792\n",
      "epoch:  28   step:  105   train loss:  0.00038133544148877263  val loss:  0.8018655776977539\n",
      "epoch:  28   step:  106   train loss:  0.0004076734185218811  val loss:  0.8006948828697205\n",
      "epoch:  28   step:  107   train loss:  0.000410985026974231  val loss:  0.8119349479675293\n",
      "epoch:  28   step:  108   train loss:  0.00021844742877874523  val loss:  0.8073581457138062\n",
      "epoch:  28   step:  109   train loss:  0.00018536752031650394  val loss:  0.8052030801773071\n",
      "epoch:  28   step:  110   train loss:  0.00047988066216930747  val loss:  0.8098368644714355\n",
      "epoch:  28   step:  111   train loss:  0.0003946764045394957  val loss:  0.806260883808136\n",
      "epoch:  28   step:  112   train loss:  0.0004473118460737169  val loss:  0.8059232234954834\n",
      "epoch:  28   step:  113   train loss:  0.0003122306370642036  val loss:  0.8060920238494873\n",
      "epoch:  28   step:  114   train loss:  0.00031304857111535966  val loss:  0.8063696622848511\n",
      "epoch:  28   step:  115   train loss:  0.0002555942046456039  val loss:  0.8035498857498169\n",
      "epoch:  28   step:  116   train loss:  0.00023486802820116282  val loss:  0.810395359992981\n",
      "epoch:  28   step:  117   train loss:  0.00027392327319830656  val loss:  0.8090544939041138\n",
      "epoch:  28   step:  118   train loss:  0.0003428046766202897  val loss:  0.816595733165741\n",
      "epoch:  28   step:  119   train loss:  0.0002363259409321472  val loss:  0.8115171790122986\n",
      "epoch:  28   step:  120   train loss:  0.00019871997938025743  val loss:  0.8166038393974304\n",
      "epoch:  28   step:  121   train loss:  0.00027272634906694293  val loss:  0.8090897798538208\n",
      "epoch:  28   step:  122   train loss:  0.00046718280646018684  val loss:  0.7999637722969055\n",
      "epoch:  28   step:  123   train loss:  0.0002423227997496724  val loss:  0.7920829057693481\n",
      "epoch:  28   step:  124   train loss:  0.00033172278199344873  val loss:  0.7935832738876343\n",
      "epoch:  28   step:  125   train loss:  0.0003918647416867316  val loss:  0.7899165153503418\n",
      "epoch:  28   step:  126   train loss:  0.0004933186573907733  val loss:  0.7848352789878845\n",
      "epoch:  28   step:  127   train loss:  0.00025385708431713283  val loss:  0.797807514667511\n",
      "epoch:  28   step:  128   train loss:  0.0005079116672277451  val loss:  0.783427894115448\n",
      "epoch:  28   step:  129   train loss:  0.000545675284229219  val loss:  0.7820353507995605\n",
      "epoch:  28   step:  130   train loss:  0.00031633744947612286  val loss:  0.7805999517440796\n",
      "epoch:  28   step:  131   train loss:  0.0002479045942891389  val loss:  0.792377233505249\n",
      "epoch:  28   step:  132   train loss:  0.0002021161199081689  val loss:  0.7799108028411865\n",
      "epoch:  28   step:  133   train loss:  0.00041328725637868047  val loss:  0.7851295471191406\n",
      "epoch:  28   step:  134   train loss:  0.00018638846813701093  val loss:  0.7846629023551941\n",
      "epoch:  28   step:  135   train loss:  0.0003626726975198835  val loss:  0.7905775904655457\n",
      "epoch:  28   step:  136   train loss:  0.00034315374796278775  val loss:  0.7901095151901245\n",
      "epoch:  28   step:  137   train loss:  0.0004253997467458248  val loss:  0.7914015054702759\n",
      "epoch:  28   step:  138   train loss:  0.0008070196490734816  val loss:  0.7791247367858887\n",
      "epoch:  28   step:  139   train loss:  0.00019116672046948224  val loss:  0.7769879102706909\n",
      "epoch:  28   step:  140   train loss:  0.00025191050372086465  val loss:  0.7739584445953369\n",
      "epoch:  28   step:  141   train loss:  0.0004759464063681662  val loss:  0.7892931699752808\n",
      "epoch:  28   step:  142   train loss:  0.00033143776818178594  val loss:  0.796798050403595\n",
      "epoch:  28   step:  143   train loss:  0.0004246892058290541  val loss:  0.7747591733932495\n",
      "epoch:  28   step:  144   train loss:  0.0003725666902028024  val loss:  0.7740865349769592\n",
      "epoch:  28   step:  145   train loss:  0.00030267395777627826  val loss:  0.7717449069023132\n",
      "epoch:  28   step:  146   train loss:  0.00034077774034813046  val loss:  0.7705016136169434\n",
      "epoch:  28   step:  147   train loss:  0.00044168709428049624  val loss:  0.7634678483009338\n",
      "epoch:  28   step:  148   train loss:  0.000328459485899657  val loss:  0.7703980803489685\n",
      "epoch:  28   step:  149   train loss:  0.00026925551355816424  val loss:  0.7632706761360168\n",
      "epoch:  28   step:  150   train loss:  0.00025197584182024  val loss:  0.7655450105667114\n",
      "epoch:  28   step:  151   train loss:  0.0004775281995534897  val loss:  0.7612316608428955\n",
      "epoch:  28   step:  152   train loss:  0.00020376509928610176  val loss:  0.7663449048995972\n",
      "epoch:  28   step:  153   train loss:  0.0003742067492567003  val loss:  0.7691959738731384\n",
      "epoch:  28   step:  154   train loss:  0.00015745239215902984  val loss:  0.7705784440040588\n",
      "epoch:  28   step:  155   train loss:  0.00017019803635776043  val loss:  0.7654111981391907\n",
      "epoch:  28   step:  156   train loss:  0.0003479204897303134  val loss:  0.7598536014556885\n",
      "epoch:  28   step:  157   train loss:  0.000524769420735538  val loss:  0.7692708969116211\n",
      "epoch:  28   step:  158   train loss:  0.00018437451217323542  val loss:  0.7719252109527588\n",
      "epoch:  28   step:  159   train loss:  0.00015459200949408114  val loss:  0.7736930847167969\n",
      "epoch:  28   step:  160   train loss:  0.00032785203075036407  val loss:  0.7717748880386353\n",
      "epoch:  28   step:  161   train loss:  0.0002802718663588166  val loss:  0.7585994601249695\n",
      "epoch:  28   step:  162   train loss:  0.0007533246534876525  val loss:  0.7546229362487793\n",
      "epoch:  28   step:  163   train loss:  0.0001366882206639275  val loss:  0.7642122507095337\n",
      "epoch:  28   step:  164   train loss:  0.00028076255694031715  val loss:  0.767451286315918\n",
      "epoch:  28   step:  165   train loss:  0.016225729137659073  val loss:  0.7558935880661011\n",
      "epoch:  29   step:  0   train loss:  0.00020456951460801065  val loss:  0.7797227501869202\n",
      "epoch:  29   step:  1   train loss:  0.0002329325652681291  val loss:  0.8089983463287354\n",
      "epoch:  29   step:  2   train loss:  0.0005185229238122702  val loss:  0.8133305907249451\n",
      "epoch:  29   step:  3   train loss:  0.0008809837163425982  val loss:  0.8268242478370667\n",
      "epoch:  29   step:  4   train loss:  0.002660759026184678  val loss:  0.8377490043640137\n",
      "epoch:  29   step:  5   train loss:  0.020956438034772873  val loss:  0.8641608953475952\n",
      "epoch:  29   step:  6   train loss:  0.0010622045956552029  val loss:  0.8571965098381042\n",
      "epoch:  29   step:  7   train loss:  0.0017128260806202888  val loss:  0.8926789164543152\n",
      "epoch:  29   step:  8   train loss:  0.0013502256479114294  val loss:  0.9324339032173157\n",
      "epoch:  29   step:  9   train loss:  0.014470246620476246  val loss:  0.9523272514343262\n",
      "epoch:  29   step:  10   train loss:  0.004630804527550936  val loss:  0.9369856119155884\n",
      "epoch:  29   step:  11   train loss:  0.002366472501307726  val loss:  0.9238603711128235\n",
      "epoch:  29   step:  12   train loss:  0.017218083143234253  val loss:  0.8779121041297913\n",
      "epoch:  29   step:  13   train loss:  0.004328677896410227  val loss:  0.8216989636421204\n",
      "epoch:  29   step:  14   train loss:  0.04707645997405052  val loss:  0.908584713935852\n",
      "epoch:  29   step:  15   train loss:  0.01822289079427719  val loss:  0.9748538136482239\n",
      "epoch:  29   step:  16   train loss:  0.03813847526907921  val loss:  0.9988851547241211\n",
      "epoch:  29   step:  17   train loss:  0.06657800823450089  val loss:  0.9253367185592651\n",
      "epoch:  29   step:  18   train loss:  0.16460327804088593  val loss:  0.8912924528121948\n",
      "epoch:  29   step:  19   train loss:  0.0007873011636547744  val loss:  0.9077233672142029\n",
      "epoch:  29   step:  20   train loss:  0.0021156882867217064  val loss:  0.9214146137237549\n",
      "epoch:  29   step:  21   train loss:  0.011050707660615444  val loss:  0.922693133354187\n",
      "epoch:  29   step:  22   train loss:  0.033946048468351364  val loss:  0.8974011540412903\n",
      "epoch:  29   step:  23   train loss:  0.007472456432878971  val loss:  0.8621257543563843\n",
      "epoch:  29   step:  24   train loss:  0.004075605422258377  val loss:  0.8400024771690369\n",
      "epoch:  29   step:  25   train loss:  0.0700865238904953  val loss:  0.9007003307342529\n",
      "epoch:  29   step:  26   train loss:  0.09640991687774658  val loss:  0.951454758644104\n",
      "epoch:  29   step:  27   train loss:  0.05007431283593178  val loss:  0.9528331756591797\n",
      "epoch:  29   step:  28   train loss:  0.011912264861166477  val loss:  0.9584077000617981\n",
      "epoch:  29   step:  29   train loss:  0.016951806843280792  val loss:  0.9123325347900391\n",
      "epoch:  29   step:  30   train loss:  0.005242796614766121  val loss:  0.86616450548172\n",
      "epoch:  29   step:  31   train loss:  0.14900770783424377  val loss:  0.8070845007896423\n",
      "epoch:  29   step:  32   train loss:  0.40107691287994385  val loss:  1.0276646614074707\n",
      "epoch:  29   step:  33   train loss:  0.7150096893310547  val loss:  1.0747572183609009\n",
      "epoch:  29   step:  34   train loss:  0.10394679009914398  val loss:  1.1079021692276\n",
      "epoch:  29   step:  35   train loss:  0.3470904231071472  val loss:  0.9709593057632446\n",
      "epoch:  29   step:  36   train loss:  0.3984326422214508  val loss:  0.7975711822509766\n",
      "epoch:  29   step:  37   train loss:  0.6185786724090576  val loss:  0.7176486253738403\n",
      "epoch:  29   step:  38   train loss:  0.607140302658081  val loss:  0.6095758676528931\n",
      "epoch:  29   step:  39   train loss:  0.28518912196159363  val loss:  0.5380139946937561\n",
      "epoch:  29   step:  40   train loss:  0.0712837353348732  val loss:  0.4680447578430176\n",
      "epoch:  29   step:  41   train loss:  0.07470330595970154  val loss:  0.44143110513687134\n",
      "epoch:  29   step:  42   train loss:  0.5194931030273438  val loss:  0.569879949092865\n",
      "epoch:  29   step:  43   train loss:  0.13629239797592163  val loss:  0.7303417921066284\n",
      "epoch:  29   step:  44   train loss:  0.3600103557109833  val loss:  0.8591868281364441\n",
      "epoch:  29   step:  45   train loss:  0.3165232539176941  val loss:  0.929122269153595\n",
      "epoch:  29   step:  46   train loss:  0.48057088255882263  val loss:  1.009238362312317\n",
      "epoch:  29   step:  47   train loss:  0.31946051120758057  val loss:  1.1073640584945679\n",
      "epoch:  29   step:  48   train loss:  0.11404149234294891  val loss:  1.290688395500183\n",
      "epoch:  29   step:  49   train loss:  0.45357394218444824  val loss:  1.3755581378936768\n",
      "epoch:  29   step:  50   train loss:  0.26559194922447205  val loss:  1.305495262145996\n",
      "epoch:  29   step:  51   train loss:  0.2915458083152771  val loss:  1.1705055236816406\n",
      "epoch:  29   step:  52   train loss:  0.14696010947227478  val loss:  1.0575324296951294\n",
      "epoch:  29   step:  53   train loss:  0.01523378025740385  val loss:  1.0059499740600586\n",
      "epoch:  29   step:  54   train loss:  0.10061928629875183  val loss:  0.8946783542633057\n",
      "epoch:  29   step:  55   train loss:  0.04600641503930092  val loss:  0.8453444242477417\n",
      "epoch:  29   step:  56   train loss:  0.29791060090065  val loss:  0.7718384861946106\n",
      "epoch:  29   step:  57   train loss:  0.40864086151123047  val loss:  0.6716883182525635\n",
      "epoch:  29   step:  58   train loss:  0.09716079384088516  val loss:  0.6113815307617188\n",
      "epoch:  29   step:  59   train loss:  0.10099269449710846  val loss:  0.5744913220405579\n",
      "epoch:  29   step:  60   train loss:  0.07784007489681244  val loss:  0.5842456221580505\n",
      "epoch:  29   step:  61   train loss:  0.44690603017807007  val loss:  0.5875641107559204\n",
      "epoch:  29   step:  62   train loss:  0.3442158102989197  val loss:  0.5444978475570679\n",
      "epoch:  29   step:  63   train loss:  0.1375713050365448  val loss:  0.5153932571411133\n",
      "epoch:  29   step:  64   train loss:  0.17212939262390137  val loss:  0.5073094964027405\n",
      "epoch:  29   step:  65   train loss:  0.26349160075187683  val loss:  0.49815550446510315\n",
      "epoch:  29   step:  66   train loss:  0.16843613982200623  val loss:  0.45362308621406555\n",
      "epoch:  29   step:  67   train loss:  0.046123478561639786  val loss:  0.4369981288909912\n",
      "epoch:  29   step:  68   train loss:  0.293389230966568  val loss:  0.41648632287979126\n",
      "epoch:  29   step:  69   train loss:  0.11836817860603333  val loss:  0.4057914614677429\n",
      "epoch:  29   step:  70   train loss:  0.04571227729320526  val loss:  0.4006994068622589\n",
      "epoch:  29   step:  71   train loss:  0.1483193039894104  val loss:  0.41001203656196594\n",
      "epoch:  29   step:  72   train loss:  0.14977629482746124  val loss:  0.41300228238105774\n",
      "epoch:  29   step:  73   train loss:  0.05418870598077774  val loss:  0.4032345116138458\n",
      "epoch:  29   step:  74   train loss:  0.1732446253299713  val loss:  0.4122641682624817\n",
      "epoch:  29   step:  75   train loss:  0.07799771428108215  val loss:  0.4295065701007843\n",
      "epoch:  29   step:  76   train loss:  0.05063905194401741  val loss:  0.437823623418808\n",
      "epoch:  29   step:  77   train loss:  0.0794726088643074  val loss:  0.4640235900878906\n",
      "epoch:  29   step:  78   train loss:  0.0548081248998642  val loss:  0.48313429951667786\n",
      "epoch:  29   step:  79   train loss:  0.06614486873149872  val loss:  0.5030851364135742\n",
      "epoch:  29   step:  80   train loss:  0.034667372703552246  val loss:  0.5200623273849487\n",
      "epoch:  29   step:  81   train loss:  0.10358929634094238  val loss:  0.539487361907959\n",
      "epoch:  29   step:  82   train loss:  0.035518545657396317  val loss:  0.55083829164505\n",
      "epoch:  29   step:  83   train loss:  0.05308004841208458  val loss:  0.5548998713493347\n",
      "epoch:  29   step:  84   train loss:  0.12127041071653366  val loss:  0.5632622241973877\n",
      "epoch:  29   step:  85   train loss:  0.09890864789485931  val loss:  0.5666207075119019\n",
      "epoch:  29   step:  86   train loss:  0.07525672018527985  val loss:  0.5817983746528625\n",
      "epoch:  29   step:  87   train loss:  0.04119580611586571  val loss:  0.5986688733100891\n",
      "epoch:  29   step:  88   train loss:  0.09167757630348206  val loss:  0.5782068371772766\n",
      "epoch:  29   step:  89   train loss:  0.12949420511722565  val loss:  0.5927712321281433\n",
      "epoch:  29   step:  90   train loss:  0.039818983525037766  val loss:  0.6207833290100098\n",
      "epoch:  29   step:  91   train loss:  0.04443586617708206  val loss:  0.6508460640907288\n",
      "epoch:  29   step:  92   train loss:  0.0349101722240448  val loss:  0.666284441947937\n",
      "epoch:  29   step:  93   train loss:  0.2751990556716919  val loss:  0.6707766056060791\n",
      "epoch:  29   step:  94   train loss:  0.05134612321853638  val loss:  0.6784119009971619\n",
      "epoch:  29   step:  95   train loss:  0.1037479117512703  val loss:  0.680186927318573\n",
      "epoch:  29   step:  96   train loss:  0.13402585685253143  val loss:  0.6863752603530884\n",
      "epoch:  29   step:  97   train loss:  0.23324063420295715  val loss:  0.6625251770019531\n",
      "epoch:  29   step:  98   train loss:  0.1256924867630005  val loss:  0.6261475086212158\n",
      "epoch:  29   step:  99   train loss:  0.07795876264572144  val loss:  0.5956334471702576\n",
      "epoch:  29   step:  100   train loss:  0.12741513550281525  val loss:  0.5473324656486511\n",
      "epoch:  29   step:  101   train loss:  0.07705026865005493  val loss:  0.5188961029052734\n",
      "epoch:  29   step:  102   train loss:  0.05948405712842941  val loss:  0.5199874043464661\n",
      "epoch:  29   step:  103   train loss:  0.042167436331510544  val loss:  0.552301824092865\n",
      "epoch:  29   step:  104   train loss:  0.11523846536874771  val loss:  0.5804086327552795\n",
      "epoch:  29   step:  105   train loss:  0.11571051180362701  val loss:  0.5858576893806458\n",
      "epoch:  29   step:  106   train loss:  0.23947513103485107  val loss:  0.5664607286453247\n",
      "epoch:  29   step:  107   train loss:  0.18859022855758667  val loss:  0.5157245397567749\n",
      "epoch:  29   step:  108   train loss:  0.135134756565094  val loss:  0.4914649724960327\n",
      "epoch:  29   step:  109   train loss:  0.17763946950435638  val loss:  0.47511428594589233\n",
      "epoch:  29   step:  110   train loss:  0.034690000116825104  val loss:  0.4791019856929779\n",
      "epoch:  29   step:  111   train loss:  0.1589038372039795  val loss:  0.46472030878067017\n",
      "epoch:  29   step:  112   train loss:  0.19248196482658386  val loss:  0.40841543674468994\n",
      "epoch:  29   step:  113   train loss:  0.034053221344947815  val loss:  0.37454965710639954\n",
      "epoch:  29   step:  114   train loss:  0.021728970110416412  val loss:  0.35300731658935547\n",
      "epoch:  29   step:  115   train loss:  0.011721577495336533  val loss:  0.3448241651058197\n",
      "epoch:  29   step:  116   train loss:  0.09294095635414124  val loss:  0.33746030926704407\n",
      "epoch:  29   step:  117   train loss:  0.08865442126989365  val loss:  0.3341939151287079\n",
      "epoch:  29   step:  118   train loss:  0.08069780468940735  val loss:  0.3290712535381317\n",
      "epoch:  29   step:  119   train loss:  0.10814942419528961  val loss:  0.3276287913322449\n",
      "epoch:  29   step:  120   train loss:  0.1080358475446701  val loss:  0.3333677351474762\n",
      "epoch:  29   step:  121   train loss:  0.1267942637205124  val loss:  0.3377055525779724\n",
      "epoch:  29   step:  122   train loss:  0.10937049984931946  val loss:  0.36778563261032104\n",
      "epoch:  29   step:  123   train loss:  0.13023127615451813  val loss:  0.4327985346317291\n",
      "epoch:  29   step:  124   train loss:  0.08897994458675385  val loss:  0.4624679386615753\n",
      "epoch:  29   step:  125   train loss:  0.09288386255502701  val loss:  0.49113479256629944\n",
      "epoch:  29   step:  126   train loss:  0.11800254881381989  val loss:  0.4977315068244934\n",
      "epoch:  29   step:  127   train loss:  0.2844703793525696  val loss:  0.4642643928527832\n",
      "epoch:  29   step:  128   train loss:  0.03287655860185623  val loss:  0.4405987858772278\n",
      "epoch:  29   step:  129   train loss:  0.038513414561748505  val loss:  0.4365076422691345\n",
      "epoch:  29   step:  130   train loss:  0.06117674708366394  val loss:  0.4387758672237396\n",
      "epoch:  29   step:  131   train loss:  0.022315990179777145  val loss:  0.4451964497566223\n",
      "epoch:  29   step:  132   train loss:  0.145609050989151  val loss:  0.4293437898159027\n",
      "epoch:  29   step:  133   train loss:  0.08530472218990326  val loss:  0.40556392073631287\n",
      "epoch:  29   step:  134   train loss:  0.019587678834795952  val loss:  0.38629305362701416\n",
      "epoch:  29   step:  135   train loss:  0.04034208133816719  val loss:  0.37417805194854736\n",
      "epoch:  29   step:  136   train loss:  0.2094387412071228  val loss:  0.36624008417129517\n",
      "epoch:  29   step:  137   train loss:  0.05476289615035057  val loss:  0.36098429560661316\n",
      "epoch:  29   step:  138   train loss:  0.0658343955874443  val loss:  0.3637521266937256\n",
      "epoch:  29   step:  139   train loss:  0.02147792838513851  val loss:  0.3679695129394531\n",
      "epoch:  29   step:  140   train loss:  0.02341969683766365  val loss:  0.3766431212425232\n",
      "epoch:  29   step:  141   train loss:  0.1354304552078247  val loss:  0.3572583794593811\n",
      "epoch:  29   step:  142   train loss:  0.09042196720838547  val loss:  0.3523675501346588\n",
      "epoch:  29   step:  143   train loss:  0.07520081847906113  val loss:  0.34097519516944885\n",
      "epoch:  29   step:  144   train loss:  0.09799063205718994  val loss:  0.33684012293815613\n",
      "epoch:  29   step:  145   train loss:  0.02324279583990574  val loss:  0.3313521146774292\n",
      "epoch:  29   step:  146   train loss:  0.057070136070251465  val loss:  0.32863348722457886\n",
      "epoch:  29   step:  147   train loss:  0.03609698265790939  val loss:  0.32880374789237976\n",
      "epoch:  29   step:  148   train loss:  0.010100804269313812  val loss:  0.3323250412940979\n",
      "epoch:  29   step:  149   train loss:  0.19359150528907776  val loss:  0.32777640223503113\n",
      "epoch:  29   step:  150   train loss:  0.1912318468093872  val loss:  0.351264625787735\n",
      "epoch:  29   step:  151   train loss:  0.013425803743302822  val loss:  0.3827829360961914\n",
      "epoch:  29   step:  152   train loss:  0.008031603880226612  val loss:  0.4077688455581665\n",
      "epoch:  29   step:  153   train loss:  0.04208581522107124  val loss:  0.4200707674026489\n",
      "epoch:  29   step:  154   train loss:  0.10035569220781326  val loss:  0.41854479908943176\n",
      "epoch:  29   step:  155   train loss:  0.016885297372937202  val loss:  0.41783690452575684\n",
      "epoch:  29   step:  156   train loss:  0.015340535901486874  val loss:  0.4172627627849579\n",
      "epoch:  29   step:  157   train loss:  0.04012244567275047  val loss:  0.418222039937973\n",
      "epoch:  29   step:  158   train loss:  0.025990929454565048  val loss:  0.4145828187465668\n",
      "epoch:  29   step:  159   train loss:  0.04060930758714676  val loss:  0.4104250967502594\n",
      "epoch:  29   step:  160   train loss:  0.16846975684165955  val loss:  0.38229644298553467\n",
      "epoch:  29   step:  161   train loss:  0.040409110486507416  val loss:  0.3559950292110443\n",
      "epoch:  29   step:  162   train loss:  0.09854582697153091  val loss:  0.3435097932815552\n",
      "epoch:  29   step:  163   train loss:  0.06320114433765411  val loss:  0.3472912013530731\n",
      "epoch:  29   step:  164   train loss:  0.04106329381465912  val loss:  0.3582310676574707\n",
      "epoch:  29   step:  165   train loss:  0.11951999366283417  val loss:  0.37724658846855164\n",
      "epoch:  30   step:  0   train loss:  0.014146883971989155  val loss:  0.42160865664482117\n",
      "epoch:  30   step:  1   train loss:  0.01985916867852211  val loss:  0.46370330452919006\n",
      "epoch:  30   step:  2   train loss:  0.006984827108681202  val loss:  0.5020710825920105\n",
      "epoch:  30   step:  3   train loss:  0.0054656341671943665  val loss:  0.5364928841590881\n",
      "epoch:  30   step:  4   train loss:  0.021763313561677933  val loss:  0.565017819404602\n",
      "epoch:  30   step:  5   train loss:  0.008411383256316185  val loss:  0.5848899483680725\n",
      "epoch:  30   step:  6   train loss:  0.11540855467319489  val loss:  0.5790810585021973\n",
      "epoch:  30   step:  7   train loss:  0.0721685141324997  val loss:  0.5592198967933655\n",
      "epoch:  30   step:  8   train loss:  0.026106074452400208  val loss:  0.5227882266044617\n",
      "epoch:  30   step:  9   train loss:  0.024873290210962296  val loss:  0.4875390827655792\n",
      "epoch:  30   step:  10   train loss:  0.02276381105184555  val loss:  0.46306678652763367\n",
      "epoch:  30   step:  11   train loss:  0.05015290528535843  val loss:  0.4626394808292389\n",
      "epoch:  30   step:  12   train loss:  0.03878854215145111  val loss:  0.48030465841293335\n",
      "epoch:  30   step:  13   train loss:  0.022654978558421135  val loss:  0.5000298619270325\n",
      "epoch:  30   step:  14   train loss:  0.10739167779684067  val loss:  0.5099204778671265\n",
      "epoch:  30   step:  15   train loss:  0.063706174492836  val loss:  0.5140879154205322\n",
      "epoch:  30   step:  16   train loss:  0.027768222615122795  val loss:  0.5192381143569946\n",
      "epoch:  30   step:  17   train loss:  0.016365651041269302  val loss:  0.5281062722206116\n",
      "epoch:  30   step:  18   train loss:  0.007108430843800306  val loss:  0.5424240827560425\n",
      "epoch:  30   step:  19   train loss:  0.014936564490199089  val loss:  0.5579710602760315\n",
      "epoch:  30   step:  20   train loss:  0.036933355033397675  val loss:  0.5841381549835205\n",
      "epoch:  30   step:  21   train loss:  0.05407489091157913  val loss:  0.5999910235404968\n",
      "epoch:  30   step:  22   train loss:  0.022318845614790916  val loss:  0.6197189092636108\n",
      "epoch:  30   step:  23   train loss:  0.03836008906364441  val loss:  0.6346345543861389\n",
      "epoch:  30   step:  24   train loss:  0.04962391406297684  val loss:  0.6264890432357788\n",
      "epoch:  30   step:  25   train loss:  0.12047258764505386  val loss:  0.5613800883293152\n",
      "epoch:  30   step:  26   train loss:  0.03357532247900963  val loss:  0.5049750208854675\n",
      "epoch:  30   step:  27   train loss:  0.05778671056032181  val loss:  0.4604114890098572\n",
      "epoch:  30   step:  28   train loss:  0.00590467918664217  val loss:  0.42754364013671875\n",
      "epoch:  30   step:  29   train loss:  0.10111013799905777  val loss:  0.3971487581729889\n",
      "epoch:  30   step:  30   train loss:  0.19476036727428436  val loss:  0.3650710880756378\n",
      "epoch:  30   step:  31   train loss:  0.03612352907657623  val loss:  0.33320707082748413\n",
      "epoch:  30   step:  32   train loss:  0.008238588459789753  val loss:  0.31315672397613525\n",
      "epoch:  30   step:  33   train loss:  0.025094304233789444  val loss:  0.3009571135044098\n",
      "epoch:  30   step:  34   train loss:  0.09277442842721939  val loss:  0.3045184016227722\n",
      "epoch:  30   step:  35   train loss:  0.0306355282664299  val loss:  0.3054196834564209\n",
      "epoch:  30   step:  36   train loss:  0.22889655828475952  val loss:  0.29256904125213623\n",
      "epoch:  30   step:  37   train loss:  0.13874442875385284  val loss:  0.2842147946357727\n",
      "epoch:  30   step:  38   train loss:  0.0023189233615994453  val loss:  0.31243813037872314\n",
      "epoch:  30   step:  39   train loss:  0.00782149937003851  val loss:  0.36532989144325256\n",
      "epoch:  30   step:  40   train loss:  0.025924189016222954  val loss:  0.4140087366104126\n",
      "epoch:  30   step:  41   train loss:  0.10270176827907562  val loss:  0.468208372592926\n",
      "epoch:  30   step:  42   train loss:  0.013269038870930672  val loss:  0.503081202507019\n",
      "epoch:  30   step:  43   train loss:  0.024154579266905785  val loss:  0.528759241104126\n",
      "epoch:  30   step:  44   train loss:  0.08079640567302704  val loss:  0.5319454073905945\n",
      "epoch:  30   step:  45   train loss:  0.03016546368598938  val loss:  0.5299740433692932\n",
      "epoch:  30   step:  46   train loss:  0.10709460079669952  val loss:  0.5142282843589783\n",
      "epoch:  30   step:  47   train loss:  0.035628944635391235  val loss:  0.5035722851753235\n",
      "epoch:  30   step:  48   train loss:  0.019723396748304367  val loss:  0.49729764461517334\n",
      "epoch:  30   step:  49   train loss:  0.0037062910851091146  val loss:  0.48865455389022827\n",
      "epoch:  30   step:  50   train loss:  0.024992935359477997  val loss:  0.4784085750579834\n",
      "epoch:  30   step:  51   train loss:  0.01719844713807106  val loss:  0.48111969232559204\n",
      "epoch:  30   step:  52   train loss:  0.033077068626880646  val loss:  0.47451671957969666\n",
      "epoch:  30   step:  53   train loss:  0.005950555671006441  val loss:  0.46710413694381714\n",
      "epoch:  30   step:  54   train loss:  0.1054498553276062  val loss:  0.4554155766963959\n",
      "epoch:  30   step:  55   train loss:  0.006630226504057646  val loss:  0.45288294553756714\n",
      "epoch:  30   step:  56   train loss:  0.018598312512040138  val loss:  0.45392996072769165\n",
      "epoch:  30   step:  57   train loss:  0.09840555489063263  val loss:  0.4597180485725403\n",
      "epoch:  30   step:  58   train loss:  0.07405473291873932  val loss:  0.46587812900543213\n",
      "epoch:  30   step:  59   train loss:  0.003913505002856255  val loss:  0.4686912000179291\n",
      "epoch:  30   step:  60   train loss:  0.029486369341611862  val loss:  0.46240389347076416\n",
      "epoch:  30   step:  61   train loss:  0.0190508421510458  val loss:  0.4565126597881317\n",
      "epoch:  30   step:  62   train loss:  0.008825595490634441  val loss:  0.4501610994338989\n",
      "epoch:  30   step:  63   train loss:  0.017892474308609962  val loss:  0.43692922592163086\n",
      "epoch:  30   step:  64   train loss:  0.003761100582778454  val loss:  0.437744140625\n",
      "epoch:  30   step:  65   train loss:  0.02773098461329937  val loss:  0.441491037607193\n",
      "epoch:  30   step:  66   train loss:  0.012942373752593994  val loss:  0.44882985949516296\n",
      "epoch:  30   step:  67   train loss:  0.013700638897716999  val loss:  0.45617416501045227\n",
      "epoch:  30   step:  68   train loss:  0.012304103001952171  val loss:  0.4663897454738617\n",
      "epoch:  30   step:  69   train loss:  0.04787558317184448  val loss:  0.4598950743675232\n",
      "epoch:  30   step:  70   train loss:  0.008064355701208115  val loss:  0.4549008905887604\n",
      "epoch:  30   step:  71   train loss:  0.021150685846805573  val loss:  0.442013144493103\n",
      "epoch:  30   step:  72   train loss:  0.02305985987186432  val loss:  0.4401423931121826\n",
      "epoch:  30   step:  73   train loss:  0.004018373787403107  val loss:  0.43836265802383423\n",
      "epoch:  30   step:  74   train loss:  0.03700967878103256  val loss:  0.43476203083992004\n",
      "epoch:  30   step:  75   train loss:  0.0034006007481366396  val loss:  0.44220998883247375\n",
      "epoch:  30   step:  76   train loss:  0.046244002878665924  val loss:  0.4423579275608063\n",
      "epoch:  30   step:  77   train loss:  0.0043025752529501915  val loss:  0.4418218433856964\n",
      "epoch:  30   step:  78   train loss:  0.017562350258231163  val loss:  0.4446462392807007\n",
      "epoch:  30   step:  79   train loss:  0.01313275471329689  val loss:  0.43400710821151733\n",
      "epoch:  30   step:  80   train loss:  0.04731143265962601  val loss:  0.4339865744113922\n",
      "epoch:  30   step:  81   train loss:  0.011196241714060307  val loss:  0.4418730139732361\n",
      "epoch:  30   step:  82   train loss:  0.013102620840072632  val loss:  0.4473121464252472\n",
      "epoch:  30   step:  83   train loss:  0.004453013651072979  val loss:  0.4612201452255249\n",
      "epoch:  30   step:  84   train loss:  0.02763807773590088  val loss:  0.47662353515625\n",
      "epoch:  30   step:  85   train loss:  0.006675688549876213  val loss:  0.48646390438079834\n",
      "epoch:  30   step:  86   train loss:  0.010051250457763672  val loss:  0.49017539620399475\n",
      "epoch:  30   step:  87   train loss:  0.02296290174126625  val loss:  0.49510595202445984\n",
      "epoch:  30   step:  88   train loss:  0.002357811201363802  val loss:  0.5022087693214417\n",
      "epoch:  30   step:  89   train loss:  0.008998704142868519  val loss:  0.5051416158676147\n",
      "epoch:  30   step:  90   train loss:  0.007420017383992672  val loss:  0.4976564645767212\n",
      "epoch:  30   step:  91   train loss:  0.006445722188800573  val loss:  0.5052886009216309\n",
      "epoch:  30   step:  92   train loss:  0.009939431212842464  val loss:  0.5141648650169373\n",
      "epoch:  30   step:  93   train loss:  0.004118038807064295  val loss:  0.5200590491294861\n",
      "epoch:  30   step:  94   train loss:  0.07604465633630753  val loss:  0.521428644657135\n",
      "epoch:  30   step:  95   train loss:  0.003487028181552887  val loss:  0.526826798915863\n",
      "epoch:  30   step:  96   train loss:  0.00962253287434578  val loss:  0.5116752982139587\n",
      "epoch:  30   step:  97   train loss:  0.007260414306074381  val loss:  0.5160532593727112\n",
      "epoch:  30   step:  98   train loss:  0.02348455972969532  val loss:  0.5064910054206848\n",
      "epoch:  30   step:  99   train loss:  0.056735433638095856  val loss:  0.49760007858276367\n",
      "epoch:  30   step:  100   train loss:  0.011712191626429558  val loss:  0.48326659202575684\n",
      "epoch:  30   step:  101   train loss:  0.01958232745528221  val loss:  0.4718379080295563\n",
      "epoch:  30   step:  102   train loss:  0.00659224484115839  val loss:  0.4621692895889282\n",
      "epoch:  30   step:  103   train loss:  0.021853983402252197  val loss:  0.45176708698272705\n",
      "epoch:  30   step:  104   train loss:  0.02097626030445099  val loss:  0.43980100750923157\n",
      "epoch:  30   step:  105   train loss:  0.005929622799158096  val loss:  0.4367285966873169\n",
      "epoch:  30   step:  106   train loss:  0.0026705991476774216  val loss:  0.43128111958503723\n",
      "epoch:  30   step:  107   train loss:  0.03132183849811554  val loss:  0.44822004437446594\n",
      "epoch:  30   step:  108   train loss:  0.00949106365442276  val loss:  0.4488467276096344\n",
      "epoch:  30   step:  109   train loss:  0.024146610870957375  val loss:  0.45381689071655273\n",
      "epoch:  30   step:  110   train loss:  0.007421369664371014  val loss:  0.4564477503299713\n",
      "epoch:  30   step:  111   train loss:  0.006574686616659164  val loss:  0.4618389904499054\n",
      "epoch:  30   step:  112   train loss:  0.0035948301665484905  val loss:  0.46195468306541443\n",
      "epoch:  30   step:  113   train loss:  0.020928822457790375  val loss:  0.46270909905433655\n",
      "epoch:  30   step:  114   train loss:  0.03375355899333954  val loss:  0.4664348065853119\n",
      "epoch:  30   step:  115   train loss:  0.03585823252797127  val loss:  0.4602808654308319\n",
      "epoch:  30   step:  116   train loss:  0.021990880370140076  val loss:  0.44697532057762146\n",
      "epoch:  30   step:  117   train loss:  0.053898222744464874  val loss:  0.43536943197250366\n",
      "epoch:  30   step:  118   train loss:  0.0036255267914384604  val loss:  0.43247339129447937\n",
      "epoch:  30   step:  119   train loss:  0.014401289634406567  val loss:  0.4426986575126648\n",
      "epoch:  30   step:  120   train loss:  0.006785928271710873  val loss:  0.4571774899959564\n",
      "epoch:  30   step:  121   train loss:  0.0036670006811618805  val loss:  0.4758928120136261\n",
      "epoch:  30   step:  122   train loss:  0.00972564984112978  val loss:  0.4727833569049835\n",
      "epoch:  30   step:  123   train loss:  0.02401047945022583  val loss:  0.48196956515312195\n",
      "epoch:  30   step:  124   train loss:  0.0169095266610384  val loss:  0.4767378568649292\n",
      "epoch:  30   step:  125   train loss:  0.005916407331824303  val loss:  0.49365320801734924\n",
      "epoch:  30   step:  126   train loss:  0.01554667018353939  val loss:  0.5015754103660583\n",
      "epoch:  30   step:  127   train loss:  0.011428235098719597  val loss:  0.49894198775291443\n",
      "epoch:  30   step:  128   train loss:  0.002951387083157897  val loss:  0.5009177923202515\n",
      "epoch:  30   step:  129   train loss:  0.0682707354426384  val loss:  0.5010935664176941\n",
      "epoch:  30   step:  130   train loss:  0.010109975934028625  val loss:  0.49329042434692383\n",
      "epoch:  30   step:  131   train loss:  0.04148520901799202  val loss:  0.50556480884552\n",
      "epoch:  30   step:  132   train loss:  0.03946125507354736  val loss:  0.5197548866271973\n",
      "epoch:  30   step:  133   train loss:  0.011138000525534153  val loss:  0.5289345979690552\n",
      "epoch:  30   step:  134   train loss:  0.011631934903562069  val loss:  0.5419551730155945\n",
      "epoch:  30   step:  135   train loss:  0.052573300898075104  val loss:  0.5487231612205505\n",
      "epoch:  30   step:  136   train loss:  0.041287779808044434  val loss:  0.5331979393959045\n",
      "epoch:  30   step:  137   train loss:  0.050465576350688934  val loss:  0.5165823698043823\n",
      "epoch:  30   step:  138   train loss:  0.012853067368268967  val loss:  0.5022060871124268\n",
      "epoch:  30   step:  139   train loss:  0.024124223738908768  val loss:  0.49097365140914917\n",
      "epoch:  30   step:  140   train loss:  0.10202111303806305  val loss:  0.492011696100235\n",
      "epoch:  30   step:  141   train loss:  0.006148826330900192  val loss:  0.482883095741272\n",
      "epoch:  30   step:  142   train loss:  0.016370773315429688  val loss:  0.46097129583358765\n",
      "epoch:  30   step:  143   train loss:  0.025936447083950043  val loss:  0.4610627293586731\n",
      "epoch:  30   step:  144   train loss:  0.005261672660708427  val loss:  0.45195966958999634\n",
      "epoch:  30   step:  145   train loss:  0.019000325351953506  val loss:  0.45886024832725525\n",
      "epoch:  30   step:  146   train loss:  0.017592038959264755  val loss:  0.45933786034584045\n",
      "epoch:  30   step:  147   train loss:  0.11256465315818787  val loss:  0.4847021996974945\n",
      "epoch:  30   step:  148   train loss:  0.009838253259658813  val loss:  0.5146986842155457\n",
      "epoch:  30   step:  149   train loss:  0.003077819012105465  val loss:  0.5387035012245178\n",
      "epoch:  30   step:  150   train loss:  0.003508141729980707  val loss:  0.5629020929336548\n",
      "epoch:  30   step:  151   train loss:  0.0035084362607449293  val loss:  0.6068540215492249\n",
      "epoch:  30   step:  152   train loss:  0.010178772732615471  val loss:  0.6250994801521301\n",
      "epoch:  30   step:  153   train loss:  0.1397571563720703  val loss:  0.6252011060714722\n",
      "epoch:  30   step:  154   train loss:  0.0072974408976733685  val loss:  0.614018440246582\n",
      "epoch:  30   step:  155   train loss:  0.009936206042766571  val loss:  0.6152141094207764\n",
      "epoch:  30   step:  156   train loss:  0.013308866880834103  val loss:  0.6068796515464783\n",
      "epoch:  30   step:  157   train loss:  0.027324313297867775  val loss:  0.6162551641464233\n",
      "epoch:  30   step:  158   train loss:  0.015615057200193405  val loss:  0.6181211471557617\n",
      "epoch:  30   step:  159   train loss:  0.13538016378879547  val loss:  0.6207076907157898\n",
      "epoch:  30   step:  160   train loss:  0.02055204100906849  val loss:  0.6180137395858765\n",
      "epoch:  30   step:  161   train loss:  0.008392635732889175  val loss:  0.6303423643112183\n",
      "epoch:  30   step:  162   train loss:  0.006619486957788467  val loss:  0.6408510208129883\n",
      "epoch:  30   step:  163   train loss:  0.009954595938324928  val loss:  0.6526007056236267\n",
      "epoch:  30   step:  164   train loss:  0.005193921737372875  val loss:  0.6543787121772766\n",
      "epoch:  30   step:  165   train loss:  0.010544074699282646  val loss:  0.6405457854270935\n",
      "epoch:  31   step:  0   train loss:  0.0237239021807909  val loss:  0.6472662687301636\n",
      "epoch:  31   step:  1   train loss:  0.01847699284553528  val loss:  0.6487636566162109\n",
      "epoch:  31   step:  2   train loss:  0.012147041969001293  val loss:  0.6442937850952148\n",
      "epoch:  31   step:  3   train loss:  0.0799526646733284  val loss:  0.6543588638305664\n",
      "epoch:  31   step:  4   train loss:  0.04037430137395859  val loss:  0.6400827765464783\n",
      "epoch:  31   step:  5   train loss:  0.004016757011413574  val loss:  0.6274275183677673\n",
      "epoch:  31   step:  6   train loss:  0.002968757413327694  val loss:  0.6122756600379944\n",
      "epoch:  31   step:  7   train loss:  0.010677633807063103  val loss:  0.6141456961631775\n",
      "epoch:  31   step:  8   train loss:  0.018382394686341286  val loss:  0.6159093976020813\n",
      "epoch:  31   step:  9   train loss:  0.007113104686141014  val loss:  0.6252687573432922\n",
      "epoch:  31   step:  10   train loss:  0.004211731255054474  val loss:  0.6267992258071899\n",
      "epoch:  31   step:  11   train loss:  0.001031969441100955  val loss:  0.6319664120674133\n",
      "epoch:  31   step:  12   train loss:  0.0022494448348879814  val loss:  0.633916437625885\n",
      "epoch:  31   step:  13   train loss:  0.002535229781642556  val loss:  0.6396819949150085\n",
      "epoch:  31   step:  14   train loss:  0.03187447413802147  val loss:  0.6557831168174744\n",
      "epoch:  31   step:  15   train loss:  0.036372993141412735  val loss:  0.6741465330123901\n",
      "epoch:  31   step:  16   train loss:  0.02295546419918537  val loss:  0.6740320324897766\n",
      "epoch:  31   step:  17   train loss:  0.018991462886333466  val loss:  0.6588221788406372\n",
      "epoch:  31   step:  18   train loss:  0.00192990992218256  val loss:  0.6449258327484131\n",
      "epoch:  31   step:  19   train loss:  0.007658580783754587  val loss:  0.6337683200836182\n",
      "epoch:  31   step:  20   train loss:  0.006702944170683622  val loss:  0.6220908761024475\n",
      "epoch:  31   step:  21   train loss:  0.07983053475618362  val loss:  0.5967326760292053\n",
      "epoch:  31   step:  22   train loss:  0.002312184078618884  val loss:  0.5794361233711243\n",
      "epoch:  31   step:  23   train loss:  0.008095581084489822  val loss:  0.5561237335205078\n",
      "epoch:  31   step:  24   train loss:  0.00811708252876997  val loss:  0.53759765625\n",
      "epoch:  31   step:  25   train loss:  0.008688179776072502  val loss:  0.5203326940536499\n",
      "epoch:  31   step:  26   train loss:  0.04884468764066696  val loss:  0.49743449687957764\n",
      "epoch:  31   step:  27   train loss:  0.016226526349782944  val loss:  0.48849010467529297\n",
      "epoch:  31   step:  28   train loss:  0.002926040906459093  val loss:  0.4773414433002472\n",
      "epoch:  31   step:  29   train loss:  0.05477441847324371  val loss:  0.4891382157802582\n",
      "epoch:  31   step:  30   train loss:  0.01148660946637392  val loss:  0.4999988377094269\n",
      "epoch:  31   step:  31   train loss:  0.005869744345545769  val loss:  0.506572961807251\n",
      "epoch:  31   step:  32   train loss:  0.0056089600548148155  val loss:  0.5103206038475037\n",
      "epoch:  31   step:  33   train loss:  0.003573330119252205  val loss:  0.521195650100708\n",
      "epoch:  31   step:  34   train loss:  0.013123496435582638  val loss:  0.529502809047699\n",
      "epoch:  31   step:  35   train loss:  0.006295366678386927  val loss:  0.5439052581787109\n",
      "epoch:  31   step:  36   train loss:  0.0024877567775547504  val loss:  0.5585308074951172\n",
      "epoch:  31   step:  37   train loss:  0.0015473542734980583  val loss:  0.566057026386261\n",
      "epoch:  31   step:  38   train loss:  0.002318920800462365  val loss:  0.5742319822311401\n",
      "epoch:  31   step:  39   train loss:  0.0008082150015980005  val loss:  0.5763105154037476\n",
      "epoch:  31   step:  40   train loss:  0.02244490198791027  val loss:  0.5898360013961792\n",
      "epoch:  31   step:  41   train loss:  0.0031813313253223896  val loss:  0.5995609760284424\n",
      "epoch:  31   step:  42   train loss:  0.007291657850146294  val loss:  0.6148198843002319\n",
      "epoch:  31   step:  43   train loss:  0.09433887898921967  val loss:  0.6184536218643188\n",
      "epoch:  31   step:  44   train loss:  0.010724393650889397  val loss:  0.6127758026123047\n",
      "epoch:  31   step:  45   train loss:  0.005429447162896395  val loss:  0.6089778542518616\n",
      "epoch:  31   step:  46   train loss:  0.01870023086667061  val loss:  0.6199951767921448\n",
      "epoch:  31   step:  47   train loss:  0.007553652860224247  val loss:  0.6260906457901001\n",
      "epoch:  31   step:  48   train loss:  0.005169084761291742  val loss:  0.6309225559234619\n",
      "epoch:  31   step:  49   train loss:  0.007463201880455017  val loss:  0.6348530650138855\n",
      "epoch:  31   step:  50   train loss:  0.009609263390302658  val loss:  0.6245137453079224\n",
      "epoch:  31   step:  51   train loss:  0.0018085232004523277  val loss:  0.6198588609695435\n",
      "epoch:  31   step:  52   train loss:  0.008517783135175705  val loss:  0.6169664859771729\n",
      "epoch:  31   step:  53   train loss:  0.020882103592157364  val loss:  0.6160836815834045\n",
      "epoch:  31   step:  54   train loss:  0.003555641509592533  val loss:  0.6057878136634827\n",
      "epoch:  31   step:  55   train loss:  0.0030366259161382914  val loss:  0.5933181047439575\n",
      "epoch:  31   step:  56   train loss:  0.012799205258488655  val loss:  0.5811256170272827\n",
      "epoch:  31   step:  57   train loss:  0.00167459761723876  val loss:  0.5753331780433655\n",
      "epoch:  31   step:  58   train loss:  0.04449904337525368  val loss:  0.5665594935417175\n",
      "epoch:  31   step:  59   train loss:  0.007938883267343044  val loss:  0.5629220008850098\n",
      "epoch:  31   step:  60   train loss:  0.007795970421284437  val loss:  0.569633960723877\n",
      "epoch:  31   step:  61   train loss:  0.002619365695863962  val loss:  0.5702506303787231\n",
      "epoch:  31   step:  62   train loss:  0.005093902349472046  val loss:  0.5732059478759766\n",
      "epoch:  31   step:  63   train loss:  0.004261248745024204  val loss:  0.5763307213783264\n",
      "epoch:  31   step:  64   train loss:  0.008039485663175583  val loss:  0.573147714138031\n",
      "epoch:  31   step:  65   train loss:  0.0027998192235827446  val loss:  0.5751761198043823\n",
      "epoch:  31   step:  66   train loss:  0.004125301260501146  val loss:  0.5858990550041199\n",
      "epoch:  31   step:  67   train loss:  0.00318263191729784  val loss:  0.5859538316726685\n",
      "epoch:  31   step:  68   train loss:  0.003248427528887987  val loss:  0.5821470618247986\n",
      "epoch:  31   step:  69   train loss:  0.0039130765944719315  val loss:  0.587943434715271\n",
      "epoch:  31   step:  70   train loss:  0.0027201988268643618  val loss:  0.5798609256744385\n",
      "epoch:  31   step:  71   train loss:  0.0036480436101555824  val loss:  0.5817771553993225\n",
      "epoch:  31   step:  72   train loss:  0.0010557726491242647  val loss:  0.5782528519630432\n",
      "epoch:  31   step:  73   train loss:  0.006304965354502201  val loss:  0.585098385810852\n",
      "epoch:  31   step:  74   train loss:  0.04751155897974968  val loss:  0.5980408191680908\n",
      "epoch:  31   step:  75   train loss:  0.0022131665609776974  val loss:  0.630034327507019\n",
      "epoch:  31   step:  76   train loss:  0.0062958309426903725  val loss:  0.6403053402900696\n",
      "epoch:  31   step:  77   train loss:  0.017183667048811913  val loss:  0.6549007892608643\n",
      "epoch:  31   step:  78   train loss:  0.021321242675185204  val loss:  0.671590268611908\n",
      "epoch:  31   step:  79   train loss:  0.005528316367417574  val loss:  0.6827061176300049\n",
      "epoch:  31   step:  80   train loss:  0.007337579037994146  val loss:  0.6842404007911682\n",
      "epoch:  31   step:  81   train loss:  0.00920245423913002  val loss:  0.682431161403656\n",
      "epoch:  31   step:  82   train loss:  0.007824449799954891  val loss:  0.6839655637741089\n",
      "epoch:  31   step:  83   train loss:  0.002288402058184147  val loss:  0.6788091659545898\n",
      "epoch:  31   step:  84   train loss:  0.011390353552997112  val loss:  0.6846761107444763\n",
      "epoch:  31   step:  85   train loss:  0.03527909517288208  val loss:  0.674944281578064\n",
      "epoch:  31   step:  86   train loss:  0.015138637274503708  val loss:  0.6739818453788757\n",
      "epoch:  31   step:  87   train loss:  0.0014958839165046811  val loss:  0.6568779349327087\n",
      "epoch:  31   step:  88   train loss:  0.00365272955968976  val loss:  0.6465590596199036\n",
      "epoch:  31   step:  89   train loss:  0.004860440269112587  val loss:  0.6403537392616272\n",
      "epoch:  31   step:  90   train loss:  0.000978362513706088  val loss:  0.6421365141868591\n",
      "epoch:  31   step:  91   train loss:  0.0020959859248250723  val loss:  0.6375309824943542\n",
      "epoch:  31   step:  92   train loss:  0.002342434134334326  val loss:  0.6252193450927734\n",
      "epoch:  31   step:  93   train loss:  0.0022917361930012703  val loss:  0.6178221106529236\n",
      "epoch:  31   step:  94   train loss:  0.0032224226742982864  val loss:  0.6279336214065552\n",
      "epoch:  31   step:  95   train loss:  0.0036446249578148127  val loss:  0.6267426013946533\n",
      "epoch:  31   step:  96   train loss:  0.00100623385515064  val loss:  0.6244491934776306\n",
      "epoch:  31   step:  97   train loss:  0.008380145765841007  val loss:  0.6119239926338196\n",
      "epoch:  31   step:  98   train loss:  0.004647472407668829  val loss:  0.6124824285507202\n",
      "epoch:  31   step:  99   train loss:  0.011754213832318783  val loss:  0.6136237978935242\n",
      "epoch:  31   step:  100   train loss:  0.01058382261544466  val loss:  0.6113460063934326\n",
      "epoch:  31   step:  101   train loss:  0.0020346525125205517  val loss:  0.6230205297470093\n",
      "epoch:  31   step:  102   train loss:  0.0024822019040584564  val loss:  0.6325315833091736\n",
      "epoch:  31   step:  103   train loss:  0.0069537353701889515  val loss:  0.6375762820243835\n",
      "epoch:  31   step:  104   train loss:  0.004924299195408821  val loss:  0.62986820936203\n",
      "epoch:  31   step:  105   train loss:  0.005586117506027222  val loss:  0.6502487063407898\n",
      "epoch:  31   step:  106   train loss:  0.019000735133886337  val loss:  0.6412346363067627\n",
      "epoch:  31   step:  107   train loss:  0.00207503791898489  val loss:  0.6254764199256897\n",
      "epoch:  31   step:  108   train loss:  0.007220141123980284  val loss:  0.6349491477012634\n",
      "epoch:  31   step:  109   train loss:  0.014820197597146034  val loss:  0.6190124750137329\n",
      "epoch:  31   step:  110   train loss:  0.0029085089918226004  val loss:  0.6001953482627869\n",
      "epoch:  31   step:  111   train loss:  0.0007947090780362487  val loss:  0.5891580581665039\n",
      "epoch:  31   step:  112   train loss:  0.002293819095939398  val loss:  0.5776196718215942\n",
      "epoch:  31   step:  113   train loss:  0.008523556403815746  val loss:  0.5719208121299744\n",
      "epoch:  31   step:  114   train loss:  0.004547243472188711  val loss:  0.5682809352874756\n",
      "epoch:  31   step:  115   train loss:  0.01955106481909752  val loss:  0.5634889006614685\n",
      "epoch:  31   step:  116   train loss:  0.002963077975437045  val loss:  0.5548830032348633\n",
      "epoch:  31   step:  117   train loss:  0.011254452168941498  val loss:  0.5474042296409607\n",
      "epoch:  31   step:  118   train loss:  0.0019337148405611515  val loss:  0.5428867340087891\n",
      "epoch:  31   step:  119   train loss:  0.005804926622658968  val loss:  0.5382359027862549\n",
      "epoch:  31   step:  120   train loss:  0.007557098288089037  val loss:  0.5563732385635376\n",
      "epoch:  31   step:  121   train loss:  0.004835482221096754  val loss:  0.5533581972122192\n",
      "epoch:  31   step:  122   train loss:  0.011855771765112877  val loss:  0.5611385703086853\n",
      "epoch:  31   step:  123   train loss:  0.0010776612907648087  val loss:  0.5581377148628235\n",
      "epoch:  31   step:  124   train loss:  0.0010528648272156715  val loss:  0.562554657459259\n",
      "epoch:  31   step:  125   train loss:  0.0015469207428395748  val loss:  0.5631380081176758\n",
      "epoch:  31   step:  126   train loss:  0.003340652910992503  val loss:  0.5601158738136292\n",
      "epoch:  31   step:  127   train loss:  0.0010491126449778676  val loss:  0.5581498742103577\n",
      "epoch:  31   step:  128   train loss:  0.0019238917157053947  val loss:  0.5609005093574524\n",
      "epoch:  31   step:  129   train loss:  0.0036474382504820824  val loss:  0.5553104281425476\n",
      "epoch:  31   step:  130   train loss:  0.0010301339207217097  val loss:  0.5574311017990112\n",
      "epoch:  31   step:  131   train loss:  0.0027854135259985924  val loss:  0.556603729724884\n",
      "epoch:  31   step:  132   train loss:  0.0011213170364499092  val loss:  0.5659698843955994\n",
      "epoch:  31   step:  133   train loss:  0.0028908199165016413  val loss:  0.5695736408233643\n",
      "epoch:  31   step:  134   train loss:  0.0005647664074786007  val loss:  0.568503201007843\n",
      "epoch:  31   step:  135   train loss:  0.021390270441770554  val loss:  0.5754087567329407\n",
      "epoch:  31   step:  136   train loss:  0.0009410327184014022  val loss:  0.5642154216766357\n",
      "epoch:  31   step:  137   train loss:  0.002397526754066348  val loss:  0.5634315609931946\n",
      "epoch:  31   step:  138   train loss:  0.003150569275021553  val loss:  0.5467226505279541\n",
      "epoch:  31   step:  139   train loss:  0.0018382748821750283  val loss:  0.5434502959251404\n",
      "epoch:  31   step:  140   train loss:  0.0011511467164382339  val loss:  0.5401952862739563\n",
      "epoch:  31   step:  141   train loss:  0.00785290077328682  val loss:  0.5280131697654724\n",
      "epoch:  31   step:  142   train loss:  0.007558159064501524  val loss:  0.52531898021698\n",
      "epoch:  31   step:  143   train loss:  0.0014459025114774704  val loss:  0.5206467509269714\n",
      "epoch:  31   step:  144   train loss:  0.004283279180526733  val loss:  0.5323438048362732\n",
      "epoch:  31   step:  145   train loss:  0.002232081489637494  val loss:  0.5314432978630066\n",
      "epoch:  31   step:  146   train loss:  0.002619876991957426  val loss:  0.5332335233688354\n",
      "epoch:  31   step:  147   train loss:  0.002652898896485567  val loss:  0.5322095155715942\n",
      "epoch:  31   step:  148   train loss:  0.0018874516244977713  val loss:  0.5146293044090271\n",
      "epoch:  31   step:  149   train loss:  0.0021325419656932354  val loss:  0.511935293674469\n",
      "epoch:  31   step:  150   train loss:  0.0011869531590491533  val loss:  0.5186992287635803\n",
      "epoch:  31   step:  151   train loss:  0.0029806834645569324  val loss:  0.5211420655250549\n",
      "epoch:  31   step:  152   train loss:  0.0017396019538864493  val loss:  0.536393940448761\n",
      "epoch:  31   step:  153   train loss:  0.02872239425778389  val loss:  0.5434836745262146\n",
      "epoch:  31   step:  154   train loss:  0.0018824918661266565  val loss:  0.5640518069267273\n",
      "epoch:  31   step:  155   train loss:  0.006014631129801273  val loss:  0.5826820135116577\n",
      "epoch:  31   step:  156   train loss:  0.0005386517732404172  val loss:  0.5836213231086731\n",
      "epoch:  31   step:  157   train loss:  0.011863191612064838  val loss:  0.5937827229499817\n",
      "epoch:  31   step:  158   train loss:  0.0004028767580166459  val loss:  0.6048807501792908\n",
      "epoch:  31   step:  159   train loss:  0.0007609458407387137  val loss:  0.605222225189209\n",
      "epoch:  31   step:  160   train loss:  0.010147955268621445  val loss:  0.62169349193573\n",
      "epoch:  31   step:  161   train loss:  0.01780548505485058  val loss:  0.6248783469200134\n",
      "epoch:  31   step:  162   train loss:  0.0016950471326708794  val loss:  0.6266048550605774\n",
      "epoch:  31   step:  163   train loss:  0.0010828303638845682  val loss:  0.6222872138023376\n",
      "epoch:  31   step:  164   train loss:  0.017644789069890976  val loss:  0.6234287023544312\n",
      "epoch:  31   step:  165   train loss:  0.0010854059364646673  val loss:  0.6212271451950073\n",
      "epoch:  32   step:  0   train loss:  0.000981770223006606  val loss:  0.6390097737312317\n",
      "epoch:  32   step:  1   train loss:  0.0017251366516575217  val loss:  0.6316112875938416\n",
      "epoch:  32   step:  2   train loss:  0.0023746516089886427  val loss:  0.65085768699646\n",
      "epoch:  32   step:  3   train loss:  0.0010430810507386923  val loss:  0.6525353789329529\n",
      "epoch:  32   step:  4   train loss:  0.00302358390763402  val loss:  0.6622418761253357\n",
      "epoch:  32   step:  5   train loss:  0.0006598432664759457  val loss:  0.6614900827407837\n",
      "epoch:  32   step:  6   train loss:  0.0007802252075634897  val loss:  0.6679384708404541\n",
      "epoch:  32   step:  7   train loss:  0.0007555070333182812  val loss:  0.6689904928207397\n",
      "epoch:  32   step:  8   train loss:  0.0022184927947819233  val loss:  0.6692403554916382\n",
      "epoch:  32   step:  9   train loss:  0.0006861129077151418  val loss:  0.6664626002311707\n",
      "epoch:  32   step:  10   train loss:  0.0006513252155855298  val loss:  0.659227192401886\n",
      "epoch:  32   step:  11   train loss:  0.0006205284735187888  val loss:  0.648503303527832\n",
      "epoch:  32   step:  12   train loss:  0.002181700896471739  val loss:  0.6457293629646301\n",
      "epoch:  32   step:  13   train loss:  0.00417940691113472  val loss:  0.6518942713737488\n",
      "epoch:  32   step:  14   train loss:  0.001243822043761611  val loss:  0.647328794002533\n",
      "epoch:  32   step:  15   train loss:  0.027203503996133804  val loss:  0.639944314956665\n",
      "epoch:  32   step:  16   train loss:  0.0016086780233308673  val loss:  0.6414842009544373\n",
      "epoch:  32   step:  17   train loss:  0.0018403613939881325  val loss:  0.6294882893562317\n",
      "epoch:  32   step:  18   train loss:  0.0007876220624893904  val loss:  0.6319615244865417\n",
      "epoch:  32   step:  19   train loss:  0.0019383819308131933  val loss:  0.6260358095169067\n",
      "epoch:  32   step:  20   train loss:  0.0009908868232741952  val loss:  0.6263691782951355\n",
      "epoch:  32   step:  21   train loss:  0.0010521367657929659  val loss:  0.6282991170883179\n",
      "epoch:  32   step:  22   train loss:  0.0011831700103357434  val loss:  0.6354602575302124\n",
      "epoch:  32   step:  23   train loss:  0.0016404032940045  val loss:  0.6376405954360962\n",
      "epoch:  32   step:  24   train loss:  0.009032175876200199  val loss:  0.6125844717025757\n",
      "epoch:  32   step:  25   train loss:  0.002146798651665449  val loss:  0.6137756109237671\n",
      "epoch:  32   step:  26   train loss:  0.0019849282689392567  val loss:  0.6015598773956299\n",
      "epoch:  32   step:  27   train loss:  0.0018346752040088177  val loss:  0.5969993472099304\n",
      "epoch:  32   step:  28   train loss:  0.0009238135535269976  val loss:  0.6101205348968506\n",
      "epoch:  32   step:  29   train loss:  0.002845229348167777  val loss:  0.6234544515609741\n",
      "epoch:  32   step:  30   train loss:  0.001627438934519887  val loss:  0.6413312554359436\n",
      "epoch:  32   step:  31   train loss:  0.0023580212146043777  val loss:  0.6426904797554016\n",
      "epoch:  32   step:  32   train loss:  0.0043508498929440975  val loss:  0.6416184306144714\n",
      "epoch:  32   step:  33   train loss:  0.0015300564700737596  val loss:  0.6488122940063477\n",
      "epoch:  32   step:  34   train loss:  0.001521238824352622  val loss:  0.6419798135757446\n",
      "epoch:  32   step:  35   train loss:  0.002924026921391487  val loss:  0.630250096321106\n",
      "epoch:  32   step:  36   train loss:  0.006111050955951214  val loss:  0.6240542531013489\n",
      "epoch:  32   step:  37   train loss:  0.0009282274404540658  val loss:  0.6169472932815552\n",
      "epoch:  32   step:  38   train loss:  0.0010225947480648756  val loss:  0.6256874799728394\n",
      "epoch:  32   step:  39   train loss:  0.0025512930005788803  val loss:  0.6234081387519836\n",
      "epoch:  32   step:  40   train loss:  0.0007308683125302196  val loss:  0.6155557632446289\n",
      "epoch:  32   step:  41   train loss:  0.0026828311383724213  val loss:  0.6153045296669006\n",
      "epoch:  32   step:  42   train loss:  0.000798541062977165  val loss:  0.6068581938743591\n",
      "epoch:  32   step:  43   train loss:  0.0015481540467590094  val loss:  0.6070652008056641\n",
      "epoch:  32   step:  44   train loss:  0.002234208397567272  val loss:  0.6054268479347229\n",
      "epoch:  32   step:  45   train loss:  0.002835584105923772  val loss:  0.5951935648918152\n",
      "epoch:  32   step:  46   train loss:  0.0006186738610267639  val loss:  0.5938684940338135\n",
      "epoch:  32   step:  47   train loss:  0.0013588045258074999  val loss:  0.5817117691040039\n",
      "epoch:  32   step:  48   train loss:  0.001795144286006689  val loss:  0.5772409439086914\n",
      "epoch:  32   step:  49   train loss:  0.000896437035407871  val loss:  0.5731214284896851\n",
      "epoch:  32   step:  50   train loss:  0.0009610435809008777  val loss:  0.5732676386833191\n",
      "epoch:  32   step:  51   train loss:  0.002036824356764555  val loss:  0.5590469837188721\n",
      "epoch:  32   step:  52   train loss:  0.000844193622469902  val loss:  0.5600029826164246\n",
      "epoch:  32   step:  53   train loss:  0.008614230901002884  val loss:  0.5463710427284241\n",
      "epoch:  32   step:  54   train loss:  0.0011688118102028966  val loss:  0.5526249408721924\n",
      "epoch:  32   step:  55   train loss:  0.0008035212522372603  val loss:  0.5525867938995361\n",
      "epoch:  32   step:  56   train loss:  0.0007941900403238833  val loss:  0.5625193119049072\n",
      "epoch:  32   step:  57   train loss:  0.0072577656246721745  val loss:  0.5583803653717041\n",
      "epoch:  32   step:  58   train loss:  0.0004500297945924103  val loss:  0.5559677481651306\n",
      "epoch:  32   step:  59   train loss:  0.0024312154855579138  val loss:  0.5583137273788452\n",
      "epoch:  32   step:  60   train loss:  0.0008609899086877704  val loss:  0.5498158931732178\n",
      "epoch:  32   step:  61   train loss:  0.0010699612321332097  val loss:  0.5590649247169495\n",
      "epoch:  32   step:  62   train loss:  0.0009663393720984459  val loss:  0.5663601160049438\n",
      "epoch:  32   step:  63   train loss:  0.0010983888059854507  val loss:  0.5659116506576538\n",
      "epoch:  32   step:  64   train loss:  0.0010321293957531452  val loss:  0.5626083016395569\n",
      "epoch:  32   step:  65   train loss:  0.001362469862215221  val loss:  0.5549494624137878\n",
      "epoch:  32   step:  66   train loss:  0.0005367908161133528  val loss:  0.5692108273506165\n",
      "epoch:  32   step:  67   train loss:  0.0008123072329908609  val loss:  0.5795022249221802\n",
      "epoch:  32   step:  68   train loss:  0.003647737205028534  val loss:  0.5916468501091003\n",
      "epoch:  32   step:  69   train loss:  0.0016562168020755053  val loss:  0.5901780724525452\n",
      "epoch:  32   step:  70   train loss:  0.00265908264555037  val loss:  0.5826672911643982\n",
      "epoch:  32   step:  71   train loss:  0.005371049977838993  val loss:  0.6011953353881836\n",
      "epoch:  32   step:  72   train loss:  0.0014324347721412778  val loss:  0.6011385321617126\n",
      "epoch:  32   step:  73   train loss:  0.0007425235817208886  val loss:  0.5994361639022827\n",
      "epoch:  32   step:  74   train loss:  0.0014132235664874315  val loss:  0.5915068984031677\n",
      "epoch:  32   step:  75   train loss:  0.0007316825212910771  val loss:  0.5917016267776489\n",
      "epoch:  32   step:  76   train loss:  0.0012801832053810358  val loss:  0.5863158106803894\n",
      "epoch:  32   step:  77   train loss:  0.0022271506022661924  val loss:  0.5918655395507812\n",
      "epoch:  32   step:  78   train loss:  0.002533335704356432  val loss:  0.5927761793136597\n",
      "epoch:  32   step:  79   train loss:  0.0043591102585196495  val loss:  0.6141128540039062\n",
      "epoch:  32   step:  80   train loss:  0.0033925441093742847  val loss:  0.6271070241928101\n",
      "epoch:  32   step:  81   train loss:  0.0007560113444924355  val loss:  0.6346977949142456\n",
      "epoch:  32   step:  82   train loss:  0.0005525264423340559  val loss:  0.6299715042114258\n",
      "epoch:  32   step:  83   train loss:  0.0010358564322814345  val loss:  0.625646710395813\n",
      "epoch:  32   step:  84   train loss:  0.0009780845139175653  val loss:  0.6191472411155701\n",
      "epoch:  32   step:  85   train loss:  0.0029401995707303286  val loss:  0.6187494993209839\n",
      "epoch:  32   step:  86   train loss:  0.0011595876421779394  val loss:  0.6265820264816284\n",
      "epoch:  32   step:  87   train loss:  0.03125770762562752  val loss:  0.6108018755912781\n",
      "epoch:  32   step:  88   train loss:  0.00230224896222353  val loss:  0.6073240041732788\n",
      "epoch:  32   step:  89   train loss:  0.0010146282147616148  val loss:  0.6148378849029541\n",
      "epoch:  32   step:  90   train loss:  0.0004850389959756285  val loss:  0.6100936532020569\n",
      "epoch:  32   step:  91   train loss:  0.004462897311896086  val loss:  0.600403368473053\n",
      "epoch:  32   step:  92   train loss:  0.005080674774944782  val loss:  0.6155996918678284\n",
      "epoch:  32   step:  93   train loss:  0.003039451315999031  val loss:  0.6253753304481506\n",
      "epoch:  32   step:  94   train loss:  0.009760744869709015  val loss:  0.6243186593055725\n",
      "epoch:  32   step:  95   train loss:  0.0010156321804970503  val loss:  0.6247572898864746\n",
      "epoch:  32   step:  96   train loss:  0.0010255915112793446  val loss:  0.6257317662239075\n",
      "epoch:  32   step:  97   train loss:  0.006386506836861372  val loss:  0.6391362547874451\n",
      "epoch:  32   step:  98   train loss:  0.0025119511410593987  val loss:  0.6529554128646851\n",
      "epoch:  32   step:  99   train loss:  0.0008362571825273335  val loss:  0.6525165438652039\n",
      "epoch:  32   step:  100   train loss:  0.008232343010604382  val loss:  0.655860424041748\n",
      "epoch:  32   step:  101   train loss:  0.006921227090060711  val loss:  0.6321157217025757\n",
      "epoch:  32   step:  102   train loss:  0.001527413260191679  val loss:  0.626099705696106\n",
      "epoch:  32   step:  103   train loss:  0.01349000632762909  val loss:  0.6282809376716614\n",
      "epoch:  32   step:  104   train loss:  0.0005578544223681092  val loss:  0.642020583152771\n",
      "epoch:  32   step:  105   train loss:  0.0006413980154320598  val loss:  0.6387380957603455\n",
      "epoch:  32   step:  106   train loss:  0.001196420518681407  val loss:  0.6352596282958984\n",
      "epoch:  32   step:  107   train loss:  0.0024293228052556515  val loss:  0.6270025968551636\n",
      "epoch:  32   step:  108   train loss:  0.0022750040516257286  val loss:  0.6303050518035889\n",
      "epoch:  32   step:  109   train loss:  0.0022811386734247208  val loss:  0.6386589407920837\n",
      "epoch:  32   step:  110   train loss:  0.0008566845208406448  val loss:  0.638323187828064\n",
      "epoch:  32   step:  111   train loss:  0.0034558502957224846  val loss:  0.6422744393348694\n",
      "epoch:  32   step:  112   train loss:  0.0006103703053668141  val loss:  0.6448283791542053\n",
      "epoch:  32   step:  113   train loss:  0.0005867889849469066  val loss:  0.6520165801048279\n",
      "epoch:  32   step:  114   train loss:  0.0007254595402628183  val loss:  0.6558181047439575\n",
      "epoch:  32   step:  115   train loss:  0.002625791123136878  val loss:  0.6545475125312805\n",
      "epoch:  32   step:  116   train loss:  0.0007794387056492269  val loss:  0.6662867665290833\n",
      "epoch:  32   step:  117   train loss:  0.001043279655277729  val loss:  0.6709867119789124\n",
      "epoch:  32   step:  118   train loss:  0.00048622998292557895  val loss:  0.6714172959327698\n",
      "epoch:  32   step:  119   train loss:  0.003039904870092869  val loss:  0.6858865022659302\n",
      "epoch:  32   step:  120   train loss:  0.0033110054209828377  val loss:  0.6893728971481323\n",
      "epoch:  32   step:  121   train loss:  0.0012773401103913784  val loss:  0.6958280801773071\n",
      "epoch:  32   step:  122   train loss:  0.0008545136079192162  val loss:  0.7016937732696533\n",
      "epoch:  32   step:  123   train loss:  0.0011429388541728258  val loss:  0.7045602202415466\n",
      "epoch:  32   step:  124   train loss:  0.0031875467393547297  val loss:  0.6972758173942566\n",
      "epoch:  32   step:  125   train loss:  0.0036118950229138136  val loss:  0.6896122694015503\n",
      "epoch:  32   step:  126   train loss:  0.0011207761708647013  val loss:  0.6951768398284912\n",
      "epoch:  32   step:  127   train loss:  0.0007607642328366637  val loss:  0.7028762698173523\n",
      "epoch:  32   step:  128   train loss:  0.005748195573687553  val loss:  0.7218416929244995\n",
      "epoch:  32   step:  129   train loss:  0.0024190854746848345  val loss:  0.7252187132835388\n",
      "epoch:  32   step:  130   train loss:  0.0011183553142473102  val loss:  0.719664990901947\n",
      "epoch:  32   step:  131   train loss:  0.001374312094412744  val loss:  0.7110256552696228\n",
      "epoch:  32   step:  132   train loss:  0.00034490664256736636  val loss:  0.6999337077140808\n",
      "epoch:  32   step:  133   train loss:  0.0011258646845817566  val loss:  0.6872990131378174\n",
      "epoch:  32   step:  134   train loss:  0.0023478721268475056  val loss:  0.6858014464378357\n",
      "epoch:  32   step:  135   train loss:  0.0013952970039099455  val loss:  0.6887163519859314\n",
      "epoch:  32   step:  136   train loss:  0.001092254649847746  val loss:  0.6919389367103577\n",
      "epoch:  32   step:  137   train loss:  0.0006906085764057934  val loss:  0.698566198348999\n",
      "epoch:  32   step:  138   train loss:  0.0005873177433386445  val loss:  0.7134483456611633\n",
      "epoch:  32   step:  139   train loss:  0.0006209624698385596  val loss:  0.7127470374107361\n",
      "epoch:  32   step:  140   train loss:  0.00043515843572095037  val loss:  0.7017738223075867\n",
      "epoch:  32   step:  141   train loss:  0.0009686353732831776  val loss:  0.707836389541626\n",
      "epoch:  32   step:  142   train loss:  0.0006848882185295224  val loss:  0.7129206657409668\n",
      "epoch:  32   step:  143   train loss:  0.0004671725910156965  val loss:  0.7170524597167969\n",
      "epoch:  32   step:  144   train loss:  0.0008517713285982609  val loss:  0.7235329747200012\n",
      "epoch:  32   step:  145   train loss:  0.006958177778869867  val loss:  0.7086932063102722\n",
      "epoch:  32   step:  146   train loss:  0.0010074246674776077  val loss:  0.706628680229187\n",
      "epoch:  32   step:  147   train loss:  0.0008333916775882244  val loss:  0.7062035202980042\n",
      "epoch:  32   step:  148   train loss:  0.0019328026100993156  val loss:  0.6913427114486694\n",
      "epoch:  32   step:  149   train loss:  0.0010503262747079134  val loss:  0.6785966157913208\n",
      "epoch:  32   step:  150   train loss:  0.0021922606974840164  val loss:  0.6722654104232788\n",
      "epoch:  32   step:  151   train loss:  0.0004967107670381665  val loss:  0.6707507371902466\n",
      "epoch:  32   step:  152   train loss:  0.0031679924577474594  val loss:  0.6681858897209167\n",
      "epoch:  32   step:  153   train loss:  0.0007118690409697592  val loss:  0.6697990298271179\n",
      "epoch:  32   step:  154   train loss:  0.001279877033084631  val loss:  0.6800746917724609\n",
      "epoch:  32   step:  155   train loss:  0.0016518074553459883  val loss:  0.6689966917037964\n",
      "epoch:  32   step:  156   train loss:  0.0009036773699335754  val loss:  0.6642961502075195\n",
      "epoch:  32   step:  157   train loss:  0.0011276850709691644  val loss:  0.652441680431366\n",
      "epoch:  32   step:  158   train loss:  0.006970461457967758  val loss:  0.6672616600990295\n",
      "epoch:  32   step:  159   train loss:  0.0008210413507185876  val loss:  0.6745102405548096\n",
      "epoch:  32   step:  160   train loss:  0.000474173401016742  val loss:  0.6799455881118774\n",
      "epoch:  32   step:  161   train loss:  0.002130580600351095  val loss:  0.6705381870269775\n",
      "epoch:  32   step:  162   train loss:  0.011097269132733345  val loss:  0.6737643480300903\n",
      "epoch:  32   step:  163   train loss:  0.00040083719068206847  val loss:  0.6595821976661682\n",
      "epoch:  32   step:  164   train loss:  0.0008147619082592428  val loss:  0.6543724536895752\n",
      "epoch:  32   step:  165   train loss:  0.0009851763024926186  val loss:  0.6578712463378906\n",
      "epoch:  33   step:  0   train loss:  0.0006464349571615458  val loss:  0.6585075259208679\n",
      "epoch:  33   step:  1   train loss:  0.00048780342331156135  val loss:  0.6571955680847168\n",
      "epoch:  33   step:  2   train loss:  0.001063282135874033  val loss:  0.6639571189880371\n",
      "epoch:  33   step:  3   train loss:  0.0005420873640105128  val loss:  0.6597156524658203\n",
      "epoch:  33   step:  4   train loss:  0.0025263414718210697  val loss:  0.6693695187568665\n",
      "epoch:  33   step:  5   train loss:  0.0004098199715372175  val loss:  0.6520282626152039\n",
      "epoch:  33   step:  6   train loss:  0.0006625132518820465  val loss:  0.650214433670044\n",
      "epoch:  33   step:  7   train loss:  0.0011637491406872869  val loss:  0.6418685913085938\n",
      "epoch:  33   step:  8   train loss:  0.003977639600634575  val loss:  0.6316551566123962\n",
      "epoch:  33   step:  9   train loss:  0.000713070563506335  val loss:  0.6264177560806274\n",
      "epoch:  33   step:  10   train loss:  0.002531444886699319  val loss:  0.6164602041244507\n",
      "epoch:  33   step:  11   train loss:  0.0006470214575529099  val loss:  0.6334941983222961\n",
      "epoch:  33   step:  12   train loss:  0.0005212788237258792  val loss:  0.6243037581443787\n",
      "epoch:  33   step:  13   train loss:  0.00031236070208251476  val loss:  0.6171852946281433\n",
      "epoch:  33   step:  14   train loss:  0.0009798932587727904  val loss:  0.624858021736145\n",
      "epoch:  33   step:  15   train loss:  0.0007034208974801004  val loss:  0.627747654914856\n",
      "epoch:  33   step:  16   train loss:  0.0009410773636773229  val loss:  0.6298321485519409\n",
      "epoch:  33   step:  17   train loss:  0.0003963364288210869  val loss:  0.6454770565032959\n",
      "epoch:  33   step:  18   train loss:  0.00027157532167620957  val loss:  0.6510551571846008\n",
      "epoch:  33   step:  19   train loss:  0.0004010208649560809  val loss:  0.6497130393981934\n",
      "epoch:  33   step:  20   train loss:  0.0013821942266076803  val loss:  0.640971302986145\n",
      "epoch:  33   step:  21   train loss:  0.00035094004124403  val loss:  0.6519075632095337\n",
      "epoch:  33   step:  22   train loss:  0.0010525326943024993  val loss:  0.6463958621025085\n",
      "epoch:  33   step:  23   train loss:  0.0008656237041577697  val loss:  0.6519298553466797\n",
      "epoch:  33   step:  24   train loss:  0.00044630258344113827  val loss:  0.6527100801467896\n",
      "epoch:  33   step:  25   train loss:  0.0006214027525857091  val loss:  0.6549393534660339\n",
      "epoch:  33   step:  26   train loss:  0.0018405683804303408  val loss:  0.6506377458572388\n",
      "epoch:  33   step:  27   train loss:  0.0010047891410067677  val loss:  0.6508392691612244\n",
      "epoch:  33   step:  28   train loss:  0.0007224382134154439  val loss:  0.6620664596557617\n",
      "epoch:  33   step:  29   train loss:  0.0005419943481683731  val loss:  0.6640602946281433\n",
      "epoch:  33   step:  30   train loss:  0.0012826023157685995  val loss:  0.6568464040756226\n",
      "epoch:  33   step:  31   train loss:  0.0005502243875525892  val loss:  0.6565769910812378\n",
      "epoch:  33   step:  32   train loss:  0.0004502242954913527  val loss:  0.6599549055099487\n",
      "epoch:  33   step:  33   train loss:  0.0010467679239809513  val loss:  0.666677713394165\n",
      "epoch:  33   step:  34   train loss:  0.0005339921917766333  val loss:  0.6688094139099121\n",
      "epoch:  33   step:  35   train loss:  0.0005324514931999147  val loss:  0.6609887480735779\n",
      "epoch:  33   step:  36   train loss:  0.000232662569032982  val loss:  0.6623227596282959\n",
      "epoch:  33   step:  37   train loss:  0.000857625447679311  val loss:  0.658015787601471\n",
      "epoch:  33   step:  38   train loss:  0.0004110843874514103  val loss:  0.6536433100700378\n",
      "epoch:  33   step:  39   train loss:  0.0003956306027248502  val loss:  0.6513184905052185\n",
      "epoch:  33   step:  40   train loss:  0.0005835779011249542  val loss:  0.6469084620475769\n",
      "epoch:  33   step:  41   train loss:  0.0008062846027314663  val loss:  0.6581161618232727\n",
      "epoch:  33   step:  42   train loss:  0.0004170801257714629  val loss:  0.6563547253608704\n",
      "epoch:  33   step:  43   train loss:  0.0010741585865616798  val loss:  0.6639828085899353\n",
      "epoch:  33   step:  44   train loss:  0.0008288732497021556  val loss:  0.6639083623886108\n",
      "epoch:  33   step:  45   train loss:  0.00072294328128919  val loss:  0.6653284430503845\n",
      "epoch:  33   step:  46   train loss:  0.0007769331568852067  val loss:  0.666183590888977\n",
      "epoch:  33   step:  47   train loss:  0.001175048528239131  val loss:  0.6629475951194763\n",
      "epoch:  33   step:  48   train loss:  0.000491672835778445  val loss:  0.6562058925628662\n",
      "epoch:  33   step:  49   train loss:  0.000889437971636653  val loss:  0.6535704135894775\n",
      "epoch:  33   step:  50   train loss:  0.0009760402608662844  val loss:  0.6584969162940979\n",
      "epoch:  33   step:  51   train loss:  0.00040725874714553356  val loss:  0.6466415524482727\n",
      "epoch:  33   step:  52   train loss:  0.00026403891388326883  val loss:  0.6493333578109741\n",
      "epoch:  33   step:  53   train loss:  0.000972556765191257  val loss:  0.6543130874633789\n",
      "epoch:  33   step:  54   train loss:  0.0009112050756812096  val loss:  0.6621432900428772\n",
      "epoch:  33   step:  55   train loss:  0.0005469440948218107  val loss:  0.6524184942245483\n",
      "epoch:  33   step:  56   train loss:  0.0009241898660548031  val loss:  0.655437707901001\n",
      "epoch:  33   step:  57   train loss:  0.0006665281835012138  val loss:  0.660823404788971\n",
      "epoch:  33   step:  58   train loss:  0.000488309480715543  val loss:  0.6734786033630371\n",
      "epoch:  33   step:  59   train loss:  0.002281718421727419  val loss:  0.6695800423622131\n",
      "epoch:  33   step:  60   train loss:  0.000995819689705968  val loss:  0.6693335771560669\n",
      "epoch:  33   step:  61   train loss:  0.0009539753664284945  val loss:  0.6651897430419922\n",
      "epoch:  33   step:  62   train loss:  0.0009593553841114044  val loss:  0.6661752462387085\n",
      "epoch:  33   step:  63   train loss:  0.003956047352403402  val loss:  0.6607829928398132\n",
      "epoch:  33   step:  64   train loss:  0.0010095598408952355  val loss:  0.6411359906196594\n",
      "epoch:  33   step:  65   train loss:  0.0015347205335274339  val loss:  0.6354075074195862\n",
      "epoch:  33   step:  66   train loss:  0.0008593369857408106  val loss:  0.6399635672569275\n",
      "epoch:  33   step:  67   train loss:  0.00034458868321962655  val loss:  0.6410765647888184\n",
      "epoch:  33   step:  68   train loss:  0.00036130179069004953  val loss:  0.6327597498893738\n",
      "epoch:  33   step:  69   train loss:  0.0010182603728026152  val loss:  0.6372274160385132\n",
      "epoch:  33   step:  70   train loss:  0.0012352934572845697  val loss:  0.6457501649856567\n",
      "epoch:  33   step:  71   train loss:  0.0005558918928727508  val loss:  0.6489735245704651\n",
      "epoch:  33   step:  72   train loss:  0.0003826241591013968  val loss:  0.6594471335411072\n",
      "epoch:  33   step:  73   train loss:  0.0007672490319237113  val loss:  0.6610439419746399\n",
      "epoch:  33   step:  74   train loss:  0.0010432824492454529  val loss:  0.6524695158004761\n",
      "epoch:  33   step:  75   train loss:  0.0006252031307667494  val loss:  0.6476094126701355\n",
      "epoch:  33   step:  76   train loss:  0.0069968318566679955  val loss:  0.672113835811615\n",
      "epoch:  33   step:  77   train loss:  0.0017822601366788149  val loss:  0.6721355319023132\n",
      "epoch:  33   step:  78   train loss:  0.0007400011527352035  val loss:  0.6812475323677063\n",
      "epoch:  33   step:  79   train loss:  0.00048752862494438887  val loss:  0.6883940100669861\n",
      "epoch:  33   step:  80   train loss:  0.0005570930079557002  val loss:  0.6939850449562073\n",
      "epoch:  33   step:  81   train loss:  0.00163360801525414  val loss:  0.6884589791297913\n",
      "epoch:  33   step:  82   train loss:  0.0014893831685185432  val loss:  0.6806114912033081\n",
      "epoch:  33   step:  83   train loss:  0.0009315162897109985  val loss:  0.6748652458190918\n",
      "epoch:  33   step:  84   train loss:  0.0006496670539490879  val loss:  0.6782872676849365\n",
      "epoch:  33   step:  85   train loss:  0.00041100173257291317  val loss:  0.6777017116546631\n",
      "epoch:  33   step:  86   train loss:  0.0006927173817530274  val loss:  0.6688598990440369\n",
      "epoch:  33   step:  87   train loss:  0.0007771085947751999  val loss:  0.6521100401878357\n",
      "epoch:  33   step:  88   train loss:  0.000653435243293643  val loss:  0.6530118584632874\n",
      "epoch:  33   step:  89   train loss:  0.0009042161982506514  val loss:  0.6656512022018433\n",
      "epoch:  33   step:  90   train loss:  0.0003092450206167996  val loss:  0.6677687764167786\n",
      "epoch:  33   step:  91   train loss:  0.0006091272225603461  val loss:  0.6693134903907776\n",
      "epoch:  33   step:  92   train loss:  0.0013205953873693943  val loss:  0.6740515232086182\n",
      "epoch:  33   step:  93   train loss:  0.00045825267443433404  val loss:  0.6659058928489685\n",
      "epoch:  33   step:  94   train loss:  0.0004312492674216628  val loss:  0.6634340286254883\n",
      "epoch:  33   step:  95   train loss:  0.0018828327301889658  val loss:  0.6659835577011108\n",
      "epoch:  33   step:  96   train loss:  0.0018498939462006092  val loss:  0.6635271906852722\n",
      "epoch:  33   step:  97   train loss:  0.0017027201829478145  val loss:  0.675442099571228\n",
      "epoch:  33   step:  98   train loss:  0.0007537540514022112  val loss:  0.6712548732757568\n",
      "epoch:  33   step:  99   train loss:  0.000885042711161077  val loss:  0.6689295172691345\n",
      "epoch:  33   step:  100   train loss:  0.0006599684711545706  val loss:  0.6714867949485779\n",
      "epoch:  33   step:  101   train loss:  0.0005058142123743892  val loss:  0.6600421071052551\n",
      "epoch:  33   step:  102   train loss:  0.00059573381440714  val loss:  0.6551414132118225\n",
      "epoch:  33   step:  103   train loss:  0.0006290475139394403  val loss:  0.6561791896820068\n",
      "epoch:  33   step:  104   train loss:  0.002303492045029998  val loss:  0.6540051698684692\n",
      "epoch:  33   step:  105   train loss:  0.0027165221981704235  val loss:  0.6616753935813904\n",
      "epoch:  33   step:  106   train loss:  0.00037223187973722816  val loss:  0.6676891446113586\n",
      "epoch:  33   step:  107   train loss:  0.0005989086348563433  val loss:  0.671976625919342\n",
      "epoch:  33   step:  108   train loss:  0.0011131090577691793  val loss:  0.660365879535675\n",
      "epoch:  33   step:  109   train loss:  0.0004452174180187285  val loss:  0.6642355918884277\n",
      "epoch:  33   step:  110   train loss:  0.00025721234851516783  val loss:  0.6664093732833862\n",
      "epoch:  33   step:  111   train loss:  0.0005802082596346736  val loss:  0.6726486682891846\n",
      "epoch:  33   step:  112   train loss:  0.0007100753718987107  val loss:  0.6772554516792297\n",
      "epoch:  33   step:  113   train loss:  0.0011245375499129295  val loss:  0.6768185496330261\n",
      "epoch:  33   step:  114   train loss:  0.00044311227975413203  val loss:  0.6781257390975952\n",
      "epoch:  33   step:  115   train loss:  0.0007043242221698165  val loss:  0.676429033279419\n",
      "epoch:  33   step:  116   train loss:  0.0005390935693867505  val loss:  0.6756680011749268\n",
      "epoch:  33   step:  117   train loss:  0.0006972068804316223  val loss:  0.6661038994789124\n",
      "epoch:  33   step:  118   train loss:  0.0005310905398800969  val loss:  0.6552929282188416\n",
      "epoch:  33   step:  119   train loss:  0.001182258827611804  val loss:  0.645010232925415\n",
      "epoch:  33   step:  120   train loss:  0.0014717194717377424  val loss:  0.6488363742828369\n",
      "epoch:  33   step:  121   train loss:  0.00044534221524372697  val loss:  0.6477795243263245\n",
      "epoch:  33   step:  122   train loss:  0.00039340840885415673  val loss:  0.6579550504684448\n",
      "epoch:  33   step:  123   train loss:  0.00042028346797451377  val loss:  0.6520691514015198\n",
      "epoch:  33   step:  124   train loss:  0.0003902020980603993  val loss:  0.658890426158905\n",
      "epoch:  33   step:  125   train loss:  0.0011143420124426484  val loss:  0.653423011302948\n",
      "epoch:  33   step:  126   train loss:  0.0006323525449261069  val loss:  0.6484560370445251\n",
      "epoch:  33   step:  127   train loss:  0.0008583181770518422  val loss:  0.6520017385482788\n",
      "epoch:  33   step:  128   train loss:  0.0010308593045920134  val loss:  0.647528350353241\n",
      "epoch:  33   step:  129   train loss:  0.0014160950668156147  val loss:  0.6439892649650574\n",
      "epoch:  33   step:  130   train loss:  0.00038555823266506195  val loss:  0.6541228890419006\n",
      "epoch:  33   step:  131   train loss:  0.0011715281289070845  val loss:  0.6532318592071533\n",
      "epoch:  33   step:  132   train loss:  0.0005834538605995476  val loss:  0.6410043835639954\n",
      "epoch:  33   step:  133   train loss:  0.0009723055991344154  val loss:  0.6392740607261658\n",
      "epoch:  33   step:  134   train loss:  0.0014644729671999812  val loss:  0.6310725808143616\n",
      "epoch:  33   step:  135   train loss:  0.0004331246018409729  val loss:  0.6307841539382935\n",
      "epoch:  33   step:  136   train loss:  0.002014490310102701  val loss:  0.6481245160102844\n",
      "epoch:  33   step:  137   train loss:  0.0004892316646873951  val loss:  0.653601884841919\n",
      "epoch:  33   step:  138   train loss:  0.0006486551137641072  val loss:  0.661334216594696\n",
      "epoch:  33   step:  139   train loss:  0.0005238163867034018  val loss:  0.6730517148971558\n",
      "epoch:  33   step:  140   train loss:  0.0011511582415550947  val loss:  0.6651446223258972\n",
      "epoch:  33   step:  141   train loss:  0.0023363756481558084  val loss:  0.658069908618927\n",
      "epoch:  33   step:  142   train loss:  0.0006294661434367299  val loss:  0.6560317873954773\n",
      "epoch:  33   step:  143   train loss:  0.00031389162177219987  val loss:  0.6639516353607178\n",
      "epoch:  33   step:  144   train loss:  0.0005152106750756502  val loss:  0.6518323421478271\n",
      "epoch:  33   step:  145   train loss:  0.0030416594818234444  val loss:  0.6393942832946777\n",
      "epoch:  33   step:  146   train loss:  0.0007449575932696462  val loss:  0.6412795782089233\n",
      "epoch:  33   step:  147   train loss:  0.00045693659922108054  val loss:  0.641866147518158\n",
      "epoch:  33   step:  148   train loss:  0.0009677648777142167  val loss:  0.6390398144721985\n",
      "epoch:  33   step:  149   train loss:  0.0007923591183498502  val loss:  0.6391786336898804\n",
      "epoch:  33   step:  150   train loss:  0.00044920554501004517  val loss:  0.637725830078125\n",
      "epoch:  33   step:  151   train loss:  0.0003497081343084574  val loss:  0.6402880549430847\n",
      "epoch:  33   step:  152   train loss:  0.0005764006054960191  val loss:  0.6476206183433533\n",
      "epoch:  33   step:  153   train loss:  0.00044106546556577086  val loss:  0.6596930623054504\n",
      "epoch:  33   step:  154   train loss:  0.000736287038307637  val loss:  0.659687340259552\n",
      "epoch:  33   step:  155   train loss:  0.0013822645414620638  val loss:  0.6593655943870544\n",
      "epoch:  33   step:  156   train loss:  0.0010038954205811024  val loss:  0.6598974466323853\n",
      "epoch:  33   step:  157   train loss:  0.0014488212764263153  val loss:  0.653133749961853\n",
      "epoch:  33   step:  158   train loss:  0.00047062610974535346  val loss:  0.658561110496521\n",
      "epoch:  33   step:  159   train loss:  0.00037330909981392324  val loss:  0.6498987674713135\n",
      "epoch:  33   step:  160   train loss:  0.0007709548808634281  val loss:  0.6488233804702759\n",
      "epoch:  33   step:  161   train loss:  0.0006503306212835014  val loss:  0.6523895859718323\n",
      "epoch:  33   step:  162   train loss:  0.00047252397052943707  val loss:  0.6525211930274963\n",
      "epoch:  33   step:  163   train loss:  0.0008381694788113236  val loss:  0.6542162895202637\n",
      "epoch:  33   step:  164   train loss:  0.0002625391643960029  val loss:  0.6450603604316711\n",
      "epoch:  33   step:  165   train loss:  0.0008771887514740229  val loss:  0.6630110144615173\n",
      "epoch:  34   step:  0   train loss:  0.0005322325741872191  val loss:  0.6579168438911438\n",
      "epoch:  34   step:  1   train loss:  0.00041717724525369704  val loss:  0.6545912027359009\n",
      "epoch:  34   step:  2   train loss:  0.0004880228661932051  val loss:  0.6713837385177612\n",
      "epoch:  34   step:  3   train loss:  0.0005166184855625033  val loss:  0.6731646656990051\n",
      "epoch:  34   step:  4   train loss:  0.00032283575274050236  val loss:  0.6698019504547119\n",
      "epoch:  34   step:  5   train loss:  0.0005961303832009435  val loss:  0.6800638437271118\n",
      "epoch:  34   step:  6   train loss:  0.000834887265227735  val loss:  0.6737880706787109\n",
      "epoch:  34   step:  7   train loss:  0.0005595386028289795  val loss:  0.6748577952384949\n",
      "epoch:  34   step:  8   train loss:  0.0002593371318653226  val loss:  0.6780145168304443\n",
      "epoch:  34   step:  9   train loss:  0.000432740111136809  val loss:  0.6814612150192261\n",
      "epoch:  34   step:  10   train loss:  0.00016664157737977803  val loss:  0.6811386942863464\n",
      "epoch:  34   step:  11   train loss:  0.00023709396191406995  val loss:  0.6871709227561951\n",
      "epoch:  34   step:  12   train loss:  0.0010418309830129147  val loss:  0.6718332767486572\n",
      "epoch:  34   step:  13   train loss:  0.0038045570254325867  val loss:  0.6917672753334045\n",
      "epoch:  34   step:  14   train loss:  0.0007914061425253749  val loss:  0.6876367330551147\n",
      "epoch:  34   step:  15   train loss:  0.0008287562523037195  val loss:  0.6960231065750122\n",
      "epoch:  34   step:  16   train loss:  0.0005429101875051856  val loss:  0.7035626173019409\n",
      "epoch:  34   step:  17   train loss:  0.0004168527666479349  val loss:  0.7010500431060791\n",
      "epoch:  34   step:  18   train loss:  0.00023444821999873966  val loss:  0.6911982297897339\n",
      "epoch:  34   step:  19   train loss:  0.00027338910149410367  val loss:  0.68234783411026\n",
      "epoch:  34   step:  20   train loss:  0.0008965819724835455  val loss:  0.6976240277290344\n",
      "epoch:  34   step:  21   train loss:  0.0003579663170967251  val loss:  0.6948879957199097\n",
      "epoch:  34   step:  22   train loss:  0.0005506385350599885  val loss:  0.6968600153923035\n",
      "epoch:  34   step:  23   train loss:  0.0004382600891403854  val loss:  0.6846728324890137\n",
      "epoch:  34   step:  24   train loss:  0.0004025436064694077  val loss:  0.6876509189605713\n",
      "epoch:  34   step:  25   train loss:  0.0005719844484701753  val loss:  0.6932594776153564\n",
      "epoch:  34   step:  26   train loss:  0.001004726393148303  val loss:  0.6992857456207275\n",
      "epoch:  34   step:  27   train loss:  0.000244502502027899  val loss:  0.6991210579872131\n",
      "epoch:  34   step:  28   train loss:  0.0007384318159893155  val loss:  0.7001187205314636\n",
      "epoch:  34   step:  29   train loss:  0.0013625511201098561  val loss:  0.6893147230148315\n",
      "epoch:  34   step:  30   train loss:  0.00041269668145105243  val loss:  0.6895155310630798\n",
      "epoch:  34   step:  31   train loss:  0.0004904448287561536  val loss:  0.6829221248626709\n",
      "epoch:  34   step:  32   train loss:  0.0010202230187132955  val loss:  0.6842750906944275\n",
      "epoch:  34   step:  33   train loss:  0.00047921534860506654  val loss:  0.6794467568397522\n",
      "epoch:  34   step:  34   train loss:  0.00016396056162193418  val loss:  0.6788564324378967\n",
      "epoch:  34   step:  35   train loss:  0.0004318070423323661  val loss:  0.671991765499115\n",
      "epoch:  34   step:  36   train loss:  0.00035913518513552845  val loss:  0.6719284057617188\n",
      "epoch:  34   step:  37   train loss:  0.000438544200733304  val loss:  0.6693261861801147\n",
      "epoch:  34   step:  38   train loss:  0.00023779660114087164  val loss:  0.6588437557220459\n",
      "epoch:  34   step:  39   train loss:  0.00034681506804190576  val loss:  0.6610633134841919\n",
      "epoch:  34   step:  40   train loss:  0.0003873974201269448  val loss:  0.65976881980896\n",
      "epoch:  34   step:  41   train loss:  0.0004909099079668522  val loss:  0.6553051471710205\n",
      "epoch:  34   step:  42   train loss:  0.0004222513525746763  val loss:  0.6509361267089844\n",
      "epoch:  34   step:  43   train loss:  0.00023352514836005867  val loss:  0.6589716672897339\n",
      "epoch:  34   step:  44   train loss:  0.0029863088857382536  val loss:  0.659917414188385\n",
      "epoch:  34   step:  45   train loss:  0.0005998924607411027  val loss:  0.6647284626960754\n",
      "epoch:  34   step:  46   train loss:  0.0006509273662231863  val loss:  0.6654990315437317\n",
      "epoch:  34   step:  47   train loss:  0.0013649947941303253  val loss:  0.6718312501907349\n",
      "epoch:  34   step:  48   train loss:  0.0007313966052606702  val loss:  0.6620897054672241\n",
      "epoch:  34   step:  49   train loss:  0.0005185505142435431  val loss:  0.657307505607605\n",
      "epoch:  34   step:  50   train loss:  0.0002846214920282364  val loss:  0.6690681576728821\n",
      "epoch:  34   step:  51   train loss:  0.0004436264862306416  val loss:  0.6686210632324219\n",
      "epoch:  34   step:  52   train loss:  0.0007808084483258426  val loss:  0.6684610843658447\n",
      "epoch:  34   step:  53   train loss:  0.001350081292912364  val loss:  0.6629313230514526\n",
      "epoch:  34   step:  54   train loss:  0.0018513480899855494  val loss:  0.6644915342330933\n",
      "epoch:  34   step:  55   train loss:  0.00031836976995691657  val loss:  0.6702651977539062\n",
      "epoch:  34   step:  56   train loss:  0.00030605908250436187  val loss:  0.6754052639007568\n",
      "epoch:  34   step:  57   train loss:  0.0009634083253331482  val loss:  0.6709800362586975\n",
      "epoch:  34   step:  58   train loss:  0.0004795916029252112  val loss:  0.6676002144813538\n",
      "epoch:  34   step:  59   train loss:  0.00030366514693014324  val loss:  0.6687764525413513\n",
      "epoch:  34   step:  60   train loss:  0.0025650514289736748  val loss:  0.6671829223632812\n",
      "epoch:  34   step:  61   train loss:  0.00048356797196902335  val loss:  0.6686388254165649\n",
      "epoch:  34   step:  62   train loss:  0.0006241544615477324  val loss:  0.6616156697273254\n",
      "epoch:  34   step:  63   train loss:  0.0006469845538958907  val loss:  0.6604554057121277\n",
      "epoch:  34   step:  64   train loss:  0.00043956312583759427  val loss:  0.6654845476150513\n",
      "epoch:  34   step:  65   train loss:  0.0010825702920556068  val loss:  0.6687076687812805\n",
      "epoch:  34   step:  66   train loss:  0.00030392681946977973  val loss:  0.669676661491394\n",
      "epoch:  34   step:  67   train loss:  0.0008379406062886119  val loss:  0.6736083030700684\n",
      "epoch:  34   step:  68   train loss:  0.0007069401326589286  val loss:  0.6789303421974182\n",
      "epoch:  34   step:  69   train loss:  0.000244651862885803  val loss:  0.6754095554351807\n",
      "epoch:  34   step:  70   train loss:  0.00058990519028157  val loss:  0.6636709570884705\n",
      "epoch:  34   step:  71   train loss:  0.0005149665521457791  val loss:  0.667439877986908\n",
      "epoch:  34   step:  72   train loss:  0.00014832236047368497  val loss:  0.6607032418251038\n",
      "epoch:  34   step:  73   train loss:  0.0007942934753373265  val loss:  0.6586705446243286\n",
      "epoch:  34   step:  74   train loss:  0.00021718049538321793  val loss:  0.6771023273468018\n",
      "epoch:  34   step:  75   train loss:  0.0005399691290222108  val loss:  0.6772167682647705\n",
      "epoch:  34   step:  76   train loss:  0.0005290395929478109  val loss:  0.6836451292037964\n",
      "epoch:  34   step:  77   train loss:  0.00038612299249507487  val loss:  0.6873090267181396\n",
      "epoch:  34   step:  78   train loss:  0.00039941331488080323  val loss:  0.6876053214073181\n",
      "epoch:  34   step:  79   train loss:  0.00018643919611349702  val loss:  0.6850556135177612\n",
      "epoch:  34   step:  80   train loss:  0.0008228236110880971  val loss:  0.6744658350944519\n",
      "epoch:  34   step:  81   train loss:  0.0004321944434195757  val loss:  0.6680669188499451\n",
      "epoch:  34   step:  82   train loss:  0.000733730907086283  val loss:  0.6838755011558533\n",
      "epoch:  34   step:  83   train loss:  0.0012781950645148754  val loss:  0.6765174865722656\n",
      "epoch:  34   step:  84   train loss:  0.00037377997068688273  val loss:  0.6745027899742126\n",
      "epoch:  34   step:  85   train loss:  0.00040507089579477906  val loss:  0.6718693375587463\n",
      "epoch:  34   step:  86   train loss:  0.0005759405321441591  val loss:  0.6660844087600708\n",
      "epoch:  34   step:  87   train loss:  0.003145686350762844  val loss:  0.6549257636070251\n",
      "epoch:  34   step:  88   train loss:  0.00045103253796696663  val loss:  0.670658528804779\n",
      "epoch:  34   step:  89   train loss:  0.0006304819835349917  val loss:  0.6736730337142944\n",
      "epoch:  34   step:  90   train loss:  0.0005493212956935167  val loss:  0.6746490597724915\n",
      "epoch:  34   step:  91   train loss:  0.0003987298987340182  val loss:  0.6761044859886169\n",
      "epoch:  34   step:  92   train loss:  0.0003827658947557211  val loss:  0.6779373288154602\n",
      "epoch:  34   step:  93   train loss:  0.00045230903197079897  val loss:  0.6786361336708069\n",
      "epoch:  34   step:  94   train loss:  0.00031081982888281345  val loss:  0.6780253648757935\n",
      "epoch:  34   step:  95   train loss:  0.00044168074964545667  val loss:  0.6757228970527649\n",
      "epoch:  34   step:  96   train loss:  0.0006908015348017216  val loss:  0.6748486161231995\n",
      "epoch:  34   step:  97   train loss:  0.0005350791034288704  val loss:  0.6731271147727966\n",
      "epoch:  34   step:  98   train loss:  0.000709755695424974  val loss:  0.6713677644729614\n",
      "epoch:  34   step:  99   train loss:  0.0008038548985496163  val loss:  0.6770815849304199\n",
      "epoch:  34   step:  100   train loss:  0.00029630574863404036  val loss:  0.677306056022644\n",
      "epoch:  34   step:  101   train loss:  0.0009595277952030301  val loss:  0.6866167783737183\n",
      "epoch:  34   step:  102   train loss:  0.00032500881934538484  val loss:  0.6882997155189514\n",
      "epoch:  34   step:  103   train loss:  0.0002607456408441067  val loss:  0.6961921453475952\n",
      "epoch:  34   step:  104   train loss:  0.0002261144109070301  val loss:  0.6907394528388977\n",
      "epoch:  34   step:  105   train loss:  0.0003778681275434792  val loss:  0.6964626908302307\n",
      "epoch:  34   step:  106   train loss:  0.0002648067893460393  val loss:  0.6961061954498291\n",
      "epoch:  34   step:  107   train loss:  0.0008124395972117782  val loss:  0.6959770321846008\n",
      "epoch:  34   step:  108   train loss:  0.0007170132012106478  val loss:  0.6892175674438477\n",
      "epoch:  34   step:  109   train loss:  0.0011674781562760472  val loss:  0.681743323802948\n",
      "epoch:  34   step:  110   train loss:  0.0005411885213106871  val loss:  0.6879477500915527\n",
      "epoch:  34   step:  111   train loss:  0.0002922172425314784  val loss:  0.6845083236694336\n",
      "epoch:  34   step:  112   train loss:  0.0005196452257223427  val loss:  0.6904400587081909\n",
      "epoch:  34   step:  113   train loss:  0.0006811765488237143  val loss:  0.6963157653808594\n",
      "epoch:  34   step:  114   train loss:  0.0006741747492924333  val loss:  0.7071263194084167\n",
      "epoch:  34   step:  115   train loss:  0.0005038309027440846  val loss:  0.7153263688087463\n",
      "epoch:  34   step:  116   train loss:  0.00039778780774213374  val loss:  0.7148122191429138\n",
      "epoch:  34   step:  117   train loss:  0.00019330006034579128  val loss:  0.7067473530769348\n",
      "epoch:  34   step:  118   train loss:  0.00038578527164645493  val loss:  0.701565682888031\n",
      "epoch:  34   step:  119   train loss:  0.000511260237544775  val loss:  0.6977152228355408\n",
      "epoch:  34   step:  120   train loss:  0.0017884779954329133  val loss:  0.6976632475852966\n",
      "epoch:  34   step:  121   train loss:  0.0005498980171978474  val loss:  0.698837399482727\n",
      "epoch:  34   step:  122   train loss:  0.001278152223676443  val loss:  0.6885703802108765\n",
      "epoch:  34   step:  123   train loss:  0.00027318656793795526  val loss:  0.6910749077796936\n",
      "epoch:  34   step:  124   train loss:  0.0001902854855870828  val loss:  0.6884729862213135\n",
      "epoch:  34   step:  125   train loss:  0.0003165569214615971  val loss:  0.690588116645813\n",
      "epoch:  34   step:  126   train loss:  0.0008343281224370003  val loss:  0.6781173348426819\n",
      "epoch:  34   step:  127   train loss:  0.0011943522840738297  val loss:  0.6714633703231812\n",
      "epoch:  34   step:  128   train loss:  0.0006580660701729357  val loss:  0.6788599491119385\n",
      "epoch:  34   step:  129   train loss:  0.0009364127181470394  val loss:  0.6854507327079773\n",
      "epoch:  34   step:  130   train loss:  0.00045031955232843757  val loss:  0.686284601688385\n",
      "epoch:  34   step:  131   train loss:  0.00033554379479028285  val loss:  0.6969854831695557\n",
      "epoch:  34   step:  132   train loss:  0.0003351643099449575  val loss:  0.7008206248283386\n",
      "epoch:  34   step:  133   train loss:  0.00013943458907306194  val loss:  0.7052268981933594\n",
      "epoch:  34   step:  134   train loss:  0.00035982183180749416  val loss:  0.7006274461746216\n",
      "epoch:  34   step:  135   train loss:  0.00021226712851785123  val loss:  0.6989637017250061\n",
      "epoch:  34   step:  136   train loss:  0.00051694962894544  val loss:  0.6965687870979309\n",
      "epoch:  34   step:  137   train loss:  0.0002095580566674471  val loss:  0.6950017213821411\n",
      "epoch:  34   step:  138   train loss:  0.0003185464011039585  val loss:  0.6959204077720642\n",
      "epoch:  34   step:  139   train loss:  0.0002666852669790387  val loss:  0.6903795003890991\n",
      "epoch:  34   step:  140   train loss:  0.00046057780855335295  val loss:  0.6910580396652222\n",
      "epoch:  34   step:  141   train loss:  0.00031565933022648096  val loss:  0.6895416975021362\n",
      "epoch:  34   step:  142   train loss:  0.0009680156945250928  val loss:  0.6895807981491089\n",
      "epoch:  34   step:  143   train loss:  0.000516552827320993  val loss:  0.7000274658203125\n",
      "epoch:  34   step:  144   train loss:  0.0004802817420568317  val loss:  0.7030289769172668\n",
      "epoch:  34   step:  145   train loss:  0.0002850649761967361  val loss:  0.7020841240882874\n",
      "epoch:  34   step:  146   train loss:  0.0005923866992816329  val loss:  0.7105759978294373\n",
      "epoch:  34   step:  147   train loss:  0.0005882589030079544  val loss:  0.723318338394165\n",
      "epoch:  34   step:  148   train loss:  0.0004579388187266886  val loss:  0.7280171513557434\n",
      "epoch:  34   step:  149   train loss:  0.00048743479419499636  val loss:  0.7205209732055664\n",
      "epoch:  34   step:  150   train loss:  0.0004000691697001457  val loss:  0.7294180989265442\n",
      "epoch:  34   step:  151   train loss:  0.0003653877356555313  val loss:  0.7302935719490051\n",
      "epoch:  34   step:  152   train loss:  0.0012063593603670597  val loss:  0.7376422882080078\n",
      "epoch:  34   step:  153   train loss:  0.0002587650087662041  val loss:  0.7471151351928711\n",
      "epoch:  34   step:  154   train loss:  0.0002627788344398141  val loss:  0.7468521595001221\n",
      "epoch:  34   step:  155   train loss:  0.00019457898451946676  val loss:  0.7362074255943298\n",
      "epoch:  34   step:  156   train loss:  0.000306443776935339  val loss:  0.7267752885818481\n",
      "epoch:  34   step:  157   train loss:  0.0003809104091487825  val loss:  0.7216737270355225\n",
      "epoch:  34   step:  158   train loss:  0.0005007954896427691  val loss:  0.7239693999290466\n",
      "epoch:  34   step:  159   train loss:  0.0004269105556886643  val loss:  0.7160118222236633\n",
      "epoch:  34   step:  160   train loss:  0.00020888951257802546  val loss:  0.71421879529953\n",
      "epoch:  34   step:  161   train loss:  0.0009184195077978075  val loss:  0.7038073539733887\n",
      "epoch:  34   step:  162   train loss:  0.0005730876000598073  val loss:  0.6951712369918823\n",
      "epoch:  34   step:  163   train loss:  0.0002883996639866382  val loss:  0.7013877034187317\n",
      "epoch:  34   step:  164   train loss:  0.00022297550458461046  val loss:  0.7040440440177917\n",
      "epoch:  34   step:  165   train loss:  0.0018105767667293549  val loss:  0.7185748219490051\n",
      "epoch:  35   step:  0   train loss:  0.00036431950866244733  val loss:  0.7049846649169922\n",
      "epoch:  35   step:  1   train loss:  0.0002307207032572478  val loss:  0.7029128074645996\n",
      "epoch:  35   step:  2   train loss:  0.0003854677197523415  val loss:  0.6988161206245422\n",
      "epoch:  35   step:  3   train loss:  0.0007754886173643172  val loss:  0.694640040397644\n",
      "epoch:  35   step:  4   train loss:  0.00018997631559614092  val loss:  0.6878932118415833\n",
      "epoch:  35   step:  5   train loss:  0.00034669903106987476  val loss:  0.6789162158966064\n",
      "epoch:  35   step:  6   train loss:  0.0003464658511802554  val loss:  0.6848964691162109\n",
      "epoch:  35   step:  7   train loss:  0.0006405339809134603  val loss:  0.6757882833480835\n",
      "epoch:  35   step:  8   train loss:  0.0002949983172584325  val loss:  0.6772239804267883\n",
      "epoch:  35   step:  9   train loss:  0.0004623454296961427  val loss:  0.68455570936203\n",
      "epoch:  35   step:  10   train loss:  0.00020037966896779835  val loss:  0.6880921125411987\n",
      "epoch:  35   step:  11   train loss:  0.0002952288486994803  val loss:  0.6919610500335693\n",
      "epoch:  35   step:  12   train loss:  0.0003711959579959512  val loss:  0.6930817365646362\n",
      "epoch:  35   step:  13   train loss:  0.0005615932168439031  val loss:  0.6922063827514648\n",
      "epoch:  35   step:  14   train loss:  0.0003483288164716214  val loss:  0.6819205284118652\n",
      "epoch:  35   step:  15   train loss:  0.0002204251941293478  val loss:  0.6780156493186951\n",
      "epoch:  35   step:  16   train loss:  0.0001994665653910488  val loss:  0.6671627759933472\n",
      "epoch:  35   step:  17   train loss:  0.0004173277411609888  val loss:  0.6663732528686523\n",
      "epoch:  35   step:  18   train loss:  0.0008274627034552395  val loss:  0.6761295795440674\n",
      "epoch:  35   step:  19   train loss:  0.00035735557321459055  val loss:  0.6917539834976196\n",
      "epoch:  35   step:  20   train loss:  0.00030325306579470634  val loss:  0.6989242434501648\n",
      "epoch:  35   step:  21   train loss:  0.0003343871794641018  val loss:  0.6953048706054688\n",
      "epoch:  35   step:  22   train loss:  0.0006127672968432307  val loss:  0.7012967467308044\n",
      "epoch:  35   step:  23   train loss:  0.0005723258946090937  val loss:  0.7051463723182678\n",
      "epoch:  35   step:  24   train loss:  0.0004932447918690741  val loss:  0.694524884223938\n",
      "epoch:  35   step:  25   train loss:  0.0003660210350062698  val loss:  0.6968609690666199\n",
      "epoch:  35   step:  26   train loss:  0.00026096327928826213  val loss:  0.6908883452415466\n",
      "epoch:  35   step:  27   train loss:  0.00028076086891815066  val loss:  0.7001888155937195\n",
      "epoch:  35   step:  28   train loss:  0.0003172583528794348  val loss:  0.6964597702026367\n",
      "epoch:  35   step:  29   train loss:  0.0004013765137642622  val loss:  0.7069368362426758\n",
      "epoch:  35   step:  30   train loss:  0.0005239894962869585  val loss:  0.7110818028450012\n",
      "epoch:  35   step:  31   train loss:  0.00021897500846534967  val loss:  0.7117042541503906\n",
      "epoch:  35   step:  32   train loss:  0.00045498035615310073  val loss:  0.7171931266784668\n",
      "epoch:  35   step:  33   train loss:  0.00043861480662599206  val loss:  0.7142327427864075\n",
      "epoch:  35   step:  34   train loss:  0.0005436408100649714  val loss:  0.7090405225753784\n",
      "epoch:  35   step:  35   train loss:  0.0004291737568564713  val loss:  0.7136761546134949\n",
      "epoch:  35   step:  36   train loss:  0.00037998074549250305  val loss:  0.7204606533050537\n",
      "epoch:  35   step:  37   train loss:  0.0003296902868896723  val loss:  0.7218789458274841\n",
      "epoch:  35   step:  38   train loss:  0.0002930070913862437  val loss:  0.7134178280830383\n",
      "epoch:  35   step:  39   train loss:  0.0007987189455889165  val loss:  0.7025931477546692\n",
      "epoch:  35   step:  40   train loss:  0.0005999889108352363  val loss:  0.6869844794273376\n",
      "epoch:  35   step:  41   train loss:  0.0009982724441215396  val loss:  0.6852748990058899\n",
      "epoch:  35   step:  42   train loss:  0.0004964857362210751  val loss:  0.6801785826683044\n",
      "epoch:  35   step:  43   train loss:  0.00027159444289281964  val loss:  0.681163489818573\n",
      "epoch:  35   step:  44   train loss:  0.0003451695083640516  val loss:  0.6813576221466064\n",
      "epoch:  35   step:  45   train loss:  0.0017110884655267  val loss:  0.6756881475448608\n",
      "epoch:  35   step:  46   train loss:  0.00020124559523537755  val loss:  0.6813510060310364\n",
      "epoch:  35   step:  47   train loss:  0.00026041921228170395  val loss:  0.6911839246749878\n",
      "epoch:  35   step:  48   train loss:  0.0003293406334705651  val loss:  0.7007680535316467\n",
      "epoch:  35   step:  49   train loss:  0.00033002416603267193  val loss:  0.6890348196029663\n",
      "epoch:  35   step:  50   train loss:  0.00044579224777407944  val loss:  0.6905597448348999\n",
      "epoch:  35   step:  51   train loss:  0.0005105512100271881  val loss:  0.6966619491577148\n",
      "epoch:  35   step:  52   train loss:  0.0002850597957149148  val loss:  0.701140820980072\n",
      "epoch:  35   step:  53   train loss:  0.0001430800766684115  val loss:  0.6906851530075073\n",
      "epoch:  35   step:  54   train loss:  0.0013060993514955044  val loss:  0.708659827709198\n",
      "epoch:  35   step:  55   train loss:  0.00026597699616104364  val loss:  0.7145204544067383\n",
      "epoch:  35   step:  56   train loss:  0.0005710487021133304  val loss:  0.7311875820159912\n",
      "epoch:  35   step:  57   train loss:  0.0003354476939421147  val loss:  0.7316473722457886\n",
      "epoch:  35   step:  58   train loss:  0.00022861924662720412  val loss:  0.7311072945594788\n",
      "epoch:  35   step:  59   train loss:  0.0006930544041097164  val loss:  0.7230169773101807\n",
      "epoch:  35   step:  60   train loss:  0.0007062629447318614  val loss:  0.7141854763031006\n",
      "epoch:  35   step:  61   train loss:  0.0002377378405071795  val loss:  0.7155707478523254\n",
      "epoch:  35   step:  62   train loss:  0.0005033228080719709  val loss:  0.7134664058685303\n",
      "epoch:  35   step:  63   train loss:  0.00025015772553160787  val loss:  0.7098168134689331\n",
      "epoch:  35   step:  64   train loss:  0.0003114498977083713  val loss:  0.7124836444854736\n",
      "epoch:  35   step:  65   train loss:  0.00033822873956523836  val loss:  0.7056142091751099\n",
      "epoch:  35   step:  66   train loss:  0.0006087068468332291  val loss:  0.7038908004760742\n",
      "epoch:  35   step:  67   train loss:  0.0005102322902530432  val loss:  0.6934759616851807\n",
      "epoch:  35   step:  68   train loss:  0.0004777616122737527  val loss:  0.7100138664245605\n",
      "epoch:  35   step:  69   train loss:  0.000347381632309407  val loss:  0.7124882936477661\n",
      "epoch:  35   step:  70   train loss:  0.0004618086386471987  val loss:  0.7026898264884949\n",
      "epoch:  35   step:  71   train loss:  0.00033932676888071  val loss:  0.7143950462341309\n",
      "epoch:  35   step:  72   train loss:  0.0004223166615702212  val loss:  0.706666111946106\n",
      "epoch:  35   step:  73   train loss:  0.00033009418984875083  val loss:  0.7218005061149597\n",
      "epoch:  35   step:  74   train loss:  0.0007535414770245552  val loss:  0.716358482837677\n",
      "epoch:  35   step:  75   train loss:  0.0005725404480472207  val loss:  0.706289529800415\n",
      "epoch:  35   step:  76   train loss:  0.00026311431429348886  val loss:  0.6969534158706665\n",
      "epoch:  35   step:  77   train loss:  0.0003484821936581284  val loss:  0.6815869212150574\n",
      "epoch:  35   step:  78   train loss:  0.00016687468450982124  val loss:  0.6771300435066223\n",
      "epoch:  35   step:  79   train loss:  0.0007143390830606222  val loss:  0.6759337186813354\n",
      "epoch:  35   step:  80   train loss:  0.0004553215694613755  val loss:  0.6705756783485413\n",
      "epoch:  35   step:  81   train loss:  0.00035026328987441957  val loss:  0.6802303791046143\n",
      "epoch:  35   step:  82   train loss:  0.00018220033962279558  val loss:  0.6795678734779358\n",
      "epoch:  35   step:  83   train loss:  0.00021238667250145227  val loss:  0.680950939655304\n",
      "epoch:  35   step:  84   train loss:  0.0004504409444052726  val loss:  0.6902532577514648\n",
      "epoch:  35   step:  85   train loss:  0.0005121403373777866  val loss:  0.695271909236908\n",
      "epoch:  35   step:  86   train loss:  0.00022823247127234936  val loss:  0.7117525935173035\n",
      "epoch:  35   step:  87   train loss:  0.0005646799108944833  val loss:  0.7103503942489624\n",
      "epoch:  35   step:  88   train loss:  0.0009802160784602165  val loss:  0.7163042426109314\n",
      "epoch:  35   step:  89   train loss:  0.0005780502106063068  val loss:  0.7232721447944641\n",
      "epoch:  35   step:  90   train loss:  0.00015172900748439133  val loss:  0.726527750492096\n",
      "epoch:  35   step:  91   train loss:  0.00048477103700861335  val loss:  0.7370531558990479\n",
      "epoch:  35   step:  92   train loss:  0.00042621151078492403  val loss:  0.7374122738838196\n",
      "epoch:  35   step:  93   train loss:  0.00022879679454490542  val loss:  0.7389810085296631\n",
      "epoch:  35   step:  94   train loss:  0.0013231057673692703  val loss:  0.7597883939743042\n",
      "epoch:  35   step:  95   train loss:  0.000300704239634797  val loss:  0.7672353982925415\n",
      "epoch:  35   step:  96   train loss:  0.0003297966904938221  val loss:  0.7724729776382446\n",
      "epoch:  35   step:  97   train loss:  0.0003032292879652232  val loss:  0.7747027277946472\n",
      "epoch:  35   step:  98   train loss:  0.0003148306277580559  val loss:  0.7701895833015442\n",
      "epoch:  35   step:  99   train loss:  0.0004921468207612634  val loss:  0.7620536088943481\n",
      "epoch:  35   step:  100   train loss:  0.00016202233382500708  val loss:  0.764527440071106\n",
      "epoch:  35   step:  101   train loss:  0.00038976219366304576  val loss:  0.7588447332382202\n",
      "epoch:  35   step:  102   train loss:  0.00026292385882698  val loss:  0.7581008076667786\n",
      "epoch:  35   step:  103   train loss:  0.00033478683326393366  val loss:  0.7528823614120483\n",
      "epoch:  35   step:  104   train loss:  0.0003346366574987769  val loss:  0.7453746795654297\n",
      "epoch:  35   step:  105   train loss:  0.00043761858250945807  val loss:  0.7393105030059814\n",
      "epoch:  35   step:  106   train loss:  0.0003835897077806294  val loss:  0.7474328279495239\n",
      "epoch:  35   step:  107   train loss:  0.000738290254957974  val loss:  0.7435933351516724\n",
      "epoch:  35   step:  108   train loss:  0.0003409752098377794  val loss:  0.7320747375488281\n",
      "epoch:  35   step:  109   train loss:  0.001472181873396039  val loss:  0.7211523056030273\n",
      "epoch:  35   step:  110   train loss:  0.0009486941853538156  val loss:  0.7074316143989563\n",
      "epoch:  35   step:  111   train loss:  0.0003710156015586108  val loss:  0.7034786939620972\n",
      "epoch:  35   step:  112   train loss:  0.00033884303411468863  val loss:  0.7137118577957153\n",
      "epoch:  35   step:  113   train loss:  0.00022571312729269266  val loss:  0.7111876606941223\n",
      "epoch:  35   step:  114   train loss:  0.0006226033437997103  val loss:  0.7081912159919739\n",
      "epoch:  35   step:  115   train loss:  0.0002803959941957146  val loss:  0.701360821723938\n",
      "epoch:  35   step:  116   train loss:  0.001859034295193851  val loss:  0.7072675824165344\n",
      "epoch:  35   step:  117   train loss:  0.0002681959012988955  val loss:  0.7077523469924927\n",
      "epoch:  35   step:  118   train loss:  0.00012337764201220125  val loss:  0.6944015622138977\n",
      "epoch:  35   step:  119   train loss:  0.0003813715884461999  val loss:  0.68660569190979\n",
      "epoch:  35   step:  120   train loss:  0.0001866459206212312  val loss:  0.680815577507019\n",
      "epoch:  35   step:  121   train loss:  0.0002789277059491724  val loss:  0.6905057430267334\n",
      "epoch:  35   step:  122   train loss:  0.0005054048378951848  val loss:  0.6861637234687805\n",
      "epoch:  35   step:  123   train loss:  0.0003899470320902765  val loss:  0.6753416061401367\n",
      "epoch:  35   step:  124   train loss:  0.0007535571348853409  val loss:  0.690113365650177\n",
      "epoch:  35   step:  125   train loss:  0.0004475139721762389  val loss:  0.6881202459335327\n",
      "epoch:  35   step:  126   train loss:  0.0011498009553179145  val loss:  0.682510495185852\n",
      "epoch:  35   step:  127   train loss:  0.00047291978262364864  val loss:  0.6871798634529114\n",
      "epoch:  35   step:  128   train loss:  0.000200354857952334  val loss:  0.6969747543334961\n",
      "epoch:  35   step:  129   train loss:  0.0004135850758757442  val loss:  0.6916595101356506\n",
      "epoch:  35   step:  130   train loss:  0.0008758642943575978  val loss:  0.6935722231864929\n",
      "epoch:  35   step:  131   train loss:  0.0003958644811064005  val loss:  0.6897063255310059\n",
      "epoch:  35   step:  132   train loss:  0.0004044685629196465  val loss:  0.6827306747436523\n",
      "epoch:  35   step:  133   train loss:  0.00017637532437220216  val loss:  0.6853050589561462\n",
      "epoch:  35   step:  134   train loss:  0.0002754907472990453  val loss:  0.6927391886711121\n",
      "epoch:  35   step:  135   train loss:  0.002309362171217799  val loss:  0.6864182949066162\n",
      "epoch:  35   step:  136   train loss:  0.0003334406646899879  val loss:  0.6850405931472778\n",
      "epoch:  35   step:  137   train loss:  0.0005205966299399734  val loss:  0.683512806892395\n",
      "epoch:  35   step:  138   train loss:  0.000370834197383374  val loss:  0.6829602718353271\n",
      "epoch:  35   step:  139   train loss:  0.0006216004258021712  val loss:  0.6743466258049011\n",
      "epoch:  35   step:  140   train loss:  0.00017854059115052223  val loss:  0.6876000165939331\n",
      "epoch:  35   step:  141   train loss:  0.0004910991992801428  val loss:  0.6996141076087952\n",
      "epoch:  35   step:  142   train loss:  0.0002570876677054912  val loss:  0.6988281011581421\n",
      "epoch:  35   step:  143   train loss:  0.0007272734073922038  val loss:  0.694750189781189\n",
      "epoch:  35   step:  144   train loss:  0.0002124877937603742  val loss:  0.702957034111023\n",
      "epoch:  35   step:  145   train loss:  0.0004012869030702859  val loss:  0.700958251953125\n",
      "epoch:  35   step:  146   train loss:  0.00041469233110547066  val loss:  0.7010027766227722\n",
      "epoch:  35   step:  147   train loss:  0.00044993014307692647  val loss:  0.6953928470611572\n",
      "epoch:  35   step:  148   train loss:  0.00047188912867568433  val loss:  0.6932294964790344\n",
      "epoch:  35   step:  149   train loss:  0.00013851623225491494  val loss:  0.6971138715744019\n",
      "epoch:  35   step:  150   train loss:  0.0003411754150874913  val loss:  0.692297101020813\n",
      "epoch:  35   step:  151   train loss:  0.0004687721375375986  val loss:  0.6876477003097534\n",
      "epoch:  35   step:  152   train loss:  0.0008013197802938521  val loss:  0.6852849721908569\n",
      "epoch:  35   step:  153   train loss:  0.0002369697467656806  val loss:  0.692075788974762\n",
      "epoch:  35   step:  154   train loss:  0.0007635807851329446  val loss:  0.6846430897712708\n",
      "epoch:  35   step:  155   train loss:  0.0003713240148499608  val loss:  0.688202440738678\n",
      "epoch:  35   step:  156   train loss:  0.00024309344007633626  val loss:  0.6925249099731445\n",
      "epoch:  35   step:  157   train loss:  0.00011330289271427318  val loss:  0.6970460414886475\n",
      "epoch:  35   step:  158   train loss:  0.00037913554115220904  val loss:  0.7007318139076233\n",
      "epoch:  35   step:  159   train loss:  0.0005593314999714494  val loss:  0.6994086503982544\n",
      "epoch:  35   step:  160   train loss:  0.00038302765460684896  val loss:  0.7081958055496216\n",
      "epoch:  35   step:  161   train loss:  0.0003774059296119958  val loss:  0.7104218006134033\n",
      "epoch:  35   step:  162   train loss:  0.000847845571115613  val loss:  0.7058688402175903\n",
      "epoch:  35   step:  163   train loss:  0.0004860241897404194  val loss:  0.6990816593170166\n",
      "epoch:  35   step:  164   train loss:  0.00021769542945548892  val loss:  0.7065636515617371\n",
      "epoch:  35   step:  165   train loss:  0.0002366316766710952  val loss:  0.6983830332756042\n",
      "epoch:  36   step:  0   train loss:  0.0006478318246081471  val loss:  0.6948648691177368\n",
      "epoch:  36   step:  1   train loss:  0.0003974806750193238  val loss:  0.6871258616447449\n",
      "epoch:  36   step:  2   train loss:  0.0003971524420194328  val loss:  0.7068060040473938\n",
      "epoch:  36   step:  3   train loss:  0.00025874178390949965  val loss:  0.7017396092414856\n",
      "epoch:  36   step:  4   train loss:  0.0002738014154601842  val loss:  0.701061487197876\n",
      "epoch:  36   step:  5   train loss:  0.00018557239673100412  val loss:  0.6972816586494446\n",
      "epoch:  36   step:  6   train loss:  0.0005081634153611958  val loss:  0.7046183347702026\n",
      "epoch:  36   step:  7   train loss:  0.0009994740830734372  val loss:  0.6977719068527222\n",
      "epoch:  36   step:  8   train loss:  0.00017989202751778066  val loss:  0.7013155817985535\n",
      "epoch:  36   step:  9   train loss:  0.00011244280176470056  val loss:  0.6996806859970093\n",
      "epoch:  36   step:  10   train loss:  0.0003705648414324969  val loss:  0.6963078379631042\n",
      "epoch:  36   step:  11   train loss:  0.00029895093757659197  val loss:  0.7190372943878174\n",
      "epoch:  36   step:  12   train loss:  0.00029879220528528094  val loss:  0.7203279733657837\n",
      "epoch:  36   step:  13   train loss:  0.00041965971468016505  val loss:  0.7175214290618896\n",
      "epoch:  36   step:  14   train loss:  0.00027668237453326583  val loss:  0.7267705202102661\n",
      "epoch:  36   step:  15   train loss:  0.000227191427256912  val loss:  0.7135735750198364\n",
      "epoch:  36   step:  16   train loss:  0.00022749508207198232  val loss:  0.7173044681549072\n",
      "epoch:  36   step:  17   train loss:  0.00034726716694422066  val loss:  0.7165298461914062\n",
      "epoch:  36   step:  18   train loss:  0.00047103501856327057  val loss:  0.7057051658630371\n",
      "epoch:  36   step:  19   train loss:  0.0001803428604034707  val loss:  0.7086269855499268\n",
      "epoch:  36   step:  20   train loss:  0.0005667248624376953  val loss:  0.7059857249259949\n",
      "epoch:  36   step:  21   train loss:  0.00033176291617564857  val loss:  0.7096021771430969\n",
      "epoch:  36   step:  22   train loss:  0.00015602947678416967  val loss:  0.7086503505706787\n",
      "epoch:  36   step:  23   train loss:  0.0003171728167217225  val loss:  0.7108064293861389\n",
      "epoch:  36   step:  24   train loss:  0.0003927119541913271  val loss:  0.7085860967636108\n",
      "epoch:  36   step:  25   train loss:  0.00041887073894031346  val loss:  0.7016342282295227\n",
      "epoch:  36   step:  26   train loss:  0.0003168614348396659  val loss:  0.7159562706947327\n",
      "epoch:  36   step:  27   train loss:  0.000553982681594789  val loss:  0.7257217764854431\n",
      "epoch:  36   step:  28   train loss:  0.00038389413384720683  val loss:  0.7427147030830383\n",
      "epoch:  36   step:  29   train loss:  0.00028377832495607436  val loss:  0.741736114025116\n",
      "epoch:  36   step:  30   train loss:  0.00016916758613660932  val loss:  0.7396463751792908\n",
      "epoch:  36   step:  31   train loss:  0.0005428323638625443  val loss:  0.7409700751304626\n",
      "epoch:  36   step:  32   train loss:  0.0007694580126553774  val loss:  0.7356134057044983\n",
      "epoch:  36   step:  33   train loss:  0.00025401206221431494  val loss:  0.7188599705696106\n",
      "epoch:  36   step:  34   train loss:  0.00025697489036247134  val loss:  0.7206982374191284\n",
      "epoch:  36   step:  35   train loss:  0.00021142743935342878  val loss:  0.7275151014328003\n",
      "epoch:  36   step:  36   train loss:  0.0003125747898593545  val loss:  0.7165305018424988\n",
      "epoch:  36   step:  37   train loss:  0.00015697715571150184  val loss:  0.7256639003753662\n",
      "epoch:  36   step:  38   train loss:  0.0013634971110150218  val loss:  0.7046632766723633\n",
      "epoch:  36   step:  39   train loss:  0.0009633912704885006  val loss:  0.7179489135742188\n",
      "epoch:  36   step:  40   train loss:  0.000526266812812537  val loss:  0.7317603826522827\n",
      "epoch:  36   step:  41   train loss:  0.0006366363959386945  val loss:  0.744913637638092\n",
      "epoch:  36   step:  42   train loss:  0.00024634276633150876  val loss:  0.7362186908721924\n",
      "epoch:  36   step:  43   train loss:  0.0004037503676954657  val loss:  0.7336302995681763\n",
      "epoch:  36   step:  44   train loss:  0.00034381545265205204  val loss:  0.7341454029083252\n",
      "epoch:  36   step:  45   train loss:  0.00017910185852088034  val loss:  0.7376349568367004\n",
      "epoch:  36   step:  46   train loss:  0.0006521558971144259  val loss:  0.7215647101402283\n",
      "epoch:  36   step:  47   train loss:  0.0002892223128583282  val loss:  0.7233450412750244\n",
      "epoch:  36   step:  48   train loss:  0.0002394338371232152  val loss:  0.7300869822502136\n",
      "epoch:  36   step:  49   train loss:  0.00020094765932299197  val loss:  0.7200491428375244\n",
      "epoch:  36   step:  50   train loss:  0.0002898094244301319  val loss:  0.7323232293128967\n",
      "epoch:  36   step:  51   train loss:  0.00033101747976616025  val loss:  0.7255064845085144\n",
      "epoch:  36   step:  52   train loss:  0.0003749816969502717  val loss:  0.7162362337112427\n",
      "epoch:  36   step:  53   train loss:  0.00019126161350868642  val loss:  0.7194143533706665\n",
      "epoch:  36   step:  54   train loss:  0.0010187994921579957  val loss:  0.7273191809654236\n",
      "epoch:  36   step:  55   train loss:  0.000135836613480933  val loss:  0.7265067100524902\n",
      "epoch:  36   step:  56   train loss:  0.00022163393441587687  val loss:  0.7235062122344971\n",
      "epoch:  36   step:  57   train loss:  0.00045025371946394444  val loss:  0.7229396104812622\n",
      "epoch:  36   step:  58   train loss:  0.00031952091376297176  val loss:  0.7200334668159485\n",
      "epoch:  36   step:  59   train loss:  0.00016608048463240266  val loss:  0.7199843525886536\n",
      "epoch:  36   step:  60   train loss:  0.0002783403906505555  val loss:  0.7248652577400208\n",
      "epoch:  36   step:  61   train loss:  0.00022473506396636367  val loss:  0.7133554816246033\n",
      "epoch:  36   step:  62   train loss:  0.0008579482091590762  val loss:  0.7006399631500244\n",
      "epoch:  36   step:  63   train loss:  0.00022410924430005252  val loss:  0.6955928802490234\n",
      "epoch:  36   step:  64   train loss:  0.00029438029741868377  val loss:  0.6883989572525024\n",
      "epoch:  36   step:  65   train loss:  0.000372482871171087  val loss:  0.6976533532142639\n",
      "epoch:  36   step:  66   train loss:  0.000507441523950547  val loss:  0.7022994756698608\n",
      "epoch:  36   step:  67   train loss:  0.0002712517452891916  val loss:  0.6983383297920227\n",
      "epoch:  36   step:  68   train loss:  0.0003138130996376276  val loss:  0.6973627805709839\n",
      "epoch:  36   step:  69   train loss:  0.001039658673107624  val loss:  0.6982190608978271\n",
      "epoch:  36   step:  70   train loss:  0.00046132353600114584  val loss:  0.7014824748039246\n",
      "epoch:  36   step:  71   train loss:  0.0004984888364560902  val loss:  0.7126603722572327\n",
      "epoch:  36   step:  72   train loss:  0.0002823182148858905  val loss:  0.7115618586540222\n",
      "epoch:  36   step:  73   train loss:  0.0001293201930820942  val loss:  0.6994435787200928\n",
      "epoch:  36   step:  74   train loss:  9.58860109676607e-05  val loss:  0.6928275227546692\n",
      "epoch:  36   step:  75   train loss:  0.00022355405963025987  val loss:  0.6964457631111145\n",
      "epoch:  36   step:  76   train loss:  0.00020070963364560157  val loss:  0.6931169629096985\n",
      "epoch:  36   step:  77   train loss:  0.00047730343067087233  val loss:  0.6976147294044495\n",
      "epoch:  36   step:  78   train loss:  0.0005195335252210498  val loss:  0.7000822424888611\n",
      "epoch:  36   step:  79   train loss:  0.0003643419477157295  val loss:  0.701423168182373\n",
      "epoch:  36   step:  80   train loss:  0.0013684248551726341  val loss:  0.7124934792518616\n",
      "epoch:  36   step:  81   train loss:  8.871476165950298e-05  val loss:  0.7207452058792114\n",
      "epoch:  36   step:  82   train loss:  0.00012617820175364614  val loss:  0.7199581861495972\n",
      "epoch:  36   step:  83   train loss:  0.0003994910221081227  val loss:  0.7290039658546448\n",
      "epoch:  36   step:  84   train loss:  0.0007744558388367295  val loss:  0.7259753942489624\n",
      "epoch:  36   step:  85   train loss:  0.00032181863207370043  val loss:  0.7299889922142029\n",
      "epoch:  36   step:  86   train loss:  0.00025335210375487804  val loss:  0.7341177463531494\n",
      "epoch:  36   step:  87   train loss:  0.00025063299108296633  val loss:  0.7286823987960815\n",
      "epoch:  36   step:  88   train loss:  0.00029202314908616245  val loss:  0.7330938577651978\n",
      "epoch:  36   step:  89   train loss:  0.0001868534309323877  val loss:  0.7325781583786011\n",
      "epoch:  36   step:  90   train loss:  0.00048394728219136596  val loss:  0.7253055572509766\n",
      "epoch:  36   step:  91   train loss:  0.0003645814140327275  val loss:  0.7247009873390198\n",
      "epoch:  36   step:  92   train loss:  0.000216495172935538  val loss:  0.7302566170692444\n",
      "epoch:  36   step:  93   train loss:  0.0007341249147430062  val loss:  0.7429074048995972\n",
      "epoch:  36   step:  94   train loss:  0.0004341034800745547  val loss:  0.7420330047607422\n",
      "epoch:  36   step:  95   train loss:  0.0001818675664253533  val loss:  0.7335562705993652\n",
      "epoch:  36   step:  96   train loss:  0.0010244797449558973  val loss:  0.7146125435829163\n",
      "epoch:  36   step:  97   train loss:  0.00031144279637373984  val loss:  0.7109735012054443\n",
      "epoch:  36   step:  98   train loss:  0.0001539043732918799  val loss:  0.7200926542282104\n",
      "epoch:  36   step:  99   train loss:  0.00022362213348969817  val loss:  0.7290205359458923\n",
      "epoch:  36   step:  100   train loss:  0.000266017799731344  val loss:  0.7299699783325195\n",
      "epoch:  36   step:  101   train loss:  0.0003516811993904412  val loss:  0.7348102927207947\n",
      "epoch:  36   step:  102   train loss:  0.00033372570760548115  val loss:  0.7306519746780396\n",
      "epoch:  36   step:  103   train loss:  0.0003826489264611155  val loss:  0.723980188369751\n",
      "epoch:  36   step:  104   train loss:  0.00040032967808656394  val loss:  0.7261703610420227\n",
      "epoch:  36   step:  105   train loss:  0.0002012114564422518  val loss:  0.7196444272994995\n",
      "epoch:  36   step:  106   train loss:  0.0004911634605377913  val loss:  0.7161905765533447\n",
      "epoch:  36   step:  107   train loss:  0.0003205504617653787  val loss:  0.7131102085113525\n",
      "epoch:  36   step:  108   train loss:  0.0019181895768269897  val loss:  0.7104622721672058\n",
      "epoch:  36   step:  109   train loss:  0.00020992342615500093  val loss:  0.708351731300354\n",
      "epoch:  36   step:  110   train loss:  0.00031858895090408623  val loss:  0.7079911828041077\n",
      "epoch:  36   step:  111   train loss:  0.0002851220197044313  val loss:  0.7095357179641724\n",
      "epoch:  36   step:  112   train loss:  0.00013334061077330261  val loss:  0.7055904269218445\n",
      "epoch:  36   step:  113   train loss:  0.00024105090415105224  val loss:  0.7102413177490234\n",
      "epoch:  36   step:  114   train loss:  0.0003553595161065459  val loss:  0.7170034646987915\n",
      "epoch:  36   step:  115   train loss:  0.00035376986488699913  val loss:  0.7212778329849243\n",
      "epoch:  36   step:  116   train loss:  0.00032183696748688817  val loss:  0.7166712880134583\n",
      "epoch:  36   step:  117   train loss:  0.00022137115593068302  val loss:  0.7214562296867371\n",
      "epoch:  36   step:  118   train loss:  0.00022529487614519894  val loss:  0.7194566130638123\n",
      "epoch:  36   step:  119   train loss:  0.00020659757137764245  val loss:  0.7250164151191711\n",
      "epoch:  36   step:  120   train loss:  0.00021121191093698144  val loss:  0.7256907224655151\n",
      "epoch:  36   step:  121   train loss:  0.00031707208836451173  val loss:  0.731011688709259\n",
      "epoch:  36   step:  122   train loss:  0.00014386816474143416  val loss:  0.7398905754089355\n",
      "epoch:  36   step:  123   train loss:  0.0002437412040308118  val loss:  0.7397889494895935\n",
      "epoch:  36   step:  124   train loss:  0.00018719857325777411  val loss:  0.7412149906158447\n",
      "epoch:  36   step:  125   train loss:  0.00027524144388735294  val loss:  0.7361616492271423\n",
      "epoch:  36   step:  126   train loss:  0.00019810641242656857  val loss:  0.7337709665298462\n",
      "epoch:  36   step:  127   train loss:  0.0002950281195808202  val loss:  0.7224947214126587\n",
      "epoch:  36   step:  128   train loss:  0.00036133744288235903  val loss:  0.7307334542274475\n",
      "epoch:  36   step:  129   train loss:  0.00023551740741822869  val loss:  0.7337080836296082\n",
      "epoch:  36   step:  130   train loss:  0.000517331063747406  val loss:  0.7405129075050354\n",
      "epoch:  36   step:  131   train loss:  0.0003705025592353195  val loss:  0.7523983120918274\n",
      "epoch:  36   step:  132   train loss:  0.0003708008152898401  val loss:  0.751459538936615\n",
      "epoch:  36   step:  133   train loss:  0.0003284274134784937  val loss:  0.7573122978210449\n",
      "epoch:  36   step:  134   train loss:  0.0005397259956225753  val loss:  0.7607982754707336\n",
      "epoch:  36   step:  135   train loss:  0.00026483929832465947  val loss:  0.7617202401161194\n",
      "epoch:  36   step:  136   train loss:  0.00021045503672212362  val loss:  0.7594205141067505\n",
      "epoch:  36   step:  137   train loss:  0.0004888661205768585  val loss:  0.7468236088752747\n",
      "epoch:  36   step:  138   train loss:  0.0006384026492014527  val loss:  0.7344493865966797\n",
      "epoch:  36   step:  139   train loss:  0.0005209905211813748  val loss:  0.7335795164108276\n",
      "epoch:  36   step:  140   train loss:  0.00030137563589960337  val loss:  0.7274470329284668\n",
      "epoch:  36   step:  141   train loss:  0.00017569013289175928  val loss:  0.7327601313591003\n",
      "epoch:  36   step:  142   train loss:  0.00021461164578795433  val loss:  0.7305335998535156\n",
      "epoch:  36   step:  143   train loss:  0.0005111866048537195  val loss:  0.72628253698349\n",
      "epoch:  36   step:  144   train loss:  0.00028122542425990105  val loss:  0.7242506146430969\n",
      "epoch:  36   step:  145   train loss:  0.00025267479941248894  val loss:  0.720103919506073\n",
      "epoch:  36   step:  146   train loss:  0.0002721560304053128  val loss:  0.711788535118103\n",
      "epoch:  36   step:  147   train loss:  0.0005770132411271334  val loss:  0.7050995230674744\n",
      "epoch:  36   step:  148   train loss:  0.00014334451407194138  val loss:  0.7076128125190735\n",
      "epoch:  36   step:  149   train loss:  0.0005337108159437776  val loss:  0.7015110850334167\n",
      "epoch:  36   step:  150   train loss:  0.00016747087647672743  val loss:  0.7056295871734619\n",
      "epoch:  36   step:  151   train loss:  0.0003182056825608015  val loss:  0.7203953266143799\n",
      "epoch:  36   step:  152   train loss:  0.0001229971385328099  val loss:  0.7284227013587952\n",
      "epoch:  36   step:  153   train loss:  0.0005215600249357522  val loss:  0.7256446480751038\n",
      "epoch:  36   step:  154   train loss:  0.0001825282524805516  val loss:  0.7230220437049866\n",
      "epoch:  36   step:  155   train loss:  0.00036170147359371185  val loss:  0.7194772958755493\n",
      "epoch:  36   step:  156   train loss:  0.0002995910763274878  val loss:  0.7195818424224854\n",
      "epoch:  36   step:  157   train loss:  0.00046622121590189636  val loss:  0.7135031819343567\n",
      "epoch:  36   step:  158   train loss:  0.000564550282433629  val loss:  0.7315701842308044\n",
      "epoch:  36   step:  159   train loss:  0.00030832248739898205  val loss:  0.7327982187271118\n",
      "epoch:  36   step:  160   train loss:  0.000267433759290725  val loss:  0.7275345921516418\n",
      "epoch:  36   step:  161   train loss:  0.0003489895607344806  val loss:  0.7281985878944397\n",
      "epoch:  36   step:  162   train loss:  0.00037222582614049315  val loss:  0.7273023724555969\n",
      "epoch:  36   step:  163   train loss:  0.00028852352988906205  val loss:  0.7181519865989685\n",
      "epoch:  36   step:  164   train loss:  0.0003362517454661429  val loss:  0.7119930982589722\n",
      "epoch:  36   step:  165   train loss:  0.002758875722065568  val loss:  0.7024542689323425\n",
      "epoch:  37   step:  0   train loss:  0.00022295504459179938  val loss:  0.7155634760856628\n",
      "epoch:  37   step:  1   train loss:  0.0001669896737439558  val loss:  0.7196359038352966\n",
      "epoch:  37   step:  2   train loss:  0.0003117939631920308  val loss:  0.7204941511154175\n",
      "epoch:  37   step:  3   train loss:  0.00021151376131456345  val loss:  0.7183067202568054\n",
      "epoch:  37   step:  4   train loss:  0.00018591528350953013  val loss:  0.726185142993927\n",
      "epoch:  37   step:  5   train loss:  0.00041699688881635666  val loss:  0.7395434379577637\n",
      "epoch:  37   step:  6   train loss:  0.0003113344719167799  val loss:  0.7221899032592773\n",
      "epoch:  37   step:  7   train loss:  0.0002682661288417876  val loss:  0.7329246401786804\n",
      "epoch:  37   step:  8   train loss:  0.0002800756774377078  val loss:  0.7288746237754822\n",
      "epoch:  37   step:  9   train loss:  0.0006226867553777993  val loss:  0.7122942805290222\n",
      "epoch:  37   step:  10   train loss:  0.0002720447664614767  val loss:  0.7051504254341125\n",
      "epoch:  37   step:  11   train loss:  0.00012483945465646684  val loss:  0.7074064612388611\n",
      "epoch:  37   step:  12   train loss:  0.00017578512779437006  val loss:  0.6977762579917908\n",
      "epoch:  37   step:  13   train loss:  0.00017019369988702238  val loss:  0.6973626017570496\n",
      "epoch:  37   step:  14   train loss:  0.00022413856640923768  val loss:  0.6924005746841431\n",
      "epoch:  37   step:  15   train loss:  0.00027871981728821993  val loss:  0.69443678855896\n",
      "epoch:  37   step:  16   train loss:  0.00036907242611050606  val loss:  0.7164492607116699\n",
      "epoch:  37   step:  17   train loss:  0.00014001107774674892  val loss:  0.72053462266922\n",
      "epoch:  37   step:  18   train loss:  0.00037939404137432575  val loss:  0.7174270749092102\n",
      "epoch:  37   step:  19   train loss:  0.00021043320884928107  val loss:  0.7198882102966309\n",
      "epoch:  37   step:  20   train loss:  0.00044648844050243497  val loss:  0.7146318554878235\n",
      "epoch:  37   step:  21   train loss:  0.0002371265145484358  val loss:  0.728240966796875\n",
      "epoch:  37   step:  22   train loss:  0.00017494685016572475  val loss:  0.7261728644371033\n",
      "epoch:  37   step:  23   train loss:  0.00024898347328417003  val loss:  0.7346552014350891\n",
      "epoch:  37   step:  24   train loss:  0.00033670529955998063  val loss:  0.7384779453277588\n",
      "epoch:  37   step:  25   train loss:  0.00014956772793084383  val loss:  0.7445436120033264\n",
      "epoch:  37   step:  26   train loss:  0.00011636942508630455  val loss:  0.7476248145103455\n",
      "epoch:  37   step:  27   train loss:  0.00022869255917612463  val loss:  0.7407902479171753\n",
      "epoch:  37   step:  28   train loss:  0.00028468636446632445  val loss:  0.7338491678237915\n",
      "epoch:  37   step:  29   train loss:  0.00017970424960367382  val loss:  0.7327756285667419\n",
      "epoch:  37   step:  30   train loss:  0.00028890089015476406  val loss:  0.7250571250915527\n",
      "epoch:  37   step:  31   train loss:  0.00020158587722107768  val loss:  0.7197640538215637\n",
      "epoch:  37   step:  32   train loss:  0.0001406669762218371  val loss:  0.7146942019462585\n",
      "epoch:  37   step:  33   train loss:  0.0005633969558402896  val loss:  0.7165477871894836\n",
      "epoch:  37   step:  34   train loss:  0.00027261191280558705  val loss:  0.7136388421058655\n",
      "epoch:  37   step:  35   train loss:  0.000452376261819154  val loss:  0.7257159352302551\n",
      "epoch:  37   step:  36   train loss:  0.00033936506952159107  val loss:  0.730731189250946\n",
      "epoch:  37   step:  37   train loss:  0.00048793910536915064  val loss:  0.7441728115081787\n",
      "epoch:  37   step:  38   train loss:  0.000440518168034032  val loss:  0.7378082871437073\n",
      "epoch:  37   step:  39   train loss:  0.00030910427449271083  val loss:  0.749627947807312\n",
      "epoch:  37   step:  40   train loss:  0.00020200610742904246  val loss:  0.7421192526817322\n",
      "epoch:  37   step:  41   train loss:  0.0003361982526257634  val loss:  0.7574862241744995\n",
      "epoch:  37   step:  42   train loss:  0.00024166330695152283  val loss:  0.7579234838485718\n",
      "epoch:  37   step:  43   train loss:  0.00011964781151618809  val loss:  0.7762411236763\n",
      "epoch:  37   step:  44   train loss:  0.0002672300615813583  val loss:  0.772167444229126\n",
      "epoch:  37   step:  45   train loss:  0.0001855821319622919  val loss:  0.7631459832191467\n",
      "epoch:  37   step:  46   train loss:  0.0004437908355612308  val loss:  0.765103280544281\n",
      "epoch:  37   step:  47   train loss:  0.0003769121249206364  val loss:  0.7684350609779358\n",
      "epoch:  37   step:  48   train loss:  0.0001524535909993574  val loss:  0.7556798458099365\n",
      "epoch:  37   step:  49   train loss:  0.00017260125605389476  val loss:  0.7488820552825928\n",
      "epoch:  37   step:  50   train loss:  0.00025939373881556094  val loss:  0.7460594773292542\n",
      "epoch:  37   step:  51   train loss:  0.0003258514334447682  val loss:  0.7420755624771118\n",
      "epoch:  37   step:  52   train loss:  0.0001435648009646684  val loss:  0.7577702403068542\n",
      "epoch:  37   step:  53   train loss:  0.00013540835061576217  val loss:  0.7491869330406189\n",
      "epoch:  37   step:  54   train loss:  0.00022074455046094954  val loss:  0.7564948797225952\n",
      "epoch:  37   step:  55   train loss:  0.00023903197143226862  val loss:  0.7518804669380188\n",
      "epoch:  37   step:  56   train loss:  0.0004615028447005898  val loss:  0.7544934153556824\n",
      "epoch:  37   step:  57   train loss:  0.0001767113571986556  val loss:  0.743284285068512\n",
      "epoch:  37   step:  58   train loss:  0.00019875103316735476  val loss:  0.7498284578323364\n",
      "epoch:  37   step:  59   train loss:  0.00019552545563783497  val loss:  0.7453030943870544\n",
      "epoch:  37   step:  60   train loss:  0.00024489636416547  val loss:  0.7550414204597473\n",
      "epoch:  37   step:  61   train loss:  0.00035750679671764374  val loss:  0.7391086220741272\n",
      "epoch:  37   step:  62   train loss:  0.00037473460542969406  val loss:  0.744596004486084\n",
      "epoch:  37   step:  63   train loss:  0.00015953036199789494  val loss:  0.7384359836578369\n",
      "epoch:  37   step:  64   train loss:  0.00023959952523000538  val loss:  0.7331639528274536\n",
      "epoch:  37   step:  65   train loss:  0.0003468247887212783  val loss:  0.7369982600212097\n",
      "epoch:  37   step:  66   train loss:  0.00020024309924338013  val loss:  0.7324232459068298\n",
      "epoch:  37   step:  67   train loss:  0.00032009166898205876  val loss:  0.7263022661209106\n",
      "epoch:  37   step:  68   train loss:  0.0005030202446505427  val loss:  0.7271813154220581\n",
      "epoch:  37   step:  69   train loss:  0.00021875636593904346  val loss:  0.7179304361343384\n",
      "epoch:  37   step:  70   train loss:  0.0002304619993083179  val loss:  0.7232705354690552\n",
      "epoch:  37   step:  71   train loss:  0.00030251056887209415  val loss:  0.7219743132591248\n",
      "epoch:  37   step:  72   train loss:  0.00022814483963884413  val loss:  0.7188238501548767\n",
      "epoch:  37   step:  73   train loss:  0.00048449693713337183  val loss:  0.7379587292671204\n",
      "epoch:  37   step:  74   train loss:  0.0002252028789371252  val loss:  0.7307994961738586\n",
      "epoch:  37   step:  75   train loss:  0.00021251090220175683  val loss:  0.746850311756134\n",
      "epoch:  37   step:  76   train loss:  0.00026799540501087904  val loss:  0.7335450053215027\n",
      "epoch:  37   step:  77   train loss:  0.00014120298146735877  val loss:  0.7280796766281128\n",
      "epoch:  37   step:  78   train loss:  0.00031967050745151937  val loss:  0.7220172882080078\n",
      "epoch:  37   step:  79   train loss:  0.0002975367533508688  val loss:  0.7148080468177795\n",
      "epoch:  37   step:  80   train loss:  0.00036521966103464365  val loss:  0.7091798186302185\n",
      "epoch:  37   step:  81   train loss:  0.0002008340961765498  val loss:  0.7080982327461243\n",
      "epoch:  37   step:  82   train loss:  0.00028128124540671706  val loss:  0.7111361026763916\n",
      "epoch:  37   step:  83   train loss:  0.00018023430311586708  val loss:  0.7031457424163818\n",
      "epoch:  37   step:  84   train loss:  0.000564635032787919  val loss:  0.7165555357933044\n",
      "epoch:  37   step:  85   train loss:  0.00013648852473124862  val loss:  0.716114342212677\n",
      "epoch:  37   step:  86   train loss:  0.00031977082835510373  val loss:  0.716115415096283\n",
      "epoch:  37   step:  87   train loss:  0.00015658861957490444  val loss:  0.7139652967453003\n",
      "epoch:  37   step:  88   train loss:  0.0003972986014559865  val loss:  0.7169633507728577\n",
      "epoch:  37   step:  89   train loss:  0.0006442804005928338  val loss:  0.7383673191070557\n",
      "epoch:  37   step:  90   train loss:  0.00019355755648575723  val loss:  0.7441791892051697\n",
      "epoch:  37   step:  91   train loss:  0.00030212150886654854  val loss:  0.7481134533882141\n",
      "epoch:  37   step:  92   train loss:  0.00028765713796019554  val loss:  0.7557474374771118\n",
      "epoch:  37   step:  93   train loss:  0.0004619549436029047  val loss:  0.7632803916931152\n",
      "epoch:  37   step:  94   train loss:  0.00023188648629002273  val loss:  0.7499781847000122\n",
      "epoch:  37   step:  95   train loss:  0.00012885710748378187  val loss:  0.7310202121734619\n",
      "epoch:  37   step:  96   train loss:  0.0002154203539248556  val loss:  0.7322651743888855\n",
      "epoch:  37   step:  97   train loss:  0.00027607622905634344  val loss:  0.7412641048431396\n",
      "epoch:  37   step:  98   train loss:  0.00023472568136639893  val loss:  0.7384080290794373\n",
      "epoch:  37   step:  99   train loss:  0.0002216165594290942  val loss:  0.7390255331993103\n",
      "epoch:  37   step:  100   train loss:  0.00029835093300789595  val loss:  0.754351019859314\n",
      "epoch:  37   step:  101   train loss:  0.00014289024693425745  val loss:  0.7460804581642151\n",
      "epoch:  37   step:  102   train loss:  0.00025961510255001485  val loss:  0.7361873984336853\n",
      "epoch:  37   step:  103   train loss:  0.00015925956540741026  val loss:  0.7387098670005798\n",
      "epoch:  37   step:  104   train loss:  0.0004384812491480261  val loss:  0.7334149479866028\n",
      "epoch:  37   step:  105   train loss:  0.00017898321675602347  val loss:  0.740123987197876\n",
      "epoch:  37   step:  106   train loss:  9.860032878350466e-05  val loss:  0.7400557398796082\n",
      "epoch:  37   step:  107   train loss:  0.0002539414563216269  val loss:  0.7525027394294739\n",
      "epoch:  37   step:  108   train loss:  0.00013131333980709314  val loss:  0.7440284490585327\n",
      "epoch:  37   step:  109   train loss:  0.00013577121717389673  val loss:  0.7475507855415344\n",
      "epoch:  37   step:  110   train loss:  0.00020692520774900913  val loss:  0.7452583909034729\n",
      "epoch:  37   step:  111   train loss:  0.00017139766714535654  val loss:  0.7542757987976074\n",
      "epoch:  37   step:  112   train loss:  0.0002741730131674558  val loss:  0.7371606826782227\n",
      "epoch:  37   step:  113   train loss:  0.0002886698639485985  val loss:  0.7391061186790466\n",
      "epoch:  37   step:  114   train loss:  0.00019574286125134677  val loss:  0.744140088558197\n",
      "epoch:  37   step:  115   train loss:  0.00019189179874956608  val loss:  0.7450178265571594\n",
      "epoch:  37   step:  116   train loss:  0.00018998931045643985  val loss:  0.7467507719993591\n",
      "epoch:  37   step:  117   train loss:  0.00047699606511741877  val loss:  0.7400489449501038\n",
      "epoch:  37   step:  118   train loss:  0.00019201112445443869  val loss:  0.735379159450531\n",
      "epoch:  37   step:  119   train loss:  0.0002508906472939998  val loss:  0.7387755513191223\n",
      "epoch:  37   step:  120   train loss:  0.00025712698698043823  val loss:  0.7197326421737671\n",
      "epoch:  37   step:  121   train loss:  0.00032086443388834596  val loss:  0.7187783718109131\n",
      "epoch:  37   step:  122   train loss:  0.00015918133431114256  val loss:  0.7244860529899597\n",
      "epoch:  37   step:  123   train loss:  0.00045441807014867663  val loss:  0.721622884273529\n",
      "epoch:  37   step:  124   train loss:  0.00028781479340977967  val loss:  0.7214472889900208\n",
      "epoch:  37   step:  125   train loss:  0.00022964546224102378  val loss:  0.7172219753265381\n",
      "epoch:  37   step:  126   train loss:  0.00016764519386924803  val loss:  0.7167178392410278\n",
      "epoch:  37   step:  127   train loss:  0.00020177745318505913  val loss:  0.7128036618232727\n",
      "epoch:  37   step:  128   train loss:  0.00020933779887855053  val loss:  0.7132475972175598\n",
      "epoch:  37   step:  129   train loss:  0.0006857567932456732  val loss:  0.7273672223091125\n",
      "epoch:  37   step:  130   train loss:  0.00029014592291787267  val loss:  0.7345001697540283\n",
      "epoch:  37   step:  131   train loss:  0.00014088691386859864  val loss:  0.7550607323646545\n",
      "epoch:  37   step:  132   train loss:  0.0009631886496208608  val loss:  0.7316450476646423\n",
      "epoch:  37   step:  133   train loss:  0.00017702963668853045  val loss:  0.730828583240509\n",
      "epoch:  37   step:  134   train loss:  0.00016088948177639395  val loss:  0.7054585218429565\n",
      "epoch:  37   step:  135   train loss:  0.0003419548738747835  val loss:  0.7242799997329712\n",
      "epoch:  37   step:  136   train loss:  0.0005698063177987933  val loss:  0.733271598815918\n",
      "epoch:  37   step:  137   train loss:  0.00016311518265865743  val loss:  0.739824652671814\n",
      "epoch:  37   step:  138   train loss:  0.0003204327367711812  val loss:  0.7366052865982056\n",
      "epoch:  37   step:  139   train loss:  0.0006917783757671714  val loss:  0.7241437435150146\n",
      "epoch:  37   step:  140   train loss:  0.00047848746180534363  val loss:  0.7291315197944641\n",
      "epoch:  37   step:  141   train loss:  0.00028322444995865226  val loss:  0.7228290438652039\n",
      "epoch:  37   step:  142   train loss:  0.0001443382352590561  val loss:  0.7197851538658142\n",
      "epoch:  37   step:  143   train loss:  0.00036229455145075917  val loss:  0.724609911441803\n",
      "epoch:  37   step:  144   train loss:  0.0004485679091885686  val loss:  0.7242397665977478\n",
      "epoch:  37   step:  145   train loss:  0.00023421374498866498  val loss:  0.724010705947876\n",
      "epoch:  37   step:  146   train loss:  0.00021743749675806612  val loss:  0.730083167552948\n",
      "epoch:  37   step:  147   train loss:  9.959757153410465e-05  val loss:  0.7392914891242981\n",
      "epoch:  37   step:  148   train loss:  0.00016924506053328514  val loss:  0.7361353039741516\n",
      "epoch:  37   step:  149   train loss:  0.000867307826410979  val loss:  0.7528464198112488\n",
      "epoch:  37   step:  150   train loss:  0.0004036406462546438  val loss:  0.7613052725791931\n",
      "epoch:  37   step:  151   train loss:  0.00018028749036602676  val loss:  0.7746975421905518\n",
      "epoch:  37   step:  152   train loss:  0.00033581475145183504  val loss:  0.7699419856071472\n",
      "epoch:  37   step:  153   train loss:  0.00011068822641391307  val loss:  0.7626680731773376\n",
      "epoch:  37   step:  154   train loss:  0.0003542701306287199  val loss:  0.7469848990440369\n",
      "epoch:  37   step:  155   train loss:  0.000280535314232111  val loss:  0.7331004738807678\n",
      "epoch:  37   step:  156   train loss:  0.00035071081947535276  val loss:  0.7238280177116394\n",
      "epoch:  37   step:  157   train loss:  0.00019064338994212449  val loss:  0.7359297275543213\n",
      "epoch:  37   step:  158   train loss:  0.00031806412152945995  val loss:  0.7436006665229797\n",
      "epoch:  37   step:  159   train loss:  0.00027405592845752835  val loss:  0.7491200566291809\n",
      "epoch:  37   step:  160   train loss:  0.00031885254429653287  val loss:  0.7655898332595825\n",
      "epoch:  37   step:  161   train loss:  0.00020603369921445847  val loss:  0.7722135782241821\n",
      "epoch:  37   step:  162   train loss:  0.00022452263510785997  val loss:  0.7831677198410034\n",
      "epoch:  37   step:  163   train loss:  0.0007808655500411987  val loss:  0.765652596950531\n",
      "epoch:  37   step:  164   train loss:  0.00012548686936497688  val loss:  0.7609847187995911\n",
      "epoch:  37   step:  165   train loss:  0.0002666276413947344  val loss:  0.7504542469978333\n",
      "epoch:  38   step:  0   train loss:  0.0001323625911027193  val loss:  0.7610352635383606\n",
      "epoch:  38   step:  1   train loss:  0.0001714707468636334  val loss:  0.7539798021316528\n",
      "epoch:  38   step:  2   train loss:  0.00011368310515535995  val loss:  0.7600507140159607\n",
      "epoch:  38   step:  3   train loss:  0.00012707067071460187  val loss:  0.7553520202636719\n",
      "epoch:  38   step:  4   train loss:  0.0003332722990307957  val loss:  0.7439213395118713\n",
      "epoch:  38   step:  5   train loss:  0.00028365536127239466  val loss:  0.7445452809333801\n",
      "epoch:  38   step:  6   train loss:  0.0001713672827463597  val loss:  0.7429854273796082\n",
      "epoch:  38   step:  7   train loss:  0.00014079395623411983  val loss:  0.7427605986595154\n",
      "epoch:  38   step:  8   train loss:  0.0001186876543215476  val loss:  0.7430970668792725\n",
      "epoch:  38   step:  9   train loss:  0.00018638469919096678  val loss:  0.7466084361076355\n",
      "epoch:  38   step:  10   train loss:  0.00019824376795440912  val loss:  0.7385335564613342\n",
      "epoch:  38   step:  11   train loss:  0.00022977730259299278  val loss:  0.7313829064369202\n",
      "epoch:  38   step:  12   train loss:  0.00034391696681268513  val loss:  0.7224416136741638\n",
      "epoch:  38   step:  13   train loss:  9.903512545861304e-05  val loss:  0.7357421517372131\n",
      "epoch:  38   step:  14   train loss:  0.00015655942843295634  val loss:  0.7436202168464661\n",
      "epoch:  38   step:  15   train loss:  0.0002041081024799496  val loss:  0.7313523888587952\n",
      "epoch:  38   step:  16   train loss:  0.0004888471448794007  val loss:  0.7436129450798035\n",
      "epoch:  38   step:  17   train loss:  0.00011277366138529032  val loss:  0.7452878952026367\n",
      "epoch:  38   step:  18   train loss:  0.00022055885347072035  val loss:  0.7472937703132629\n",
      "epoch:  38   step:  19   train loss:  0.00012883185991086066  val loss:  0.7457128763198853\n",
      "epoch:  38   step:  20   train loss:  0.0004828975652344525  val loss:  0.740863025188446\n",
      "epoch:  38   step:  21   train loss:  0.00016323800082318485  val loss:  0.7307333946228027\n",
      "epoch:  38   step:  22   train loss:  0.0003344910510350019  val loss:  0.7308380603790283\n",
      "epoch:  38   step:  23   train loss:  0.00017975771334022284  val loss:  0.7412252426147461\n",
      "epoch:  38   step:  24   train loss:  6.414571544155478e-05  val loss:  0.7386708855628967\n",
      "epoch:  38   step:  25   train loss:  0.0001719872234389186  val loss:  0.7527029514312744\n",
      "epoch:  38   step:  26   train loss:  0.0009325661230832338  val loss:  0.7643082737922668\n",
      "epoch:  38   step:  27   train loss:  0.00023234289255924523  val loss:  0.7618340253829956\n",
      "epoch:  38   step:  28   train loss:  0.00025698792887851596  val loss:  0.7577816843986511\n",
      "epoch:  38   step:  29   train loss:  0.00017916865181177855  val loss:  0.7522587180137634\n",
      "epoch:  38   step:  30   train loss:  0.00021625713270623237  val loss:  0.7521931529045105\n",
      "epoch:  38   step:  31   train loss:  0.0001462767249904573  val loss:  0.7421701550483704\n",
      "epoch:  38   step:  32   train loss:  0.00028397489222697914  val loss:  0.7297710180282593\n",
      "epoch:  38   step:  33   train loss:  0.00022462185006588697  val loss:  0.7367802858352661\n",
      "epoch:  38   step:  34   train loss:  0.00012812839122489095  val loss:  0.7384235262870789\n",
      "epoch:  38   step:  35   train loss:  0.0012130076065659523  val loss:  0.7197999358177185\n",
      "epoch:  38   step:  36   train loss:  0.000375475938199088  val loss:  0.7229343056678772\n",
      "epoch:  38   step:  37   train loss:  0.00034987914841622114  val loss:  0.7338768839836121\n",
      "epoch:  38   step:  38   train loss:  0.0001291463995585218  val loss:  0.7166645526885986\n",
      "epoch:  38   step:  39   train loss:  6.925596244400367e-05  val loss:  0.7171841263771057\n",
      "epoch:  38   step:  40   train loss:  0.00020914370543323457  val loss:  0.7174822092056274\n",
      "epoch:  38   step:  41   train loss:  0.00019701436394825578  val loss:  0.7233961820602417\n",
      "epoch:  38   step:  42   train loss:  0.0003210597496945411  val loss:  0.7465203404426575\n",
      "epoch:  38   step:  43   train loss:  0.0004470155108720064  val loss:  0.7429184317588806\n",
      "epoch:  38   step:  44   train loss:  0.00012395702651701868  val loss:  0.7458336353302002\n",
      "epoch:  38   step:  45   train loss:  0.00017529964679852128  val loss:  0.7456091046333313\n",
      "epoch:  38   step:  46   train loss:  0.00010790395026560873  val loss:  0.7570583820343018\n",
      "epoch:  38   step:  47   train loss:  0.00011419237853260711  val loss:  0.7474136352539062\n",
      "epoch:  38   step:  48   train loss:  0.0001373100240016356  val loss:  0.741222620010376\n",
      "epoch:  38   step:  49   train loss:  0.0001703371381154284  val loss:  0.7439131140708923\n",
      "epoch:  38   step:  50   train loss:  0.0002182038442697376  val loss:  0.7426365613937378\n",
      "epoch:  38   step:  51   train loss:  0.0001755623088683933  val loss:  0.7542749047279358\n",
      "epoch:  38   step:  52   train loss:  0.00014536138041876256  val loss:  0.755127489566803\n",
      "epoch:  38   step:  53   train loss:  0.00019855292339343578  val loss:  0.749042272567749\n",
      "epoch:  38   step:  54   train loss:  0.00013994921755511314  val loss:  0.7471826076507568\n",
      "epoch:  38   step:  55   train loss:  0.0002323626831639558  val loss:  0.7526664733886719\n",
      "epoch:  38   step:  56   train loss:  0.00030581257306039333  val loss:  0.7333773374557495\n",
      "epoch:  38   step:  57   train loss:  0.00029426015680655837  val loss:  0.735702395439148\n",
      "epoch:  38   step:  58   train loss:  0.0002892527845688164  val loss:  0.7273920774459839\n",
      "epoch:  38   step:  59   train loss:  0.0001273700618185103  val loss:  0.7348728775978088\n",
      "epoch:  38   step:  60   train loss:  0.00018932807142846286  val loss:  0.7374774217605591\n",
      "epoch:  38   step:  61   train loss:  0.00010107607522513717  val loss:  0.7307816743850708\n",
      "epoch:  38   step:  62   train loss:  0.0002214149571955204  val loss:  0.7436079382896423\n",
      "epoch:  38   step:  63   train loss:  0.0006277104839682579  val loss:  0.7277746200561523\n",
      "epoch:  38   step:  64   train loss:  0.00013723864685744047  val loss:  0.7186357975006104\n",
      "epoch:  38   step:  65   train loss:  0.0002052238560281694  val loss:  0.7274121642112732\n",
      "epoch:  38   step:  66   train loss:  0.00017606838082429022  val loss:  0.7242339849472046\n",
      "epoch:  38   step:  67   train loss:  0.0003743793931789696  val loss:  0.7202339172363281\n",
      "epoch:  38   step:  68   train loss:  0.0002364506945014  val loss:  0.7216160297393799\n",
      "epoch:  38   step:  69   train loss:  0.00023077006335370243  val loss:  0.7320442795753479\n",
      "epoch:  38   step:  70   train loss:  0.00021736652706749737  val loss:  0.7445309162139893\n",
      "epoch:  38   step:  71   train loss:  0.0003207777626812458  val loss:  0.7395193576812744\n",
      "epoch:  38   step:  72   train loss:  0.0001970181765500456  val loss:  0.7318911552429199\n",
      "epoch:  38   step:  73   train loss:  0.0012087463401257992  val loss:  0.712610125541687\n",
      "epoch:  38   step:  74   train loss:  0.00010056261089630425  val loss:  0.7175097465515137\n",
      "epoch:  38   step:  75   train loss:  0.0002109005581587553  val loss:  0.7286285758018494\n",
      "epoch:  38   step:  76   train loss:  0.0001216536620631814  val loss:  0.7218520045280457\n",
      "epoch:  38   step:  77   train loss:  0.00012460793368518353  val loss:  0.7284664511680603\n",
      "epoch:  38   step:  78   train loss:  7.335537520702928e-05  val loss:  0.7221290469169617\n",
      "epoch:  38   step:  79   train loss:  0.00010951463627861813  val loss:  0.7183555960655212\n",
      "epoch:  38   step:  80   train loss:  0.0001995497295865789  val loss:  0.7237192988395691\n",
      "epoch:  38   step:  81   train loss:  0.0006462014862336218  val loss:  0.7332623600959778\n",
      "epoch:  38   step:  82   train loss:  0.00024039199342951179  val loss:  0.7445523142814636\n",
      "epoch:  38   step:  83   train loss:  0.00016289518680423498  val loss:  0.7469360828399658\n",
      "epoch:  38   step:  84   train loss:  0.00011531190830282867  val loss:  0.7588597536087036\n",
      "epoch:  38   step:  85   train loss:  7.974199979798868e-05  val loss:  0.7659348845481873\n",
      "epoch:  38   step:  86   train loss:  0.00011956859816564247  val loss:  0.7700102925300598\n",
      "epoch:  38   step:  87   train loss:  0.0001979309890884906  val loss:  0.7625234127044678\n",
      "epoch:  38   step:  88   train loss:  0.0002142134471796453  val loss:  0.7696352005004883\n",
      "epoch:  38   step:  89   train loss:  0.00016584877448622137  val loss:  0.7729959487915039\n",
      "epoch:  38   step:  90   train loss:  0.0001444685331080109  val loss:  0.7814726233482361\n",
      "epoch:  38   step:  91   train loss:  0.0002030278992606327  val loss:  0.7831006646156311\n",
      "epoch:  38   step:  92   train loss:  0.0002654666604939848  val loss:  0.7722752690315247\n",
      "epoch:  38   step:  93   train loss:  0.00017656713316682726  val loss:  0.7716312408447266\n",
      "epoch:  38   step:  94   train loss:  0.0001578133669681847  val loss:  0.7838069796562195\n",
      "epoch:  38   step:  95   train loss:  0.0002759874623734504  val loss:  0.7709563374519348\n",
      "epoch:  38   step:  96   train loss:  0.00018721012747846544  val loss:  0.7781751155853271\n",
      "epoch:  38   step:  97   train loss:  0.00017537637904752046  val loss:  0.776730477809906\n",
      "epoch:  38   step:  98   train loss:  0.00015419002738781273  val loss:  0.7717982530593872\n",
      "epoch:  38   step:  99   train loss:  0.00033413158962503076  val loss:  0.7820525765419006\n",
      "epoch:  38   step:  100   train loss:  0.00020494169439189136  val loss:  0.7795306444168091\n",
      "epoch:  38   step:  101   train loss:  0.00019688726752065122  val loss:  0.7712163329124451\n",
      "epoch:  38   step:  102   train loss:  0.00013354315888136625  val loss:  0.7670827507972717\n",
      "epoch:  38   step:  103   train loss:  0.00013941767974756658  val loss:  0.7732520699501038\n",
      "epoch:  38   step:  104   train loss:  0.00028361001750454307  val loss:  0.7700429558753967\n",
      "epoch:  38   step:  105   train loss:  8.456135401502252e-05  val loss:  0.7718602418899536\n",
      "epoch:  38   step:  106   train loss:  0.00011516959057189524  val loss:  0.7683417201042175\n",
      "epoch:  38   step:  107   train loss:  0.00012304481060709804  val loss:  0.7636911869049072\n",
      "epoch:  38   step:  108   train loss:  7.9029516200535e-05  val loss:  0.7713636755943298\n",
      "epoch:  38   step:  109   train loss:  0.0002229881938546896  val loss:  0.7762351632118225\n",
      "epoch:  38   step:  110   train loss:  0.00021944612672086805  val loss:  0.7877733707427979\n",
      "epoch:  38   step:  111   train loss:  0.00012963265180587769  val loss:  0.7896478772163391\n",
      "epoch:  38   step:  112   train loss:  0.00011366113903932273  val loss:  0.7947365641593933\n",
      "epoch:  38   step:  113   train loss:  9.934193803928792e-05  val loss:  0.7906216382980347\n",
      "epoch:  38   step:  114   train loss:  0.0002430976164760068  val loss:  0.8010790944099426\n",
      "epoch:  38   step:  115   train loss:  0.00014777839533053339  val loss:  0.8115728497505188\n",
      "epoch:  38   step:  116   train loss:  0.00034775931271724403  val loss:  0.8138611316680908\n",
      "epoch:  38   step:  117   train loss:  0.00016931681602727622  val loss:  0.8020181059837341\n",
      "epoch:  38   step:  118   train loss:  0.0002607134520076215  val loss:  0.7876928448677063\n",
      "epoch:  38   step:  119   train loss:  0.0003612979780882597  val loss:  0.7977393865585327\n",
      "epoch:  38   step:  120   train loss:  0.00021500172442756593  val loss:  0.7982978820800781\n",
      "epoch:  38   step:  121   train loss:  0.00011310586705803871  val loss:  0.7946942448616028\n",
      "epoch:  38   step:  122   train loss:  9.859273268375546e-05  val loss:  0.7815330624580383\n",
      "epoch:  38   step:  123   train loss:  0.00021115073468536139  val loss:  0.7883704900741577\n",
      "epoch:  38   step:  124   train loss:  9.983009658753872e-05  val loss:  0.7773522734642029\n",
      "epoch:  38   step:  125   train loss:  0.0001508281857240945  val loss:  0.7725693583488464\n",
      "epoch:  38   step:  126   train loss:  0.00016499380581080914  val loss:  0.7786298990249634\n",
      "epoch:  38   step:  127   train loss:  0.00018410556367598474  val loss:  0.7716884613037109\n",
      "epoch:  38   step:  128   train loss:  0.00014930959150660783  val loss:  0.769375205039978\n",
      "epoch:  38   step:  129   train loss:  0.0004223907017149031  val loss:  0.7736697196960449\n",
      "epoch:  38   step:  130   train loss:  7.959461800055578e-05  val loss:  0.7661986351013184\n",
      "epoch:  38   step:  131   train loss:  0.00028026520158164203  val loss:  0.7757740020751953\n",
      "epoch:  38   step:  132   train loss:  0.00020280513854231685  val loss:  0.7791077494621277\n",
      "epoch:  38   step:  133   train loss:  0.0006233197636902332  val loss:  0.7864489555358887\n",
      "epoch:  38   step:  134   train loss:  0.00015019389684312046  val loss:  0.7808728814125061\n",
      "epoch:  38   step:  135   train loss:  0.00012893375242128968  val loss:  0.7837944030761719\n",
      "epoch:  38   step:  136   train loss:  0.00016749525093473494  val loss:  0.7857257723808289\n",
      "epoch:  38   step:  137   train loss:  0.00015139718016143888  val loss:  0.7767489552497864\n",
      "epoch:  38   step:  138   train loss:  0.00011537356476765126  val loss:  0.7656367421150208\n",
      "epoch:  38   step:  139   train loss:  0.00011198515858268365  val loss:  0.7734745144844055\n",
      "epoch:  38   step:  140   train loss:  0.0001214927397086285  val loss:  0.7779743075370789\n",
      "epoch:  38   step:  141   train loss:  0.0002464519056957215  val loss:  0.7881150841712952\n",
      "epoch:  38   step:  142   train loss:  0.00018652442668098956  val loss:  0.8049982786178589\n",
      "epoch:  38   step:  143   train loss:  0.0003019268624484539  val loss:  0.7902974486351013\n",
      "epoch:  38   step:  144   train loss:  0.0002354833559365943  val loss:  0.793864369392395\n",
      "epoch:  38   step:  145   train loss:  0.00011556808021850884  val loss:  0.797825813293457\n",
      "epoch:  38   step:  146   train loss:  0.00015999426250346005  val loss:  0.7760024666786194\n",
      "epoch:  38   step:  147   train loss:  0.0001619341055629775  val loss:  0.761555552482605\n",
      "epoch:  38   step:  148   train loss:  0.0002569013158790767  val loss:  0.7566237449645996\n",
      "epoch:  38   step:  149   train loss:  0.0006603168440051377  val loss:  0.778620183467865\n",
      "epoch:  38   step:  150   train loss:  0.0002723723591770977  val loss:  0.7885470390319824\n",
      "epoch:  38   step:  151   train loss:  0.0002403989201411605  val loss:  0.7961022853851318\n",
      "epoch:  38   step:  152   train loss:  0.00010357710561947897  val loss:  0.7834487557411194\n",
      "epoch:  38   step:  153   train loss:  0.0004033996956422925  val loss:  0.7940676212310791\n",
      "epoch:  38   step:  154   train loss:  0.00017876375932246447  val loss:  0.7772741317749023\n",
      "epoch:  38   step:  155   train loss:  0.00014708234812133014  val loss:  0.7712773084640503\n",
      "epoch:  38   step:  156   train loss:  0.00022932139108888805  val loss:  0.7613315582275391\n",
      "epoch:  38   step:  157   train loss:  0.001568146632052958  val loss:  0.7801339030265808\n",
      "epoch:  38   step:  158   train loss:  0.00024836938246153295  val loss:  0.7732784748077393\n",
      "epoch:  38   step:  159   train loss:  0.00021538828150369227  val loss:  0.7721397280693054\n",
      "epoch:  38   step:  160   train loss:  0.0001079134235624224  val loss:  0.7693507075309753\n",
      "epoch:  38   step:  161   train loss:  0.0002950067282654345  val loss:  0.7573490738868713\n",
      "epoch:  38   step:  162   train loss:  0.00040554878069087863  val loss:  0.7701311111450195\n",
      "epoch:  38   step:  163   train loss:  0.0008790860883891582  val loss:  0.7838323712348938\n",
      "epoch:  38   step:  164   train loss:  0.0003404922899790108  val loss:  0.7867116332054138\n",
      "epoch:  38   step:  165   train loss:  0.0018506841734051704  val loss:  0.7800827622413635\n",
      "epoch:  39   step:  0   train loss:  0.00030772818718105555  val loss:  0.7683401107788086\n",
      "epoch:  39   step:  1   train loss:  0.0004940073704347014  val loss:  0.7988824844360352\n",
      "epoch:  39   step:  2   train loss:  0.00012140873877797276  val loss:  0.7976555228233337\n",
      "epoch:  39   step:  3   train loss:  7.623127021361142e-05  val loss:  0.7923402190208435\n",
      "epoch:  39   step:  4   train loss:  0.0002399011718807742  val loss:  0.7987905740737915\n",
      "epoch:  39   step:  5   train loss:  0.0001031215360853821  val loss:  0.8085753917694092\n",
      "epoch:  39   step:  6   train loss:  9.228987619280815e-05  val loss:  0.8097109794616699\n",
      "epoch:  39   step:  7   train loss:  0.0002567448536865413  val loss:  0.8019125461578369\n",
      "epoch:  39   step:  8   train loss:  0.00019438033632468432  val loss:  0.8029957413673401\n",
      "epoch:  39   step:  9   train loss:  0.00013462663628160954  val loss:  0.7987914085388184\n",
      "epoch:  39   step:  10   train loss:  0.00015911657828837633  val loss:  0.7955929636955261\n",
      "epoch:  39   step:  11   train loss:  0.00013013325224164873  val loss:  0.7879858016967773\n",
      "epoch:  39   step:  12   train loss:  0.00015575907309539616  val loss:  0.7874987721443176\n",
      "epoch:  39   step:  13   train loss:  0.00027572401450015604  val loss:  0.8127771019935608\n",
      "epoch:  39   step:  14   train loss:  0.000172589992871508  val loss:  0.8160168528556824\n",
      "epoch:  39   step:  15   train loss:  0.00018126718350686133  val loss:  0.8237872123718262\n",
      "epoch:  39   step:  16   train loss:  0.0002941775892395526  val loss:  0.8155860900878906\n",
      "epoch:  39   step:  17   train loss:  0.00012790921027772129  val loss:  0.8133838176727295\n",
      "epoch:  39   step:  18   train loss:  0.0001856150629464537  val loss:  0.7919870615005493\n",
      "epoch:  39   step:  19   train loss:  9.327485167887062e-05  val loss:  0.7849533557891846\n",
      "epoch:  39   step:  20   train loss:  0.00015084879123605788  val loss:  0.7716029286384583\n",
      "epoch:  39   step:  21   train loss:  0.00019181062816642225  val loss:  0.7629624605178833\n",
      "epoch:  39   step:  22   train loss:  0.00038859789492562413  val loss:  0.7722679972648621\n",
      "epoch:  39   step:  23   train loss:  0.00014135881792753935  val loss:  0.7494872808456421\n",
      "epoch:  39   step:  24   train loss:  8.603656169725582e-05  val loss:  0.7575603127479553\n",
      "epoch:  39   step:  25   train loss:  0.0001981202804017812  val loss:  0.7502270936965942\n",
      "epoch:  39   step:  26   train loss:  0.00019655139476526529  val loss:  0.7627815008163452\n",
      "epoch:  39   step:  27   train loss:  0.00010645910515449941  val loss:  0.760445773601532\n",
      "epoch:  39   step:  28   train loss:  0.0004122638492844999  val loss:  0.7372944355010986\n",
      "epoch:  39   step:  29   train loss:  7.846133667044342e-05  val loss:  0.742591917514801\n",
      "epoch:  39   step:  30   train loss:  0.00026460521621629596  val loss:  0.7310103178024292\n",
      "epoch:  39   step:  31   train loss:  0.00021631974959746003  val loss:  0.7351356148719788\n",
      "epoch:  39   step:  32   train loss:  0.0005076149245724082  val loss:  0.7490956783294678\n",
      "epoch:  39   step:  33   train loss:  0.00019404018530622125  val loss:  0.7473180294036865\n",
      "epoch:  39   step:  34   train loss:  0.00022089012782089412  val loss:  0.7494431138038635\n",
      "epoch:  39   step:  35   train loss:  0.00023334989964496344  val loss:  0.7609326839447021\n",
      "epoch:  39   step:  36   train loss:  0.00019194718333892524  val loss:  0.7621749639511108\n",
      "epoch:  39   step:  37   train loss:  0.00014357958571054041  val loss:  0.7642541527748108\n",
      "epoch:  39   step:  38   train loss:  0.00015685558901168406  val loss:  0.7715582251548767\n",
      "epoch:  39   step:  39   train loss:  9.659073839429766e-05  val loss:  0.7726972103118896\n",
      "epoch:  39   step:  40   train loss:  0.00023395052994601429  val loss:  0.766508936882019\n",
      "epoch:  39   step:  41   train loss:  9.282398241339251e-05  val loss:  0.7676024436950684\n",
      "epoch:  39   step:  42   train loss:  6.033968384144828e-05  val loss:  0.7695953845977783\n",
      "epoch:  39   step:  43   train loss:  0.0003607581020332873  val loss:  0.7511512041091919\n",
      "epoch:  39   step:  44   train loss:  0.000306202273350209  val loss:  0.7470215559005737\n",
      "epoch:  39   step:  45   train loss:  0.00010999753430951387  val loss:  0.7408556938171387\n",
      "epoch:  39   step:  46   train loss:  0.00031505210790783167  val loss:  0.7419832944869995\n",
      "epoch:  39   step:  47   train loss:  0.00024007221509236842  val loss:  0.7522671818733215\n",
      "epoch:  39   step:  48   train loss:  0.0002503295545466244  val loss:  0.7614447474479675\n",
      "epoch:  39   step:  49   train loss:  0.00016428531671408564  val loss:  0.7720410823822021\n",
      "epoch:  39   step:  50   train loss:  0.00016578016220591962  val loss:  0.7806891202926636\n",
      "epoch:  39   step:  51   train loss:  0.00021762291726190597  val loss:  0.7751905918121338\n",
      "epoch:  39   step:  52   train loss:  0.00011007615830749273  val loss:  0.7779407501220703\n",
      "epoch:  39   step:  53   train loss:  0.0001717250852379948  val loss:  0.7661168575286865\n",
      "epoch:  39   step:  54   train loss:  0.00016510765999555588  val loss:  0.7716782093048096\n",
      "epoch:  39   step:  55   train loss:  0.00025119970086961985  val loss:  0.7655272483825684\n",
      "epoch:  39   step:  56   train loss:  0.0001870244595920667  val loss:  0.7730441093444824\n",
      "epoch:  39   step:  57   train loss:  0.00022119043569546193  val loss:  0.7816267013549805\n",
      "epoch:  39   step:  58   train loss:  7.615363574586809e-05  val loss:  0.7785986661911011\n",
      "epoch:  39   step:  59   train loss:  0.0002042680571321398  val loss:  0.7762375473976135\n",
      "epoch:  39   step:  60   train loss:  0.00014670247037429363  val loss:  0.7700933814048767\n",
      "epoch:  39   step:  61   train loss:  0.00019131728913635015  val loss:  0.7676785588264465\n",
      "epoch:  39   step:  62   train loss:  0.000412736990256235  val loss:  0.7396571040153503\n",
      "epoch:  39   step:  63   train loss:  0.00010405742796137929  val loss:  0.7419902682304382\n",
      "epoch:  39   step:  64   train loss:  0.0001413376594427973  val loss:  0.7425684332847595\n",
      "epoch:  39   step:  65   train loss:  0.0002542483271099627  val loss:  0.7454893589019775\n",
      "epoch:  39   step:  66   train loss:  0.00013371663226280361  val loss:  0.7474126815795898\n",
      "epoch:  39   step:  67   train loss:  5.674547719536349e-05  val loss:  0.7440218329429626\n",
      "epoch:  39   step:  68   train loss:  8.175796392606571e-05  val loss:  0.7385337948799133\n",
      "epoch:  39   step:  69   train loss:  0.00047631748020648956  val loss:  0.7447448372840881\n",
      "epoch:  39   step:  70   train loss:  9.344049612991512e-05  val loss:  0.7558727264404297\n",
      "epoch:  39   step:  71   train loss:  0.0001325477205682546  val loss:  0.7469853162765503\n",
      "epoch:  39   step:  72   train loss:  0.00013232875789981335  val loss:  0.7479510307312012\n",
      "epoch:  39   step:  73   train loss:  0.00014800665667280555  val loss:  0.744993269443512\n",
      "epoch:  39   step:  74   train loss:  0.00023786758538335562  val loss:  0.7455609440803528\n",
      "epoch:  39   step:  75   train loss:  0.00045912686618976295  val loss:  0.7445356845855713\n",
      "epoch:  39   step:  76   train loss:  0.00015892586088739336  val loss:  0.7570374011993408\n",
      "epoch:  39   step:  77   train loss:  0.00030278548365458846  val loss:  0.74449223279953\n",
      "epoch:  39   step:  78   train loss:  0.00015890186477918178  val loss:  0.7544033527374268\n",
      "epoch:  39   step:  79   train loss:  0.00010301758447894827  val loss:  0.7577432990074158\n",
      "epoch:  39   step:  80   train loss:  0.0001448421971872449  val loss:  0.7685554623603821\n",
      "epoch:  39   step:  81   train loss:  0.0003701001114677638  val loss:  0.7617440819740295\n",
      "epoch:  39   step:  82   train loss:  0.00012160911865066737  val loss:  0.7715103626251221\n",
      "epoch:  39   step:  83   train loss:  0.00024018397380132228  val loss:  0.7765568494796753\n",
      "epoch:  39   step:  84   train loss:  0.00016443069034721702  val loss:  0.7911717295646667\n",
      "epoch:  39   step:  85   train loss:  0.000192030260222964  val loss:  0.7808129787445068\n",
      "epoch:  39   step:  86   train loss:  0.00010371047392254695  val loss:  0.7714210152626038\n",
      "epoch:  39   step:  87   train loss:  0.00010113624739460647  val loss:  0.7725982666015625\n",
      "epoch:  39   step:  88   train loss:  7.004798681009561e-05  val loss:  0.7754554748535156\n",
      "epoch:  39   step:  89   train loss:  0.00021156069124117494  val loss:  0.7978363037109375\n",
      "epoch:  39   step:  90   train loss:  0.00022544877720065415  val loss:  0.7880502343177795\n",
      "epoch:  39   step:  91   train loss:  0.00011462922702776268  val loss:  0.7917545437812805\n",
      "epoch:  39   step:  92   train loss:  0.00012756763317156583  val loss:  0.7842241525650024\n",
      "epoch:  39   step:  93   train loss:  0.0002241493493784219  val loss:  0.7870025634765625\n",
      "epoch:  39   step:  94   train loss:  0.0006193051813170314  val loss:  0.7752724885940552\n",
      "epoch:  39   step:  95   train loss:  0.0002291400742251426  val loss:  0.7724441885948181\n",
      "epoch:  39   step:  96   train loss:  0.00013534212484955788  val loss:  0.7706476449966431\n",
      "epoch:  39   step:  97   train loss:  0.00010652031778590754  val loss:  0.7760027647018433\n",
      "epoch:  39   step:  98   train loss:  0.00012790384062100202  val loss:  0.7906956076622009\n",
      "epoch:  39   step:  99   train loss:  0.00015686973347328603  val loss:  0.7873851656913757\n",
      "epoch:  39   step:  100   train loss:  0.0001437957980670035  val loss:  0.7907399535179138\n",
      "epoch:  39   step:  101   train loss:  0.00022908295795787126  val loss:  0.8024641871452332\n",
      "epoch:  39   step:  102   train loss:  0.0006096180295571685  val loss:  0.7725462317466736\n",
      "epoch:  39   step:  103   train loss:  0.0003271813038736582  val loss:  0.7731717228889465\n",
      "epoch:  39   step:  104   train loss:  0.00016427290393039584  val loss:  0.7631648778915405\n",
      "epoch:  39   step:  105   train loss:  0.00023229069483932108  val loss:  0.7566841840744019\n",
      "epoch:  39   step:  106   train loss:  8.690246613696218e-05  val loss:  0.7591975927352905\n",
      "epoch:  39   step:  107   train loss:  0.00017468041914980859  val loss:  0.7573521733283997\n",
      "epoch:  39   step:  108   train loss:  6.735153874615207e-05  val loss:  0.7569394707679749\n",
      "epoch:  39   step:  109   train loss:  0.0001929345162352547  val loss:  0.754490315914154\n",
      "epoch:  39   step:  110   train loss:  0.0001585871068527922  val loss:  0.7636238932609558\n",
      "epoch:  39   step:  111   train loss:  0.00011815711332019418  val loss:  0.7735790610313416\n",
      "epoch:  39   step:  112   train loss:  0.0001511397131253034  val loss:  0.776333212852478\n",
      "epoch:  39   step:  113   train loss:  0.00013830536045134068  val loss:  0.7732111811637878\n",
      "epoch:  39   step:  114   train loss:  0.00010672019561752677  val loss:  0.776951789855957\n",
      "epoch:  39   step:  115   train loss:  0.0001700136053841561  val loss:  0.7602322101593018\n",
      "epoch:  39   step:  116   train loss:  0.0003439940046519041  val loss:  0.7802590131759644\n",
      "epoch:  39   step:  117   train loss:  0.00019817324937321246  val loss:  0.7854827046394348\n",
      "epoch:  39   step:  118   train loss:  0.00013542143278755248  val loss:  0.7902980446815491\n",
      "epoch:  39   step:  119   train loss:  0.0002953040529973805  val loss:  0.7926306128501892\n",
      "epoch:  39   step:  120   train loss:  0.0001959686487680301  val loss:  0.8053620457649231\n",
      "epoch:  39   step:  121   train loss:  0.00015255020116455853  val loss:  0.8009089231491089\n",
      "epoch:  39   step:  122   train loss:  0.001808640081435442  val loss:  0.8162716627120972\n",
      "epoch:  39   step:  123   train loss:  0.00022215512581169605  val loss:  0.8125165104866028\n",
      "epoch:  39   step:  124   train loss:  5.694297578884289e-05  val loss:  0.7999613881111145\n",
      "epoch:  39   step:  125   train loss:  0.00012543919729068875  val loss:  0.7890464663505554\n",
      "epoch:  39   step:  126   train loss:  7.057905895635486e-05  val loss:  0.7857363820075989\n",
      "epoch:  39   step:  127   train loss:  0.00017828200361691415  val loss:  0.7920708060264587\n",
      "epoch:  39   step:  128   train loss:  0.00020769306865986437  val loss:  0.7871394753456116\n",
      "epoch:  39   step:  129   train loss:  0.00014856428606435657  val loss:  0.7859941720962524\n",
      "epoch:  39   step:  130   train loss:  0.0001628202444408089  val loss:  0.7904306054115295\n",
      "epoch:  39   step:  131   train loss:  0.00011020194506272674  val loss:  0.778380811214447\n",
      "epoch:  39   step:  132   train loss:  0.00019471917767077684  val loss:  0.7978023886680603\n",
      "epoch:  39   step:  133   train loss:  0.00010774665861390531  val loss:  0.7890037894248962\n",
      "epoch:  39   step:  134   train loss:  0.00019700682605616748  val loss:  0.7772389054298401\n",
      "epoch:  39   step:  135   train loss:  0.0004128431901335716  val loss:  0.768625795841217\n",
      "epoch:  39   step:  136   train loss:  0.00019148584397044033  val loss:  0.7708768248558044\n",
      "epoch:  39   step:  137   train loss:  0.00015433266526088119  val loss:  0.7833500504493713\n",
      "epoch:  39   step:  138   train loss:  0.00016336377302650362  val loss:  0.7832537889480591\n",
      "epoch:  39   step:  139   train loss:  0.00023867905838415027  val loss:  0.7829295992851257\n",
      "epoch:  39   step:  140   train loss:  0.00022817260469309986  val loss:  0.7739928364753723\n",
      "epoch:  39   step:  141   train loss:  8.363636152353138e-05  val loss:  0.765866219997406\n",
      "epoch:  39   step:  142   train loss:  0.00031746557215228677  val loss:  0.7616602182388306\n",
      "epoch:  39   step:  143   train loss:  0.00034373378730379045  val loss:  0.7517082095146179\n",
      "epoch:  39   step:  144   train loss:  0.00019167544087395072  val loss:  0.7566117644309998\n",
      "epoch:  39   step:  145   train loss:  0.00022004448692314327  val loss:  0.7566297054290771\n",
      "epoch:  39   step:  146   train loss:  0.000190028891665861  val loss:  0.7586071491241455\n",
      "epoch:  39   step:  147   train loss:  0.00020924456475768238  val loss:  0.7558948993682861\n",
      "epoch:  39   step:  148   train loss:  0.0002307842660229653  val loss:  0.7543310523033142\n",
      "epoch:  39   step:  149   train loss:  5.183725443203002e-05  val loss:  0.7527442574501038\n",
      "epoch:  39   step:  150   train loss:  0.00012271880405023694  val loss:  0.741603434085846\n",
      "epoch:  39   step:  151   train loss:  0.00027931176009587944  val loss:  0.7447047233581543\n",
      "epoch:  39   step:  152   train loss:  0.00014940474648028612  val loss:  0.7577155232429504\n",
      "epoch:  39   step:  153   train loss:  0.00020355773449409753  val loss:  0.751449704170227\n",
      "epoch:  39   step:  154   train loss:  0.00022677460219711065  val loss:  0.7432565689086914\n",
      "epoch:  39   step:  155   train loss:  0.00012844818411394954  val loss:  0.737576961517334\n",
      "epoch:  39   step:  156   train loss:  0.00016042418428696692  val loss:  0.7390941977500916\n",
      "epoch:  39   step:  157   train loss:  4.974463809048757e-05  val loss:  0.7364950180053711\n",
      "epoch:  39   step:  158   train loss:  0.00019822722242679447  val loss:  0.739452064037323\n",
      "epoch:  39   step:  159   train loss:  0.0006196644390001893  val loss:  0.7202471494674683\n",
      "epoch:  39   step:  160   train loss:  0.00011155954416608438  val loss:  0.7388387322425842\n",
      "epoch:  39   step:  161   train loss:  0.00017651275265961885  val loss:  0.7559850811958313\n",
      "epoch:  39   step:  162   train loss:  0.0003139513428322971  val loss:  0.7463651299476624\n",
      "epoch:  39   step:  163   train loss:  7.907319377409294e-05  val loss:  0.7482025027275085\n",
      "epoch:  39   step:  164   train loss:  9.766750736162066e-05  val loss:  0.7577386498451233\n",
      "epoch:  39   step:  165   train loss:  0.00013554407632909715  val loss:  0.7626236081123352\n",
      "epoch:  40   step:  0   train loss:  8.887522562872618e-05  val loss:  0.7628305554389954\n",
      "epoch:  40   step:  1   train loss:  0.0001831787230912596  val loss:  0.7641416192054749\n",
      "epoch:  40   step:  2   train loss:  9.441786096431315e-05  val loss:  0.7628177404403687\n",
      "epoch:  40   step:  3   train loss:  0.00011196706327609718  val loss:  0.7504852414131165\n",
      "epoch:  40   step:  4   train loss:  0.0001125392664107494  val loss:  0.760962963104248\n",
      "epoch:  40   step:  5   train loss:  0.0003269532462581992  val loss:  0.78663569688797\n",
      "epoch:  40   step:  6   train loss:  0.00022918259492143989  val loss:  0.7947965860366821\n",
      "epoch:  40   step:  7   train loss:  0.00020584349113050848  val loss:  0.8018028140068054\n",
      "epoch:  40   step:  8   train loss:  0.00011383180390112102  val loss:  0.7959869503974915\n",
      "epoch:  40   step:  9   train loss:  8.227051876019686e-05  val loss:  0.7980868220329285\n",
      "epoch:  40   step:  10   train loss:  0.0012563553173094988  val loss:  0.7723594307899475\n",
      "epoch:  40   step:  11   train loss:  0.00018994015408679843  val loss:  0.7890238165855408\n",
      "epoch:  40   step:  12   train loss:  0.00017335348820779473  val loss:  0.8034415245056152\n",
      "epoch:  40   step:  13   train loss:  0.00013124250108376145  val loss:  0.7893217206001282\n",
      "epoch:  40   step:  14   train loss:  9.423639858141541e-05  val loss:  0.7738021612167358\n",
      "epoch:  40   step:  15   train loss:  0.00014643397298641503  val loss:  0.7633599638938904\n",
      "epoch:  40   step:  16   train loss:  0.00013069261331111193  val loss:  0.7677280306816101\n",
      "epoch:  40   step:  17   train loss:  0.00020847399719059467  val loss:  0.7611841559410095\n",
      "epoch:  40   step:  18   train loss:  0.00012125082139391452  val loss:  0.7591823935508728\n",
      "epoch:  40   step:  19   train loss:  0.00021202271454967558  val loss:  0.7637139558792114\n",
      "epoch:  40   step:  20   train loss:  0.00020522749400697649  val loss:  0.7591690421104431\n",
      "epoch:  40   step:  21   train loss:  0.00011066651495639235  val loss:  0.7607089281082153\n",
      "epoch:  40   step:  22   train loss:  0.00012236300972290337  val loss:  0.7654304504394531\n",
      "epoch:  40   step:  23   train loss:  0.0008975841337814927  val loss:  0.7391952872276306\n",
      "epoch:  40   step:  24   train loss:  0.00016264915757346898  val loss:  0.7466995120048523\n",
      "epoch:  40   step:  25   train loss:  0.00013413325359579176  val loss:  0.7412978410720825\n",
      "epoch:  40   step:  26   train loss:  0.0001471659488743171  val loss:  0.7406266331672668\n",
      "epoch:  40   step:  27   train loss:  6.485519406851381e-05  val loss:  0.7492036819458008\n",
      "epoch:  40   step:  28   train loss:  0.00010410686809336767  val loss:  0.7555471658706665\n",
      "epoch:  40   step:  29   train loss:  0.00010923923400696367  val loss:  0.7748545408248901\n",
      "epoch:  40   step:  30   train loss:  9.56886651692912e-05  val loss:  0.7674476504325867\n",
      "epoch:  40   step:  31   train loss:  7.092662417562678e-05  val loss:  0.7647744417190552\n",
      "epoch:  40   step:  32   train loss:  0.00012092306860722601  val loss:  0.756460428237915\n",
      "epoch:  40   step:  33   train loss:  0.00014814265887252986  val loss:  0.744871973991394\n",
      "epoch:  40   step:  34   train loss:  0.000123263627756387  val loss:  0.7414118051528931\n",
      "epoch:  40   step:  35   train loss:  0.00021611977717839181  val loss:  0.7452741265296936\n",
      "epoch:  40   step:  36   train loss:  7.20104289939627e-05  val loss:  0.7484715580940247\n",
      "epoch:  40   step:  37   train loss:  0.0001267321058548987  val loss:  0.7547953724861145\n",
      "epoch:  40   step:  38   train loss:  0.00011919211829081178  val loss:  0.7478423118591309\n",
      "epoch:  40   step:  39   train loss:  8.481704571750015e-05  val loss:  0.75525963306427\n",
      "epoch:  40   step:  40   train loss:  0.00012263478129170835  val loss:  0.7645645141601562\n",
      "epoch:  40   step:  41   train loss:  0.00021165945508982986  val loss:  0.7662561535835266\n",
      "epoch:  40   step:  42   train loss:  0.00011772100697271526  val loss:  0.7591074109077454\n",
      "epoch:  40   step:  43   train loss:  9.938979928847402e-05  val loss:  0.754337728023529\n",
      "epoch:  40   step:  44   train loss:  0.00014067947631701827  val loss:  0.7437519431114197\n",
      "epoch:  40   step:  45   train loss:  0.00011196546256542206  val loss:  0.7512033581733704\n",
      "epoch:  40   step:  46   train loss:  0.0001317751593887806  val loss:  0.7499932050704956\n",
      "epoch:  40   step:  47   train loss:  0.00017699840827845037  val loss:  0.745612382888794\n",
      "epoch:  40   step:  48   train loss:  0.0008904028800316155  val loss:  0.7347716093063354\n",
      "epoch:  40   step:  49   train loss:  0.0001621462870389223  val loss:  0.7346252202987671\n",
      "epoch:  40   step:  50   train loss:  0.0002102699363604188  val loss:  0.740844190120697\n",
      "epoch:  40   step:  51   train loss:  0.00012405827874317765  val loss:  0.7349812984466553\n",
      "epoch:  40   step:  52   train loss:  0.0001555759517941624  val loss:  0.7194685339927673\n",
      "epoch:  40   step:  53   train loss:  0.00015509873628616333  val loss:  0.7142219543457031\n",
      "epoch:  40   step:  54   train loss:  0.00026448938297107816  val loss:  0.7359215617179871\n",
      "epoch:  40   step:  55   train loss:  0.00012203094956930727  val loss:  0.7359349131584167\n",
      "epoch:  40   step:  56   train loss:  5.598109419224784e-05  val loss:  0.7451978921890259\n",
      "epoch:  40   step:  57   train loss:  0.00013901942293159664  val loss:  0.7506333589553833\n",
      "epoch:  40   step:  58   train loss:  0.00017777507309801877  val loss:  0.7594463229179382\n",
      "epoch:  40   step:  59   train loss:  0.00010085481335408986  val loss:  0.7601194381713867\n",
      "epoch:  40   step:  60   train loss:  0.00019787484779953957  val loss:  0.7577671408653259\n",
      "epoch:  40   step:  61   train loss:  0.0001006144520943053  val loss:  0.7539161443710327\n",
      "epoch:  40   step:  62   train loss:  0.00012887129560112953  val loss:  0.7691620588302612\n",
      "epoch:  40   step:  63   train loss:  0.000117300427518785  val loss:  0.7621021270751953\n",
      "epoch:  40   step:  64   train loss:  0.00014243907935451716  val loss:  0.766058087348938\n",
      "epoch:  40   step:  65   train loss:  0.00017026100249495357  val loss:  0.7618651986122131\n",
      "epoch:  40   step:  66   train loss:  0.00011713737330865115  val loss:  0.767373263835907\n",
      "epoch:  40   step:  67   train loss:  0.0002585435868240893  val loss:  0.7903834581375122\n",
      "epoch:  40   step:  68   train loss:  0.0001388621749356389  val loss:  0.7993909120559692\n",
      "epoch:  40   step:  69   train loss:  0.00013879340258426964  val loss:  0.7963404655456543\n",
      "epoch:  40   step:  70   train loss:  0.00013746164040639997  val loss:  0.7921841144561768\n",
      "epoch:  40   step:  71   train loss:  7.956735498737544e-05  val loss:  0.7866439819335938\n",
      "epoch:  40   step:  72   train loss:  0.0002318543556611985  val loss:  0.7693053483963013\n",
      "epoch:  40   step:  73   train loss:  0.000132733941427432  val loss:  0.7638317942619324\n",
      "epoch:  40   step:  74   train loss:  0.0002868256706278771  val loss:  0.7508863806724548\n",
      "epoch:  40   step:  75   train loss:  0.00014116177044343203  val loss:  0.7618712186813354\n",
      "epoch:  40   step:  76   train loss:  0.00011815226753242314  val loss:  0.7703912258148193\n",
      "epoch:  40   step:  77   train loss:  0.00015545111091341823  val loss:  0.7792258858680725\n",
      "epoch:  40   step:  78   train loss:  0.000262830697465688  val loss:  0.7934513688087463\n",
      "epoch:  40   step:  79   train loss:  0.0001474840537412092  val loss:  0.798437237739563\n",
      "epoch:  40   step:  80   train loss:  0.00010876112355617806  val loss:  0.7803168296813965\n",
      "epoch:  40   step:  81   train loss:  0.000233807775657624  val loss:  0.769037127494812\n",
      "epoch:  40   step:  82   train loss:  0.0007835439755581319  val loss:  0.7608656883239746\n",
      "epoch:  40   step:  83   train loss:  6.868377386126667e-05  val loss:  0.766225278377533\n",
      "epoch:  40   step:  84   train loss:  8.927892486099154e-05  val loss:  0.7637649178504944\n",
      "epoch:  40   step:  85   train loss:  0.00010950704745482653  val loss:  0.7609811425209045\n",
      "epoch:  40   step:  86   train loss:  7.726460171397775e-05  val loss:  0.7613507509231567\n",
      "epoch:  40   step:  87   train loss:  5.941073686699383e-05  val loss:  0.7638422250747681\n",
      "epoch:  40   step:  88   train loss:  0.00013246978051029146  val loss:  0.7712981104850769\n",
      "epoch:  40   step:  89   train loss:  0.0002553741214796901  val loss:  0.7712653279304504\n",
      "epoch:  40   step:  90   train loss:  0.00011625614570220932  val loss:  0.7746342420578003\n",
      "epoch:  40   step:  91   train loss:  0.00010803590703289956  val loss:  0.7818168997764587\n",
      "epoch:  40   step:  92   train loss:  0.00016570065054111183  val loss:  0.7886533141136169\n",
      "epoch:  40   step:  93   train loss:  0.00020017714996356517  val loss:  0.7676172256469727\n",
      "epoch:  40   step:  94   train loss:  0.00020342155767139047  val loss:  0.758259654045105\n",
      "epoch:  40   step:  95   train loss:  0.00022502485080622137  val loss:  0.7828094363212585\n",
      "epoch:  40   step:  96   train loss:  9.882782615022734e-05  val loss:  0.7894803285598755\n",
      "epoch:  40   step:  97   train loss:  0.0002779709466267377  val loss:  0.77315753698349\n",
      "epoch:  40   step:  98   train loss:  0.00012426651665009558  val loss:  0.7852941751480103\n",
      "epoch:  40   step:  99   train loss:  7.464214286301285e-05  val loss:  0.78728187084198\n",
      "epoch:  40   step:  100   train loss:  0.00010251197818433866  val loss:  0.7761333584785461\n",
      "epoch:  40   step:  101   train loss:  0.00012345473805908114  val loss:  0.7757385969161987\n",
      "epoch:  40   step:  102   train loss:  0.00015339604578912258  val loss:  0.785682737827301\n",
      "epoch:  40   step:  103   train loss:  9.037730342242867e-05  val loss:  0.7921662926673889\n",
      "epoch:  40   step:  104   train loss:  0.00010544914402998984  val loss:  0.7890671491622925\n",
      "epoch:  40   step:  105   train loss:  0.00011953852663282305  val loss:  0.80184006690979\n",
      "epoch:  40   step:  106   train loss:  0.0008364655077457428  val loss:  0.8113965392112732\n",
      "epoch:  40   step:  107   train loss:  0.0002018635132117197  val loss:  0.8094217777252197\n",
      "epoch:  40   step:  108   train loss:  0.00014539051335304976  val loss:  0.787453830242157\n",
      "epoch:  40   step:  109   train loss:  0.0001506880798842758  val loss:  0.7927101254463196\n",
      "epoch:  40   step:  110   train loss:  0.00013144849799573421  val loss:  0.787604033946991\n",
      "epoch:  40   step:  111   train loss:  0.00011126844037789851  val loss:  0.7877225875854492\n",
      "epoch:  40   step:  112   train loss:  0.00017647873028181493  val loss:  0.7767066955566406\n",
      "epoch:  40   step:  113   train loss:  0.00015519530279561877  val loss:  0.7829675674438477\n",
      "epoch:  40   step:  114   train loss:  0.0001674873346928507  val loss:  0.7860193252563477\n",
      "epoch:  40   step:  115   train loss:  0.00015948418877087533  val loss:  0.7931036949157715\n",
      "epoch:  40   step:  116   train loss:  9.262694220524281e-05  val loss:  0.7839443683624268\n",
      "epoch:  40   step:  117   train loss:  0.00014184258179739118  val loss:  0.7883390784263611\n",
      "epoch:  40   step:  118   train loss:  0.00010950004798360169  val loss:  0.7813079953193665\n",
      "epoch:  40   step:  119   train loss:  4.4721000449499115e-05  val loss:  0.7725008130073547\n",
      "epoch:  40   step:  120   train loss:  0.00015615703887306154  val loss:  0.7761696577072144\n",
      "epoch:  40   step:  121   train loss:  7.295993418665603e-05  val loss:  0.783160924911499\n",
      "epoch:  40   step:  122   train loss:  0.00047780631575733423  val loss:  0.8027093410491943\n",
      "epoch:  40   step:  123   train loss:  0.0002172631793655455  val loss:  0.7973523736000061\n",
      "epoch:  40   step:  124   train loss:  0.0001042200019583106  val loss:  0.7908777594566345\n",
      "epoch:  40   step:  125   train loss:  0.00847718771547079  val loss:  0.8083628416061401\n",
      "epoch:  40   step:  126   train loss:  0.00012303459516260773  val loss:  0.7867026925086975\n",
      "epoch:  40   step:  127   train loss:  0.00011534929944900796  val loss:  0.7665653824806213\n",
      "epoch:  40   step:  128   train loss:  9.558848978485912e-05  val loss:  0.7549542784690857\n",
      "epoch:  40   step:  129   train loss:  0.0001744877954479307  val loss:  0.7463136315345764\n",
      "epoch:  40   step:  130   train loss:  0.00020261536701582372  val loss:  0.7403088212013245\n",
      "epoch:  40   step:  131   train loss:  0.0003617219626903534  val loss:  0.7478139400482178\n",
      "epoch:  40   step:  132   train loss:  0.00022010121028870344  val loss:  0.7507432103157043\n",
      "epoch:  40   step:  133   train loss:  0.0005953791551291943  val loss:  0.7406182885169983\n",
      "epoch:  40   step:  134   train loss:  0.009329312480986118  val loss:  0.7239649891853333\n",
      "epoch:  40   step:  135   train loss:  0.0008645258494652808  val loss:  0.7074829936027527\n",
      "epoch:  40   step:  136   train loss:  0.00026798996259458363  val loss:  0.6968962550163269\n",
      "epoch:  40   step:  137   train loss:  0.00012242498632986099  val loss:  0.6846156120300293\n",
      "epoch:  40   step:  138   train loss:  0.00020090016187168658  val loss:  0.6878036260604858\n",
      "epoch:  40   step:  139   train loss:  0.00021789688616991043  val loss:  0.6763446927070618\n",
      "epoch:  40   step:  140   train loss:  0.00022640642418991774  val loss:  0.6624011993408203\n",
      "epoch:  40   step:  141   train loss:  0.0004677891847677529  val loss:  0.6581094861030579\n",
      "epoch:  40   step:  142   train loss:  0.0005583990132436156  val loss:  0.673746645450592\n",
      "epoch:  40   step:  143   train loss:  0.00017171412764582783  val loss:  0.673408031463623\n",
      "epoch:  40   step:  144   train loss:  9.942566975951195e-05  val loss:  0.6804484128952026\n",
      "epoch:  40   step:  145   train loss:  0.0012004500022158027  val loss:  0.6918588876724243\n",
      "epoch:  40   step:  146   train loss:  0.000979444943368435  val loss:  0.6916868686676025\n",
      "epoch:  40   step:  147   train loss:  0.0005064242868684232  val loss:  0.6899425387382507\n",
      "epoch:  40   step:  148   train loss:  0.0005744780646637082  val loss:  0.6801525950431824\n",
      "epoch:  40   step:  149   train loss:  0.005031469278037548  val loss:  0.675238311290741\n",
      "epoch:  40   step:  150   train loss:  0.00012265617260709405  val loss:  0.6851295232772827\n",
      "epoch:  40   step:  151   train loss:  0.0005340665811672807  val loss:  0.7112942934036255\n",
      "epoch:  40   step:  152   train loss:  0.0003506310749799013  val loss:  0.7085307836532593\n",
      "epoch:  40   step:  153   train loss:  0.0004991954192519188  val loss:  0.7045204639434814\n",
      "epoch:  40   step:  154   train loss:  0.00031767727341502905  val loss:  0.7091122269630432\n",
      "epoch:  40   step:  155   train loss:  0.0006367971654981375  val loss:  0.7144805192947388\n",
      "epoch:  40   step:  156   train loss:  0.0001245425664819777  val loss:  0.7184203267097473\n",
      "epoch:  40   step:  157   train loss:  0.00020677839347627014  val loss:  0.717931866645813\n",
      "epoch:  40   step:  158   train loss:  0.00018149639072362334  val loss:  0.7196181416511536\n",
      "epoch:  40   step:  159   train loss:  0.0021193057764321566  val loss:  0.7194501161575317\n",
      "epoch:  40   step:  160   train loss:  0.0002242320915684104  val loss:  0.715173065662384\n",
      "epoch:  40   step:  161   train loss:  0.0014699890743941069  val loss:  0.7150231599807739\n",
      "epoch:  40   step:  162   train loss:  0.0030652182176709175  val loss:  0.7230474948883057\n",
      "epoch:  40   step:  163   train loss:  0.00022995553445070982  val loss:  0.728480875492096\n",
      "epoch:  40   step:  164   train loss:  0.0008589498465880752  val loss:  0.7502725720405579\n",
      "epoch:  40   step:  165   train loss:  0.0012019852874800563  val loss:  0.7344851493835449\n",
      "epoch:  41   step:  0   train loss:  0.00017412412853445858  val loss:  0.7318241000175476\n",
      "epoch:  41   step:  1   train loss:  0.0004242926661390811  val loss:  0.742560088634491\n",
      "epoch:  41   step:  2   train loss:  0.00047696271212771535  val loss:  0.7506103515625\n",
      "epoch:  41   step:  3   train loss:  0.00042920446139760315  val loss:  0.7698656916618347\n",
      "epoch:  41   step:  4   train loss:  0.00015117067960090935  val loss:  0.7695531249046326\n",
      "epoch:  41   step:  5   train loss:  0.00023247957869898528  val loss:  0.7811609506607056\n",
      "epoch:  41   step:  6   train loss:  0.0004410409601405263  val loss:  0.779495894908905\n",
      "epoch:  41   step:  7   train loss:  0.00037707481533288956  val loss:  0.7888473272323608\n",
      "epoch:  41   step:  8   train loss:  0.0014384977985173464  val loss:  0.7829064726829529\n",
      "epoch:  41   step:  9   train loss:  9.524557390250266e-05  val loss:  0.7967317700386047\n",
      "epoch:  41   step:  10   train loss:  0.0002306190726812929  val loss:  0.7808980941772461\n",
      "epoch:  41   step:  11   train loss:  0.0004664628067985177  val loss:  0.772695779800415\n",
      "epoch:  41   step:  12   train loss:  8.938589598983526e-05  val loss:  0.7736968994140625\n",
      "epoch:  41   step:  13   train loss:  7.659337279619649e-05  val loss:  0.7807749509811401\n",
      "epoch:  41   step:  14   train loss:  0.0001195364457089454  val loss:  0.7820760011672974\n",
      "epoch:  41   step:  15   train loss:  0.00036253355210646987  val loss:  0.7926253080368042\n",
      "epoch:  41   step:  16   train loss:  0.0004929067799821496  val loss:  0.7758963108062744\n",
      "epoch:  41   step:  17   train loss:  0.0005668931407853961  val loss:  0.7786157131195068\n",
      "epoch:  41   step:  18   train loss:  0.00022565545805264264  val loss:  0.7977386116981506\n",
      "epoch:  41   step:  19   train loss:  7.318105781450868e-05  val loss:  0.8035358786582947\n",
      "epoch:  41   step:  20   train loss:  0.00013465422671288252  val loss:  0.8076602220535278\n",
      "epoch:  41   step:  21   train loss:  0.00015280907973647118  val loss:  0.802987813949585\n",
      "epoch:  41   step:  22   train loss:  0.00035062897950410843  val loss:  0.7935263514518738\n",
      "epoch:  41   step:  23   train loss:  7.445146911777556e-05  val loss:  0.7948041558265686\n",
      "epoch:  41   step:  24   train loss:  0.0014039461966603994  val loss:  0.7738070487976074\n",
      "epoch:  41   step:  25   train loss:  0.00011166206968482584  val loss:  0.780768096446991\n",
      "epoch:  41   step:  26   train loss:  0.00013458504690788686  val loss:  0.7717512845993042\n",
      "epoch:  41   step:  27   train loss:  0.00011034978524548933  val loss:  0.7798712849617004\n",
      "epoch:  41   step:  28   train loss:  0.00014963746070861816  val loss:  0.7827132344245911\n",
      "epoch:  41   step:  29   train loss:  0.00023500260431319475  val loss:  0.7752460241317749\n",
      "epoch:  41   step:  30   train loss:  0.0004603289416991174  val loss:  0.7726642489433289\n",
      "epoch:  41   step:  31   train loss:  0.00014935381477698684  val loss:  0.7832552194595337\n",
      "epoch:  41   step:  32   train loss:  0.000129593419842422  val loss:  0.7907598614692688\n",
      "epoch:  41   step:  33   train loss:  0.0007795098936185241  val loss:  0.8128653764724731\n",
      "epoch:  41   step:  34   train loss:  0.00016816395509522408  val loss:  0.8060053586959839\n",
      "epoch:  41   step:  35   train loss:  0.000732953252736479  val loss:  0.8196248412132263\n",
      "epoch:  41   step:  36   train loss:  0.0015175488078966737  val loss:  0.8013914227485657\n",
      "epoch:  41   step:  37   train loss:  0.00010262237628921866  val loss:  0.8169771432876587\n",
      "epoch:  41   step:  38   train loss:  0.0002436465147184208  val loss:  0.8209521174430847\n",
      "epoch:  41   step:  39   train loss:  8.12851867522113e-05  val loss:  0.8209547400474548\n",
      "epoch:  41   step:  40   train loss:  0.0012364167487248778  val loss:  0.8036530613899231\n",
      "epoch:  41   step:  41   train loss:  0.00011466228170320392  val loss:  0.8055316209793091\n",
      "epoch:  41   step:  42   train loss:  0.00015298649668693542  val loss:  0.8025696277618408\n",
      "epoch:  41   step:  43   train loss:  6.771986954845488e-05  val loss:  0.7985557913780212\n",
      "epoch:  41   step:  44   train loss:  0.0002295467274961993  val loss:  0.7985542416572571\n",
      "epoch:  41   step:  45   train loss:  0.00015876986435614526  val loss:  0.7951033115386963\n",
      "epoch:  41   step:  46   train loss:  0.00016532068548258394  val loss:  0.7946102023124695\n",
      "epoch:  41   step:  47   train loss:  8.866345160640776e-05  val loss:  0.7998997569084167\n",
      "epoch:  41   step:  48   train loss:  0.00015348584565799683  val loss:  0.7980019450187683\n",
      "epoch:  41   step:  49   train loss:  0.00031716458033770323  val loss:  0.7937904000282288\n",
      "epoch:  41   step:  50   train loss:  9.996630979003385e-05  val loss:  0.800167977809906\n",
      "epoch:  41   step:  51   train loss:  0.0002672497066669166  val loss:  0.7900600433349609\n",
      "epoch:  41   step:  52   train loss:  7.040218042675406e-05  val loss:  0.7904537320137024\n",
      "epoch:  41   step:  53   train loss:  0.0018614429282024503  val loss:  0.8189063668251038\n",
      "epoch:  41   step:  54   train loss:  0.00013630863395519555  val loss:  0.8177669048309326\n",
      "epoch:  41   step:  55   train loss:  0.0002640659804455936  val loss:  0.8290970921516418\n",
      "epoch:  41   step:  56   train loss:  0.0001768724905559793  val loss:  0.8218765258789062\n",
      "epoch:  41   step:  57   train loss:  0.00012957389117218554  val loss:  0.8235226273536682\n",
      "epoch:  41   step:  58   train loss:  0.0003876877890434116  val loss:  0.8063839673995972\n",
      "epoch:  41   step:  59   train loss:  0.0002100458659697324  val loss:  0.7920964360237122\n",
      "epoch:  41   step:  60   train loss:  0.00013491639401763678  val loss:  0.7932937741279602\n",
      "epoch:  41   step:  61   train loss:  0.0012522698380053043  val loss:  0.7966074347496033\n",
      "epoch:  41   step:  62   train loss:  9.984008647734299e-05  val loss:  0.8049689531326294\n",
      "epoch:  41   step:  63   train loss:  0.0001293863751925528  val loss:  0.7996693253517151\n",
      "epoch:  41   step:  64   train loss:  0.000499128713272512  val loss:  0.7912924885749817\n",
      "epoch:  41   step:  65   train loss:  0.000205053758691065  val loss:  0.787955105304718\n",
      "epoch:  41   step:  66   train loss:  0.00018402245768811554  val loss:  0.8048412799835205\n",
      "epoch:  41   step:  67   train loss:  0.00019162641547154635  val loss:  0.8039416074752808\n",
      "epoch:  41   step:  68   train loss:  0.0005778269842267036  val loss:  0.8181008696556091\n",
      "epoch:  41   step:  69   train loss:  0.00016933397273533046  val loss:  0.813231348991394\n",
      "epoch:  41   step:  70   train loss:  0.0001856444141594693  val loss:  0.8137474656105042\n",
      "epoch:  41   step:  71   train loss:  0.00026241090381518006  val loss:  0.8279865980148315\n",
      "epoch:  41   step:  72   train loss:  9.40296013141051e-05  val loss:  0.8205965161323547\n",
      "epoch:  41   step:  73   train loss:  0.00023764892830513418  val loss:  0.8265090584754944\n",
      "epoch:  41   step:  74   train loss:  0.00014002231182530522  val loss:  0.8163249492645264\n",
      "epoch:  41   step:  75   train loss:  7.088833081070334e-05  val loss:  0.8223229050636292\n",
      "epoch:  41   step:  76   train loss:  0.00012197312025818974  val loss:  0.8079249858856201\n",
      "epoch:  41   step:  77   train loss:  0.00019905116641893983  val loss:  0.8130743503570557\n",
      "epoch:  41   step:  78   train loss:  8.694114512763917e-05  val loss:  0.8164929151535034\n",
      "epoch:  41   step:  79   train loss:  7.914084562798962e-05  val loss:  0.8109878301620483\n",
      "epoch:  41   step:  80   train loss:  0.00018068063945975155  val loss:  0.8171288967132568\n",
      "epoch:  41   step:  81   train loss:  0.0001961991365533322  val loss:  0.8276440501213074\n",
      "epoch:  41   step:  82   train loss:  0.0003440087893977761  val loss:  0.8127513527870178\n",
      "epoch:  41   step:  83   train loss:  7.964029646245763e-05  val loss:  0.8189635872840881\n",
      "epoch:  41   step:  84   train loss:  0.00011541842832230031  val loss:  0.816670298576355\n",
      "epoch:  41   step:  85   train loss:  0.00029821708449162543  val loss:  0.8024424314498901\n",
      "epoch:  41   step:  86   train loss:  0.0007007492240518332  val loss:  0.8017339706420898\n",
      "epoch:  41   step:  87   train loss:  7.405108772218227e-05  val loss:  0.7970615029335022\n",
      "epoch:  41   step:  88   train loss:  0.00014153755910228938  val loss:  0.7909119129180908\n",
      "epoch:  41   step:  89   train loss:  0.00020743026107084006  val loss:  0.786723256111145\n",
      "epoch:  41   step:  90   train loss:  0.00012065094779245555  val loss:  0.7822813391685486\n",
      "epoch:  41   step:  91   train loss:  0.00011214178812224418  val loss:  0.793964684009552\n",
      "epoch:  41   step:  92   train loss:  0.00026503042317926884  val loss:  0.7949419021606445\n",
      "epoch:  41   step:  93   train loss:  0.0001648767793085426  val loss:  0.7973161339759827\n",
      "epoch:  41   step:  94   train loss:  0.0001576101640239358  val loss:  0.7898483872413635\n",
      "epoch:  41   step:  95   train loss:  0.0001991522149182856  val loss:  0.7934015989303589\n",
      "epoch:  41   step:  96   train loss:  0.00017087822197936475  val loss:  0.7798483371734619\n",
      "epoch:  41   step:  97   train loss:  0.0001518696080893278  val loss:  0.7865062355995178\n",
      "epoch:  41   step:  98   train loss:  0.0001001912314677611  val loss:  0.78761887550354\n",
      "epoch:  41   step:  99   train loss:  0.00010572892642812803  val loss:  0.791668713092804\n",
      "epoch:  41   step:  100   train loss:  0.00022632291074842215  val loss:  0.7713204622268677\n",
      "epoch:  41   step:  101   train loss:  0.00015681888908147812  val loss:  0.7764065265655518\n",
      "epoch:  41   step:  102   train loss:  9.682867676019669e-05  val loss:  0.7825957536697388\n",
      "epoch:  41   step:  103   train loss:  8.310656994581223e-05  val loss:  0.7818096280097961\n",
      "epoch:  41   step:  104   train loss:  5.134409002494067e-05  val loss:  0.7835385203361511\n",
      "epoch:  41   step:  105   train loss:  5.607104685623199e-05  val loss:  0.7741454839706421\n",
      "epoch:  41   step:  106   train loss:  0.0005176207050681114  val loss:  0.7755395174026489\n",
      "epoch:  41   step:  107   train loss:  0.00015252429875545204  val loss:  0.7830718159675598\n",
      "epoch:  41   step:  108   train loss:  0.0002699127944651991  val loss:  0.7865273356437683\n",
      "epoch:  41   step:  109   train loss:  0.00015936291310936213  val loss:  0.7873814702033997\n",
      "epoch:  41   step:  110   train loss:  0.00017630649381317198  val loss:  0.8003225326538086\n",
      "epoch:  41   step:  111   train loss:  8.499438990838826e-05  val loss:  0.8049001693725586\n",
      "epoch:  41   step:  112   train loss:  0.00016567111015319824  val loss:  0.8012703657150269\n",
      "epoch:  41   step:  113   train loss:  0.0001053632004186511  val loss:  0.7931655049324036\n",
      "epoch:  41   step:  114   train loss:  0.0001859418407548219  val loss:  0.7952340245246887\n",
      "epoch:  41   step:  115   train loss:  7.152227044571191e-05  val loss:  0.8044997453689575\n",
      "epoch:  41   step:  116   train loss:  0.00012540514580905437  val loss:  0.816823422908783\n",
      "epoch:  41   step:  117   train loss:  5.97946964262519e-05  val loss:  0.8062512278556824\n",
      "epoch:  41   step:  118   train loss:  0.00010595312051009387  val loss:  0.810312807559967\n",
      "epoch:  41   step:  119   train loss:  0.00012075367703801021  val loss:  0.8191311359405518\n",
      "epoch:  41   step:  120   train loss:  0.00019414977577980608  val loss:  0.8378812074661255\n",
      "epoch:  41   step:  121   train loss:  0.00020674797997344285  val loss:  0.8368685245513916\n",
      "epoch:  41   step:  122   train loss:  0.00010649344039848074  val loss:  0.8393028974533081\n",
      "epoch:  41   step:  123   train loss:  5.3852440032642335e-05  val loss:  0.8359004855155945\n",
      "epoch:  41   step:  124   train loss:  0.00010748919157776982  val loss:  0.833087682723999\n",
      "epoch:  41   step:  125   train loss:  0.0001214232761412859  val loss:  0.8198893666267395\n",
      "epoch:  41   step:  126   train loss:  0.0001908312551677227  val loss:  0.8110420107841492\n",
      "epoch:  41   step:  127   train loss:  9.613859583623707e-05  val loss:  0.7999338507652283\n",
      "epoch:  41   step:  128   train loss:  0.00023230780789162964  val loss:  0.8028925657272339\n",
      "epoch:  41   step:  129   train loss:  0.0002504452131688595  val loss:  0.8124241232872009\n",
      "epoch:  41   step:  130   train loss:  6.971254333620891e-05  val loss:  0.7970563173294067\n",
      "epoch:  41   step:  131   train loss:  8.20326094981283e-05  val loss:  0.7952445149421692\n",
      "epoch:  41   step:  132   train loss:  8.243267802754417e-05  val loss:  0.7927224040031433\n",
      "epoch:  41   step:  133   train loss:  0.00021490579820238054  val loss:  0.8063192367553711\n",
      "epoch:  41   step:  134   train loss:  0.00010857991583179682  val loss:  0.7918961644172668\n",
      "epoch:  41   step:  135   train loss:  9.173930448014289e-05  val loss:  0.8024146556854248\n",
      "epoch:  41   step:  136   train loss:  0.00011313851427985355  val loss:  0.8201108574867249\n",
      "epoch:  41   step:  137   train loss:  0.00023603280715178698  val loss:  0.8235511779785156\n",
      "epoch:  41   step:  138   train loss:  0.0003971025871578604  val loss:  0.8384847640991211\n",
      "epoch:  41   step:  139   train loss:  8.735741721466184e-05  val loss:  0.8412767648696899\n",
      "epoch:  41   step:  140   train loss:  0.00029228132916614413  val loss:  0.838897705078125\n",
      "epoch:  41   step:  141   train loss:  0.00011812704906333238  val loss:  0.8272860050201416\n",
      "epoch:  41   step:  142   train loss:  0.00025874702259898186  val loss:  0.8295872211456299\n",
      "epoch:  41   step:  143   train loss:  0.00011788133269874379  val loss:  0.8316187262535095\n",
      "epoch:  41   step:  144   train loss:  0.00013161978858988732  val loss:  0.8238852024078369\n",
      "epoch:  41   step:  145   train loss:  0.00014212583482731134  val loss:  0.8109471797943115\n",
      "epoch:  41   step:  146   train loss:  0.00012288686411920935  val loss:  0.8134177923202515\n",
      "epoch:  41   step:  147   train loss:  0.00030527496710419655  val loss:  0.8303597569465637\n",
      "epoch:  41   step:  148   train loss:  0.0005067058955319226  val loss:  0.805568516254425\n",
      "epoch:  41   step:  149   train loss:  0.00014378741616383195  val loss:  0.7885222434997559\n",
      "epoch:  41   step:  150   train loss:  8.897734369384125e-05  val loss:  0.7786421179771423\n",
      "epoch:  41   step:  151   train loss:  6.88490763423033e-05  val loss:  0.7941139340400696\n",
      "epoch:  41   step:  152   train loss:  0.0001234600495081395  val loss:  0.7908265590667725\n",
      "epoch:  41   step:  153   train loss:  0.00014484234270639718  val loss:  0.7772923111915588\n",
      "epoch:  41   step:  154   train loss:  5.6175238569267094e-05  val loss:  0.7720966339111328\n",
      "epoch:  41   step:  155   train loss:  0.0010977766942232847  val loss:  0.756272554397583\n",
      "epoch:  41   step:  156   train loss:  0.00035125997965224087  val loss:  0.7809730172157288\n",
      "epoch:  41   step:  157   train loss:  7.434617873514071e-05  val loss:  0.7937606573104858\n",
      "epoch:  41   step:  158   train loss:  0.00011697600712068379  val loss:  0.7965153455734253\n",
      "epoch:  41   step:  159   train loss:  0.0001356858410872519  val loss:  0.8209728598594666\n",
      "epoch:  41   step:  160   train loss:  0.00012202898506075144  val loss:  0.8194684982299805\n",
      "epoch:  41   step:  161   train loss:  0.0001433340075891465  val loss:  0.8098328709602356\n",
      "epoch:  41   step:  162   train loss:  0.00010377635771874338  val loss:  0.8113415837287903\n",
      "epoch:  41   step:  163   train loss:  0.00046839899732731283  val loss:  0.7971951961517334\n",
      "epoch:  41   step:  164   train loss:  0.00042169480002485216  val loss:  0.7794724702835083\n",
      "epoch:  41   step:  165   train loss:  0.00026180865825153887  val loss:  0.7591959238052368\n",
      "epoch:  42   step:  0   train loss:  0.00013066262181382626  val loss:  0.7625452876091003\n",
      "epoch:  42   step:  1   train loss:  0.00011377339251339436  val loss:  0.7829715609550476\n",
      "epoch:  42   step:  2   train loss:  0.00012651912402361631  val loss:  0.7782307863235474\n",
      "epoch:  42   step:  3   train loss:  6.647930422332138e-05  val loss:  0.7750242352485657\n",
      "epoch:  42   step:  4   train loss:  0.00014547804312314838  val loss:  0.7884765267372131\n",
      "epoch:  42   step:  5   train loss:  0.00024066302285064012  val loss:  0.7810312509536743\n",
      "epoch:  42   step:  6   train loss:  8.673182310303673e-05  val loss:  0.7728318572044373\n",
      "epoch:  42   step:  7   train loss:  9.3949529400561e-05  val loss:  0.7862885594367981\n",
      "epoch:  42   step:  8   train loss:  8.889222226571292e-05  val loss:  0.800331711769104\n",
      "epoch:  42   step:  9   train loss:  0.00011921862460440025  val loss:  0.786616325378418\n",
      "epoch:  42   step:  10   train loss:  0.00013288868649397045  val loss:  0.7993358969688416\n",
      "epoch:  42   step:  11   train loss:  6.748782470822334e-05  val loss:  0.8050539493560791\n",
      "epoch:  42   step:  12   train loss:  6.651459261775017e-05  val loss:  0.8250768184661865\n",
      "epoch:  42   step:  13   train loss:  0.0001969613367691636  val loss:  0.8342762589454651\n",
      "epoch:  42   step:  14   train loss:  0.00010833209671545774  val loss:  0.8310596346855164\n",
      "epoch:  42   step:  15   train loss:  5.8008081396110356e-05  val loss:  0.8214144706726074\n",
      "epoch:  42   step:  16   train loss:  0.00022507573885377496  val loss:  0.818863570690155\n",
      "epoch:  42   step:  17   train loss:  6.527132063638419e-05  val loss:  0.825603187084198\n",
      "epoch:  42   step:  18   train loss:  0.00010671999189071357  val loss:  0.8162931799888611\n",
      "epoch:  42   step:  19   train loss:  0.00011954635556321591  val loss:  0.8131303191184998\n",
      "epoch:  42   step:  20   train loss:  6.474938709288836e-05  val loss:  0.8165722489356995\n",
      "epoch:  42   step:  21   train loss:  8.72174568939954e-05  val loss:  0.8174414038658142\n",
      "epoch:  42   step:  22   train loss:  5.405215415521525e-05  val loss:  0.8218210339546204\n",
      "epoch:  42   step:  23   train loss:  5.2507421059999615e-05  val loss:  0.8510311841964722\n",
      "epoch:  42   step:  24   train loss:  5.6167518778238446e-05  val loss:  0.8470122814178467\n",
      "epoch:  42   step:  25   train loss:  0.0001722770102787763  val loss:  0.836106538772583\n",
      "epoch:  42   step:  26   train loss:  7.2542141424492e-05  val loss:  0.8278509974479675\n",
      "epoch:  42   step:  27   train loss:  4.865415030508302e-05  val loss:  0.8234228491783142\n",
      "epoch:  42   step:  28   train loss:  8.129186608130112e-05  val loss:  0.8204309940338135\n",
      "epoch:  42   step:  29   train loss:  9.8413358500693e-05  val loss:  0.8083919882774353\n",
      "epoch:  42   step:  30   train loss:  0.00014399882638826966  val loss:  0.789474606513977\n",
      "epoch:  42   step:  31   train loss:  0.00011445736163295805  val loss:  0.7849729061126709\n",
      "epoch:  42   step:  32   train loss:  0.0001064692551153712  val loss:  0.7721961140632629\n",
      "epoch:  42   step:  33   train loss:  5.8397250541020185e-05  val loss:  0.7777153849601746\n",
      "epoch:  42   step:  34   train loss:  8.875549247022718e-05  val loss:  0.7815125584602356\n",
      "epoch:  42   step:  35   train loss:  7.672738865949214e-05  val loss:  0.784905731678009\n",
      "epoch:  42   step:  36   train loss:  8.926507143769413e-05  val loss:  0.7924812436103821\n",
      "epoch:  42   step:  37   train loss:  0.00012433648225851357  val loss:  0.7972524166107178\n",
      "epoch:  42   step:  38   train loss:  9.484963811701164e-05  val loss:  0.7986071705818176\n",
      "epoch:  42   step:  39   train loss:  0.00014115372323431075  val loss:  0.8065515160560608\n",
      "epoch:  42   step:  40   train loss:  0.0004094523610547185  val loss:  0.7934766411781311\n",
      "epoch:  42   step:  41   train loss:  6.853848026366904e-05  val loss:  0.8000955581665039\n",
      "epoch:  42   step:  42   train loss:  0.0001355672866338864  val loss:  0.7950630784034729\n",
      "epoch:  42   step:  43   train loss:  0.00010769711661851034  val loss:  0.7888228297233582\n",
      "epoch:  42   step:  44   train loss:  4.628716123988852e-05  val loss:  0.8027297854423523\n",
      "epoch:  42   step:  45   train loss:  7.428864773828536e-05  val loss:  0.8131332397460938\n",
      "epoch:  42   step:  46   train loss:  0.00011731175618479028  val loss:  0.8199063539505005\n",
      "epoch:  42   step:  47   train loss:  0.00017440404917579144  val loss:  0.8052589893341064\n",
      "epoch:  42   step:  48   train loss:  0.00012917087587993592  val loss:  0.8103320598602295\n",
      "epoch:  42   step:  49   train loss:  0.00011671453103190288  val loss:  0.8298719525337219\n",
      "epoch:  42   step:  50   train loss:  8.042193076107651e-05  val loss:  0.8515516519546509\n",
      "epoch:  42   step:  51   train loss:  0.0001007636747090146  val loss:  0.8565765023231506\n",
      "epoch:  42   step:  52   train loss:  5.975014209980145e-05  val loss:  0.8425315022468567\n",
      "epoch:  42   step:  53   train loss:  0.00012571262777782977  val loss:  0.8211958408355713\n",
      "epoch:  42   step:  54   train loss:  7.897404429968446e-05  val loss:  0.8197638392448425\n",
      "epoch:  42   step:  55   train loss:  8.628149225842208e-05  val loss:  0.815486490726471\n",
      "epoch:  42   step:  56   train loss:  7.348954386543483e-05  val loss:  0.8169796466827393\n",
      "epoch:  42   step:  57   train loss:  0.00012675147445406765  val loss:  0.8274538516998291\n",
      "epoch:  42   step:  58   train loss:  0.00015186253585852683  val loss:  0.8422120213508606\n",
      "epoch:  42   step:  59   train loss:  7.258372352225706e-05  val loss:  0.8234754800796509\n",
      "epoch:  42   step:  60   train loss:  0.0001450593990739435  val loss:  0.8141192197799683\n",
      "epoch:  42   step:  61   train loss:  0.00010193945490755141  val loss:  0.8098373413085938\n",
      "epoch:  42   step:  62   train loss:  6.0838159697595984e-05  val loss:  0.8081991672515869\n",
      "epoch:  42   step:  63   train loss:  0.00010777840361697599  val loss:  0.7955046892166138\n",
      "epoch:  42   step:  64   train loss:  4.812878978555091e-05  val loss:  0.799717903137207\n",
      "epoch:  42   step:  65   train loss:  0.00012384836736600846  val loss:  0.8057995438575745\n",
      "epoch:  42   step:  66   train loss:  0.00012277934001758695  val loss:  0.7960848212242126\n",
      "epoch:  42   step:  67   train loss:  0.00010187084990320727  val loss:  0.7983325719833374\n",
      "epoch:  42   step:  68   train loss:  0.00010141193342860788  val loss:  0.8055347800254822\n",
      "epoch:  42   step:  69   train loss:  7.951770385261625e-05  val loss:  0.8175767064094543\n",
      "epoch:  42   step:  70   train loss:  0.00013658491661772132  val loss:  0.830790102481842\n",
      "epoch:  42   step:  71   train loss:  0.00014636036939918995  val loss:  0.828274130821228\n",
      "epoch:  42   step:  72   train loss:  0.00015613822324667126  val loss:  0.8177697658538818\n",
      "epoch:  42   step:  73   train loss:  0.0002570169745013118  val loss:  0.812118649482727\n",
      "epoch:  42   step:  74   train loss:  7.146010466385633e-05  val loss:  0.8089455962181091\n",
      "epoch:  42   step:  75   train loss:  6.673592724837363e-05  val loss:  0.8022013902664185\n",
      "epoch:  42   step:  76   train loss:  7.901781646069139e-05  val loss:  0.8018232583999634\n",
      "epoch:  42   step:  77   train loss:  6.50498695904389e-05  val loss:  0.8149757981300354\n",
      "epoch:  42   step:  78   train loss:  6.470597872976214e-05  val loss:  0.8258890509605408\n",
      "epoch:  42   step:  79   train loss:  0.0002148545754607767  val loss:  0.8296422362327576\n",
      "epoch:  42   step:  80   train loss:  6.590600241906941e-05  val loss:  0.8243160247802734\n",
      "epoch:  42   step:  81   train loss:  7.358151924563572e-05  val loss:  0.8377354145050049\n",
      "epoch:  42   step:  82   train loss:  0.0002092853537760675  val loss:  0.8247076272964478\n",
      "epoch:  42   step:  83   train loss:  0.0001246387982973829  val loss:  0.8470382690429688\n",
      "epoch:  42   step:  84   train loss:  0.00011690905375871807  val loss:  0.834814190864563\n",
      "epoch:  42   step:  85   train loss:  9.498963481746614e-05  val loss:  0.8390694856643677\n",
      "epoch:  42   step:  86   train loss:  0.00011222308967262506  val loss:  0.8215049505233765\n",
      "epoch:  42   step:  87   train loss:  0.0001367763034068048  val loss:  0.8254785537719727\n",
      "epoch:  42   step:  88   train loss:  8.042868284974247e-05  val loss:  0.8323155641555786\n",
      "epoch:  42   step:  89   train loss:  3.731402102857828e-05  val loss:  0.8258222937583923\n",
      "epoch:  42   step:  90   train loss:  0.00015659202472306788  val loss:  0.8424265384674072\n",
      "epoch:  42   step:  91   train loss:  0.00010997786012012511  val loss:  0.8313877582550049\n",
      "epoch:  42   step:  92   train loss:  0.00018455696408636868  val loss:  0.821094810962677\n",
      "epoch:  42   step:  93   train loss:  0.00013126592966727912  val loss:  0.8206952214241028\n",
      "epoch:  42   step:  94   train loss:  0.00025696339434944093  val loss:  0.8303917646408081\n",
      "epoch:  42   step:  95   train loss:  6.495734851341695e-05  val loss:  0.8342393040657043\n",
      "epoch:  42   step:  96   train loss:  0.00017444575496483594  val loss:  0.8450374603271484\n",
      "epoch:  42   step:  97   train loss:  5.858813528902829e-05  val loss:  0.8580389022827148\n",
      "epoch:  42   step:  98   train loss:  0.00012173075083410367  val loss:  0.8713379502296448\n",
      "epoch:  42   step:  99   train loss:  0.00011490948963910341  val loss:  0.8690588474273682\n",
      "epoch:  42   step:  100   train loss:  0.0003903481119778007  val loss:  0.846270740032196\n",
      "epoch:  42   step:  101   train loss:  0.00012446152686607093  val loss:  0.854043185710907\n",
      "epoch:  42   step:  102   train loss:  0.0003688900324050337  val loss:  0.8302293419837952\n",
      "epoch:  42   step:  103   train loss:  0.00015083909966051579  val loss:  0.8315833210945129\n",
      "epoch:  42   step:  104   train loss:  0.00013575504999607801  val loss:  0.8271738290786743\n",
      "epoch:  42   step:  105   train loss:  8.318274194607511e-05  val loss:  0.8175739645957947\n",
      "epoch:  42   step:  106   train loss:  6.732437759637833e-05  val loss:  0.8005969524383545\n",
      "epoch:  42   step:  107   train loss:  0.00016177629004232585  val loss:  0.8131603598594666\n",
      "epoch:  42   step:  108   train loss:  0.00012871790386270732  val loss:  0.8074564337730408\n",
      "epoch:  42   step:  109   train loss:  5.093183426652104e-05  val loss:  0.8054225444793701\n",
      "epoch:  42   step:  110   train loss:  9.899475844576955e-05  val loss:  0.7996624112129211\n",
      "epoch:  42   step:  111   train loss:  0.00013314897660166025  val loss:  0.7957249879837036\n",
      "epoch:  42   step:  112   train loss:  0.00013920548371970654  val loss:  0.7733072638511658\n",
      "epoch:  42   step:  113   train loss:  0.0002807540586218238  val loss:  0.7975140810012817\n",
      "epoch:  42   step:  114   train loss:  6.70790541335009e-05  val loss:  0.7978700995445251\n",
      "epoch:  42   step:  115   train loss:  0.000987576087936759  val loss:  0.825427770614624\n",
      "epoch:  42   step:  116   train loss:  0.00014120421838015318  val loss:  0.8343749642372131\n",
      "epoch:  42   step:  117   train loss:  0.00016131930169649422  val loss:  0.828974723815918\n",
      "epoch:  42   step:  118   train loss:  7.171551987994462e-05  val loss:  0.8326051831245422\n",
      "epoch:  42   step:  119   train loss:  0.00011335714953020215  val loss:  0.8336060047149658\n",
      "epoch:  42   step:  120   train loss:  6.52516755508259e-05  val loss:  0.8263319730758667\n",
      "epoch:  42   step:  121   train loss:  6.579430191777647e-05  val loss:  0.8456913828849792\n",
      "epoch:  42   step:  122   train loss:  0.00013353972462937236  val loss:  0.8589619994163513\n",
      "epoch:  42   step:  123   train loss:  0.00023900803353171796  val loss:  0.8417481184005737\n",
      "epoch:  42   step:  124   train loss:  0.0001597152149770409  val loss:  0.8509793877601624\n",
      "epoch:  42   step:  125   train loss:  3.0788178264629096e-05  val loss:  0.8506382703781128\n",
      "epoch:  42   step:  126   train loss:  9.502169268671423e-05  val loss:  0.8637701272964478\n",
      "epoch:  42   step:  127   train loss:  5.63788489671424e-05  val loss:  0.8471157550811768\n",
      "epoch:  42   step:  128   train loss:  6.163782381918281e-05  val loss:  0.8447925448417664\n",
      "epoch:  42   step:  129   train loss:  0.00013585377018898726  val loss:  0.8523838520050049\n",
      "epoch:  42   step:  130   train loss:  9.096958092413843e-05  val loss:  0.838077962398529\n",
      "epoch:  42   step:  131   train loss:  7.641615229658782e-05  val loss:  0.8444609642028809\n",
      "epoch:  42   step:  132   train loss:  9.404611046193168e-05  val loss:  0.8444836735725403\n",
      "epoch:  42   step:  133   train loss:  0.0001415266888216138  val loss:  0.8531609177589417\n",
      "epoch:  42   step:  134   train loss:  0.00013918668264523149  val loss:  0.8656347393989563\n",
      "epoch:  42   step:  135   train loss:  0.00015948459622450173  val loss:  0.8699461817741394\n",
      "epoch:  42   step:  136   train loss:  0.00011903756239917129  val loss:  0.878832221031189\n",
      "epoch:  42   step:  137   train loss:  5.977657565381378e-05  val loss:  0.8712161183357239\n",
      "epoch:  42   step:  138   train loss:  0.00013978259812574834  val loss:  0.872920036315918\n",
      "epoch:  42   step:  139   train loss:  0.0001100022200262174  val loss:  0.8855204582214355\n",
      "epoch:  42   step:  140   train loss:  8.528360194759443e-05  val loss:  0.8743121027946472\n",
      "epoch:  42   step:  141   train loss:  0.00010648675379343331  val loss:  0.8826557993888855\n",
      "epoch:  42   step:  142   train loss:  0.00011816356709459797  val loss:  0.8888716101646423\n",
      "epoch:  42   step:  143   train loss:  0.00012005139433313161  val loss:  0.8817284107208252\n",
      "epoch:  42   step:  144   train loss:  0.00015243982488755137  val loss:  0.8994567394256592\n",
      "epoch:  42   step:  145   train loss:  8.488247112836689e-05  val loss:  0.8937689661979675\n",
      "epoch:  42   step:  146   train loss:  7.953381282277405e-05  val loss:  0.889751672744751\n",
      "epoch:  42   step:  147   train loss:  7.572630420327187e-05  val loss:  0.896316647529602\n",
      "epoch:  42   step:  148   train loss:  4.672285285778344e-05  val loss:  0.9121535420417786\n",
      "epoch:  42   step:  149   train loss:  5.001904355594888e-05  val loss:  0.9030746221542358\n",
      "epoch:  42   step:  150   train loss:  0.00010697340621845797  val loss:  0.9066746234893799\n",
      "epoch:  42   step:  151   train loss:  0.00013168741133995354  val loss:  0.890579104423523\n",
      "epoch:  42   step:  152   train loss:  0.00014793228183407336  val loss:  0.8969939947128296\n",
      "epoch:  42   step:  153   train loss:  5.4087096941657364e-05  val loss:  0.8860291838645935\n",
      "epoch:  42   step:  154   train loss:  9.23035986488685e-05  val loss:  0.8869574666023254\n",
      "epoch:  42   step:  155   train loss:  5.048672028351575e-05  val loss:  0.8716854453086853\n",
      "epoch:  42   step:  156   train loss:  0.00010530008876230568  val loss:  0.8675135374069214\n",
      "epoch:  42   step:  157   train loss:  0.00011777738836826757  val loss:  0.8610110878944397\n",
      "epoch:  42   step:  158   train loss:  0.0001457967737223953  val loss:  0.8445512056350708\n",
      "epoch:  42   step:  159   train loss:  0.00010155488416785374  val loss:  0.8329038619995117\n",
      "epoch:  42   step:  160   train loss:  0.00011343465303070843  val loss:  0.8445111513137817\n",
      "epoch:  42   step:  161   train loss:  0.00015752683975733817  val loss:  0.8415272235870361\n",
      "epoch:  42   step:  162   train loss:  4.656198507291265e-05  val loss:  0.8396633863449097\n",
      "epoch:  42   step:  163   train loss:  0.00010551905143074691  val loss:  0.8479419350624084\n",
      "epoch:  42   step:  164   train loss:  0.00020576328097376972  val loss:  0.8481773138046265\n",
      "epoch:  42   step:  165   train loss:  2.3670894734095782e-05  val loss:  0.8406863212585449\n",
      "epoch:  43   step:  0   train loss:  0.00012885336764156818  val loss:  0.8583737015724182\n",
      "epoch:  43   step:  1   train loss:  0.0001035748646245338  val loss:  0.8654304146766663\n",
      "epoch:  43   step:  2   train loss:  0.00010266817116644233  val loss:  0.8769554495811462\n",
      "epoch:  43   step:  3   train loss:  0.0001307687780354172  val loss:  0.8943936228752136\n",
      "epoch:  43   step:  4   train loss:  0.00019029089889954776  val loss:  0.8829224109649658\n",
      "epoch:  43   step:  5   train loss:  6.172253779368475e-05  val loss:  0.8697605729103088\n",
      "epoch:  43   step:  6   train loss:  7.527988054789603e-05  val loss:  0.8751166462898254\n",
      "epoch:  43   step:  7   train loss:  0.00016239378601312637  val loss:  0.8654584884643555\n",
      "epoch:  43   step:  8   train loss:  6.631576979998499e-05  val loss:  0.8579075336456299\n",
      "epoch:  43   step:  9   train loss:  6.585403752978891e-05  val loss:  0.854607880115509\n",
      "epoch:  43   step:  10   train loss:  0.00017803465016186237  val loss:  0.8327866792678833\n",
      "epoch:  43   step:  11   train loss:  4.6412256779149175e-05  val loss:  0.8397228121757507\n",
      "epoch:  43   step:  12   train loss:  9.70633263932541e-05  val loss:  0.8428375124931335\n",
      "epoch:  43   step:  13   train loss:  4.573562182486057e-05  val loss:  0.8511765003204346\n",
      "epoch:  43   step:  14   train loss:  4.0896935388445854e-05  val loss:  0.8537358641624451\n",
      "epoch:  43   step:  15   train loss:  6.417207623599097e-05  val loss:  0.8606411814689636\n",
      "epoch:  43   step:  16   train loss:  3.9894886867841706e-05  val loss:  0.8403993248939514\n",
      "epoch:  43   step:  17   train loss:  0.00017859265790320933  val loss:  0.869463324546814\n",
      "epoch:  43   step:  18   train loss:  4.951169103151187e-05  val loss:  0.8569362163543701\n",
      "epoch:  43   step:  19   train loss:  6.192683940753341e-05  val loss:  0.8631693124771118\n",
      "epoch:  43   step:  20   train loss:  3.765695873880759e-05  val loss:  0.8638308644294739\n",
      "epoch:  43   step:  21   train loss:  4.5593842514790595e-05  val loss:  0.8684845566749573\n",
      "epoch:  43   step:  22   train loss:  7.974370964802802e-05  val loss:  0.86555016040802\n",
      "epoch:  43   step:  23   train loss:  0.00017944085993804038  val loss:  0.8882113695144653\n",
      "epoch:  43   step:  24   train loss:  6.648922862950712e-05  val loss:  0.8791477084159851\n",
      "epoch:  43   step:  25   train loss:  0.00010588963777991012  val loss:  0.8701574802398682\n",
      "epoch:  43   step:  26   train loss:  9.637153561925516e-05  val loss:  0.8786795735359192\n",
      "epoch:  43   step:  27   train loss:  6.0853955801576376e-05  val loss:  0.8845492601394653\n",
      "epoch:  43   step:  28   train loss:  8.383023669011891e-05  val loss:  0.8954663872718811\n",
      "epoch:  43   step:  29   train loss:  9.759378735907376e-05  val loss:  0.9101248979568481\n",
      "epoch:  43   step:  30   train loss:  6.424156890716404e-05  val loss:  0.9070414304733276\n",
      "epoch:  43   step:  31   train loss:  5.918529495829716e-05  val loss:  0.8990208506584167\n",
      "epoch:  43   step:  32   train loss:  6.784955621697009e-05  val loss:  0.9070671796798706\n",
      "epoch:  43   step:  33   train loss:  0.00010037697938969359  val loss:  0.9087038040161133\n",
      "epoch:  43   step:  34   train loss:  0.000102948397397995  val loss:  0.8980549573898315\n",
      "epoch:  43   step:  35   train loss:  4.533471292234026e-05  val loss:  0.8876097202301025\n",
      "epoch:  43   step:  36   train loss:  8.051610348047689e-05  val loss:  0.8730798363685608\n",
      "epoch:  43   step:  37   train loss:  0.00010734132956713438  val loss:  0.8729407787322998\n",
      "epoch:  43   step:  38   train loss:  6.527864752570167e-05  val loss:  0.8805259466171265\n",
      "epoch:  43   step:  39   train loss:  6.689297151751816e-05  val loss:  0.8967326283454895\n",
      "epoch:  43   step:  40   train loss:  0.0001012218272080645  val loss:  0.9004557728767395\n",
      "epoch:  43   step:  41   train loss:  6.767790182493627e-05  val loss:  0.8947976231575012\n",
      "epoch:  43   step:  42   train loss:  0.00017589630442671478  val loss:  0.8765853643417358\n",
      "epoch:  43   step:  43   train loss:  0.00011607135093072429  val loss:  0.874138593673706\n",
      "epoch:  43   step:  44   train loss:  0.0001280706492252648  val loss:  0.8837899565696716\n",
      "epoch:  43   step:  45   train loss:  6.429991481127217e-05  val loss:  0.8822892308235168\n",
      "epoch:  43   step:  46   train loss:  8.782666554907337e-05  val loss:  0.8879904747009277\n",
      "epoch:  43   step:  47   train loss:  0.0001409165997756645  val loss:  0.8733809590339661\n",
      "epoch:  43   step:  48   train loss:  7.106098928488791e-05  val loss:  0.8639802932739258\n",
      "epoch:  43   step:  49   train loss:  5.013831832911819e-05  val loss:  0.8547511696815491\n",
      "epoch:  43   step:  50   train loss:  5.32003614353016e-05  val loss:  0.862847626209259\n",
      "epoch:  43   step:  51   train loss:  7.018218457233161e-05  val loss:  0.8603148460388184\n",
      "epoch:  43   step:  52   train loss:  8.416230411967263e-05  val loss:  0.8573210835456848\n",
      "epoch:  43   step:  53   train loss:  0.00010606436990201473  val loss:  0.860482931137085\n",
      "epoch:  43   step:  54   train loss:  0.00013568268332164735  val loss:  0.8467761278152466\n",
      "epoch:  43   step:  55   train loss:  6.025200127623975e-05  val loss:  0.849244236946106\n",
      "epoch:  43   step:  56   train loss:  9.246417903341353e-05  val loss:  0.8437344431877136\n",
      "epoch:  43   step:  57   train loss:  4.64506410935428e-05  val loss:  0.8514618277549744\n",
      "epoch:  43   step:  58   train loss:  0.00013325695181265473  val loss:  0.8490540981292725\n",
      "epoch:  43   step:  59   train loss:  5.439406959339976e-05  val loss:  0.8386326432228088\n",
      "epoch:  43   step:  60   train loss:  3.374538937350735e-05  val loss:  0.8504872918128967\n",
      "epoch:  43   step:  61   train loss:  9.808500908548012e-05  val loss:  0.8513374328613281\n",
      "epoch:  43   step:  62   train loss:  5.591683293459937e-05  val loss:  0.860246479511261\n",
      "epoch:  43   step:  63   train loss:  8.728599641472101e-05  val loss:  0.8549903631210327\n",
      "epoch:  43   step:  64   train loss:  0.00021404039580374956  val loss:  0.8596227765083313\n",
      "epoch:  43   step:  65   train loss:  0.0001317090936936438  val loss:  0.8542710542678833\n",
      "epoch:  43   step:  66   train loss:  4.9516667786519974e-05  val loss:  0.8583540320396423\n",
      "epoch:  43   step:  67   train loss:  0.00017621922597754747  val loss:  0.8427459597587585\n",
      "epoch:  43   step:  68   train loss:  0.00010133989417226985  val loss:  0.838478147983551\n",
      "epoch:  43   step:  69   train loss:  7.47779049561359e-05  val loss:  0.8389414548873901\n",
      "epoch:  43   step:  70   train loss:  4.9530350224813446e-05  val loss:  0.8327921032905579\n",
      "epoch:  43   step:  71   train loss:  4.982226528227329e-05  val loss:  0.8268988132476807\n",
      "epoch:  43   step:  72   train loss:  0.00010361475870013237  val loss:  0.8285395503044128\n",
      "epoch:  43   step:  73   train loss:  0.00011128210462629795  val loss:  0.8408565521240234\n",
      "epoch:  43   step:  74   train loss:  5.290907938615419e-05  val loss:  0.8429893255233765\n",
      "epoch:  43   step:  75   train loss:  0.0001319578441325575  val loss:  0.836887776851654\n",
      "epoch:  43   step:  76   train loss:  0.00019801969756372273  val loss:  0.8255108594894409\n",
      "epoch:  43   step:  77   train loss:  5.7160948927048594e-05  val loss:  0.829007089138031\n",
      "epoch:  43   step:  78   train loss:  6.443470192607492e-05  val loss:  0.836644172668457\n",
      "epoch:  43   step:  79   train loss:  0.00015098098083399236  val loss:  0.8343786597251892\n",
      "epoch:  43   step:  80   train loss:  5.9185778809478506e-05  val loss:  0.8297661542892456\n",
      "epoch:  43   step:  81   train loss:  0.00012839498231187463  val loss:  0.8488804697990417\n",
      "epoch:  43   step:  82   train loss:  0.0001326794154010713  val loss:  0.8487005233764648\n",
      "epoch:  43   step:  83   train loss:  4.158983574598096e-05  val loss:  0.8452194929122925\n",
      "epoch:  43   step:  84   train loss:  9.150939877144992e-05  val loss:  0.8445968627929688\n",
      "epoch:  43   step:  85   train loss:  6.680944352410734e-05  val loss:  0.8442348837852478\n",
      "epoch:  43   step:  86   train loss:  0.00012774555943906307  val loss:  0.8631436228752136\n",
      "epoch:  43   step:  87   train loss:  5.286852683639154e-05  val loss:  0.8612561821937561\n",
      "epoch:  43   step:  88   train loss:  0.00012189159315312281  val loss:  0.8649520874023438\n",
      "epoch:  43   step:  89   train loss:  0.00010730017675086856  val loss:  0.8559802770614624\n",
      "epoch:  43   step:  90   train loss:  2.918965765275061e-05  val loss:  0.8438250422477722\n",
      "epoch:  43   step:  91   train loss:  3.4140106436097994e-05  val loss:  0.8518845438957214\n",
      "epoch:  43   step:  92   train loss:  4.822252594749443e-05  val loss:  0.8570886850357056\n",
      "epoch:  43   step:  93   train loss:  9.463430615141988e-05  val loss:  0.8661722540855408\n",
      "epoch:  43   step:  94   train loss:  0.00011667330545606092  val loss:  0.8602402806282043\n",
      "epoch:  43   step:  95   train loss:  9.814076474867761e-05  val loss:  0.8558404445648193\n",
      "epoch:  43   step:  96   train loss:  9.684958058642223e-05  val loss:  0.8527219891548157\n",
      "epoch:  43   step:  97   train loss:  5.992328442516737e-05  val loss:  0.8553453087806702\n",
      "epoch:  43   step:  98   train loss:  0.00017045732238329947  val loss:  0.8512048125267029\n",
      "epoch:  43   step:  99   train loss:  7.89238401921466e-05  val loss:  0.8637123703956604\n",
      "epoch:  43   step:  100   train loss:  0.00011342015204718336  val loss:  0.8585726618766785\n",
      "epoch:  43   step:  101   train loss:  2.773951564449817e-05  val loss:  0.8764770030975342\n",
      "epoch:  43   step:  102   train loss:  8.204058394767344e-05  val loss:  0.8814905881881714\n",
      "epoch:  43   step:  103   train loss:  9.186744864564389e-05  val loss:  0.8782601952552795\n",
      "epoch:  43   step:  104   train loss:  9.331230830866843e-05  val loss:  0.8797410130500793\n",
      "epoch:  43   step:  105   train loss:  0.00011313092545606196  val loss:  0.8868495225906372\n",
      "epoch:  43   step:  106   train loss:  4.076999903190881e-05  val loss:  0.8820826411247253\n",
      "epoch:  43   step:  107   train loss:  6.63423998048529e-05  val loss:  0.8669635057449341\n",
      "epoch:  43   step:  108   train loss:  0.00018253088637720793  val loss:  0.8715834021568298\n",
      "epoch:  43   step:  109   train loss:  7.46629448258318e-05  val loss:  0.8763651251792908\n",
      "epoch:  43   step:  110   train loss:  5.5186239478643984e-05  val loss:  0.8817747235298157\n",
      "epoch:  43   step:  111   train loss:  4.19513598899357e-05  val loss:  0.8936266899108887\n",
      "epoch:  43   step:  112   train loss:  6.639136699959636e-05  val loss:  0.8980284929275513\n",
      "epoch:  43   step:  113   train loss:  6.940618914086372e-05  val loss:  0.8995435833930969\n",
      "epoch:  43   step:  114   train loss:  0.00015825912123546004  val loss:  0.8814077377319336\n",
      "epoch:  43   step:  115   train loss:  0.00014074663340579718  val loss:  0.879391074180603\n",
      "epoch:  43   step:  116   train loss:  4.072213778272271e-05  val loss:  0.8699319362640381\n",
      "epoch:  43   step:  117   train loss:  4.906707181362435e-05  val loss:  0.8654501438140869\n",
      "epoch:  43   step:  118   train loss:  9.864936873782426e-05  val loss:  0.8647093176841736\n",
      "epoch:  43   step:  119   train loss:  5.5818982218625024e-05  val loss:  0.8732130527496338\n",
      "epoch:  43   step:  120   train loss:  7.265854947036132e-05  val loss:  0.8778818845748901\n",
      "epoch:  43   step:  121   train loss:  3.279456723248586e-05  val loss:  0.8618485331535339\n",
      "epoch:  43   step:  122   train loss:  0.0028801560401916504  val loss:  0.8448247909545898\n",
      "epoch:  43   step:  123   train loss:  6.595472223125398e-05  val loss:  0.8614757061004639\n",
      "epoch:  43   step:  124   train loss:  0.0001145954302046448  val loss:  0.8706663250923157\n",
      "epoch:  43   step:  125   train loss:  0.00012424224405549467  val loss:  0.8813619613647461\n",
      "epoch:  43   step:  126   train loss:  9.145015064859763e-05  val loss:  0.8832910656929016\n",
      "epoch:  43   step:  127   train loss:  2.3628086637472734e-05  val loss:  0.8899601697921753\n",
      "epoch:  43   step:  128   train loss:  7.695338717894629e-05  val loss:  0.8929608464241028\n",
      "epoch:  43   step:  129   train loss:  0.0011024799896404147  val loss:  0.9073247909545898\n",
      "epoch:  43   step:  130   train loss:  9.083181794267148e-05  val loss:  0.8894156217575073\n",
      "epoch:  43   step:  131   train loss:  0.0001417022867826745  val loss:  0.8927002549171448\n",
      "epoch:  43   step:  132   train loss:  7.605551945744082e-05  val loss:  0.9018495082855225\n",
      "epoch:  43   step:  133   train loss:  0.0001330556842731312  val loss:  0.9020249247550964\n",
      "epoch:  43   step:  134   train loss:  0.0001239508274011314  val loss:  0.893693745136261\n",
      "epoch:  43   step:  135   train loss:  0.00014172759256325662  val loss:  0.9038927555084229\n",
      "epoch:  43   step:  136   train loss:  0.00012993140262551606  val loss:  0.9023767709732056\n",
      "epoch:  43   step:  137   train loss:  0.00010671911877579987  val loss:  0.8815075755119324\n",
      "epoch:  43   step:  138   train loss:  6.136031151982024e-05  val loss:  0.8723945617675781\n",
      "epoch:  43   step:  139   train loss:  9.859622514341027e-05  val loss:  0.8704420328140259\n",
      "epoch:  43   step:  140   train loss:  7.20583411748521e-05  val loss:  0.8449357151985168\n",
      "epoch:  43   step:  141   train loss:  9.310961468145251e-05  val loss:  0.8303097486495972\n",
      "epoch:  43   step:  142   train loss:  0.0001759181177476421  val loss:  0.8158586621284485\n",
      "epoch:  43   step:  143   train loss:  0.00018532646936364472  val loss:  0.8215852379798889\n",
      "epoch:  43   step:  144   train loss:  0.00014898648078087717  val loss:  0.8049010038375854\n",
      "epoch:  43   step:  145   train loss:  0.00023322433116845787  val loss:  0.7995314002037048\n",
      "epoch:  43   step:  146   train loss:  5.585936378338374e-05  val loss:  0.810073733329773\n",
      "epoch:  43   step:  147   train loss:  0.0002513694344088435  val loss:  0.8290729522705078\n",
      "epoch:  43   step:  148   train loss:  0.0001276302500627935  val loss:  0.8363716006278992\n",
      "epoch:  43   step:  149   train loss:  6.255601329030469e-05  val loss:  0.8485752940177917\n",
      "epoch:  43   step:  150   train loss:  7.89249679655768e-05  val loss:  0.8531374931335449\n",
      "epoch:  43   step:  151   train loss:  0.00017900114471558481  val loss:  0.8388127088546753\n",
      "epoch:  43   step:  152   train loss:  9.457966370973736e-05  val loss:  0.8538855314254761\n",
      "epoch:  43   step:  153   train loss:  7.406387885566801e-05  val loss:  0.8716156482696533\n",
      "epoch:  43   step:  154   train loss:  0.00012802948185708374  val loss:  0.8746070861816406\n",
      "epoch:  43   step:  155   train loss:  0.00011311784328427166  val loss:  0.8680000305175781\n",
      "epoch:  43   step:  156   train loss:  0.00011849450675072148  val loss:  0.8640040159225464\n",
      "epoch:  43   step:  157   train loss:  4.397408338263631e-05  val loss:  0.8628587126731873\n",
      "epoch:  43   step:  158   train loss:  8.51620061439462e-05  val loss:  0.8692488670349121\n",
      "epoch:  43   step:  159   train loss:  6.29818023298867e-05  val loss:  0.8744858503341675\n",
      "epoch:  43   step:  160   train loss:  0.00012487336061894894  val loss:  0.8808977603912354\n",
      "epoch:  43   step:  161   train loss:  0.00018938515859190375  val loss:  0.8874822854995728\n",
      "epoch:  43   step:  162   train loss:  0.0002253020938951522  val loss:  0.8674780130386353\n",
      "epoch:  43   step:  163   train loss:  0.00014116194506641477  val loss:  0.8801939487457275\n",
      "epoch:  43   step:  164   train loss:  0.00010833728447323665  val loss:  0.881017804145813\n",
      "epoch:  43   step:  165   train loss:  5.081524432171136e-05  val loss:  0.8659086227416992\n",
      "epoch:  44   step:  0   train loss:  7.19885720172897e-05  val loss:  0.8702760934829712\n",
      "epoch:  44   step:  1   train loss:  0.00010676011152099818  val loss:  0.8574445247650146\n",
      "epoch:  44   step:  2   train loss:  5.5885255278553814e-05  val loss:  0.8361018896102905\n",
      "epoch:  44   step:  3   train loss:  8.760807395447046e-05  val loss:  0.8390960693359375\n",
      "epoch:  44   step:  4   train loss:  0.0001319377770414576  val loss:  0.8501560688018799\n",
      "epoch:  44   step:  5   train loss:  7.16870345058851e-05  val loss:  0.8680325150489807\n",
      "epoch:  44   step:  6   train loss:  0.00011910249304492027  val loss:  0.8652123212814331\n",
      "epoch:  44   step:  7   train loss:  8.097304817056283e-05  val loss:  0.8638944029808044\n",
      "epoch:  44   step:  8   train loss:  0.00013057634350843728  val loss:  0.8560969829559326\n",
      "epoch:  44   step:  9   train loss:  3.808078326983377e-05  val loss:  0.8435412645339966\n",
      "epoch:  44   step:  10   train loss:  5.5660904763499275e-05  val loss:  0.8384162783622742\n",
      "epoch:  44   step:  11   train loss:  9.527322254143655e-05  val loss:  0.8299604058265686\n",
      "epoch:  44   step:  12   train loss:  3.559983815648593e-05  val loss:  0.8287867307662964\n",
      "epoch:  44   step:  13   train loss:  8.517317473888397e-05  val loss:  0.8359647393226624\n",
      "epoch:  44   step:  14   train loss:  4.439471013029106e-05  val loss:  0.8329054117202759\n",
      "epoch:  44   step:  15   train loss:  4.7724475734867156e-05  val loss:  0.8378973007202148\n",
      "epoch:  44   step:  16   train loss:  6.133476563263685e-05  val loss:  0.8401342034339905\n",
      "epoch:  44   step:  17   train loss:  0.00011914197239093482  val loss:  0.847137987613678\n",
      "epoch:  44   step:  18   train loss:  2.24512186832726e-05  val loss:  0.8554914593696594\n",
      "epoch:  44   step:  19   train loss:  5.025412610848434e-05  val loss:  0.8367882370948792\n",
      "epoch:  44   step:  20   train loss:  3.377889152034186e-05  val loss:  0.8392816185951233\n",
      "epoch:  44   step:  21   train loss:  7.810446550138295e-05  val loss:  0.8440723419189453\n",
      "epoch:  44   step:  22   train loss:  0.0001011210260912776  val loss:  0.8417664766311646\n",
      "epoch:  44   step:  23   train loss:  8.414001058554277e-05  val loss:  0.8418945670127869\n",
      "epoch:  44   step:  24   train loss:  0.0001006187085295096  val loss:  0.8386610746383667\n",
      "epoch:  44   step:  25   train loss:  8.91319359652698e-05  val loss:  0.8460232615470886\n",
      "epoch:  44   step:  26   train loss:  0.00019707554019987583  val loss:  0.8404875993728638\n",
      "epoch:  44   step:  27   train loss:  0.00010120362276211381  val loss:  0.8458791971206665\n",
      "epoch:  44   step:  28   train loss:  6.645830580964684e-05  val loss:  0.843248724937439\n",
      "epoch:  44   step:  29   train loss:  6.391346687451005e-05  val loss:  0.8514983654022217\n",
      "epoch:  44   step:  30   train loss:  8.273882122011855e-05  val loss:  0.8543084263801575\n",
      "epoch:  44   step:  31   train loss:  5.8023630117531866e-05  val loss:  0.85201096534729\n",
      "epoch:  44   step:  32   train loss:  0.00013295456301420927  val loss:  0.866055428981781\n",
      "epoch:  44   step:  33   train loss:  8.060302934609354e-05  val loss:  0.8657616376876831\n",
      "epoch:  44   step:  34   train loss:  0.00015205271483864635  val loss:  0.8860324025154114\n",
      "epoch:  44   step:  35   train loss:  5.314871668815613e-05  val loss:  0.8844013214111328\n",
      "epoch:  44   step:  36   train loss:  5.880398384761065e-05  val loss:  0.863953709602356\n",
      "epoch:  44   step:  37   train loss:  0.00011969944898737594  val loss:  0.8568545579910278\n",
      "epoch:  44   step:  38   train loss:  9.155747102340683e-05  val loss:  0.8706626892089844\n",
      "epoch:  44   step:  39   train loss:  5.122964284964837e-05  val loss:  0.8793594837188721\n",
      "epoch:  44   step:  40   train loss:  5.455432255985215e-05  val loss:  0.8773899078369141\n",
      "epoch:  44   step:  41   train loss:  6.805146404076368e-05  val loss:  0.8665818572044373\n",
      "epoch:  44   step:  42   train loss:  0.00017879213555715978  val loss:  0.8828839659690857\n",
      "epoch:  44   step:  43   train loss:  3.941515751648694e-05  val loss:  0.8800873160362244\n",
      "epoch:  44   step:  44   train loss:  0.0001175504585262388  val loss:  0.867798924446106\n",
      "epoch:  44   step:  45   train loss:  9.637324546929449e-05  val loss:  0.866708517074585\n",
      "epoch:  44   step:  46   train loss:  4.9467867938801646e-05  val loss:  0.8633142113685608\n",
      "epoch:  44   step:  47   train loss:  4.57430724054575e-05  val loss:  0.8695804476737976\n",
      "epoch:  44   step:  48   train loss:  7.639917748747393e-05  val loss:  0.8837878704071045\n",
      "epoch:  44   step:  49   train loss:  7.338382420130074e-05  val loss:  0.8923012614250183\n",
      "epoch:  44   step:  50   train loss:  5.286665691528469e-05  val loss:  0.8873172402381897\n",
      "epoch:  44   step:  51   train loss:  5.272823909763247e-05  val loss:  0.8979114890098572\n",
      "epoch:  44   step:  52   train loss:  5.63220601179637e-05  val loss:  0.8935225009918213\n",
      "epoch:  44   step:  53   train loss:  0.0001487480039941147  val loss:  0.8835943937301636\n",
      "epoch:  44   step:  54   train loss:  6.727383879479021e-05  val loss:  0.894987940788269\n",
      "epoch:  44   step:  55   train loss:  8.41013970784843e-05  val loss:  0.8871856331825256\n",
      "epoch:  44   step:  56   train loss:  0.00014027676661498845  val loss:  0.8754025101661682\n",
      "epoch:  44   step:  57   train loss:  0.00010235951049253345  val loss:  0.8862121105194092\n",
      "epoch:  44   step:  58   train loss:  8.221484313253313e-05  val loss:  0.8899831771850586\n",
      "epoch:  44   step:  59   train loss:  6.928123912075534e-05  val loss:  0.8849943280220032\n",
      "epoch:  44   step:  60   train loss:  9.395046072313562e-05  val loss:  0.8648379445075989\n",
      "epoch:  44   step:  61   train loss:  7.630571781191975e-05  val loss:  0.8650270104408264\n",
      "epoch:  44   step:  62   train loss:  4.333509423304349e-05  val loss:  0.8610670566558838\n",
      "epoch:  44   step:  63   train loss:  8.81725936778821e-05  val loss:  0.8677990436553955\n",
      "epoch:  44   step:  64   train loss:  0.00023732618137728423  val loss:  0.8679145574569702\n",
      "epoch:  44   step:  65   train loss:  8.605901530245319e-05  val loss:  0.8802774548530579\n",
      "epoch:  44   step:  66   train loss:  8.370103751076385e-05  val loss:  0.8965966701507568\n",
      "epoch:  44   step:  67   train loss:  8.289601100841537e-05  val loss:  0.9052470922470093\n",
      "epoch:  44   step:  68   train loss:  6.139751349110156e-05  val loss:  0.9053226113319397\n",
      "epoch:  44   step:  69   train loss:  0.00015754134801682085  val loss:  0.8898531198501587\n",
      "epoch:  44   step:  70   train loss:  4.4509935833048075e-05  val loss:  0.8795807361602783\n",
      "epoch:  44   step:  71   train loss:  5.68543364352081e-05  val loss:  0.8733011484146118\n",
      "epoch:  44   step:  72   train loss:  2.9331906262086704e-05  val loss:  0.8697147369384766\n",
      "epoch:  44   step:  73   train loss:  0.00017741670308168977  val loss:  0.8732928037643433\n",
      "epoch:  44   step:  74   train loss:  5.607321145362221e-05  val loss:  0.862138032913208\n",
      "epoch:  44   step:  75   train loss:  2.5301029381807894e-05  val loss:  0.865771472454071\n",
      "epoch:  44   step:  76   train loss:  0.00012392274220474064  val loss:  0.883418619632721\n",
      "epoch:  44   step:  77   train loss:  8.516870002495125e-05  val loss:  0.8663374185562134\n",
      "epoch:  44   step:  78   train loss:  4.613247438101098e-05  val loss:  0.8667026162147522\n",
      "epoch:  44   step:  79   train loss:  3.854574970318936e-05  val loss:  0.8665594458580017\n",
      "epoch:  44   step:  80   train loss:  0.00014229139196686447  val loss:  0.8867776393890381\n",
      "epoch:  44   step:  81   train loss:  0.00010940379434032366  val loss:  0.8797155022621155\n",
      "epoch:  44   step:  82   train loss:  8.898664964362979e-05  val loss:  0.8833805918693542\n",
      "epoch:  44   step:  83   train loss:  9.390129707753658e-05  val loss:  0.8905363082885742\n",
      "epoch:  44   step:  84   train loss:  7.994482439244166e-05  val loss:  0.8940661549568176\n",
      "epoch:  44   step:  85   train loss:  3.691150777740404e-05  val loss:  0.8992018103599548\n",
      "epoch:  44   step:  86   train loss:  3.824107989203185e-05  val loss:  0.9094869494438171\n",
      "epoch:  44   step:  87   train loss:  6.151464185677469e-05  val loss:  0.900655210018158\n",
      "epoch:  44   step:  88   train loss:  3.355958324391395e-05  val loss:  0.8932495713233948\n",
      "epoch:  44   step:  89   train loss:  0.00011965807061642408  val loss:  0.8893823027610779\n",
      "epoch:  44   step:  90   train loss:  4.0055067074717954e-05  val loss:  0.8857675194740295\n",
      "epoch:  44   step:  91   train loss:  8.854872430674732e-05  val loss:  0.8983470797538757\n",
      "epoch:  44   step:  92   train loss:  5.135921310284175e-05  val loss:  0.8968194127082825\n",
      "epoch:  44   step:  93   train loss:  6.0844398831250146e-05  val loss:  0.886135995388031\n",
      "epoch:  44   step:  94   train loss:  0.00011086235463153571  val loss:  0.8960978388786316\n",
      "epoch:  44   step:  95   train loss:  3.744774221559055e-05  val loss:  0.8915676474571228\n",
      "epoch:  44   step:  96   train loss:  7.82943025114946e-05  val loss:  0.8803257346153259\n",
      "epoch:  44   step:  97   train loss:  4.637234815163538e-05  val loss:  0.8792653679847717\n",
      "epoch:  44   step:  98   train loss:  5.9722391597460955e-05  val loss:  0.881895899772644\n",
      "epoch:  44   step:  99   train loss:  5.5006908951327205e-05  val loss:  0.8761220574378967\n",
      "epoch:  44   step:  100   train loss:  9.231986769009382e-05  val loss:  0.8792864084243774\n",
      "epoch:  44   step:  101   train loss:  2.8097754693590105e-05  val loss:  0.8824485540390015\n",
      "epoch:  44   step:  102   train loss:  5.65488007850945e-05  val loss:  0.8813918232917786\n",
      "epoch:  44   step:  103   train loss:  4.2737374315038323e-05  val loss:  0.879989504814148\n",
      "epoch:  44   step:  104   train loss:  0.00010449168621562421  val loss:  0.8808047771453857\n",
      "epoch:  44   step:  105   train loss:  6.256316555663943e-05  val loss:  0.88504958152771\n",
      "epoch:  44   step:  106   train loss:  0.00019892647105734795  val loss:  0.8679194450378418\n",
      "epoch:  44   step:  107   train loss:  0.00011283707863185555  val loss:  0.8756630420684814\n",
      "epoch:  44   step:  108   train loss:  5.080963819636963e-05  val loss:  0.8861264586448669\n",
      "epoch:  44   step:  109   train loss:  6.452505476772785e-05  val loss:  0.8797004222869873\n",
      "epoch:  44   step:  110   train loss:  0.00012527064245659858  val loss:  0.8609875440597534\n",
      "epoch:  44   step:  111   train loss:  8.664478082209826e-05  val loss:  0.8488006591796875\n",
      "epoch:  44   step:  112   train loss:  0.00011091867054346949  val loss:  0.8454382419586182\n",
      "epoch:  44   step:  113   train loss:  0.0002354196913074702  val loss:  0.8684849143028259\n",
      "epoch:  44   step:  114   train loss:  9.58125528995879e-05  val loss:  0.8783992528915405\n",
      "epoch:  44   step:  115   train loss:  5.605868500424549e-05  val loss:  0.8808072209358215\n",
      "epoch:  44   step:  116   train loss:  5.6075314205372706e-05  val loss:  0.8833621144294739\n",
      "epoch:  44   step:  117   train loss:  0.00010978097270708531  val loss:  0.8830979466438293\n",
      "epoch:  44   step:  118   train loss:  8.144737512338907e-05  val loss:  0.9017550349235535\n",
      "epoch:  44   step:  119   train loss:  2.1680330974049866e-05  val loss:  0.9008108377456665\n",
      "epoch:  44   step:  120   train loss:  8.358000195585191e-05  val loss:  0.8903594613075256\n",
      "epoch:  44   step:  121   train loss:  8.669126691529527e-05  val loss:  0.8837087154388428\n",
      "epoch:  44   step:  122   train loss:  8.714231080375612e-05  val loss:  0.8900511860847473\n",
      "epoch:  44   step:  123   train loss:  0.00010562650277279317  val loss:  0.9004948735237122\n",
      "epoch:  44   step:  124   train loss:  0.00012195458839414641  val loss:  0.8956010937690735\n",
      "epoch:  44   step:  125   train loss:  0.0006739366799592972  val loss:  0.8974400162696838\n",
      "epoch:  44   step:  126   train loss:  7.279869168996811e-05  val loss:  0.9015673398971558\n",
      "epoch:  44   step:  127   train loss:  4.801548857358284e-05  val loss:  0.895587146282196\n",
      "epoch:  44   step:  128   train loss:  0.00010968777496600524  val loss:  0.8851600289344788\n",
      "epoch:  44   step:  129   train loss:  8.456071373075247e-05  val loss:  0.8886367678642273\n",
      "epoch:  44   step:  130   train loss:  0.00010927471157629043  val loss:  0.8956255912780762\n",
      "epoch:  44   step:  131   train loss:  7.680726412218064e-05  val loss:  0.892626166343689\n",
      "epoch:  44   step:  132   train loss:  4.286770126782358e-05  val loss:  0.9045737385749817\n",
      "epoch:  44   step:  133   train loss:  4.5060718548484147e-05  val loss:  0.8917579650878906\n",
      "epoch:  44   step:  134   train loss:  6.798464892199263e-05  val loss:  0.8943454623222351\n",
      "epoch:  44   step:  135   train loss:  6.391019269358367e-05  val loss:  0.8971874117851257\n",
      "epoch:  44   step:  136   train loss:  0.00010150975867873058  val loss:  0.880727231502533\n",
      "epoch:  44   step:  137   train loss:  4.74978587590158e-05  val loss:  0.8785822987556458\n",
      "epoch:  44   step:  138   train loss:  9.38730372581631e-05  val loss:  0.8973625898361206\n",
      "epoch:  44   step:  139   train loss:  4.259151319274679e-05  val loss:  0.9065030217170715\n",
      "epoch:  44   step:  140   train loss:  8.804252138361335e-05  val loss:  0.9147810935974121\n",
      "epoch:  44   step:  141   train loss:  5.973179213469848e-05  val loss:  0.9083024859428406\n",
      "epoch:  44   step:  142   train loss:  0.0001268296327907592  val loss:  0.9045920372009277\n",
      "epoch:  44   step:  143   train loss:  8.513947250321507e-05  val loss:  0.9091273546218872\n",
      "epoch:  44   step:  144   train loss:  0.00013572079478763044  val loss:  0.9094250202178955\n",
      "epoch:  44   step:  145   train loss:  4.6763609134359285e-05  val loss:  0.9098485112190247\n",
      "epoch:  44   step:  146   train loss:  4.5608354412252083e-05  val loss:  0.9120988249778748\n",
      "epoch:  44   step:  147   train loss:  5.224060078035109e-05  val loss:  0.9311277270317078\n",
      "epoch:  44   step:  148   train loss:  3.560620825737715e-05  val loss:  0.927646815776825\n",
      "epoch:  44   step:  149   train loss:  4.728346539195627e-05  val loss:  0.9258489608764648\n",
      "epoch:  44   step:  150   train loss:  8.783200610196218e-05  val loss:  0.9278741478919983\n",
      "epoch:  44   step:  151   train loss:  5.701644840883091e-05  val loss:  0.9211460947990417\n",
      "epoch:  44   step:  152   train loss:  0.00010025007941294461  val loss:  0.9195594787597656\n",
      "epoch:  44   step:  153   train loss:  8.360035280929878e-05  val loss:  0.9282001256942749\n",
      "epoch:  44   step:  154   train loss:  9.372839122079313e-05  val loss:  0.9135380387306213\n",
      "epoch:  44   step:  155   train loss:  6.243670941330492e-05  val loss:  0.9080988764762878\n",
      "epoch:  44   step:  156   train loss:  0.0003548326203599572  val loss:  0.9036373496055603\n",
      "epoch:  44   step:  157   train loss:  4.2602288885973394e-05  val loss:  0.904578685760498\n",
      "epoch:  44   step:  158   train loss:  0.00014423870015889406  val loss:  0.8902949094772339\n",
      "epoch:  44   step:  159   train loss:  0.00016329923528246582  val loss:  0.898292601108551\n",
      "epoch:  44   step:  160   train loss:  6.675483018625528e-05  val loss:  0.8988116979598999\n",
      "epoch:  44   step:  161   train loss:  6.559050234500319e-05  val loss:  0.9062547087669373\n",
      "epoch:  44   step:  162   train loss:  7.735562394373119e-05  val loss:  0.8957095146179199\n",
      "epoch:  44   step:  163   train loss:  9.396408859174699e-05  val loss:  0.9063716530799866\n",
      "epoch:  44   step:  164   train loss:  4.385149077279493e-05  val loss:  0.8935552835464478\n",
      "epoch:  44   step:  165   train loss:  0.00035896096960641444  val loss:  0.8837897777557373\n",
      "epoch:  45   step:  0   train loss:  5.6694472732488066e-05  val loss:  0.8888047337532043\n",
      "epoch:  45   step:  1   train loss:  4.17166156694293e-05  val loss:  0.8945838212966919\n",
      "epoch:  45   step:  2   train loss:  0.0004328404611442238  val loss:  0.8722285032272339\n",
      "epoch:  45   step:  3   train loss:  5.13194827362895e-05  val loss:  0.8841204643249512\n",
      "epoch:  45   step:  4   train loss:  6.698205834254622e-05  val loss:  0.8802807331085205\n",
      "epoch:  45   step:  5   train loss:  4.73636100650765e-05  val loss:  0.8846054673194885\n",
      "epoch:  45   step:  6   train loss:  4.5803557441104203e-05  val loss:  0.8882179856300354\n",
      "epoch:  45   step:  7   train loss:  9.73915375652723e-05  val loss:  0.9019749164581299\n",
      "epoch:  45   step:  8   train loss:  5.837935896124691e-05  val loss:  0.9098876714706421\n",
      "epoch:  45   step:  9   train loss:  5.729034819523804e-05  val loss:  0.9109998941421509\n",
      "epoch:  45   step:  10   train loss:  5.834583134856075e-05  val loss:  0.9088233709335327\n",
      "epoch:  45   step:  11   train loss:  0.00019309321942273527  val loss:  0.9309823513031006\n",
      "epoch:  45   step:  12   train loss:  6.358223618008196e-05  val loss:  0.9352776408195496\n",
      "epoch:  45   step:  13   train loss:  0.00015629456902388483  val loss:  0.9330331087112427\n",
      "epoch:  45   step:  14   train loss:  5.217896978138015e-05  val loss:  0.9438322186470032\n",
      "epoch:  45   step:  15   train loss:  3.577509778551757e-05  val loss:  0.9483299255371094\n",
      "epoch:  45   step:  16   train loss:  0.00010287832992617041  val loss:  0.9481333494186401\n",
      "epoch:  45   step:  17   train loss:  6.761778058717027e-05  val loss:  0.9335362315177917\n",
      "epoch:  45   step:  18   train loss:  9.249740105587989e-05  val loss:  0.9418484568595886\n",
      "epoch:  45   step:  19   train loss:  3.551341069396585e-05  val loss:  0.9432321190834045\n",
      "epoch:  45   step:  20   train loss:  4.953019379172474e-05  val loss:  0.9352600574493408\n",
      "epoch:  45   step:  21   train loss:  8.218469156417996e-05  val loss:  0.9366235136985779\n",
      "epoch:  45   step:  22   train loss:  4.9779308028519154e-05  val loss:  0.9240338206291199\n",
      "epoch:  45   step:  23   train loss:  0.00011546917812665924  val loss:  0.928460419178009\n",
      "epoch:  45   step:  24   train loss:  9.629252599552274e-05  val loss:  0.9239715337753296\n",
      "epoch:  45   step:  25   train loss:  7.257920515257865e-05  val loss:  0.9257094860076904\n",
      "epoch:  45   step:  26   train loss:  0.00013416925503406674  val loss:  0.9393994212150574\n",
      "epoch:  45   step:  27   train loss:  5.119586057844572e-05  val loss:  0.9393572807312012\n",
      "epoch:  45   step:  28   train loss:  7.31817854102701e-05  val loss:  0.9386010766029358\n",
      "epoch:  45   step:  29   train loss:  0.000228717370191589  val loss:  0.9428002834320068\n",
      "epoch:  45   step:  30   train loss:  7.648409518878907e-05  val loss:  0.9232627749443054\n",
      "epoch:  45   step:  31   train loss:  4.052826625411399e-05  val loss:  0.9380683898925781\n",
      "epoch:  45   step:  32   train loss:  4.91570244776085e-05  val loss:  0.9303570985794067\n",
      "epoch:  45   step:  33   train loss:  4.2136438423767686e-05  val loss:  0.9270932078361511\n",
      "epoch:  45   step:  34   train loss:  6.087373913032934e-05  val loss:  0.9361712336540222\n",
      "epoch:  45   step:  35   train loss:  0.00012244752724654973  val loss:  0.9486905932426453\n",
      "epoch:  45   step:  36   train loss:  4.951110895490274e-05  val loss:  0.9347209930419922\n",
      "epoch:  45   step:  37   train loss:  4.6122797357384115e-05  val loss:  0.9310114979743958\n",
      "epoch:  45   step:  38   train loss:  6.178502371767536e-05  val loss:  0.9336676001548767\n",
      "epoch:  45   step:  39   train loss:  6.758375820936635e-05  val loss:  0.9254008531570435\n",
      "epoch:  45   step:  40   train loss:  1.8733837350737303e-05  val loss:  0.9250780344009399\n",
      "epoch:  45   step:  41   train loss:  0.00010962654778268188  val loss:  0.9154787063598633\n",
      "epoch:  45   step:  42   train loss:  2.576641236373689e-05  val loss:  0.9098126292228699\n",
      "epoch:  45   step:  43   train loss:  6.379531987477094e-05  val loss:  0.916466474533081\n",
      "epoch:  45   step:  44   train loss:  4.165154678048566e-05  val loss:  0.9112483263015747\n",
      "epoch:  45   step:  45   train loss:  6.222841329872608e-05  val loss:  0.9149603843688965\n",
      "epoch:  45   step:  46   train loss:  7.581392128486186e-05  val loss:  0.9185375571250916\n",
      "epoch:  45   step:  47   train loss:  0.00017243874026462436  val loss:  0.9232756495475769\n",
      "epoch:  45   step:  48   train loss:  5.5399526900146157e-05  val loss:  0.9267502427101135\n",
      "epoch:  45   step:  49   train loss:  7.421456393785775e-05  val loss:  0.9222736358642578\n",
      "epoch:  45   step:  50   train loss:  3.3156607969431207e-05  val loss:  0.9175803661346436\n",
      "epoch:  45   step:  51   train loss:  4.6676832425873727e-05  val loss:  0.9281712770462036\n",
      "epoch:  45   step:  52   train loss:  5.8897923736367375e-05  val loss:  0.9382371306419373\n",
      "epoch:  45   step:  53   train loss:  7.627063314430416e-05  val loss:  0.9277913570404053\n",
      "epoch:  45   step:  54   train loss:  3.718386506079696e-05  val loss:  0.9230639338493347\n",
      "epoch:  45   step:  55   train loss:  9.44295825320296e-05  val loss:  0.9241364598274231\n",
      "epoch:  45   step:  56   train loss:  5.636651621898636e-05  val loss:  0.9286051988601685\n",
      "epoch:  45   step:  57   train loss:  4.7475208702962846e-05  val loss:  0.9324502944946289\n",
      "epoch:  45   step:  58   train loss:  4.627816451829858e-05  val loss:  0.9416878819465637\n",
      "epoch:  45   step:  59   train loss:  0.00011193998216185719  val loss:  0.935297966003418\n",
      "epoch:  45   step:  60   train loss:  6.802991265431046e-05  val loss:  0.9165635704994202\n",
      "epoch:  45   step:  61   train loss:  4.734066533274017e-05  val loss:  0.9363909959793091\n",
      "epoch:  45   step:  62   train loss:  4.298853673390113e-05  val loss:  0.9454793334007263\n",
      "epoch:  45   step:  63   train loss:  5.7203425967600197e-05  val loss:  0.9489782452583313\n",
      "epoch:  45   step:  64   train loss:  4.8879046516958624e-05  val loss:  0.9486936926841736\n",
      "epoch:  45   step:  65   train loss:  4.2807947465917096e-05  val loss:  0.9420490860939026\n",
      "epoch:  45   step:  66   train loss:  8.617064304416999e-05  val loss:  0.9449856877326965\n",
      "epoch:  45   step:  67   train loss:  3.1465868232771754e-05  val loss:  0.9389596581459045\n",
      "epoch:  45   step:  68   train loss:  0.0002796035259962082  val loss:  0.916238009929657\n",
      "epoch:  45   step:  69   train loss:  4.830931720789522e-05  val loss:  0.9092272520065308\n",
      "epoch:  45   step:  70   train loss:  5.7190969528164715e-05  val loss:  0.893320620059967\n",
      "epoch:  45   step:  71   train loss:  4.762820026371628e-05  val loss:  0.8842034935951233\n",
      "epoch:  45   step:  72   train loss:  9.441121073905379e-05  val loss:  0.8778784275054932\n",
      "epoch:  45   step:  73   train loss:  4.564738628687337e-05  val loss:  0.8879455924034119\n",
      "epoch:  45   step:  74   train loss:  8.927904127631336e-05  val loss:  0.9023776054382324\n",
      "epoch:  45   step:  75   train loss:  7.70237238612026e-05  val loss:  0.884806215763092\n",
      "epoch:  45   step:  76   train loss:  7.583848491776735e-05  val loss:  0.8649957776069641\n",
      "epoch:  45   step:  77   train loss:  6.788910832256079e-05  val loss:  0.8880427479743958\n",
      "epoch:  45   step:  78   train loss:  2.956983917101752e-05  val loss:  0.8934375643730164\n",
      "epoch:  45   step:  79   train loss:  3.3052521757781506e-05  val loss:  0.90125972032547\n",
      "epoch:  45   step:  80   train loss:  7.63877687859349e-05  val loss:  0.9187957048416138\n",
      "epoch:  45   step:  81   train loss:  6.898549327161163e-05  val loss:  0.9049962162971497\n",
      "epoch:  45   step:  82   train loss:  5.6346834753639996e-05  val loss:  0.8939814567565918\n",
      "epoch:  45   step:  83   train loss:  8.829760190565139e-05  val loss:  0.8974822759628296\n",
      "epoch:  45   step:  84   train loss:  6.235369073692709e-05  val loss:  0.8859562277793884\n",
      "epoch:  45   step:  85   train loss:  9.192264406010509e-05  val loss:  0.8798532485961914\n",
      "epoch:  45   step:  86   train loss:  6.737830699421465e-05  val loss:  0.8749516010284424\n",
      "epoch:  45   step:  87   train loss:  9.071121894521639e-05  val loss:  0.8792088627815247\n",
      "epoch:  45   step:  88   train loss:  0.00011741465277737007  val loss:  0.8883396983146667\n",
      "epoch:  45   step:  89   train loss:  0.00012531419633887708  val loss:  0.8804938793182373\n",
      "epoch:  45   step:  90   train loss:  6.414554081857204e-05  val loss:  0.8907214403152466\n",
      "epoch:  45   step:  91   train loss:  0.00012091650569345802  val loss:  0.8973324298858643\n",
      "epoch:  45   step:  92   train loss:  5.254725328995846e-05  val loss:  0.8974820375442505\n",
      "epoch:  45   step:  93   train loss:  6.953191041247919e-05  val loss:  0.9126980304718018\n",
      "epoch:  45   step:  94   train loss:  7.853605347918347e-05  val loss:  0.9137622117996216\n",
      "epoch:  45   step:  95   train loss:  2.6932650143862702e-05  val loss:  0.9067301750183105\n",
      "epoch:  45   step:  96   train loss:  0.00011546998575795442  val loss:  0.9157682657241821\n",
      "epoch:  45   step:  97   train loss:  5.377484194468707e-05  val loss:  0.9189193248748779\n",
      "epoch:  45   step:  98   train loss:  0.00011312885180814192  val loss:  0.9315997958183289\n",
      "epoch:  45   step:  99   train loss:  5.866089850314893e-05  val loss:  0.9128284454345703\n",
      "epoch:  45   step:  100   train loss:  7.398245361400768e-05  val loss:  0.9187487363815308\n",
      "epoch:  45   step:  101   train loss:  6.259275687625632e-05  val loss:  0.9208724498748779\n",
      "epoch:  45   step:  102   train loss:  3.4919154131785035e-05  val loss:  0.9151875376701355\n",
      "epoch:  45   step:  103   train loss:  2.4603650672361255e-05  val loss:  0.9163374304771423\n",
      "epoch:  45   step:  104   train loss:  4.3877029384020716e-05  val loss:  0.9112293720245361\n",
      "epoch:  45   step:  105   train loss:  0.0001402847410645336  val loss:  0.8899871110916138\n",
      "epoch:  45   step:  106   train loss:  8.35981554700993e-05  val loss:  0.8781877160072327\n",
      "epoch:  45   step:  107   train loss:  6.819477130193263e-05  val loss:  0.8814433217048645\n",
      "epoch:  45   step:  108   train loss:  7.187172013800591e-05  val loss:  0.8837102055549622\n",
      "epoch:  45   step:  109   train loss:  9.899280121317133e-05  val loss:  0.9004166722297668\n",
      "epoch:  45   step:  110   train loss:  0.00016542358207516372  val loss:  0.896291196346283\n",
      "epoch:  45   step:  111   train loss:  5.3834552090847865e-05  val loss:  0.8987817168235779\n",
      "epoch:  45   step:  112   train loss:  3.741392356459983e-05  val loss:  0.9093062877655029\n",
      "epoch:  45   step:  113   train loss:  5.7490771723678336e-05  val loss:  0.8962736129760742\n",
      "epoch:  45   step:  114   train loss:  3.427315459703095e-05  val loss:  0.8962087631225586\n",
      "epoch:  45   step:  115   train loss:  4.236440145177767e-05  val loss:  0.88631671667099\n",
      "epoch:  45   step:  116   train loss:  3.3309057471342385e-05  val loss:  0.8914060592651367\n",
      "epoch:  45   step:  117   train loss:  6.424305320251733e-05  val loss:  0.8937661051750183\n",
      "epoch:  45   step:  118   train loss:  0.00011666142381727695  val loss:  0.9096914529800415\n",
      "epoch:  45   step:  119   train loss:  4.1899329517036676e-05  val loss:  0.9061039686203003\n",
      "epoch:  45   step:  120   train loss:  0.00010973840835504234  val loss:  0.8881765007972717\n",
      "epoch:  45   step:  121   train loss:  5.691148908226751e-05  val loss:  0.8833874464035034\n",
      "epoch:  45   step:  122   train loss:  5.623064862447791e-05  val loss:  0.882710874080658\n",
      "epoch:  45   step:  123   train loss:  0.00011299781908746809  val loss:  0.8905619382858276\n",
      "epoch:  45   step:  124   train loss:  7.573542825412005e-05  val loss:  0.8898658752441406\n",
      "epoch:  45   step:  125   train loss:  4.571189492708072e-05  val loss:  0.8953548073768616\n",
      "epoch:  45   step:  126   train loss:  0.000129447114886716  val loss:  0.9049444198608398\n",
      "epoch:  45   step:  127   train loss:  0.00010724729509092867  val loss:  0.9026774168014526\n",
      "epoch:  45   step:  128   train loss:  4.494975655688904e-05  val loss:  0.8982661962509155\n",
      "epoch:  45   step:  129   train loss:  7.466100214514881e-05  val loss:  0.8918025493621826\n",
      "epoch:  45   step:  130   train loss:  4.065193570568226e-05  val loss:  0.8868909478187561\n",
      "epoch:  45   step:  131   train loss:  0.00011876432836288586  val loss:  0.8802332878112793\n",
      "epoch:  45   step:  132   train loss:  4.8288995458278805e-05  val loss:  0.8737656474113464\n",
      "epoch:  45   step:  133   train loss:  3.337572343298234e-05  val loss:  0.8765208125114441\n",
      "epoch:  45   step:  134   train loss:  5.651046376442537e-05  val loss:  0.8890308141708374\n",
      "epoch:  45   step:  135   train loss:  5.149427670403384e-05  val loss:  0.9030479788780212\n",
      "epoch:  45   step:  136   train loss:  5.08454286318738e-05  val loss:  0.8873676061630249\n",
      "epoch:  45   step:  137   train loss:  4.466938116820529e-05  val loss:  0.8739073872566223\n",
      "epoch:  45   step:  138   train loss:  4.485556564759463e-05  val loss:  0.8896021246910095\n",
      "epoch:  45   step:  139   train loss:  2.328917253180407e-05  val loss:  0.8964685797691345\n",
      "epoch:  45   step:  140   train loss:  2.7986698114546016e-05  val loss:  0.8890106678009033\n",
      "epoch:  45   step:  141   train loss:  3.536598524078727e-05  val loss:  0.8918587565422058\n",
      "epoch:  45   step:  142   train loss:  0.00019490727572701871  val loss:  0.9069416522979736\n",
      "epoch:  45   step:  143   train loss:  6.41497244942002e-05  val loss:  0.9121901392936707\n",
      "epoch:  45   step:  144   train loss:  5.6694298109505326e-05  val loss:  0.9121807813644409\n",
      "epoch:  45   step:  145   train loss:  8.073207573033869e-05  val loss:  0.9206520915031433\n",
      "epoch:  45   step:  146   train loss:  0.00018000851559918374  val loss:  0.9187695980072021\n",
      "epoch:  45   step:  147   train loss:  3.168902185279876e-05  val loss:  0.9112671613693237\n",
      "epoch:  45   step:  148   train loss:  7.231038762256503e-05  val loss:  0.9153933525085449\n",
      "epoch:  45   step:  149   train loss:  2.186672281823121e-05  val loss:  0.9118238091468811\n",
      "epoch:  45   step:  150   train loss:  0.00010379886225564405  val loss:  0.902045488357544\n",
      "epoch:  45   step:  151   train loss:  5.981722642900422e-05  val loss:  0.8999476432800293\n",
      "epoch:  45   step:  152   train loss:  7.405287760775536e-05  val loss:  0.903523325920105\n",
      "epoch:  45   step:  153   train loss:  3.802055289270356e-05  val loss:  0.9041723608970642\n",
      "epoch:  45   step:  154   train loss:  7.18542723916471e-05  val loss:  0.8964759707450867\n",
      "epoch:  45   step:  155   train loss:  3.685610863612965e-05  val loss:  0.8878886103630066\n",
      "epoch:  45   step:  156   train loss:  7.28092563804239e-05  val loss:  0.8873730301856995\n",
      "epoch:  45   step:  157   train loss:  0.00016695016529411077  val loss:  0.8765885233879089\n",
      "epoch:  45   step:  158   train loss:  8.209385123336688e-05  val loss:  0.8702248930931091\n",
      "epoch:  45   step:  159   train loss:  8.442520629614592e-05  val loss:  0.8749564290046692\n",
      "epoch:  45   step:  160   train loss:  3.957537774113007e-05  val loss:  0.8919509053230286\n",
      "epoch:  45   step:  161   train loss:  5.263301864033565e-05  val loss:  0.8759024739265442\n",
      "epoch:  45   step:  162   train loss:  5.7458099036011845e-05  val loss:  0.87930828332901\n",
      "epoch:  45   step:  163   train loss:  0.00010973497410304844  val loss:  0.8790634870529175\n",
      "epoch:  45   step:  164   train loss:  4.438997711986303e-05  val loss:  0.8845586776733398\n",
      "epoch:  45   step:  165   train loss:  8.643071487313136e-05  val loss:  0.8948371410369873\n",
      "epoch:  46   step:  0   train loss:  5.5610180424991995e-05  val loss:  0.9072830080986023\n",
      "epoch:  46   step:  1   train loss:  5.112327198730782e-05  val loss:  0.913314163684845\n",
      "epoch:  46   step:  2   train loss:  4.290058132028207e-05  val loss:  0.929520845413208\n",
      "epoch:  46   step:  3   train loss:  0.0001375314313918352  val loss:  0.9266021251678467\n",
      "epoch:  46   step:  4   train loss:  4.1637253161752596e-05  val loss:  0.924952507019043\n",
      "epoch:  46   step:  5   train loss:  0.00012876762775704265  val loss:  0.9359399080276489\n",
      "epoch:  46   step:  6   train loss:  2.853345540643204e-05  val loss:  0.9315109848976135\n",
      "epoch:  46   step:  7   train loss:  1.4486991858575493e-05  val loss:  0.9123580455780029\n",
      "epoch:  46   step:  8   train loss:  5.427928772405721e-05  val loss:  0.9021589756011963\n",
      "epoch:  46   step:  9   train loss:  3.543588536558673e-05  val loss:  0.9044739603996277\n",
      "epoch:  46   step:  10   train loss:  4.301562512409873e-05  val loss:  0.9053577780723572\n",
      "epoch:  46   step:  11   train loss:  9.473062527831644e-05  val loss:  0.8920945525169373\n",
      "epoch:  46   step:  12   train loss:  0.00012467510532587767  val loss:  0.8959370851516724\n",
      "epoch:  46   step:  13   train loss:  3.7779838748974726e-05  val loss:  0.8961760401725769\n",
      "epoch:  46   step:  14   train loss:  6.861984729766846e-05  val loss:  0.8926923871040344\n",
      "epoch:  46   step:  15   train loss:  3.90343957406003e-05  val loss:  0.8965088725090027\n",
      "epoch:  46   step:  16   train loss:  4.0097947930917144e-05  val loss:  0.894133448600769\n",
      "epoch:  46   step:  17   train loss:  5.148044147063047e-05  val loss:  0.8810845017433167\n",
      "epoch:  46   step:  18   train loss:  4.0314902435056865e-05  val loss:  0.8829460740089417\n",
      "epoch:  46   step:  19   train loss:  9.618193871574476e-05  val loss:  0.8702197670936584\n",
      "epoch:  46   step:  20   train loss:  3.4691303881118074e-05  val loss:  0.8683603405952454\n",
      "epoch:  46   step:  21   train loss:  3.969716635765508e-05  val loss:  0.8709346652030945\n",
      "epoch:  46   step:  22   train loss:  6.188808765728027e-05  val loss:  0.8877905011177063\n",
      "epoch:  46   step:  23   train loss:  3.8448633858934045e-05  val loss:  0.8951706290245056\n",
      "epoch:  46   step:  24   train loss:  5.8457793784327805e-05  val loss:  0.8938524723052979\n",
      "epoch:  46   step:  25   train loss:  3.349992402945645e-05  val loss:  0.9054470658302307\n",
      "epoch:  46   step:  26   train loss:  6.252422463148832e-05  val loss:  0.8859856128692627\n",
      "epoch:  46   step:  27   train loss:  6.877542182337493e-05  val loss:  0.901139497756958\n",
      "epoch:  46   step:  28   train loss:  0.00037484115455299616  val loss:  0.8902163505554199\n",
      "epoch:  46   step:  29   train loss:  6.212830339791253e-05  val loss:  0.8839634656906128\n",
      "epoch:  46   step:  30   train loss:  4.243498551659286e-05  val loss:  0.9105620384216309\n",
      "epoch:  46   step:  31   train loss:  4.909807466901839e-05  val loss:  0.9197364449501038\n",
      "epoch:  46   step:  32   train loss:  6.249880243558437e-05  val loss:  0.9066478610038757\n",
      "epoch:  46   step:  33   train loss:  2.7774254704127088e-05  val loss:  0.9051247239112854\n",
      "epoch:  46   step:  34   train loss:  2.8693448257399723e-05  val loss:  0.8889899849891663\n",
      "epoch:  46   step:  35   train loss:  4.3241772800683975e-05  val loss:  0.8788090348243713\n",
      "epoch:  46   step:  36   train loss:  0.0001539940421935171  val loss:  0.8929040431976318\n",
      "epoch:  46   step:  37   train loss:  4.155865462962538e-05  val loss:  0.8825408816337585\n",
      "epoch:  46   step:  38   train loss:  3.6467743484536186e-05  val loss:  0.8877730965614319\n",
      "epoch:  46   step:  39   train loss:  0.0001571419561514631  val loss:  0.9064909815788269\n",
      "epoch:  46   step:  40   train loss:  0.0001016613095998764  val loss:  0.9013707637786865\n",
      "epoch:  46   step:  41   train loss:  8.625315240351483e-05  val loss:  0.9000251293182373\n",
      "epoch:  46   step:  42   train loss:  5.603072713711299e-05  val loss:  0.897612452507019\n",
      "epoch:  46   step:  43   train loss:  4.377995719551109e-05  val loss:  0.9098092913627625\n",
      "epoch:  46   step:  44   train loss:  4.806639481103048e-05  val loss:  0.9078044891357422\n",
      "epoch:  46   step:  45   train loss:  0.00014122728316579014  val loss:  0.911479115486145\n",
      "epoch:  46   step:  46   train loss:  5.277177842799574e-05  val loss:  0.908791184425354\n",
      "epoch:  46   step:  47   train loss:  4.084780812263489e-05  val loss:  0.9166366457939148\n",
      "epoch:  46   step:  48   train loss:  3.69558620150201e-05  val loss:  0.9234557747840881\n",
      "epoch:  46   step:  49   train loss:  3.041093259525951e-05  val loss:  0.9184072613716125\n",
      "epoch:  46   step:  50   train loss:  6.253908213693649e-05  val loss:  0.9276630878448486\n",
      "epoch:  46   step:  51   train loss:  0.0014670579694211483  val loss:  0.9171974658966064\n",
      "epoch:  46   step:  52   train loss:  5.31934347236529e-05  val loss:  0.9048351049423218\n",
      "epoch:  46   step:  53   train loss:  5.320324271451682e-05  val loss:  0.8967334032058716\n",
      "epoch:  46   step:  54   train loss:  0.00021621183259412646  val loss:  0.8850602507591248\n",
      "epoch:  46   step:  55   train loss:  5.0759448640746996e-05  val loss:  0.8775618672370911\n",
      "epoch:  46   step:  56   train loss:  6.625906098634005e-05  val loss:  0.856164813041687\n",
      "epoch:  46   step:  57   train loss:  4.2927160393446684e-05  val loss:  0.8537652492523193\n",
      "epoch:  46   step:  58   train loss:  8.390792936552316e-05  val loss:  0.8478353023529053\n",
      "epoch:  46   step:  59   train loss:  8.305952360387892e-05  val loss:  0.831733226776123\n",
      "epoch:  46   step:  60   train loss:  0.0005130307981744409  val loss:  0.8213690519332886\n",
      "epoch:  46   step:  61   train loss:  5.823790343129076e-05  val loss:  0.8078815937042236\n",
      "epoch:  46   step:  62   train loss:  0.00011190800432814285  val loss:  0.8067227602005005\n",
      "epoch:  46   step:  63   train loss:  6.122924969531596e-05  val loss:  0.8154892325401306\n",
      "epoch:  46   step:  64   train loss:  7.344082405325025e-05  val loss:  0.8186904788017273\n",
      "epoch:  46   step:  65   train loss:  6.386120367096737e-05  val loss:  0.8251806497573853\n",
      "epoch:  46   step:  66   train loss:  4.262064612703398e-05  val loss:  0.8220265507698059\n",
      "epoch:  46   step:  67   train loss:  6.059585575712845e-05  val loss:  0.8346152901649475\n",
      "epoch:  46   step:  68   train loss:  4.034218000015244e-05  val loss:  0.8457542061805725\n",
      "epoch:  46   step:  69   train loss:  5.760333078796975e-05  val loss:  0.8444649577140808\n",
      "epoch:  46   step:  70   train loss:  5.1442366384435445e-05  val loss:  0.8463720679283142\n",
      "epoch:  46   step:  71   train loss:  9.839574340730906e-05  val loss:  0.8451053500175476\n",
      "epoch:  46   step:  72   train loss:  4.1042876546271145e-05  val loss:  0.8479481339454651\n",
      "epoch:  46   step:  73   train loss:  0.00011376039037713781  val loss:  0.8515772819519043\n",
      "epoch:  46   step:  74   train loss:  5.3991214372217655e-05  val loss:  0.8515661954879761\n",
      "epoch:  46   step:  75   train loss:  6.150789704406634e-05  val loss:  0.843526303768158\n",
      "epoch:  46   step:  76   train loss:  5.53059799131006e-05  val loss:  0.8328677415847778\n",
      "epoch:  46   step:  77   train loss:  4.447637184057385e-05  val loss:  0.8308621048927307\n",
      "epoch:  46   step:  78   train loss:  5.772668373538181e-05  val loss:  0.8349058628082275\n",
      "epoch:  46   step:  79   train loss:  0.00017269447562284768  val loss:  0.8266409039497375\n",
      "epoch:  46   step:  80   train loss:  6.397465040208772e-05  val loss:  0.8122923374176025\n",
      "epoch:  46   step:  81   train loss:  0.00010698887490434572  val loss:  0.8104763031005859\n",
      "epoch:  46   step:  82   train loss:  4.465445090318099e-05  val loss:  0.8242297768592834\n",
      "epoch:  46   step:  83   train loss:  0.00021830524201504886  val loss:  0.8203406929969788\n",
      "epoch:  46   step:  84   train loss:  8.64930625539273e-05  val loss:  0.8169112205505371\n",
      "epoch:  46   step:  85   train loss:  7.206611917354167e-05  val loss:  0.80887770652771\n",
      "epoch:  46   step:  86   train loss:  2.4332710381713696e-05  val loss:  0.8218837380409241\n",
      "epoch:  46   step:  87   train loss:  5.095138476463035e-05  val loss:  0.826362133026123\n",
      "epoch:  46   step:  88   train loss:  0.0001254450180567801  val loss:  0.840930700302124\n",
      "epoch:  46   step:  89   train loss:  0.0001143222616519779  val loss:  0.817341685295105\n",
      "epoch:  46   step:  90   train loss:  6.836252578068525e-05  val loss:  0.8160551190376282\n",
      "epoch:  46   step:  91   train loss:  4.965351035934873e-05  val loss:  0.7997380495071411\n",
      "epoch:  46   step:  92   train loss:  2.6646142941899598e-05  val loss:  0.8161265850067139\n",
      "epoch:  46   step:  93   train loss:  5.332511500455439e-05  val loss:  0.8122069239616394\n",
      "epoch:  46   step:  94   train loss:  5.5911674280650914e-05  val loss:  0.8406869173049927\n",
      "epoch:  46   step:  95   train loss:  5.872621841263026e-05  val loss:  0.8341500163078308\n",
      "epoch:  46   step:  96   train loss:  0.00012060118751833215  val loss:  0.8293463587760925\n",
      "epoch:  46   step:  97   train loss:  6.749823660356924e-05  val loss:  0.8446705937385559\n",
      "epoch:  46   step:  98   train loss:  8.253114356193691e-05  val loss:  0.838141143321991\n",
      "epoch:  46   step:  99   train loss:  4.664964217226952e-05  val loss:  0.8226149082183838\n",
      "epoch:  46   step:  100   train loss:  3.115660365438089e-05  val loss:  0.8361607193946838\n",
      "epoch:  46   step:  101   train loss:  2.6817080652108416e-05  val loss:  0.843119204044342\n",
      "epoch:  46   step:  102   train loss:  3.640133218141273e-05  val loss:  0.8313267230987549\n",
      "epoch:  46   step:  103   train loss:  0.0002327546535525471  val loss:  0.862292468547821\n",
      "epoch:  46   step:  104   train loss:  2.4410746846115217e-05  val loss:  0.8666881322860718\n",
      "epoch:  46   step:  105   train loss:  2.4336097339983098e-05  val loss:  0.8704167604446411\n",
      "epoch:  46   step:  106   train loss:  5.4976248065941036e-05  val loss:  0.8637207746505737\n",
      "epoch:  46   step:  107   train loss:  2.973004120576661e-05  val loss:  0.854978084564209\n",
      "epoch:  46   step:  108   train loss:  6.827089964644983e-05  val loss:  0.8551879525184631\n",
      "epoch:  46   step:  109   train loss:  6.370549817802384e-05  val loss:  0.870174765586853\n",
      "epoch:  46   step:  110   train loss:  6.0761929489672184e-05  val loss:  0.8813496828079224\n",
      "epoch:  46   step:  111   train loss:  0.00015237698971759528  val loss:  0.859738290309906\n",
      "epoch:  46   step:  112   train loss:  3.563817517715506e-05  val loss:  0.8649570345878601\n",
      "epoch:  46   step:  113   train loss:  2.827005482686218e-05  val loss:  0.8639693856239319\n",
      "epoch:  46   step:  114   train loss:  8.236795838456601e-05  val loss:  0.8576627373695374\n",
      "epoch:  46   step:  115   train loss:  2.077513636322692e-05  val loss:  0.8522870540618896\n",
      "epoch:  46   step:  116   train loss:  6.186382233863696e-05  val loss:  0.8576533794403076\n",
      "epoch:  46   step:  117   train loss:  2.463372948113829e-05  val loss:  0.8555818200111389\n",
      "epoch:  46   step:  118   train loss:  3.821158315986395e-05  val loss:  0.8583037853240967\n",
      "epoch:  46   step:  119   train loss:  5.755211896030232e-05  val loss:  0.8663036823272705\n",
      "epoch:  46   step:  120   train loss:  2.3252487153513357e-05  val loss:  0.8638948798179626\n",
      "epoch:  46   step:  121   train loss:  3.7835543480468914e-05  val loss:  0.8575138449668884\n",
      "epoch:  46   step:  122   train loss:  7.068191189318895e-05  val loss:  0.8469421863555908\n",
      "epoch:  46   step:  123   train loss:  7.194186036940664e-05  val loss:  0.8487461805343628\n",
      "epoch:  46   step:  124   train loss:  8.077957318164408e-05  val loss:  0.8687034249305725\n",
      "epoch:  46   step:  125   train loss:  6.144814688013867e-05  val loss:  0.8889198303222656\n",
      "epoch:  46   step:  126   train loss:  2.8076374292140827e-05  val loss:  0.8869186043739319\n",
      "epoch:  46   step:  127   train loss:  3.2947409636108205e-05  val loss:  0.8837971091270447\n",
      "epoch:  46   step:  128   train loss:  0.00020085915457457304  val loss:  0.8790420293807983\n",
      "epoch:  46   step:  129   train loss:  3.3111839002231136e-05  val loss:  0.883429765701294\n",
      "epoch:  46   step:  130   train loss:  2.3970831534825265e-05  val loss:  0.8759509325027466\n",
      "epoch:  46   step:  131   train loss:  8.41217624838464e-05  val loss:  0.8659080862998962\n",
      "epoch:  46   step:  132   train loss:  3.112708873231895e-05  val loss:  0.8698837161064148\n",
      "epoch:  46   step:  133   train loss:  5.126927499077283e-05  val loss:  0.8747701644897461\n",
      "epoch:  46   step:  134   train loss:  4.9231999582843855e-05  val loss:  0.8852674961090088\n",
      "epoch:  46   step:  135   train loss:  3.2251718948828056e-05  val loss:  0.8797053098678589\n",
      "epoch:  46   step:  136   train loss:  0.00011345725215505809  val loss:  0.8753359317779541\n",
      "epoch:  46   step:  137   train loss:  8.240339229814708e-05  val loss:  0.8660134077072144\n",
      "epoch:  46   step:  138   train loss:  5.5307638831436634e-05  val loss:  0.8544124960899353\n",
      "epoch:  46   step:  139   train loss:  2.8575526812346652e-05  val loss:  0.8546556830406189\n",
      "epoch:  46   step:  140   train loss:  3.838680640910752e-05  val loss:  0.8446295261383057\n",
      "epoch:  46   step:  141   train loss:  5.7472498156130314e-05  val loss:  0.8473947048187256\n",
      "epoch:  46   step:  142   train loss:  3.587195533327758e-05  val loss:  0.8550421595573425\n",
      "epoch:  46   step:  143   train loss:  0.00011950832413276657  val loss:  0.8607451915740967\n",
      "epoch:  46   step:  144   train loss:  7.361060124821961e-05  val loss:  0.8700413107872009\n",
      "epoch:  46   step:  145   train loss:  2.7405840228311718e-05  val loss:  0.8742902874946594\n",
      "epoch:  46   step:  146   train loss:  6.201757059898227e-05  val loss:  0.8709982633590698\n",
      "epoch:  46   step:  147   train loss:  4.883016663370654e-05  val loss:  0.881761908531189\n",
      "epoch:  46   step:  148   train loss:  3.482878673821688e-05  val loss:  0.8908777236938477\n",
      "epoch:  46   step:  149   train loss:  5.926420635660179e-05  val loss:  0.8764915466308594\n",
      "epoch:  46   step:  150   train loss:  5.3730527724837884e-05  val loss:  0.8824049830436707\n",
      "epoch:  46   step:  151   train loss:  5.708014577976428e-05  val loss:  0.8770452737808228\n"
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.SGD(params = submodule2_net.parameters(), lr=1e-4, momentum=0.9, weight_decay=2e-5)\n",
    "optimizer = torch.optim.Adam(params = submodule2_net.parameters(), lr=1e-4)\n",
    "#optimizer = torch.optim.SGD(params = submodule2_net.parameters(), lr=1e-4)\n",
    "#loss_func = torch.nn.CrossEntropyLoss(weight=class_weights).cuda()  # the target label is NOT an one-hotted\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "val_submodule2_data_input_1 = val_submodule2_data_input_1.cuda().float()\n",
    "val_submodule2_data_input_2 = val_submodule2_data_input_2.cuda().float()\n",
    "val_submodule2_data_label = val_submodule2_data_label.cuda().float()\n",
    "\n",
    "min_val_loss_print=float('inf')\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for step, (batch_data_1, batch_data_2, batch_label) in enumerate(loader):\n",
    "        batch_data_1 = batch_data_1.cuda().float()\n",
    "        batch_data_2 = batch_data_2.cuda().float()\n",
    "        batch_label = batch_label.cuda().float()\n",
    "        submodule2_net.train()\n",
    "        output  = submodule2_net(batch_data_1, batch_data_2)\n",
    "        train_loss = loss_func(output,batch_label)\n",
    "        #print(train_loss)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_print = train_loss.data.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            submodule2_net.eval()\n",
    "            val_output  = submodule2_net(val_submodule2_data_input_1, val_submodule2_data_input_2)\n",
    "            #print(val_output)\n",
    "            val_loss = loss_func(val_output,val_submodule2_data_label)\n",
    "            val_loss_print = val_loss.data.item()\n",
    "            torch.cuda.empty_cache()\n",
    "        #print('epoch: ', epoch, '  step: ', step, '  train loss: ', train_loss_print, 'a val loss: ', val_loss_print)\n",
    "        print('epoch: ', epoch, '  step: ', step, '  train loss: ', train_loss_print, ' val loss: ', val_loss_print)\n",
    "        if val_loss_print < min_val_loss_print:\n",
    "            torch.save(submodule2_net.state_dict(), 'net_params_20210814/net_params_20210814_pitch_roll_1/epoch_'+str(epoch)+'.pkl') \n",
    "            min_val_loss_print = val_loss_print\n",
    "            print('min_val_loss_print', min_val_loss_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
