{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "import scipy.ndimage\n",
    "BATCH_SIZE =  32\n",
    "NUM_EPOCH = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Construct customized ResNet\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    " \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    " \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    " \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    " \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    " \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    " \n",
    "        return out\n",
    "\n",
    "class Submodule2(nn.Module):\n",
    "        \n",
    "    def __init__(self, pcpt_block, pcpt_layers, scoop_block, scoop_layers, h, w, pcpt_is_upsample=0, scoop_is_upsample=0):\n",
    "        self.inplanes = 64\n",
    "        self.pcpt_is_upsample = pcpt_is_upsample\n",
    "        super(Submodule2, self).__init__()\n",
    "        self.pcpt_conv1 = nn.Conv2d(4, 64, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.pcpt_bn1 = nn.BatchNorm2d(64)\n",
    "        self.pcpt_relu = nn.ReLU(inplace=True)\n",
    "        self.pcpt_maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.pcpt_upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.pcpt_layer1 = self._make_layer(pcpt_block, 128, pcpt_layers[0])\n",
    "        self.pcpt_layer2 = self._make_layer(pcpt_block, 256, pcpt_layers[1])\n",
    "        self.pcpt_layer3 = self._make_layer(pcpt_block, 512, pcpt_layers[2])\n",
    "\n",
    "        self.inplanes = 512\n",
    "        self.scoop_is_upsample = scoop_is_upsample\n",
    "        self.scoop_upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.scoop_layer1 = self._make_layer(scoop_block, 256, scoop_layers[0])\n",
    "        self.scoop_layer2 = self._make_layer(scoop_block, 128, scoop_layers[1])\n",
    "        self.scoop_layer3 = self._make_layer(scoop_block, 64, scoop_layers[2])\n",
    "        self.scoop_conv1 = nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.scoop_bn1 = nn.BatchNorm2d(1)\n",
    "        self.scoop_conv2 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.scoop_bn2 = nn.BatchNorm2d(3)\n",
    "        self.scoop_relu = nn.ReLU(inplace=True)\n",
    "\n",
    "            \n",
    "            \n",
    "        self.x1_hidden = torch.nn.Linear(3*200*200, 200)\n",
    "        self.x2_hidden = torch.nn.Linear(2, 200)\n",
    "        self.x_hidden1 = torch.nn.Linear(400, 200)\n",
    "        self.x_hidden2 = torch.nn.Linear(200, 50)\n",
    "        self.x_hidden3 = torch.nn.Linear(50, 1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.pcpt_conv1(x1)\n",
    "        x1 = self.pcpt_bn1(x1)\n",
    "        x1 = self.pcpt_relu(x1)\n",
    "        x1 = self.pcpt_maxpool(x1)\n",
    "\n",
    "        x1 = self.pcpt_layer1(x1)\n",
    "        x1 = self.pcpt_maxpool(x1)\n",
    "        x1 = self.pcpt_layer2(x1)\n",
    "        x1 = self.pcpt_layer3(x1)\n",
    "\n",
    "        x1 = self.scoop_layer1(x1)\n",
    "        x1 = self.scoop_layer2(x1)\n",
    "        x1 = self.scoop_upsample(x1)\n",
    "        x1 = self.scoop_layer3(x1)\n",
    "        x1 = self.scoop_upsample(x1)\n",
    "\n",
    "        x1 = self.scoop_conv2(x1)\n",
    "        x1 = self.scoop_bn2(x1)\n",
    "        x1 = self.scoop_relu(x1)       \n",
    "        x1 = x1.reshape(x1.shape[0],-1)\n",
    "        \n",
    "        x1 = F.relu(self.x1_hidden(x1))\n",
    "        \n",
    "        x2 = F.relu(self.x2_hidden(x2))\n",
    "        \n",
    "        x = torch.cat((x1, x2), dim=-1)\n",
    "        \n",
    "        x = F.relu(self.x_hidden1(x))\n",
    "        x = F.relu(self.x_hidden2(x))\n",
    "        x = F.relu(self.x_hidden3(x))\n",
    "        #x=torch.reshape(x,(-1,3,200))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodule2(\n",
      "  (pcpt_conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (pcpt_bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pcpt_relu): ReLU(inplace=True)\n",
      "  (pcpt_maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pcpt_upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (pcpt_layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pcpt_layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pcpt_layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_upsample): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "  (scoop_layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (scoop_conv1): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (scoop_bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (scoop_conv2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (scoop_bn2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (scoop_relu): ReLU(inplace=True)\n",
      "  (x1_hidden): Linear(in_features=120000, out_features=200, bias=True)\n",
      "  (x2_hidden): Linear(in_features=2, out_features=200, bias=True)\n",
      "  (x_hidden1): Linear(in_features=400, out_features=200, bias=True)\n",
      "  (x_hidden2): Linear(in_features=200, out_features=50, bias=True)\n",
      "  (x_hidden3): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntry_data_input1=torch.from_numpy(np.load(\"data_20210605/input_data_array_submodule2_image.npy\")/255.0).permute(0,3,1,2)[0:2, :, :, :].cuda().float()\\ntry_data_input2=torch.from_numpy(np.array([[50,60], [30,40]])).cuda().float()\\naaa=submodule2_net(try_data_input1, try_data_input2)\\nprint(aaa.shape)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submodule2_net = Submodule2(pcpt_block=BasicBlock, pcpt_layers=[1,1,1], scoop_block=BasicBlock, scoop_layers=[1,1,1], h=200, w=200).cuda()\n",
    "print(submodule2_net)\n",
    "\n",
    "'''\n",
    "try_data_input1=torch.from_numpy(np.load(\"data_20210605/input_data_array_submodule2_image.npy\")/255.0).permute(0,3,1,2)[0:2, :, :, :].cuda().float()\n",
    "try_data_input2=torch.from_numpy(np.array([[50,60], [30,40]])).cuda().float()\n",
    "aaa=submodule2_net(try_data_input1, try_data_input2)\n",
    "print(aaa.shape)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6727, 200, 200, 4)\n",
      "(6727, 2)\n",
      "(6727, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "submodule2_data_input_1 = np.load(\"data_20210711/input_data_array_submodule2_image_20210711.npy\")/255.0\n",
    "print(submodule2_data_input_1.shape)\n",
    "\n",
    "temp_index_set = random.sample(range(len(submodule2_data_input_1)), 50)\n",
    "temp_index_set_other = list(set(range(len(submodule2_data_input_1))).difference(set(temp_index_set)))\n",
    "\n",
    "submodule2_data_input_1 = torch.from_numpy(submodule2_data_input_1).permute(0,3,1,2)\n",
    "train_submodule2_data_input_1 = submodule2_data_input_1[temp_index_set_other, :, :, :]\n",
    "val_submodule2_data_input_1 = submodule2_data_input_1[temp_index_set, :, :, :]\n",
    "\n",
    "submodule2_data_input_2 = np.load(\"data_20210711/input_data_array_submodule2_finger_position_20210711.npy\")\n",
    "print(submodule2_data_input_2.shape)\n",
    "submodule2_data_input_2 = torch.from_numpy(submodule2_data_input_2)\n",
    "train_submodule2_data_input_2 = submodule2_data_input_2[temp_index_set_other, :]\n",
    "val_submodule2_data_input_2 = submodule2_data_input_2[temp_index_set, :]\n",
    "\n",
    "#print(train_submodule2_data_input_1)\n",
    "\n",
    "\n",
    "submodule2_data_label = np.load(\"data_20210711/label_data_array_submodule2_thumb_position_20210711.npy\")\n",
    "print(submodule2_data_label.shape)\n",
    "\n",
    "submodule2_data_label = torch.from_numpy(submodule2_data_label).long()\n",
    "train_submodule2_data_label = submodule2_data_label[temp_index_set_other, :]\n",
    "val_submodule2_data_label = submodule2_data_label[temp_index_set, :]\n",
    "\n",
    "submodule2_train_torch_dataset = Data.TensorDataset(train_submodule2_data_input_1, train_submodule2_data_input_2, train_submodule2_data_label)\n",
    "loader = Data.DataLoader(dataset=submodule2_train_torch_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "htmap_h = submodule2_data_input_1.shape[2]\n",
    "htmap_w = submodule2_data_input_1.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   step:  0   train loss:  65.59323120117188  val loss:  60.90070343017578  val L1 loss:  61.4007\n",
      "min_val_loss_print 60.90070343017578\n",
      "epoch:  0   step:  1   train loss:  60.5460205078125  val loss:  58.14142608642578  val L1 loss:  58.6361\n",
      "min_val_loss_print 58.14142608642578\n",
      "epoch:  0   step:  2   train loss:  57.863765716552734  val loss:  56.21961975097656  val L1 loss:  56.7196\n",
      "min_val_loss_print 56.21961975097656\n",
      "epoch:  0   step:  3   train loss:  52.14617156982422  val loss:  54.10171890258789  val L1 loss:  54.6017\n",
      "min_val_loss_print 54.10171890258789\n",
      "epoch:  0   step:  4   train loss:  42.818687438964844  val loss:  52.03583908081055  val L1 loss:  52.5358\n",
      "min_val_loss_print 52.03583908081055\n",
      "epoch:  0   step:  5   train loss:  55.885494232177734  val loss:  50.21784973144531  val L1 loss:  50.7178\n",
      "min_val_loss_print 50.21784973144531\n",
      "epoch:  0   step:  6   train loss:  47.47514343261719  val loss:  48.39632797241211  val L1 loss:  48.8963\n",
      "min_val_loss_print 48.39632797241211\n",
      "epoch:  0   step:  7   train loss:  45.452674865722656  val loss:  46.52093505859375  val L1 loss:  47.0174\n",
      "min_val_loss_print 46.52093505859375\n",
      "epoch:  0   step:  8   train loss:  43.353240966796875  val loss:  44.447792053222656  val L1 loss:  44.9478\n",
      "min_val_loss_print 44.447792053222656\n",
      "epoch:  0   step:  9   train loss:  43.113407135009766  val loss:  41.928123474121094  val L1 loss:  42.4281\n",
      "min_val_loss_print 41.928123474121094\n",
      "epoch:  0   step:  10   train loss:  37.48908233642578  val loss:  38.383567810058594  val L1 loss:  38.8833\n",
      "min_val_loss_print 38.383567810058594\n",
      "epoch:  0   step:  11   train loss:  39.90139389038086  val loss:  34.05880355834961  val L1 loss:  34.5587\n",
      "min_val_loss_print 34.05880355834961\n",
      "epoch:  0   step:  12   train loss:  35.15301513671875  val loss:  28.589317321777344  val L1 loss:  29.0889\n",
      "min_val_loss_print 28.589317321777344\n",
      "epoch:  0   step:  13   train loss:  30.176410675048828  val loss:  23.709266662597656  val L1 loss:  24.2016\n",
      "min_val_loss_print 23.709266662597656\n",
      "epoch:  0   step:  14   train loss:  37.821258544921875  val loss:  22.618120193481445  val L1 loss:  23.1104\n",
      "min_val_loss_print 22.618120193481445\n",
      "epoch:  0   step:  15   train loss:  39.29759216308594  val loss:  22.28607940673828  val L1 loss:  22.7837\n",
      "min_val_loss_print 22.28607940673828\n",
      "epoch:  0   step:  16   train loss:  36.880836486816406  val loss:  21.94972038269043  val L1 loss:  22.4433\n",
      "min_val_loss_print 21.94972038269043\n",
      "epoch:  0   step:  17   train loss:  33.06221008300781  val loss:  21.706777572631836  val L1 loss:  22.1975\n",
      "min_val_loss_print 21.706777572631836\n",
      "epoch:  0   step:  18   train loss:  23.649263381958008  val loss:  21.27383804321289  val L1 loss:  21.7678\n",
      "min_val_loss_print 21.27383804321289\n",
      "epoch:  0   step:  19   train loss:  23.589027404785156  val loss:  21.00701904296875  val L1 loss:  21.501\n",
      "min_val_loss_print 21.00701904296875\n",
      "epoch:  0   step:  20   train loss:  28.10419464111328  val loss:  21.413986206054688  val L1 loss:  21.914\n",
      "epoch:  0   step:  21   train loss:  24.44070816040039  val loss:  22.160112380981445  val L1 loss:  22.6601\n",
      "epoch:  0   step:  22   train loss:  23.890268325805664  val loss:  22.853734970092773  val L1 loss:  23.3496\n",
      "epoch:  0   step:  23   train loss:  28.789016723632812  val loss:  24.187049865722656  val L1 loss:  24.6796\n",
      "epoch:  0   step:  24   train loss:  21.050134658813477  val loss:  25.444623947143555  val L1 loss:  25.9446\n",
      "epoch:  0   step:  25   train loss:  26.81897735595703  val loss:  25.945831298828125  val L1 loss:  26.4458\n",
      "epoch:  0   step:  26   train loss:  27.374614715576172  val loss:  26.21758460998535  val L1 loss:  26.7176\n",
      "epoch:  0   step:  27   train loss:  20.821067810058594  val loss:  26.14558982849121  val L1 loss:  26.6361\n",
      "epoch:  0   step:  28   train loss:  27.312435150146484  val loss:  25.376991271972656  val L1 loss:  25.866\n",
      "epoch:  0   step:  29   train loss:  23.923521041870117  val loss:  24.20360565185547  val L1 loss:  24.6973\n",
      "epoch:  0   step:  30   train loss:  25.007652282714844  val loss:  23.13624382019043  val L1 loss:  23.6354\n",
      "epoch:  0   step:  31   train loss:  17.72979164123535  val loss:  21.906417846679688  val L1 loss:  22.3997\n",
      "epoch:  0   step:  32   train loss:  21.59534454345703  val loss:  21.321157455444336  val L1 loss:  21.8103\n",
      "epoch:  0   step:  33   train loss:  25.241249084472656  val loss:  21.19207763671875  val L1 loss:  21.6874\n",
      "epoch:  0   step:  34   train loss:  19.980649948120117  val loss:  21.217376708984375  val L1 loss:  21.7016\n",
      "epoch:  0   step:  35   train loss:  19.316444396972656  val loss:  21.41585922241211  val L1 loss:  21.9106\n",
      "epoch:  0   step:  36   train loss:  23.203832626342773  val loss:  21.757904052734375  val L1 loss:  22.2509\n",
      "epoch:  0   step:  37   train loss:  21.691747665405273  val loss:  21.40938377380371  val L1 loss:  21.9059\n",
      "epoch:  0   step:  38   train loss:  21.83538818359375  val loss:  20.888916015625  val L1 loss:  21.3783\n",
      "min_val_loss_print 20.888916015625\n",
      "epoch:  0   step:  39   train loss:  24.295391082763672  val loss:  20.333385467529297  val L1 loss:  20.8326\n",
      "min_val_loss_print 20.333385467529297\n",
      "epoch:  0   step:  40   train loss:  22.203266143798828  val loss:  19.667558670043945  val L1 loss:  20.1676\n",
      "min_val_loss_print 19.667558670043945\n",
      "epoch:  0   step:  41   train loss:  20.206600189208984  val loss:  19.2121524810791  val L1 loss:  19.7122\n",
      "min_val_loss_print 19.2121524810791\n",
      "epoch:  0   step:  42   train loss:  21.162601470947266  val loss:  18.763185501098633  val L1 loss:  19.2578\n",
      "min_val_loss_print 18.763185501098633\n",
      "epoch:  0   step:  43   train loss:  22.24884605407715  val loss:  18.267372131347656  val L1 loss:  18.7607\n",
      "min_val_loss_print 18.267372131347656\n",
      "epoch:  0   step:  44   train loss:  17.631702423095703  val loss:  17.886329650878906  val L1 loss:  18.3827\n",
      "min_val_loss_print 17.886329650878906\n",
      "epoch:  0   step:  45   train loss:  18.817272186279297  val loss:  17.528139114379883  val L1 loss:  18.0207\n",
      "min_val_loss_print 17.528139114379883\n",
      "epoch:  0   step:  46   train loss:  19.486133575439453  val loss:  17.292020797729492  val L1 loss:  17.7833\n",
      "min_val_loss_print 17.292020797729492\n",
      "epoch:  0   step:  47   train loss:  18.03443145751953  val loss:  17.17359733581543  val L1 loss:  17.6621\n",
      "min_val_loss_print 17.17359733581543\n",
      "epoch:  0   step:  48   train loss:  16.72393035888672  val loss:  17.134153366088867  val L1 loss:  17.6226\n",
      "min_val_loss_print 17.134153366088867\n",
      "epoch:  0   step:  49   train loss:  20.810989379882812  val loss:  16.91742706298828  val L1 loss:  17.3956\n",
      "min_val_loss_print 16.91742706298828\n",
      "epoch:  0   step:  50   train loss:  25.00071144104004  val loss:  16.606409072875977  val L1 loss:  17.0919\n",
      "min_val_loss_print 16.606409072875977\n",
      "epoch:  0   step:  51   train loss:  16.38634490966797  val loss:  16.394044876098633  val L1 loss:  16.886\n",
      "min_val_loss_print 16.394044876098633\n",
      "epoch:  0   step:  52   train loss:  22.946535110473633  val loss:  16.220314025878906  val L1 loss:  16.7048\n",
      "min_val_loss_print 16.220314025878906\n",
      "epoch:  0   step:  53   train loss:  21.76150131225586  val loss:  16.074716567993164  val L1 loss:  16.5618\n",
      "min_val_loss_print 16.074716567993164\n",
      "epoch:  0   step:  54   train loss:  20.328208923339844  val loss:  15.943395614624023  val L1 loss:  16.4276\n",
      "min_val_loss_print 15.943395614624023\n",
      "epoch:  0   step:  55   train loss:  18.399192810058594  val loss:  15.939806938171387  val L1 loss:  16.4363\n",
      "min_val_loss_print 15.939806938171387\n",
      "epoch:  0   step:  56   train loss:  21.21725082397461  val loss:  16.084087371826172  val L1 loss:  16.5792\n",
      "epoch:  0   step:  57   train loss:  19.41252899169922  val loss:  16.34996223449707  val L1 loss:  16.85\n",
      "epoch:  0   step:  58   train loss:  20.40875244140625  val loss:  16.406156539916992  val L1 loss:  16.9062\n",
      "epoch:  0   step:  59   train loss:  20.703502655029297  val loss:  16.272560119628906  val L1 loss:  16.7709\n",
      "epoch:  0   step:  60   train loss:  19.746074676513672  val loss:  16.028697967529297  val L1 loss:  16.5272\n",
      "epoch:  0   step:  61   train loss:  20.909664154052734  val loss:  15.846725463867188  val L1 loss:  16.3398\n",
      "min_val_loss_print 15.846725463867188\n",
      "epoch:  0   step:  62   train loss:  16.58509063720703  val loss:  15.616830825805664  val L1 loss:  16.1049\n",
      "min_val_loss_print 15.616830825805664\n",
      "epoch:  0   step:  63   train loss:  19.160493850708008  val loss:  15.344239234924316  val L1 loss:  15.8214\n",
      "min_val_loss_print 15.344239234924316\n",
      "epoch:  0   step:  64   train loss:  18.444547653198242  val loss:  15.162392616271973  val L1 loss:  15.6507\n",
      "min_val_loss_print 15.162392616271973\n",
      "epoch:  0   step:  65   train loss:  18.95903205871582  val loss:  15.060644149780273  val L1 loss:  15.5361\n",
      "min_val_loss_print 15.060644149780273\n",
      "epoch:  0   step:  66   train loss:  16.123519897460938  val loss:  15.04382610321045  val L1 loss:  15.5395\n",
      "min_val_loss_print 15.04382610321045\n",
      "epoch:  0   step:  67   train loss:  16.589473724365234  val loss:  15.013920783996582  val L1 loss:  15.5135\n",
      "min_val_loss_print 15.013920783996582\n",
      "epoch:  0   step:  68   train loss:  19.16175079345703  val loss:  14.954713821411133  val L1 loss:  15.4501\n",
      "min_val_loss_print 14.954713821411133\n",
      "epoch:  0   step:  69   train loss:  16.061922073364258  val loss:  14.88968563079834  val L1 loss:  15.376\n",
      "min_val_loss_print 14.88968563079834\n",
      "epoch:  0   step:  70   train loss:  20.932544708251953  val loss:  14.813032150268555  val L1 loss:  15.3088\n",
      "min_val_loss_print 14.813032150268555\n",
      "epoch:  0   step:  71   train loss:  16.643840789794922  val loss:  14.900510787963867  val L1 loss:  15.4005\n",
      "epoch:  0   step:  72   train loss:  18.945119857788086  val loss:  14.882704734802246  val L1 loss:  15.3822\n",
      "epoch:  0   step:  73   train loss:  15.780603408813477  val loss:  14.765153884887695  val L1 loss:  15.264\n",
      "min_val_loss_print 14.765153884887695\n",
      "epoch:  0   step:  74   train loss:  17.833580017089844  val loss:  14.737658500671387  val L1 loss:  15.2326\n",
      "min_val_loss_print 14.737658500671387\n",
      "epoch:  0   step:  75   train loss:  16.883834838867188  val loss:  14.761893272399902  val L1 loss:  15.2493\n",
      "epoch:  0   step:  76   train loss:  18.073637008666992  val loss:  14.613187789916992  val L1 loss:  15.1064\n",
      "min_val_loss_print 14.613187789916992\n",
      "epoch:  0   step:  77   train loss:  11.359429359436035  val loss:  14.499526023864746  val L1 loss:  14.9972\n",
      "min_val_loss_print 14.499526023864746\n",
      "epoch:  0   step:  78   train loss:  19.424823760986328  val loss:  14.460911750793457  val L1 loss:  14.9577\n",
      "min_val_loss_print 14.460911750793457\n",
      "epoch:  0   step:  79   train loss:  15.636317253112793  val loss:  14.275092124938965  val L1 loss:  14.76\n",
      "min_val_loss_print 14.275092124938965\n",
      "epoch:  0   step:  80   train loss:  17.132076263427734  val loss:  13.94664478302002  val L1 loss:  14.4318\n",
      "min_val_loss_print 13.94664478302002\n",
      "epoch:  0   step:  81   train loss:  18.972118377685547  val loss:  13.776198387145996  val L1 loss:  14.2596\n",
      "min_val_loss_print 13.776198387145996\n",
      "epoch:  0   step:  82   train loss:  15.707547187805176  val loss:  13.77432632446289  val L1 loss:  14.2642\n",
      "min_val_loss_print 13.77432632446289\n",
      "epoch:  0   step:  83   train loss:  16.775039672851562  val loss:  13.72518539428711  val L1 loss:  14.217\n",
      "min_val_loss_print 13.72518539428711\n",
      "epoch:  0   step:  84   train loss:  15.29764461517334  val loss:  13.705414772033691  val L1 loss:  14.1997\n",
      "min_val_loss_print 13.705414772033691\n",
      "epoch:  0   step:  85   train loss:  19.528575897216797  val loss:  13.577714920043945  val L1 loss:  14.0739\n",
      "min_val_loss_print 13.577714920043945\n",
      "epoch:  0   step:  86   train loss:  13.815868377685547  val loss:  13.455588340759277  val L1 loss:  13.9407\n",
      "min_val_loss_print 13.455588340759277\n",
      "epoch:  0   step:  87   train loss:  13.681661605834961  val loss:  13.337620735168457  val L1 loss:  13.831\n",
      "min_val_loss_print 13.337620735168457\n",
      "epoch:  0   step:  88   train loss:  16.429594039916992  val loss:  13.191765785217285  val L1 loss:  13.6899\n",
      "min_val_loss_print 13.191765785217285\n",
      "epoch:  0   step:  89   train loss:  15.167524337768555  val loss:  13.012106895446777  val L1 loss:  13.5093\n",
      "min_val_loss_print 13.012106895446777\n",
      "epoch:  0   step:  90   train loss:  15.78563117980957  val loss:  12.70488452911377  val L1 loss:  13.1954\n",
      "min_val_loss_print 12.70488452911377\n",
      "epoch:  0   step:  91   train loss:  18.703399658203125  val loss:  12.536730766296387  val L1 loss:  13.0008\n",
      "min_val_loss_print 12.536730766296387\n",
      "epoch:  0   step:  92   train loss:  12.706770896911621  val loss:  12.50633716583252  val L1 loss:  12.9824\n",
      "min_val_loss_print 12.50633716583252\n",
      "epoch:  0   step:  93   train loss:  15.084362030029297  val loss:  12.580000877380371  val L1 loss:  13.0601\n",
      "epoch:  0   step:  94   train loss:  14.347733497619629  val loss:  12.60563850402832  val L1 loss:  13.0796\n",
      "epoch:  0   step:  95   train loss:  14.916874885559082  val loss:  12.712039947509766  val L1 loss:  13.2046\n",
      "epoch:  0   step:  96   train loss:  14.234853744506836  val loss:  12.816447257995605  val L1 loss:  13.3032\n",
      "epoch:  0   step:  97   train loss:  16.397485733032227  val loss:  12.845358848571777  val L1 loss:  13.3373\n",
      "epoch:  0   step:  98   train loss:  12.953147888183594  val loss:  12.89815902709961  val L1 loss:  13.3777\n",
      "epoch:  0   step:  99   train loss:  15.178221702575684  val loss:  13.05841064453125  val L1 loss:  13.534\n",
      "epoch:  0   step:  100   train loss:  15.624805450439453  val loss:  13.488696098327637  val L1 loss:  13.9882\n",
      "epoch:  0   step:  101   train loss:  13.375221252441406  val loss:  13.910802841186523  val L1 loss:  14.4108\n",
      "epoch:  0   step:  102   train loss:  13.436029434204102  val loss:  14.32408618927002  val L1 loss:  14.8177\n",
      "epoch:  0   step:  103   train loss:  13.157355308532715  val loss:  14.699132919311523  val L1 loss:  15.1914\n",
      "epoch:  0   step:  104   train loss:  13.476527214050293  val loss:  14.760961532592773  val L1 loss:  15.2602\n",
      "epoch:  0   step:  105   train loss:  14.171506881713867  val loss:  14.287939071655273  val L1 loss:  14.7753\n",
      "epoch:  0   step:  106   train loss:  14.196514129638672  val loss:  13.672696113586426  val L1 loss:  14.1694\n",
      "epoch:  0   step:  107   train loss:  15.872709274291992  val loss:  12.801515579223633  val L1 loss:  13.301\n",
      "epoch:  0   step:  108   train loss:  13.397375106811523  val loss:  12.21702766418457  val L1 loss:  12.7168\n",
      "min_val_loss_print 12.21702766418457\n",
      "epoch:  0   step:  109   train loss:  13.823094367980957  val loss:  12.12704086303711  val L1 loss:  12.6216\n",
      "min_val_loss_print 12.12704086303711\n",
      "epoch:  0   step:  110   train loss:  11.227432250976562  val loss:  12.950153350830078  val L1 loss:  13.4253\n",
      "epoch:  0   step:  111   train loss:  13.707128524780273  val loss:  13.739262580871582  val L1 loss:  14.2393\n",
      "epoch:  0   step:  112   train loss:  13.634233474731445  val loss:  13.996112823486328  val L1 loss:  14.4939\n",
      "epoch:  0   step:  113   train loss:  12.615270614624023  val loss:  13.700361251831055  val L1 loss:  14.1985\n",
      "epoch:  0   step:  114   train loss:  13.071744918823242  val loss:  13.099620819091797  val L1 loss:  13.5878\n",
      "epoch:  0   step:  115   train loss:  11.902031898498535  val loss:  12.473421096801758  val L1 loss:  12.9604\n",
      "epoch:  0   step:  116   train loss:  12.739517211914062  val loss:  11.91723346710205  val L1 loss:  12.4143\n",
      "min_val_loss_print 11.91723346710205\n",
      "epoch:  0   step:  117   train loss:  10.878020286560059  val loss:  11.700112342834473  val L1 loss:  12.1885\n",
      "min_val_loss_print 11.700112342834473\n",
      "epoch:  0   step:  118   train loss:  13.325143814086914  val loss:  11.59123706817627  val L1 loss:  12.087\n",
      "min_val_loss_print 11.59123706817627\n",
      "epoch:  0   step:  119   train loss:  10.685781478881836  val loss:  11.631134986877441  val L1 loss:  12.1177\n",
      "epoch:  0   step:  120   train loss:  13.302068710327148  val loss:  11.839316368103027  val L1 loss:  12.3231\n",
      "epoch:  0   step:  121   train loss:  14.979642868041992  val loss:  11.859123229980469  val L1 loss:  12.3589\n",
      "epoch:  0   step:  122   train loss:  13.045759201049805  val loss:  11.745392799377441  val L1 loss:  12.2344\n",
      "epoch:  0   step:  123   train loss:  13.870851516723633  val loss:  11.469243049621582  val L1 loss:  11.952\n",
      "min_val_loss_print 11.469243049621582\n",
      "epoch:  0   step:  124   train loss:  11.994281768798828  val loss:  11.428373336791992  val L1 loss:  11.9277\n",
      "min_val_loss_print 11.428373336791992\n",
      "epoch:  0   step:  125   train loss:  14.768301963806152  val loss:  11.520666122436523  val L1 loss:  12.0125\n",
      "epoch:  0   step:  126   train loss:  10.641897201538086  val loss:  11.735187530517578  val L1 loss:  12.2167\n",
      "epoch:  0   step:  127   train loss:  14.188934326171875  val loss:  11.99837589263916  val L1 loss:  12.4802\n",
      "epoch:  0   step:  128   train loss:  11.72566032409668  val loss:  12.099970817565918  val L1 loss:  12.5928\n",
      "epoch:  0   step:  129   train loss:  12.48261833190918  val loss:  11.96590518951416  val L1 loss:  12.4555\n",
      "epoch:  0   step:  130   train loss:  9.924222946166992  val loss:  11.749014854431152  val L1 loss:  12.244\n",
      "epoch:  0   step:  131   train loss:  9.054401397705078  val loss:  11.32564640045166  val L1 loss:  11.8021\n",
      "min_val_loss_print 11.32564640045166\n",
      "epoch:  0   step:  132   train loss:  14.458745956420898  val loss:  11.098311424255371  val L1 loss:  11.5873\n",
      "min_val_loss_print 11.098311424255371\n",
      "epoch:  0   step:  133   train loss:  9.80997085571289  val loss:  10.79965591430664  val L1 loss:  11.2945\n",
      "min_val_loss_print 10.79965591430664\n",
      "epoch:  0   step:  134   train loss:  10.624103546142578  val loss:  10.632376670837402  val L1 loss:  11.1174\n",
      "min_val_loss_print 10.632376670837402\n",
      "epoch:  0   step:  135   train loss:  8.632643699645996  val loss:  10.557558059692383  val L1 loss:  11.0496\n",
      "min_val_loss_print 10.557558059692383\n",
      "epoch:  0   step:  136   train loss:  11.671003341674805  val loss:  10.520084381103516  val L1 loss:  11.0177\n",
      "min_val_loss_print 10.520084381103516\n",
      "epoch:  0   step:  137   train loss:  10.535543441772461  val loss:  10.424357414245605  val L1 loss:  10.9141\n",
      "min_val_loss_print 10.424357414245605\n",
      "epoch:  0   step:  138   train loss:  7.938821792602539  val loss:  10.240693092346191  val L1 loss:  10.7248\n",
      "min_val_loss_print 10.240693092346191\n",
      "epoch:  0   step:  139   train loss:  9.317970275878906  val loss:  10.145830154418945  val L1 loss:  10.6423\n",
      "min_val_loss_print 10.145830154418945\n",
      "epoch:  0   step:  140   train loss:  8.543584823608398  val loss:  10.11591911315918  val L1 loss:  10.6045\n",
      "min_val_loss_print 10.11591911315918\n",
      "epoch:  0   step:  141   train loss:  11.048175811767578  val loss:  10.16195297241211  val L1 loss:  10.6493\n",
      "epoch:  0   step:  142   train loss:  9.604690551757812  val loss:  10.484663009643555  val L1 loss:  10.9781\n",
      "epoch:  0   step:  143   train loss:  9.520190238952637  val loss:  10.748724937438965  val L1 loss:  11.2403\n",
      "epoch:  0   step:  144   train loss:  7.77454948425293  val loss:  11.071757316589355  val L1 loss:  11.5661\n",
      "epoch:  0   step:  145   train loss:  13.770401000976562  val loss:  11.349379539489746  val L1 loss:  11.8428\n",
      "epoch:  0   step:  146   train loss:  10.89361572265625  val loss:  11.249311447143555  val L1 loss:  11.7407\n",
      "epoch:  0   step:  147   train loss:  11.104434967041016  val loss:  11.03470230102539  val L1 loss:  11.5258\n",
      "epoch:  0   step:  148   train loss:  12.10464859008789  val loss:  10.789688110351562  val L1 loss:  11.2853\n",
      "epoch:  0   step:  149   train loss:  10.507219314575195  val loss:  10.685443878173828  val L1 loss:  11.1684\n",
      "epoch:  0   step:  150   train loss:  11.291680335998535  val loss:  10.55891227722168  val L1 loss:  11.058\n",
      "epoch:  0   step:  151   train loss:  10.166094779968262  val loss:  10.46790599822998  val L1 loss:  10.9544\n",
      "epoch:  0   step:  152   train loss:  10.959175109863281  val loss:  10.337604522705078  val L1 loss:  10.8248\n",
      "epoch:  0   step:  153   train loss:  9.18491268157959  val loss:  10.185770988464355  val L1 loss:  10.6568\n",
      "epoch:  0   step:  154   train loss:  10.295352935791016  val loss:  10.110962867736816  val L1 loss:  10.5972\n",
      "min_val_loss_print 10.110962867736816\n",
      "epoch:  0   step:  155   train loss:  9.663618087768555  val loss:  10.040166854858398  val L1 loss:  10.5263\n",
      "min_val_loss_print 10.040166854858398\n",
      "epoch:  0   step:  156   train loss:  7.92879581451416  val loss:  9.90578556060791  val L1 loss:  10.3891\n",
      "min_val_loss_print 9.90578556060791\n",
      "epoch:  0   step:  157   train loss:  6.993914604187012  val loss:  9.872238159179688  val L1 loss:  10.3613\n",
      "min_val_loss_print 9.872238159179688\n",
      "epoch:  0   step:  158   train loss:  11.8292236328125  val loss:  9.852339744567871  val L1 loss:  10.3466\n",
      "min_val_loss_print 9.852339744567871\n",
      "epoch:  0   step:  159   train loss:  9.154498100280762  val loss:  9.505754470825195  val L1 loss:  10.0057\n",
      "min_val_loss_print 9.505754470825195\n",
      "epoch:  0   step:  160   train loss:  8.276139259338379  val loss:  9.197476387023926  val L1 loss:  9.6908\n",
      "min_val_loss_print 9.197476387023926\n",
      "epoch:  0   step:  161   train loss:  9.061388969421387  val loss:  8.932221412658691  val L1 loss:  9.4153\n",
      "min_val_loss_print 8.932221412658691\n",
      "epoch:  0   step:  162   train loss:  10.199411392211914  val loss:  8.959269523620605  val L1 loss:  9.4353\n",
      "epoch:  0   step:  163   train loss:  8.00247573852539  val loss:  9.213879585266113  val L1 loss:  9.7052\n",
      "epoch:  0   step:  164   train loss:  10.723831176757812  val loss:  9.47817325592041  val L1 loss:  9.9559\n",
      "epoch:  0   step:  165   train loss:  7.700357437133789  val loss:  9.953996658325195  val L1 loss:  10.4479\n",
      "epoch:  0   step:  166   train loss:  7.593160152435303  val loss:  9.914543151855469  val L1 loss:  10.411\n",
      "epoch:  0   step:  167   train loss:  10.202041625976562  val loss:  9.374846458435059  val L1 loss:  9.8508\n",
      "epoch:  0   step:  168   train loss:  7.666847229003906  val loss:  9.204513549804688  val L1 loss:  9.682\n",
      "epoch:  0   step:  169   train loss:  9.444119453430176  val loss:  9.335419654846191  val L1 loss:  9.8148\n",
      "epoch:  0   step:  170   train loss:  9.561397552490234  val loss:  9.355700492858887  val L1 loss:  9.8398\n",
      "epoch:  0   step:  171   train loss:  8.10158920288086  val loss:  9.437065124511719  val L1 loss:  9.9368\n",
      "epoch:  0   step:  172   train loss:  8.946799278259277  val loss:  9.454289436340332  val L1 loss:  9.9543\n",
      "epoch:  0   step:  173   train loss:  7.290454387664795  val loss:  9.196799278259277  val L1 loss:  9.675\n",
      "epoch:  0   step:  174   train loss:  9.272124290466309  val loss:  9.137683868408203  val L1 loss:  9.6343\n",
      "epoch:  0   step:  175   train loss:  9.043723106384277  val loss:  9.060328483581543  val L1 loss:  9.5583\n",
      "epoch:  0   step:  176   train loss:  6.09290075302124  val loss:  9.049630165100098  val L1 loss:  9.5402\n",
      "epoch:  0   step:  177   train loss:  8.69023323059082  val loss:  8.958075523376465  val L1 loss:  9.4549\n",
      "epoch:  0   step:  178   train loss:  7.8909430503845215  val loss:  9.17997932434082  val L1 loss:  9.6556\n",
      "epoch:  0   step:  179   train loss:  6.109829425811768  val loss:  9.36249828338623  val L1 loss:  9.8546\n",
      "epoch:  0   step:  180   train loss:  7.1246137619018555  val loss:  9.590063095092773  val L1 loss:  10.073\n",
      "epoch:  0   step:  181   train loss:  7.884624004364014  val loss:  9.793636322021484  val L1 loss:  10.2757\n",
      "epoch:  0   step:  182   train loss:  9.087812423706055  val loss:  9.429489135742188  val L1 loss:  9.9183\n",
      "epoch:  0   step:  183   train loss:  10.100015640258789  val loss:  9.2626314163208  val L1 loss:  9.7459\n",
      "epoch:  0   step:  184   train loss:  8.142814636230469  val loss:  9.17121696472168  val L1 loss:  9.6462\n",
      "epoch:  0   step:  185   train loss:  8.195919036865234  val loss:  9.001776695251465  val L1 loss:  9.4926\n",
      "epoch:  0   step:  186   train loss:  7.711897850036621  val loss:  8.849597930908203  val L1 loss:  9.3466\n",
      "min_val_loss_print 8.849597930908203\n",
      "epoch:  0   step:  187   train loss:  7.22004508972168  val loss:  8.701348304748535  val L1 loss:  9.1876\n",
      "min_val_loss_print 8.701348304748535\n",
      "epoch:  0   step:  188   train loss:  7.139810562133789  val loss:  8.563224792480469  val L1 loss:  9.0415\n",
      "min_val_loss_print 8.563224792480469\n",
      "epoch:  0   step:  189   train loss:  6.132370948791504  val loss:  8.580208778381348  val L1 loss:  9.0757\n",
      "epoch:  0   step:  190   train loss:  6.854117393493652  val loss:  8.595375061035156  val L1 loss:  9.0898\n",
      "epoch:  0   step:  191   train loss:  6.28645133972168  val loss:  8.471428871154785  val L1 loss:  8.9651\n",
      "min_val_loss_print 8.471428871154785\n",
      "epoch:  0   step:  192   train loss:  6.000354766845703  val loss:  8.421430587768555  val L1 loss:  8.8976\n",
      "min_val_loss_print 8.421430587768555\n",
      "epoch:  0   step:  193   train loss:  10.763383865356445  val loss:  8.447470664978027  val L1 loss:  8.9317\n",
      "epoch:  0   step:  194   train loss:  8.79025936126709  val loss:  8.555313110351562  val L1 loss:  9.035\n",
      "epoch:  0   step:  195   train loss:  10.260677337646484  val loss:  8.648032188415527  val L1 loss:  9.1339\n",
      "epoch:  0   step:  196   train loss:  8.83808422088623  val loss:  8.631893157958984  val L1 loss:  9.1148\n",
      "epoch:  0   step:  197   train loss:  7.0989274978637695  val loss:  8.504288673400879  val L1 loss:  8.9849\n",
      "epoch:  0   step:  198   train loss:  7.067608833312988  val loss:  8.640429496765137  val L1 loss:  9.1308\n",
      "epoch:  0   step:  199   train loss:  7.678322792053223  val loss:  9.182475090026855  val L1 loss:  9.672\n",
      "epoch:  0   step:  200   train loss:  4.6171088218688965  val loss:  9.687724113464355  val L1 loss:  10.1858\n",
      "epoch:  0   step:  201   train loss:  7.752403259277344  val loss:  9.968955039978027  val L1 loss:  10.469\n",
      "epoch:  0   step:  202   train loss:  9.610875129699707  val loss:  10.28253173828125  val L1 loss:  10.7825\n",
      "epoch:  0   step:  203   train loss:  6.633601188659668  val loss:  10.488532066345215  val L1 loss:  10.9885\n",
      "epoch:  0   step:  204   train loss:  10.494179725646973  val loss:  10.602697372436523  val L1 loss:  11.1027\n",
      "epoch:  0   step:  205   train loss:  5.57904052734375  val loss:  10.483194351196289  val L1 loss:  10.9832\n",
      "epoch:  0   step:  206   train loss:  5.6573615074157715  val loss:  10.162038803100586  val L1 loss:  10.6588\n",
      "epoch:  0   step:  207   train loss:  6.388202667236328  val loss:  9.763702392578125  val L1 loss:  10.2606\n",
      "epoch:  0   step:  208   train loss:  5.8554911613464355  val loss:  9.437976837158203  val L1 loss:  9.9267\n",
      "epoch:  1   step:  0   train loss:  5.361891269683838  val loss:  9.070667266845703  val L1 loss:  9.5612\n",
      "epoch:  1   step:  1   train loss:  7.229774475097656  val loss:  8.841436386108398  val L1 loss:  9.3275\n",
      "epoch:  1   step:  2   train loss:  6.69837760925293  val loss:  8.652801513671875  val L1 loss:  9.1376\n",
      "epoch:  1   step:  3   train loss:  8.348353385925293  val loss:  8.54020881652832  val L1 loss:  9.0352\n",
      "epoch:  1   step:  4   train loss:  7.111132621765137  val loss:  8.510066032409668  val L1 loss:  8.9805\n",
      "epoch:  1   step:  5   train loss:  7.693840980529785  val loss:  8.576719284057617  val L1 loss:  9.0598\n",
      "epoch:  1   step:  6   train loss:  5.888932228088379  val loss:  8.484036445617676  val L1 loss:  8.9771\n",
      "epoch:  1   step:  7   train loss:  6.741106033325195  val loss:  8.327850341796875  val L1 loss:  8.8102\n",
      "min_val_loss_print 8.327850341796875\n",
      "epoch:  1   step:  8   train loss:  9.902444839477539  val loss:  8.210549354553223  val L1 loss:  8.6849\n",
      "min_val_loss_print 8.210549354553223\n",
      "epoch:  1   step:  9   train loss:  6.477206230163574  val loss:  8.084118843078613  val L1 loss:  8.5609\n",
      "min_val_loss_print 8.084118843078613\n",
      "epoch:  1   step:  10   train loss:  4.2659783363342285  val loss:  8.068117141723633  val L1 loss:  8.529\n",
      "min_val_loss_print 8.068117141723633\n",
      "epoch:  1   step:  11   train loss:  5.521027088165283  val loss:  8.283082962036133  val L1 loss:  8.7687\n",
      "epoch:  1   step:  12   train loss:  8.21224594116211  val loss:  7.944519996643066  val L1 loss:  8.4188\n",
      "min_val_loss_print 7.944519996643066\n",
      "epoch:  1   step:  13   train loss:  5.007217884063721  val loss:  7.703657627105713  val L1 loss:  8.1882\n",
      "min_val_loss_print 7.703657627105713\n",
      "epoch:  1   step:  14   train loss:  5.69826602935791  val loss:  7.690284252166748  val L1 loss:  8.1832\n",
      "min_val_loss_print 7.690284252166748\n",
      "epoch:  1   step:  15   train loss:  6.698448181152344  val loss:  7.699149131774902  val L1 loss:  8.1787\n",
      "epoch:  1   step:  16   train loss:  7.963253974914551  val loss:  7.826902866363525  val L1 loss:  8.3232\n",
      "epoch:  1   step:  17   train loss:  8.640621185302734  val loss:  8.396111488342285  val L1 loss:  8.8892\n",
      "epoch:  1   step:  18   train loss:  5.9331464767456055  val loss:  9.142117500305176  val L1 loss:  9.6358\n",
      "epoch:  1   step:  19   train loss:  8.246431350708008  val loss:  9.684900283813477  val L1 loss:  10.1841\n",
      "epoch:  1   step:  20   train loss:  6.916265964508057  val loss:  9.364897727966309  val L1 loss:  9.8556\n",
      "epoch:  1   step:  21   train loss:  6.319695949554443  val loss:  8.560601234436035  val L1 loss:  9.0508\n",
      "epoch:  1   step:  22   train loss:  4.897325038909912  val loss:  8.153937339782715  val L1 loss:  8.6404\n",
      "epoch:  1   step:  23   train loss:  7.836016654968262  val loss:  7.885435581207275  val L1 loss:  8.3766\n",
      "epoch:  1   step:  24   train loss:  6.443600654602051  val loss:  7.624311447143555  val L1 loss:  8.1082\n",
      "min_val_loss_print 7.624311447143555\n",
      "epoch:  1   step:  25   train loss:  5.959075927734375  val loss:  7.871375560760498  val L1 loss:  8.3598\n",
      "epoch:  1   step:  26   train loss:  6.946084499359131  val loss:  8.534165382385254  val L1 loss:  9.024\n",
      "epoch:  1   step:  27   train loss:  7.3229875564575195  val loss:  8.726546287536621  val L1 loss:  9.2221\n",
      "epoch:  1   step:  28   train loss:  8.741388320922852  val loss:  8.185124397277832  val L1 loss:  8.671\n",
      "epoch:  1   step:  29   train loss:  6.471612453460693  val loss:  7.603742599487305  val L1 loss:  8.1008\n",
      "min_val_loss_print 7.603742599487305\n",
      "epoch:  1   step:  30   train loss:  7.709352016448975  val loss:  7.580102920532227  val L1 loss:  8.0557\n",
      "min_val_loss_print 7.580102920532227\n",
      "epoch:  1   step:  31   train loss:  6.321938514709473  val loss:  8.03510570526123  val L1 loss:  8.5183\n",
      "epoch:  1   step:  32   train loss:  8.065592765808105  val loss:  8.32882022857666  val L1 loss:  8.8194\n",
      "epoch:  1   step:  33   train loss:  5.691280841827393  val loss:  8.063651084899902  val L1 loss:  8.5405\n",
      "epoch:  1   step:  34   train loss:  6.431578636169434  val loss:  7.672017574310303  val L1 loss:  8.1524\n",
      "epoch:  1   step:  35   train loss:  6.797711372375488  val loss:  7.796702861785889  val L1 loss:  8.2966\n",
      "epoch:  1   step:  36   train loss:  6.2469682693481445  val loss:  8.045536994934082  val L1 loss:  8.5128\n",
      "epoch:  1   step:  37   train loss:  6.389567852020264  val loss:  7.846616744995117  val L1 loss:  8.3348\n",
      "epoch:  1   step:  38   train loss:  9.253296852111816  val loss:  7.540132999420166  val L1 loss:  8.0227\n",
      "min_val_loss_print 7.540132999420166\n",
      "epoch:  1   step:  39   train loss:  7.212277412414551  val loss:  7.517034530639648  val L1 loss:  7.9992\n",
      "min_val_loss_print 7.517034530639648\n",
      "epoch:  1   step:  40   train loss:  5.485788822174072  val loss:  7.677133560180664  val L1 loss:  8.1589\n",
      "epoch:  1   step:  41   train loss:  7.305186748504639  val loss:  7.619287014007568  val L1 loss:  8.0963\n",
      "epoch:  1   step:  42   train loss:  7.500067710876465  val loss:  7.548691272735596  val L1 loss:  8.0193\n",
      "epoch:  1   step:  43   train loss:  5.174094200134277  val loss:  7.40478515625  val L1 loss:  7.8944\n",
      "min_val_loss_print 7.40478515625\n",
      "epoch:  1   step:  44   train loss:  5.309240818023682  val loss:  7.806313514709473  val L1 loss:  8.3001\n",
      "epoch:  1   step:  45   train loss:  8.787784576416016  val loss:  8.351631164550781  val L1 loss:  8.8489\n",
      "epoch:  1   step:  46   train loss:  7.747834205627441  val loss:  8.274760246276855  val L1 loss:  8.7672\n",
      "epoch:  1   step:  47   train loss:  7.503668785095215  val loss:  7.973951816558838  val L1 loss:  8.4689\n",
      "epoch:  1   step:  48   train loss:  6.060561180114746  val loss:  7.695764064788818  val L1 loss:  8.1858\n",
      "epoch:  1   step:  49   train loss:  5.316897392272949  val loss:  7.750864028930664  val L1 loss:  8.231\n",
      "epoch:  1   step:  50   train loss:  5.782435894012451  val loss:  7.845261096954346  val L1 loss:  8.3207\n",
      "epoch:  1   step:  51   train loss:  7.544733047485352  val loss:  7.795430660247803  val L1 loss:  8.2801\n",
      "epoch:  1   step:  52   train loss:  5.409523963928223  val loss:  7.73335599899292  val L1 loss:  8.2138\n",
      "epoch:  1   step:  53   train loss:  4.640820503234863  val loss:  7.761466026306152  val L1 loss:  8.2565\n",
      "epoch:  1   step:  54   train loss:  9.590771675109863  val loss:  7.67767333984375  val L1 loss:  8.172\n",
      "epoch:  1   step:  55   train loss:  4.911412715911865  val loss:  7.506280422210693  val L1 loss:  7.994\n",
      "epoch:  1   step:  56   train loss:  5.759819030761719  val loss:  7.504796504974365  val L1 loss:  7.9941\n",
      "epoch:  1   step:  57   train loss:  5.20194673538208  val loss:  7.607337474822998  val L1 loss:  8.0971\n",
      "epoch:  1   step:  58   train loss:  7.441459655761719  val loss:  7.828067302703857  val L1 loss:  8.3154\n",
      "epoch:  1   step:  59   train loss:  7.843346118927002  val loss:  7.957082271575928  val L1 loss:  8.4343\n",
      "epoch:  1   step:  60   train loss:  4.899847984313965  val loss:  7.82751989364624  val L1 loss:  8.3215\n",
      "epoch:  1   step:  61   train loss:  6.9638471603393555  val loss:  7.71274471282959  val L1 loss:  8.2021\n",
      "epoch:  1   step:  62   train loss:  4.560399055480957  val loss:  7.7226433753967285  val L1 loss:  8.2162\n",
      "epoch:  1   step:  63   train loss:  7.337078094482422  val loss:  7.84625244140625  val L1 loss:  8.3422\n",
      "epoch:  1   step:  64   train loss:  8.288028717041016  val loss:  7.666966915130615  val L1 loss:  8.1536\n",
      "epoch:  1   step:  65   train loss:  7.075205326080322  val loss:  7.431703090667725  val L1 loss:  7.9273\n",
      "epoch:  1   step:  66   train loss:  5.249051570892334  val loss:  7.385560035705566  val L1 loss:  7.86\n",
      "min_val_loss_print 7.385560035705566\n",
      "epoch:  1   step:  67   train loss:  6.019726753234863  val loss:  7.330442428588867  val L1 loss:  7.8045\n",
      "min_val_loss_print 7.330442428588867\n",
      "epoch:  1   step:  68   train loss:  6.8146562576293945  val loss:  7.278556823730469  val L1 loss:  7.7653\n",
      "min_val_loss_print 7.278556823730469\n",
      "epoch:  1   step:  69   train loss:  4.383464336395264  val loss:  7.362819194793701  val L1 loss:  7.8581\n",
      "epoch:  1   step:  70   train loss:  4.871695518493652  val loss:  7.518507480621338  val L1 loss:  8.0083\n",
      "epoch:  1   step:  71   train loss:  5.096929550170898  val loss:  7.501434326171875  val L1 loss:  7.9856\n",
      "epoch:  1   step:  72   train loss:  6.938625335693359  val loss:  7.784988880157471  val L1 loss:  8.2546\n",
      "epoch:  1   step:  73   train loss:  6.285428047180176  val loss:  7.982957363128662  val L1 loss:  8.4675\n",
      "epoch:  1   step:  74   train loss:  8.126575469970703  val loss:  7.809384822845459  val L1 loss:  8.2907\n",
      "epoch:  1   step:  75   train loss:  3.34613299369812  val loss:  7.570350646972656  val L1 loss:  8.0507\n",
      "epoch:  1   step:  76   train loss:  6.21391487121582  val loss:  7.300293922424316  val L1 loss:  7.7703\n",
      "epoch:  1   step:  77   train loss:  5.801508903503418  val loss:  7.1103949546813965  val L1 loss:  7.6092\n",
      "min_val_loss_print 7.1103949546813965\n",
      "epoch:  1   step:  78   train loss:  6.7378082275390625  val loss:  6.948572158813477  val L1 loss:  7.437\n",
      "min_val_loss_print 6.948572158813477\n",
      "epoch:  1   step:  79   train loss:  6.164301872253418  val loss:  6.978151798248291  val L1 loss:  7.4636\n",
      "epoch:  1   step:  80   train loss:  5.894281387329102  val loss:  6.930364608764648  val L1 loss:  7.416\n",
      "min_val_loss_print 6.930364608764648\n",
      "epoch:  1   step:  81   train loss:  5.590176582336426  val loss:  6.990441799163818  val L1 loss:  7.4802\n",
      "epoch:  1   step:  82   train loss:  7.933501243591309  val loss:  6.937749862670898  val L1 loss:  7.4284\n",
      "epoch:  1   step:  83   train loss:  7.353700637817383  val loss:  6.858489036560059  val L1 loss:  7.3548\n",
      "min_val_loss_print 6.858489036560059\n",
      "epoch:  1   step:  84   train loss:  5.3878679275512695  val loss:  6.816140651702881  val L1 loss:  7.2932\n",
      "min_val_loss_print 6.816140651702881\n",
      "epoch:  1   step:  85   train loss:  7.026869773864746  val loss:  6.851186275482178  val L1 loss:  7.3328\n",
      "epoch:  1   step:  86   train loss:  4.4328694343566895  val loss:  6.837220191955566  val L1 loss:  7.3173\n",
      "epoch:  1   step:  87   train loss:  5.739254474639893  val loss:  6.776941776275635  val L1 loss:  7.2546\n",
      "min_val_loss_print 6.776941776275635\n",
      "epoch:  1   step:  88   train loss:  5.200989723205566  val loss:  6.860368728637695  val L1 loss:  7.3468\n",
      "epoch:  1   step:  89   train loss:  9.147727012634277  val loss:  6.9098711013793945  val L1 loss:  7.402\n",
      "epoch:  1   step:  90   train loss:  6.061513423919678  val loss:  6.846879005432129  val L1 loss:  7.336\n",
      "epoch:  1   step:  91   train loss:  5.1069841384887695  val loss:  6.618218898773193  val L1 loss:  7.1062\n",
      "min_val_loss_print 6.618218898773193\n",
      "epoch:  1   step:  92   train loss:  6.5343732833862305  val loss:  6.584900856018066  val L1 loss:  7.0668\n",
      "min_val_loss_print 6.584900856018066\n",
      "epoch:  1   step:  93   train loss:  5.915543079376221  val loss:  6.573630332946777  val L1 loss:  7.061\n",
      "min_val_loss_print 6.573630332946777\n",
      "epoch:  1   step:  94   train loss:  7.222989082336426  val loss:  6.605094909667969  val L1 loss:  7.097\n",
      "epoch:  1   step:  95   train loss:  6.5135297775268555  val loss:  6.641600131988525  val L1 loss:  7.1271\n",
      "epoch:  1   step:  96   train loss:  5.529764175415039  val loss:  6.7112555503845215  val L1 loss:  7.2008\n",
      "epoch:  1   step:  97   train loss:  5.049521446228027  val loss:  6.838651657104492  val L1 loss:  7.3121\n",
      "epoch:  1   step:  98   train loss:  5.572911262512207  val loss:  6.894620895385742  val L1 loss:  7.373\n",
      "epoch:  1   step:  99   train loss:  4.844388961791992  val loss:  7.037150859832764  val L1 loss:  7.5178\n",
      "epoch:  1   step:  100   train loss:  6.036809921264648  val loss:  7.401060581207275  val L1 loss:  7.8817\n",
      "epoch:  1   step:  101   train loss:  4.062248706817627  val loss:  7.949367523193359  val L1 loss:  8.4423\n",
      "epoch:  1   step:  102   train loss:  5.170248985290527  val loss:  7.914242267608643  val L1 loss:  8.4017\n",
      "epoch:  1   step:  103   train loss:  5.384062767028809  val loss:  7.538312911987305  val L1 loss:  8.0097\n",
      "epoch:  1   step:  104   train loss:  5.428372383117676  val loss:  7.397544860839844  val L1 loss:  7.8659\n",
      "epoch:  1   step:  105   train loss:  7.256927490234375  val loss:  7.438782691955566  val L1 loss:  7.9224\n",
      "epoch:  1   step:  106   train loss:  6.267735004425049  val loss:  7.526095867156982  val L1 loss:  7.9999\n",
      "epoch:  1   step:  107   train loss:  9.596782684326172  val loss:  7.659583568572998  val L1 loss:  8.1389\n",
      "epoch:  1   step:  108   train loss:  6.573846817016602  val loss:  7.456691741943359  val L1 loss:  7.9443\n",
      "epoch:  1   step:  109   train loss:  7.163968086242676  val loss:  7.244376182556152  val L1 loss:  7.7134\n",
      "epoch:  1   step:  110   train loss:  3.1560792922973633  val loss:  7.077260494232178  val L1 loss:  7.5676\n",
      "epoch:  1   step:  111   train loss:  8.56627082824707  val loss:  7.021211624145508  val L1 loss:  7.4771\n",
      "epoch:  1   step:  112   train loss:  9.651874542236328  val loss:  7.482040882110596  val L1 loss:  7.97\n",
      "epoch:  1   step:  113   train loss:  4.978678226470947  val loss:  7.598313808441162  val L1 loss:  8.0815\n",
      "epoch:  1   step:  114   train loss:  4.745967864990234  val loss:  7.351052284240723  val L1 loss:  7.8338\n",
      "epoch:  1   step:  115   train loss:  5.09158182144165  val loss:  7.189645767211914  val L1 loss:  7.6714\n",
      "epoch:  1   step:  116   train loss:  5.957005023956299  val loss:  7.145094871520996  val L1 loss:  7.6391\n",
      "epoch:  1   step:  117   train loss:  7.272791862487793  val loss:  7.069809913635254  val L1 loss:  7.5544\n",
      "epoch:  1   step:  118   train loss:  4.649190902709961  val loss:  7.11151647567749  val L1 loss:  7.58\n",
      "epoch:  1   step:  119   train loss:  5.8029890060424805  val loss:  6.989711761474609  val L1 loss:  7.4578\n",
      "epoch:  1   step:  120   train loss:  6.160137176513672  val loss:  6.84505558013916  val L1 loss:  7.3403\n",
      "epoch:  1   step:  121   train loss:  5.653418064117432  val loss:  7.091063022613525  val L1 loss:  7.5766\n",
      "epoch:  1   step:  122   train loss:  4.0491108894348145  val loss:  7.267171382904053  val L1 loss:  7.7439\n",
      "epoch:  1   step:  123   train loss:  4.397968292236328  val loss:  7.218976974487305  val L1 loss:  7.7061\n",
      "epoch:  1   step:  124   train loss:  8.15894603729248  val loss:  6.815953254699707  val L1 loss:  7.3021\n",
      "epoch:  1   step:  125   train loss:  4.770057201385498  val loss:  6.679204940795898  val L1 loss:  7.1334\n",
      "epoch:  1   step:  126   train loss:  3.1484785079956055  val loss:  6.65598726272583  val L1 loss:  7.1049\n",
      "epoch:  1   step:  127   train loss:  5.839186668395996  val loss:  6.708982944488525  val L1 loss:  7.1893\n",
      "epoch:  1   step:  128   train loss:  8.672258377075195  val loss:  6.777163028717041  val L1 loss:  7.2625\n",
      "epoch:  1   step:  129   train loss:  8.430797576904297  val loss:  6.713344097137451  val L1 loss:  7.1688\n",
      "epoch:  1   step:  130   train loss:  3.7092573642730713  val loss:  6.968752384185791  val L1 loss:  7.454\n",
      "epoch:  1   step:  131   train loss:  6.199224948883057  val loss:  6.980417728424072  val L1 loss:  7.4662\n",
      "epoch:  1   step:  132   train loss:  6.915097713470459  val loss:  6.944255352020264  val L1 loss:  7.4276\n",
      "epoch:  1   step:  133   train loss:  4.536792755126953  val loss:  6.981306552886963  val L1 loss:  7.4642\n",
      "epoch:  1   step:  134   train loss:  5.382046222686768  val loss:  6.982017517089844  val L1 loss:  7.4664\n",
      "epoch:  1   step:  135   train loss:  4.439487934112549  val loss:  6.839794158935547  val L1 loss:  7.3237\n",
      "epoch:  1   step:  136   train loss:  4.484108924865723  val loss:  6.844694137573242  val L1 loss:  7.3342\n",
      "epoch:  1   step:  137   train loss:  5.490684509277344  val loss:  6.804727554321289  val L1 loss:  7.2751\n",
      "epoch:  1   step:  138   train loss:  5.473265171051025  val loss:  6.894296646118164  val L1 loss:  7.3601\n",
      "epoch:  1   step:  139   train loss:  5.837009429931641  val loss:  6.975623607635498  val L1 loss:  7.461\n",
      "epoch:  1   step:  140   train loss:  9.429429054260254  val loss:  7.094659328460693  val L1 loss:  7.5798\n",
      "epoch:  1   step:  141   train loss:  5.605547904968262  val loss:  7.245279312133789  val L1 loss:  7.7202\n",
      "epoch:  1   step:  142   train loss:  4.617132186889648  val loss:  7.51037073135376  val L1 loss:  7.9988\n",
      "epoch:  1   step:  143   train loss:  5.533699989318848  val loss:  7.747899532318115  val L1 loss:  8.2249\n",
      "epoch:  1   step:  144   train loss:  5.648787498474121  val loss:  7.518547058105469  val L1 loss:  7.993\n",
      "epoch:  1   step:  145   train loss:  4.831759452819824  val loss:  7.176976203918457  val L1 loss:  7.6477\n",
      "epoch:  1   step:  146   train loss:  6.352758407592773  val loss:  7.346662521362305  val L1 loss:  7.8358\n",
      "epoch:  1   step:  147   train loss:  7.426471710205078  val loss:  7.447830677032471  val L1 loss:  7.9409\n",
      "epoch:  1   step:  148   train loss:  7.870151996612549  val loss:  7.325351238250732  val L1 loss:  7.8201\n",
      "epoch:  1   step:  149   train loss:  6.496307373046875  val loss:  7.203718185424805  val L1 loss:  7.6935\n",
      "epoch:  1   step:  150   train loss:  7.742425918579102  val loss:  7.353627681732178  val L1 loss:  7.8266\n",
      "epoch:  1   step:  151   train loss:  4.89237117767334  val loss:  7.441728115081787  val L1 loss:  7.9135\n",
      "epoch:  1   step:  152   train loss:  7.703257083892822  val loss:  7.350264549255371  val L1 loss:  7.8334\n",
      "epoch:  1   step:  153   train loss:  7.258793354034424  val loss:  7.055047512054443  val L1 loss:  7.5451\n",
      "epoch:  1   step:  154   train loss:  5.0622453689575195  val loss:  7.066619873046875  val L1 loss:  7.557\n",
      "epoch:  1   step:  155   train loss:  6.520271301269531  val loss:  7.2229461669921875  val L1 loss:  7.7072\n",
      "epoch:  1   step:  156   train loss:  4.72169828414917  val loss:  7.327336311340332  val L1 loss:  7.8168\n",
      "epoch:  1   step:  157   train loss:  9.150297164916992  val loss:  7.037089824676514  val L1 loss:  7.5233\n",
      "epoch:  1   step:  158   train loss:  5.5615644454956055  val loss:  6.81052827835083  val L1 loss:  7.2996\n",
      "epoch:  1   step:  159   train loss:  5.147479057312012  val loss:  6.771194934844971  val L1 loss:  7.2607\n",
      "epoch:  1   step:  160   train loss:  5.05296516418457  val loss:  6.752265453338623  val L1 loss:  7.229\n",
      "epoch:  1   step:  161   train loss:  4.722848892211914  val loss:  6.7380218505859375  val L1 loss:  7.2172\n",
      "epoch:  1   step:  162   train loss:  11.110799789428711  val loss:  6.764284610748291  val L1 loss:  7.2471\n",
      "epoch:  1   step:  163   train loss:  6.427465438842773  val loss:  6.925757884979248  val L1 loss:  7.4171\n",
      "epoch:  1   step:  164   train loss:  5.289236068725586  val loss:  6.95005464553833  val L1 loss:  7.4345\n",
      "epoch:  1   step:  165   train loss:  4.504651069641113  val loss:  7.205411434173584  val L1 loss:  7.6899\n",
      "epoch:  1   step:  166   train loss:  4.021538257598877  val loss:  7.239477157592773  val L1 loss:  7.7299\n",
      "epoch:  1   step:  167   train loss:  6.465686798095703  val loss:  7.320962905883789  val L1 loss:  7.8177\n",
      "epoch:  1   step:  168   train loss:  7.519832611083984  val loss:  7.208002090454102  val L1 loss:  7.7038\n",
      "epoch:  1   step:  169   train loss:  4.80810022354126  val loss:  6.632070064544678  val L1 loss:  7.1098\n",
      "epoch:  1   step:  170   train loss:  8.193199157714844  val loss:  6.521759986877441  val L1 loss:  6.9906\n",
      "min_val_loss_print 6.521759986877441\n",
      "epoch:  1   step:  171   train loss:  4.942998886108398  val loss:  6.580611228942871  val L1 loss:  7.0412\n",
      "epoch:  1   step:  172   train loss:  5.504004001617432  val loss:  6.555977821350098  val L1 loss:  7.0282\n",
      "epoch:  1   step:  173   train loss:  8.055624961853027  val loss:  6.675450325012207  val L1 loss:  7.1575\n",
      "epoch:  1   step:  174   train loss:  4.123668670654297  val loss:  6.997190475463867  val L1 loss:  7.4921\n",
      "epoch:  1   step:  175   train loss:  6.941654205322266  val loss:  7.212099552154541  val L1 loss:  7.6856\n",
      "epoch:  1   step:  176   train loss:  5.031948566436768  val loss:  7.184273719787598  val L1 loss:  7.6661\n",
      "epoch:  1   step:  177   train loss:  6.4158172607421875  val loss:  6.9271240234375  val L1 loss:  7.4039\n",
      "epoch:  1   step:  178   train loss:  5.430638313293457  val loss:  6.905387878417969  val L1 loss:  7.3765\n",
      "epoch:  1   step:  179   train loss:  5.993130207061768  val loss:  6.89262580871582  val L1 loss:  7.3809\n",
      "epoch:  1   step:  180   train loss:  5.021126747131348  val loss:  6.836781978607178  val L1 loss:  7.3226\n",
      "epoch:  1   step:  181   train loss:  4.854705810546875  val loss:  6.760371685028076  val L1 loss:  7.2452\n",
      "epoch:  1   step:  182   train loss:  6.762600898742676  val loss:  6.748685359954834  val L1 loss:  7.219\n",
      "epoch:  1   step:  183   train loss:  5.554275989532471  val loss:  6.7666497230529785  val L1 loss:  7.2489\n",
      "epoch:  1   step:  184   train loss:  3.8747339248657227  val loss:  6.720478534698486  val L1 loss:  7.1978\n",
      "epoch:  1   step:  185   train loss:  5.589851379394531  val loss:  6.722131252288818  val L1 loss:  7.1888\n",
      "epoch:  1   step:  186   train loss:  6.0145111083984375  val loss:  6.777710914611816  val L1 loss:  7.2601\n",
      "epoch:  1   step:  187   train loss:  5.18964958190918  val loss:  6.88737678527832  val L1 loss:  7.3738\n",
      "epoch:  1   step:  188   train loss:  5.256500244140625  val loss:  7.011852741241455  val L1 loss:  7.4913\n",
      "epoch:  1   step:  189   train loss:  5.961134433746338  val loss:  6.862631797790527  val L1 loss:  7.3488\n",
      "epoch:  1   step:  190   train loss:  6.232413291931152  val loss:  6.780828475952148  val L1 loss:  7.2679\n",
      "epoch:  1   step:  191   train loss:  3.2611474990844727  val loss:  6.775406837463379  val L1 loss:  7.2625\n",
      "epoch:  1   step:  192   train loss:  4.735443592071533  val loss:  6.690768718719482  val L1 loss:  7.1709\n",
      "epoch:  1   step:  193   train loss:  7.81246280670166  val loss:  6.634429931640625  val L1 loss:  7.1157\n",
      "epoch:  1   step:  194   train loss:  6.681014060974121  val loss:  6.774759292602539  val L1 loss:  7.2488\n",
      "epoch:  1   step:  195   train loss:  6.4927449226379395  val loss:  6.648431301116943  val L1 loss:  7.1229\n",
      "epoch:  1   step:  196   train loss:  8.04291820526123  val loss:  6.453341484069824  val L1 loss:  6.9291\n",
      "min_val_loss_print 6.453341484069824\n",
      "epoch:  1   step:  197   train loss:  4.313401222229004  val loss:  6.476266860961914  val L1 loss:  6.9581\n",
      "epoch:  1   step:  198   train loss:  8.673410415649414  val loss:  6.813615798950195  val L1 loss:  7.3022\n",
      "epoch:  1   step:  199   train loss:  5.439995288848877  val loss:  7.35892915725708  val L1 loss:  7.8318\n",
      "epoch:  1   step:  200   train loss:  6.424862861633301  val loss:  7.403224468231201  val L1 loss:  7.8832\n",
      "epoch:  1   step:  201   train loss:  7.114572525024414  val loss:  7.001504421234131  val L1 loss:  7.4906\n",
      "epoch:  1   step:  202   train loss:  7.056042671203613  val loss:  6.72869348526001  val L1 loss:  7.2197\n",
      "epoch:  1   step:  203   train loss:  11.212433815002441  val loss:  6.521406173706055  val L1 loss:  6.9892\n",
      "epoch:  1   step:  204   train loss:  4.957279205322266  val loss:  6.5298895835876465  val L1 loss:  6.9944\n",
      "epoch:  1   step:  205   train loss:  6.769559860229492  val loss:  6.498863220214844  val L1 loss:  6.964\n",
      "epoch:  1   step:  206   train loss:  6.194772720336914  val loss:  6.527956485748291  val L1 loss:  7.0143\n",
      "epoch:  1   step:  207   train loss:  8.126333236694336  val loss:  6.525203704833984  val L1 loss:  7.0007\n",
      "epoch:  1   step:  208   train loss:  6.037858486175537  val loss:  6.667637348175049  val L1 loss:  7.1456\n",
      "epoch:  2   step:  0   train loss:  6.632431507110596  val loss:  6.963991641998291  val L1 loss:  7.439\n",
      "epoch:  2   step:  1   train loss:  6.290595054626465  val loss:  7.12077522277832  val L1 loss:  7.5915\n",
      "epoch:  2   step:  2   train loss:  6.4281182289123535  val loss:  6.814385890960693  val L1 loss:  7.2936\n",
      "epoch:  2   step:  3   train loss:  5.353451251983643  val loss:  6.561027526855469  val L1 loss:  7.0342\n",
      "epoch:  2   step:  4   train loss:  6.443024158477783  val loss:  6.747403621673584  val L1 loss:  7.2265\n",
      "epoch:  2   step:  5   train loss:  6.193863868713379  val loss:  7.239401817321777  val L1 loss:  7.7303\n",
      "epoch:  2   step:  6   train loss:  4.728952884674072  val loss:  7.451664924621582  val L1 loss:  7.9335\n",
      "epoch:  2   step:  7   train loss:  5.189349174499512  val loss:  7.250140190124512  val L1 loss:  7.7411\n",
      "epoch:  2   step:  8   train loss:  4.465240001678467  val loss:  6.805542469024658  val L1 loss:  7.2795\n",
      "epoch:  2   step:  9   train loss:  4.089463233947754  val loss:  6.842479228973389  val L1 loss:  7.3306\n",
      "epoch:  2   step:  10   train loss:  6.724567413330078  val loss:  6.707980632781982  val L1 loss:  7.1771\n",
      "epoch:  2   step:  11   train loss:  6.043424129486084  val loss:  6.690301418304443  val L1 loss:  7.1794\n",
      "epoch:  2   step:  12   train loss:  5.950654983520508  val loss:  6.616757869720459  val L1 loss:  7.1011\n",
      "epoch:  2   step:  13   train loss:  6.80517578125  val loss:  6.5887885093688965  val L1 loss:  7.0762\n",
      "epoch:  2   step:  14   train loss:  3.3968088626861572  val loss:  6.5336594581604  val L1 loss:  7.0174\n",
      "epoch:  2   step:  15   train loss:  5.189815044403076  val loss:  6.497243404388428  val L1 loss:  6.9885\n",
      "epoch:  2   step:  16   train loss:  4.903468132019043  val loss:  6.446707725524902  val L1 loss:  6.9431\n",
      "min_val_loss_print 6.446707725524902\n",
      "epoch:  2   step:  17   train loss:  4.976957321166992  val loss:  6.454413890838623  val L1 loss:  6.9541\n",
      "epoch:  2   step:  18   train loss:  5.9078874588012695  val loss:  6.47319221496582  val L1 loss:  6.9726\n",
      "epoch:  2   step:  19   train loss:  9.362018585205078  val loss:  6.598857402801514  val L1 loss:  7.0708\n",
      "epoch:  2   step:  20   train loss:  8.100943565368652  val loss:  6.729540824890137  val L1 loss:  7.2024\n",
      "epoch:  2   step:  21   train loss:  5.242298126220703  val loss:  6.658347129821777  val L1 loss:  7.1326\n",
      "epoch:  2   step:  22   train loss:  5.253064155578613  val loss:  6.6177978515625  val L1 loss:  7.0939\n",
      "epoch:  2   step:  23   train loss:  5.569483757019043  val loss:  6.602273464202881  val L1 loss:  7.0973\n",
      "epoch:  2   step:  24   train loss:  6.212988376617432  val loss:  6.695163249969482  val L1 loss:  7.1779\n",
      "epoch:  2   step:  25   train loss:  4.275993347167969  val loss:  6.779948711395264  val L1 loss:  7.2655\n",
      "epoch:  2   step:  26   train loss:  6.70838737487793  val loss:  6.898333549499512  val L1 loss:  7.3642\n",
      "epoch:  2   step:  27   train loss:  6.499784469604492  val loss:  7.314159393310547  val L1 loss:  7.8057\n",
      "epoch:  2   step:  28   train loss:  3.7149949073791504  val loss:  7.452767848968506  val L1 loss:  7.9402\n",
      "epoch:  2   step:  29   train loss:  5.495506763458252  val loss:  7.544943809509277  val L1 loss:  8.0339\n",
      "epoch:  2   step:  30   train loss:  4.974771022796631  val loss:  7.28820276260376  val L1 loss:  7.7613\n",
      "epoch:  2   step:  31   train loss:  4.193302154541016  val loss:  7.036161422729492  val L1 loss:  7.5218\n",
      "epoch:  2   step:  32   train loss:  6.210268974304199  val loss:  6.961083889007568  val L1 loss:  7.4386\n",
      "epoch:  2   step:  33   train loss:  4.851105690002441  val loss:  6.831940650939941  val L1 loss:  7.3076\n",
      "epoch:  2   step:  34   train loss:  6.128328323364258  val loss:  6.828239440917969  val L1 loss:  7.3105\n",
      "epoch:  2   step:  35   train loss:  5.131513595581055  val loss:  6.910329818725586  val L1 loss:  7.4098\n",
      "epoch:  2   step:  36   train loss:  7.285122394561768  val loss:  6.883702754974365  val L1 loss:  7.3771\n",
      "epoch:  2   step:  37   train loss:  6.594144821166992  val loss:  6.678918361663818  val L1 loss:  7.16\n",
      "epoch:  2   step:  38   train loss:  6.917754173278809  val loss:  6.683293342590332  val L1 loss:  7.1598\n",
      "epoch:  2   step:  39   train loss:  4.922649383544922  val loss:  6.818469524383545  val L1 loss:  7.295\n",
      "epoch:  2   step:  40   train loss:  4.9465179443359375  val loss:  6.636026382446289  val L1 loss:  7.1086\n",
      "epoch:  2   step:  41   train loss:  4.817732810974121  val loss:  6.425915241241455  val L1 loss:  6.8885\n",
      "min_val_loss_print 6.425915241241455\n",
      "epoch:  2   step:  42   train loss:  8.439858436584473  val loss:  6.371124267578125  val L1 loss:  6.8492\n",
      "min_val_loss_print 6.371124267578125\n",
      "epoch:  2   step:  43   train loss:  8.913222312927246  val loss:  6.537554740905762  val L1 loss:  7.0298\n",
      "epoch:  2   step:  44   train loss:  4.852190017700195  val loss:  6.706960201263428  val L1 loss:  7.2\n",
      "epoch:  2   step:  45   train loss:  4.786551475524902  val loss:  6.607300758361816  val L1 loss:  7.0833\n",
      "epoch:  2   step:  46   train loss:  5.674808025360107  val loss:  6.616598129272461  val L1 loss:  7.1121\n",
      "epoch:  2   step:  47   train loss:  6.493655204772949  val loss:  6.65012788772583  val L1 loss:  7.1165\n",
      "epoch:  2   step:  48   train loss:  6.9940104484558105  val loss:  6.637275218963623  val L1 loss:  7.0973\n",
      "epoch:  2   step:  49   train loss:  7.14811372756958  val loss:  6.5994462966918945  val L1 loss:  7.0643\n",
      "epoch:  2   step:  50   train loss:  5.06640625  val loss:  6.585237979888916  val L1 loss:  7.073\n",
      "epoch:  2   step:  51   train loss:  6.709900379180908  val loss:  6.60072660446167  val L1 loss:  7.088\n",
      "epoch:  2   step:  52   train loss:  5.26103401184082  val loss:  6.634036540985107  val L1 loss:  7.1194\n",
      "epoch:  2   step:  53   train loss:  4.88252067565918  val loss:  6.795490741729736  val L1 loss:  7.2856\n",
      "epoch:  2   step:  54   train loss:  5.8411455154418945  val loss:  6.5649590492248535  val L1 loss:  7.0363\n",
      "epoch:  2   step:  55   train loss:  8.053787231445312  val loss:  6.58533239364624  val L1 loss:  7.0659\n",
      "epoch:  2   step:  56   train loss:  3.428626537322998  val loss:  7.171745300292969  val L1 loss:  7.6446\n",
      "epoch:  2   step:  57   train loss:  6.322414398193359  val loss:  7.382808685302734  val L1 loss:  7.8598\n",
      "epoch:  2   step:  58   train loss:  7.24840784072876  val loss:  6.956713676452637  val L1 loss:  7.4264\n",
      "epoch:  2   step:  59   train loss:  4.900582313537598  val loss:  6.487898826599121  val L1 loss:  6.9511\n",
      "epoch:  2   step:  60   train loss:  8.28062629699707  val loss:  6.426976203918457  val L1 loss:  6.8976\n",
      "epoch:  2   step:  61   train loss:  7.084477424621582  val loss:  7.081757545471191  val L1 loss:  7.5758\n",
      "epoch:  2   step:  62   train loss:  8.14543342590332  val loss:  7.42501163482666  val L1 loss:  7.9121\n",
      "epoch:  2   step:  63   train loss:  5.465079307556152  val loss:  7.457401752471924  val L1 loss:  7.9403\n",
      "epoch:  2   step:  64   train loss:  6.747864723205566  val loss:  7.284156799316406  val L1 loss:  7.7734\n",
      "epoch:  2   step:  65   train loss:  4.924336910247803  val loss:  6.920950889587402  val L1 loss:  7.4098\n",
      "epoch:  2   step:  66   train loss:  6.051183223724365  val loss:  6.853431224822998  val L1 loss:  7.3323\n",
      "epoch:  2   step:  67   train loss:  5.0853471755981445  val loss:  7.16123628616333  val L1 loss:  7.6317\n",
      "epoch:  2   step:  68   train loss:  6.479280948638916  val loss:  7.442821979522705  val L1 loss:  7.9347\n",
      "epoch:  2   step:  69   train loss:  6.764993667602539  val loss:  7.388744831085205  val L1 loss:  7.8804\n",
      "epoch:  2   step:  70   train loss:  6.521257400512695  val loss:  6.8998613357543945  val L1 loss:  7.3839\n",
      "epoch:  2   step:  71   train loss:  9.851583480834961  val loss:  7.000203609466553  val L1 loss:  7.4808\n",
      "epoch:  2   step:  72   train loss:  4.079599380493164  val loss:  7.397883892059326  val L1 loss:  7.8801\n",
      "epoch:  2   step:  73   train loss:  4.4717631340026855  val loss:  7.901120662689209  val L1 loss:  8.3935\n",
      "epoch:  2   step:  74   train loss:  7.833376884460449  val loss:  7.942126750946045  val L1 loss:  8.4409\n",
      "epoch:  2   step:  75   train loss:  6.404956340789795  val loss:  7.7292985916137695  val L1 loss:  8.2232\n",
      "epoch:  2   step:  76   train loss:  7.860264301300049  val loss:  7.26406717300415  val L1 loss:  7.7396\n",
      "epoch:  2   step:  77   train loss:  4.15200662612915  val loss:  7.110962867736816  val L1 loss:  7.5911\n",
      "epoch:  2   step:  78   train loss:  5.470781326293945  val loss:  7.402689456939697  val L1 loss:  7.8848\n",
      "epoch:  2   step:  79   train loss:  6.403812885284424  val loss:  7.86525821685791  val L1 loss:  8.3482\n",
      "epoch:  2   step:  80   train loss:  10.086389541625977  val loss:  7.857138633728027  val L1 loss:  8.3381\n",
      "epoch:  2   step:  81   train loss:  6.850689888000488  val loss:  7.356253623962402  val L1 loss:  7.842\n",
      "epoch:  2   step:  82   train loss:  6.5854997634887695  val loss:  7.0926947593688965  val L1 loss:  7.5742\n",
      "epoch:  2   step:  83   train loss:  5.909287452697754  val loss:  7.208601474761963  val L1 loss:  7.689\n",
      "epoch:  2   step:  84   train loss:  6.727729320526123  val loss:  7.344053745269775  val L1 loss:  7.8272\n",
      "epoch:  2   step:  85   train loss:  4.846897602081299  val loss:  7.112412452697754  val L1 loss:  7.5971\n",
      "epoch:  2   step:  86   train loss:  4.061631679534912  val loss:  6.959965229034424  val L1 loss:  7.4411\n",
      "epoch:  2   step:  87   train loss:  4.194624423980713  val loss:  6.920800685882568  val L1 loss:  7.406\n",
      "epoch:  2   step:  88   train loss:  5.9428911209106445  val loss:  7.074687480926514  val L1 loss:  7.548\n",
      "epoch:  2   step:  89   train loss:  6.616028785705566  val loss:  7.192235469818115  val L1 loss:  7.6557\n",
      "epoch:  2   step:  90   train loss:  4.221693992614746  val loss:  7.116722106933594  val L1 loss:  7.5993\n",
      "epoch:  2   step:  91   train loss:  4.112409591674805  val loss:  6.917647361755371  val L1 loss:  7.3855\n",
      "epoch:  2   step:  92   train loss:  4.437301158905029  val loss:  6.868476390838623  val L1 loss:  7.3651\n",
      "epoch:  2   step:  93   train loss:  4.686857223510742  val loss:  6.997453689575195  val L1 loss:  7.4834\n",
      "epoch:  2   step:  94   train loss:  7.339475631713867  val loss:  7.07111930847168  val L1 loss:  7.5582\n",
      "epoch:  2   step:  95   train loss:  6.883853912353516  val loss:  7.040257453918457  val L1 loss:  7.522\n",
      "epoch:  2   step:  96   train loss:  7.72432804107666  val loss:  6.802041530609131  val L1 loss:  7.284\n",
      "epoch:  2   step:  97   train loss:  3.905447006225586  val loss:  6.906919002532959  val L1 loss:  7.3935\n",
      "epoch:  2   step:  98   train loss:  6.7827019691467285  val loss:  7.031050682067871  val L1 loss:  7.5114\n",
      "epoch:  2   step:  99   train loss:  3.7109737396240234  val loss:  7.050679683685303  val L1 loss:  7.5323\n",
      "epoch:  2   step:  100   train loss:  5.947692394256592  val loss:  6.981696605682373  val L1 loss:  7.4645\n",
      "epoch:  2   step:  101   train loss:  4.808351516723633  val loss:  6.743748664855957  val L1 loss:  7.2089\n",
      "epoch:  2   step:  102   train loss:  5.586767673492432  val loss:  6.878261089324951  val L1 loss:  7.3539\n",
      "epoch:  2   step:  103   train loss:  5.753836631774902  val loss:  7.221871852874756  val L1 loss:  7.7046\n",
      "epoch:  2   step:  104   train loss:  7.461619853973389  val loss:  7.1228485107421875  val L1 loss:  7.6112\n",
      "epoch:  2   step:  105   train loss:  6.518345832824707  val loss:  6.6220879554748535  val L1 loss:  7.0971\n",
      "epoch:  2   step:  106   train loss:  5.728231430053711  val loss:  6.596487045288086  val L1 loss:  7.0677\n",
      "epoch:  2   step:  107   train loss:  4.627895355224609  val loss:  6.847144603729248  val L1 loss:  7.3171\n",
      "epoch:  2   step:  108   train loss:  5.266192436218262  val loss:  7.016421794891357  val L1 loss:  7.4944\n",
      "epoch:  2   step:  109   train loss:  4.27625846862793  val loss:  7.056149959564209  val L1 loss:  7.5471\n",
      "epoch:  2   step:  110   train loss:  5.742201805114746  val loss:  6.675381183624268  val L1 loss:  7.1664\n",
      "epoch:  2   step:  111   train loss:  6.428109645843506  val loss:  6.396960258483887  val L1 loss:  6.8561\n",
      "epoch:  2   step:  112   train loss:  5.839012145996094  val loss:  6.560369491577148  val L1 loss:  7.0417\n",
      "epoch:  2   step:  113   train loss:  5.121872901916504  val loss:  7.01984167098999  val L1 loss:  7.5\n",
      "epoch:  2   step:  114   train loss:  7.537907600402832  val loss:  7.04798698425293  val L1 loss:  7.5329\n",
      "epoch:  2   step:  115   train loss:  6.6171793937683105  val loss:  6.734000205993652  val L1 loss:  7.2203\n",
      "epoch:  2   step:  116   train loss:  4.172885894775391  val loss:  6.502114772796631  val L1 loss:  6.9759\n",
      "epoch:  2   step:  117   train loss:  5.499351501464844  val loss:  6.931248664855957  val L1 loss:  7.4157\n",
      "epoch:  2   step:  118   train loss:  3.768187999725342  val loss:  7.04519510269165  val L1 loss:  7.5344\n",
      "epoch:  2   step:  119   train loss:  9.035348892211914  val loss:  6.698413372039795  val L1 loss:  7.1645\n",
      "epoch:  2   step:  120   train loss:  4.8000593185424805  val loss:  6.5957159996032715  val L1 loss:  7.0567\n",
      "epoch:  2   step:  121   train loss:  4.5850653648376465  val loss:  6.879148006439209  val L1 loss:  7.3674\n",
      "epoch:  2   step:  122   train loss:  6.244776248931885  val loss:  7.039372444152832  val L1 loss:  7.5144\n",
      "epoch:  2   step:  123   train loss:  8.151962280273438  val loss:  7.170754432678223  val L1 loss:  7.6539\n",
      "epoch:  2   step:  124   train loss:  4.919922828674316  val loss:  6.965079307556152  val L1 loss:  7.4387\n",
      "epoch:  2   step:  125   train loss:  8.574936866760254  val loss:  6.781189918518066  val L1 loss:  7.2664\n",
      "epoch:  2   step:  126   train loss:  7.4972052574157715  val loss:  6.612236022949219  val L1 loss:  7.0928\n",
      "epoch:  2   step:  127   train loss:  4.566989898681641  val loss:  6.530784606933594  val L1 loss:  7.0247\n",
      "epoch:  2   step:  128   train loss:  5.052489757537842  val loss:  6.513489246368408  val L1 loss:  6.9967\n",
      "epoch:  2   step:  129   train loss:  6.365357398986816  val loss:  6.515935897827148  val L1 loss:  6.9801\n",
      "epoch:  2   step:  130   train loss:  7.4290266036987305  val loss:  6.511021614074707  val L1 loss:  6.9865\n",
      "epoch:  2   step:  131   train loss:  7.346789360046387  val loss:  6.557244300842285  val L1 loss:  7.0444\n",
      "epoch:  2   step:  132   train loss:  7.053430557250977  val loss:  6.531808853149414  val L1 loss:  7.0119\n",
      "epoch:  2   step:  133   train loss:  5.813857555389404  val loss:  6.548818111419678  val L1 loss:  7.0326\n",
      "epoch:  2   step:  134   train loss:  4.153816223144531  val loss:  6.638094425201416  val L1 loss:  7.1086\n",
      "epoch:  2   step:  135   train loss:  6.249307632446289  val loss:  6.671802997589111  val L1 loss:  7.1547\n",
      "epoch:  2   step:  136   train loss:  5.6222944259643555  val loss:  6.58927059173584  val L1 loss:  7.0699\n",
      "epoch:  2   step:  137   train loss:  3.895744800567627  val loss:  6.658605098724365  val L1 loss:  7.1379\n",
      "epoch:  2   step:  138   train loss:  7.871817588806152  val loss:  6.800417423248291  val L1 loss:  7.2746\n",
      "epoch:  2   step:  139   train loss:  8.660661697387695  val loss:  6.754656791687012  val L1 loss:  7.2205\n",
      "epoch:  2   step:  140   train loss:  6.0509514808654785  val loss:  6.597414493560791  val L1 loss:  7.0806\n",
      "epoch:  2   step:  141   train loss:  6.384417533874512  val loss:  6.552120208740234  val L1 loss:  7.0308\n",
      "epoch:  2   step:  142   train loss:  4.116429328918457  val loss:  6.7182841300964355  val L1 loss:  7.2092\n",
      "epoch:  2   step:  143   train loss:  6.813626766204834  val loss:  6.8311991691589355  val L1 loss:  7.3234\n",
      "epoch:  2   step:  144   train loss:  5.372980117797852  val loss:  6.811425685882568  val L1 loss:  7.3003\n",
      "epoch:  2   step:  145   train loss:  7.671618938446045  val loss:  6.791138648986816  val L1 loss:  7.2703\n",
      "epoch:  2   step:  146   train loss:  5.624405384063721  val loss:  6.653509140014648  val L1 loss:  7.1399\n",
      "epoch:  2   step:  147   train loss:  4.834235668182373  val loss:  6.6100616455078125  val L1 loss:  7.097\n",
      "epoch:  2   step:  148   train loss:  5.363521099090576  val loss:  6.604508399963379  val L1 loss:  7.09\n",
      "epoch:  2   step:  149   train loss:  3.4195713996887207  val loss:  6.6130290031433105  val L1 loss:  7.0868\n",
      "epoch:  2   step:  150   train loss:  5.137339115142822  val loss:  6.64917516708374  val L1 loss:  7.117\n",
      "epoch:  2   step:  151   train loss:  5.178750514984131  val loss:  6.590800762176514  val L1 loss:  7.069\n",
      "epoch:  2   step:  152   train loss:  5.569157600402832  val loss:  6.526522636413574  val L1 loss:  7.002\n",
      "epoch:  2   step:  153   train loss:  6.2610859870910645  val loss:  6.551354885101318  val L1 loss:  7.0294\n",
      "epoch:  2   step:  154   train loss:  4.792686462402344  val loss:  6.450509071350098  val L1 loss:  6.9304\n",
      "epoch:  2   step:  155   train loss:  6.615710735321045  val loss:  6.451252460479736  val L1 loss:  6.9361\n",
      "epoch:  2   step:  156   train loss:  4.396632671356201  val loss:  6.4470977783203125  val L1 loss:  6.9279\n",
      "epoch:  2   step:  157   train loss:  5.663077354431152  val loss:  6.440114498138428  val L1 loss:  6.9216\n",
      "epoch:  2   step:  158   train loss:  7.065427303314209  val loss:  6.519631385803223  val L1 loss:  7.0008\n",
      "epoch:  2   step:  159   train loss:  4.337584972381592  val loss:  6.685575008392334  val L1 loss:  7.1712\n",
      "epoch:  2   step:  160   train loss:  5.394397735595703  val loss:  6.964602470397949  val L1 loss:  7.452\n",
      "epoch:  2   step:  161   train loss:  5.959705352783203  val loss:  6.980573654174805  val L1 loss:  7.461\n",
      "epoch:  2   step:  162   train loss:  6.939180850982666  val loss:  6.801873683929443  val L1 loss:  7.2782\n",
      "epoch:  2   step:  163   train loss:  4.912314414978027  val loss:  6.646486759185791  val L1 loss:  7.1315\n",
      "epoch:  2   step:  164   train loss:  5.2845587730407715  val loss:  6.611487865447998  val L1 loss:  7.0935\n",
      "epoch:  2   step:  165   train loss:  7.625814437866211  val loss:  6.59245491027832  val L1 loss:  7.0755\n",
      "epoch:  2   step:  166   train loss:  3.2681846618652344  val loss:  6.595263481140137  val L1 loss:  7.0713\n",
      "epoch:  2   step:  167   train loss:  4.121494293212891  val loss:  6.554912090301514  val L1 loss:  7.0353\n",
      "epoch:  2   step:  168   train loss:  4.632473945617676  val loss:  6.495206356048584  val L1 loss:  6.973\n",
      "epoch:  2   step:  169   train loss:  3.796264410018921  val loss:  6.455783367156982  val L1 loss:  6.93\n",
      "epoch:  2   step:  170   train loss:  4.242783546447754  val loss:  6.509462833404541  val L1 loss:  6.9719\n",
      "epoch:  2   step:  171   train loss:  7.997089385986328  val loss:  6.592241287231445  val L1 loss:  7.07\n",
      "epoch:  2   step:  172   train loss:  4.6604695320129395  val loss:  6.524263858795166  val L1 loss:  6.9848\n",
      "epoch:  2   step:  173   train loss:  4.496529579162598  val loss:  6.469309329986572  val L1 loss:  6.9536\n",
      "epoch:  2   step:  174   train loss:  8.340957641601562  val loss:  6.47809362411499  val L1 loss:  6.9485\n",
      "epoch:  2   step:  175   train loss:  6.074493885040283  val loss:  6.512324810028076  val L1 loss:  6.9893\n",
      "epoch:  2   step:  176   train loss:  7.758050441741943  val loss:  6.5735602378845215  val L1 loss:  7.0498\n",
      "epoch:  2   step:  177   train loss:  4.159003257751465  val loss:  7.05864143371582  val L1 loss:  7.5433\n",
      "epoch:  2   step:  178   train loss:  5.022834777832031  val loss:  7.136108875274658  val L1 loss:  7.6221\n",
      "epoch:  2   step:  179   train loss:  4.725733757019043  val loss:  6.837527751922607  val L1 loss:  7.328\n",
      "epoch:  2   step:  180   train loss:  6.95203161239624  val loss:  6.62657356262207  val L1 loss:  7.1087\n",
      "epoch:  2   step:  181   train loss:  6.758871078491211  val loss:  6.382077693939209  val L1 loss:  6.8479\n",
      "epoch:  2   step:  182   train loss:  3.376263380050659  val loss:  6.317629337310791  val L1 loss:  6.8025\n",
      "min_val_loss_print 6.317629337310791\n",
      "epoch:  2   step:  183   train loss:  3.3332719802856445  val loss:  6.279033660888672  val L1 loss:  6.7515\n",
      "min_val_loss_print 6.279033660888672\n",
      "epoch:  2   step:  184   train loss:  5.004499912261963  val loss:  6.316843032836914  val L1 loss:  6.7923\n",
      "epoch:  2   step:  185   train loss:  5.8557868003845215  val loss:  6.361172199249268  val L1 loss:  6.8147\n",
      "epoch:  2   step:  186   train loss:  5.222087860107422  val loss:  6.4170966148376465  val L1 loss:  6.8924\n",
      "epoch:  2   step:  187   train loss:  8.497835159301758  val loss:  6.416678428649902  val L1 loss:  6.8883\n",
      "epoch:  2   step:  188   train loss:  5.420820713043213  val loss:  6.3959879875183105  val L1 loss:  6.8889\n",
      "epoch:  2   step:  189   train loss:  3.8996429443359375  val loss:  6.405681133270264  val L1 loss:  6.8951\n",
      "epoch:  2   step:  190   train loss:  4.022474765777588  val loss:  6.469666481018066  val L1 loss:  6.9526\n",
      "epoch:  2   step:  191   train loss:  5.793793678283691  val loss:  6.517596244812012  val L1 loss:  6.9958\n",
      "epoch:  2   step:  192   train loss:  6.529641151428223  val loss:  6.466048240661621  val L1 loss:  6.9511\n",
      "epoch:  2   step:  193   train loss:  6.304622650146484  val loss:  6.4993414878845215  val L1 loss:  6.9765\n",
      "epoch:  2   step:  194   train loss:  4.290361404418945  val loss:  6.503825664520264  val L1 loss:  6.9749\n",
      "epoch:  2   step:  195   train loss:  5.5110039710998535  val loss:  6.54594612121582  val L1 loss:  7.0284\n",
      "epoch:  2   step:  196   train loss:  4.18496036529541  val loss:  6.675158500671387  val L1 loss:  7.1718\n",
      "epoch:  2   step:  197   train loss:  6.544891834259033  val loss:  6.741485595703125  val L1 loss:  7.2257\n",
      "epoch:  2   step:  198   train loss:  5.712128639221191  val loss:  6.779857158660889  val L1 loss:  7.2564\n",
      "epoch:  2   step:  199   train loss:  3.889601230621338  val loss:  6.705422878265381  val L1 loss:  7.1809\n",
      "epoch:  2   step:  200   train loss:  9.626949310302734  val loss:  6.590981960296631  val L1 loss:  7.0831\n",
      "epoch:  2   step:  201   train loss:  5.135146141052246  val loss:  6.596245765686035  val L1 loss:  7.0802\n",
      "epoch:  2   step:  202   train loss:  5.726495265960693  val loss:  6.58045768737793  val L1 loss:  7.0658\n",
      "epoch:  2   step:  203   train loss:  4.427216529846191  val loss:  6.587461948394775  val L1 loss:  7.074\n",
      "epoch:  2   step:  204   train loss:  4.324038505554199  val loss:  6.527066707611084  val L1 loss:  7.0157\n",
      "epoch:  2   step:  205   train loss:  7.421825408935547  val loss:  6.506041526794434  val L1 loss:  6.9884\n",
      "epoch:  2   step:  206   train loss:  3.246854543685913  val loss:  6.459782123565674  val L1 loss:  6.9393\n",
      "epoch:  2   step:  207   train loss:  4.385181903839111  val loss:  6.447017669677734  val L1 loss:  6.9263\n",
      "epoch:  2   step:  208   train loss:  4.042151927947998  val loss:  6.4152655601501465  val L1 loss:  6.9041\n",
      "epoch:  3   step:  0   train loss:  5.285494804382324  val loss:  6.386017799377441  val L1 loss:  6.8767\n",
      "epoch:  3   step:  1   train loss:  5.908624649047852  val loss:  6.387964248657227  val L1 loss:  6.8781\n",
      "epoch:  3   step:  2   train loss:  7.8582258224487305  val loss:  6.382881164550781  val L1 loss:  6.8703\n",
      "epoch:  3   step:  3   train loss:  4.060439586639404  val loss:  6.40273904800415  val L1 loss:  6.8858\n",
      "epoch:  3   step:  4   train loss:  5.5491814613342285  val loss:  6.468080997467041  val L1 loss:  6.948\n",
      "epoch:  3   step:  5   train loss:  2.855870246887207  val loss:  6.46428918838501  val L1 loss:  6.9428\n",
      "epoch:  3   step:  6   train loss:  5.1402130126953125  val loss:  6.543182373046875  val L1 loss:  7.0246\n",
      "epoch:  3   step:  7   train loss:  5.71721076965332  val loss:  6.554407596588135  val L1 loss:  7.0404\n",
      "epoch:  3   step:  8   train loss:  2.9055023193359375  val loss:  6.719443321228027  val L1 loss:  7.1862\n",
      "epoch:  3   step:  9   train loss:  5.841002941131592  val loss:  7.283875942230225  val L1 loss:  7.7576\n",
      "epoch:  3   step:  10   train loss:  5.126245498657227  val loss:  7.941832065582275  val L1 loss:  8.4315\n",
      "epoch:  3   step:  11   train loss:  4.0464372634887695  val loss:  8.16213321685791  val L1 loss:  8.6559\n",
      "epoch:  3   step:  12   train loss:  6.856955528259277  val loss:  8.141773223876953  val L1 loss:  8.64\n",
      "epoch:  3   step:  13   train loss:  5.06145715713501  val loss:  7.601198673248291  val L1 loss:  8.0822\n",
      "epoch:  3   step:  14   train loss:  6.521840572357178  val loss:  6.973363876342773  val L1 loss:  7.4593\n",
      "epoch:  3   step:  15   train loss:  5.5931715965271  val loss:  6.747568130493164  val L1 loss:  7.2246\n",
      "epoch:  3   step:  16   train loss:  3.7597250938415527  val loss:  6.591796875  val L1 loss:  7.0724\n",
      "epoch:  3   step:  17   train loss:  4.585538864135742  val loss:  6.49157190322876  val L1 loss:  6.9716\n",
      "epoch:  3   step:  18   train loss:  6.38852071762085  val loss:  6.485970497131348  val L1 loss:  6.9483\n",
      "epoch:  3   step:  19   train loss:  3.646113634109497  val loss:  6.498721599578857  val L1 loss:  6.9678\n",
      "epoch:  3   step:  20   train loss:  4.612860679626465  val loss:  6.436368465423584  val L1 loss:  6.9217\n",
      "epoch:  3   step:  21   train loss:  4.703636169433594  val loss:  6.456814289093018  val L1 loss:  6.935\n",
      "epoch:  3   step:  22   train loss:  3.940539836883545  val loss:  6.751291275024414  val L1 loss:  7.2218\n",
      "epoch:  3   step:  23   train loss:  7.53048038482666  val loss:  7.075453758239746  val L1 loss:  7.5427\n",
      "epoch:  3   step:  24   train loss:  7.2122907638549805  val loss:  6.98514986038208  val L1 loss:  7.4478\n",
      "epoch:  3   step:  25   train loss:  4.698063373565674  val loss:  6.914363861083984  val L1 loss:  7.4013\n",
      "epoch:  3   step:  26   train loss:  3.798532724380493  val loss:  6.818459510803223  val L1 loss:  7.3034\n",
      "epoch:  3   step:  27   train loss:  6.3130316734313965  val loss:  6.694210529327393  val L1 loss:  7.161\n",
      "epoch:  3   step:  28   train loss:  5.92674446105957  val loss:  6.686063289642334  val L1 loss:  7.1411\n",
      "epoch:  3   step:  29   train loss:  3.0486607551574707  val loss:  6.662288665771484  val L1 loss:  7.1311\n",
      "epoch:  3   step:  30   train loss:  5.1880950927734375  val loss:  6.662559509277344  val L1 loss:  7.1363\n",
      "epoch:  3   step:  31   train loss:  4.3087921142578125  val loss:  6.643211364746094  val L1 loss:  7.1371\n",
      "epoch:  3   step:  32   train loss:  5.45086145401001  val loss:  6.641852378845215  val L1 loss:  7.1356\n",
      "epoch:  3   step:  33   train loss:  4.930792808532715  val loss:  6.7275004386901855  val L1 loss:  7.2034\n",
      "epoch:  3   step:  34   train loss:  5.233536720275879  val loss:  6.833284854888916  val L1 loss:  7.3236\n",
      "epoch:  3   step:  35   train loss:  4.481297969818115  val loss:  6.8489670753479  val L1 loss:  7.3284\n",
      "epoch:  3   step:  36   train loss:  4.389582633972168  val loss:  6.759462833404541  val L1 loss:  7.2323\n",
      "epoch:  3   step:  37   train loss:  5.3318867683410645  val loss:  6.612204551696777  val L1 loss:  7.1032\n",
      "epoch:  3   step:  38   train loss:  4.871481418609619  val loss:  6.535954475402832  val L1 loss:  7.0226\n",
      "epoch:  3   step:  39   train loss:  3.948087215423584  val loss:  6.625723838806152  val L1 loss:  7.1058\n",
      "epoch:  3   step:  40   train loss:  6.088008880615234  val loss:  6.641451358795166  val L1 loss:  7.1198\n",
      "epoch:  3   step:  41   train loss:  6.389953136444092  val loss:  6.532325744628906  val L1 loss:  7.0095\n",
      "epoch:  3   step:  42   train loss:  6.001842021942139  val loss:  6.50791597366333  val L1 loss:  6.9693\n",
      "epoch:  3   step:  43   train loss:  6.7616424560546875  val loss:  6.617667198181152  val L1 loss:  7.0957\n",
      "epoch:  3   step:  44   train loss:  3.4923014640808105  val loss:  6.7532477378845215  val L1 loss:  7.2419\n",
      "epoch:  3   step:  45   train loss:  6.292407035827637  val loss:  6.648680210113525  val L1 loss:  7.1187\n",
      "epoch:  3   step:  46   train loss:  5.142033576965332  val loss:  6.551785945892334  val L1 loss:  7.0214\n",
      "epoch:  3   step:  47   train loss:  4.905433654785156  val loss:  6.578265190124512  val L1 loss:  7.0652\n",
      "epoch:  3   step:  48   train loss:  4.5876994132995605  val loss:  6.87303352355957  val L1 loss:  7.3532\n",
      "epoch:  3   step:  49   train loss:  7.206280708312988  val loss:  7.039294242858887  val L1 loss:  7.5186\n",
      "epoch:  3   step:  50   train loss:  8.31655502319336  val loss:  6.744612693786621  val L1 loss:  7.2196\n",
      "epoch:  3   step:  51   train loss:  7.067025661468506  val loss:  6.409476280212402  val L1 loss:  6.8952\n",
      "epoch:  3   step:  52   train loss:  4.997130393981934  val loss:  6.513050556182861  val L1 loss:  7.0081\n",
      "epoch:  3   step:  53   train loss:  3.714254379272461  val loss:  6.6802659034729  val L1 loss:  7.1678\n",
      "epoch:  3   step:  54   train loss:  5.503499984741211  val loss:  6.666597843170166  val L1 loss:  7.149\n",
      "epoch:  3   step:  55   train loss:  5.075377464294434  val loss:  6.26981782913208  val L1 loss:  6.7544\n",
      "min_val_loss_print 6.26981782913208\n",
      "epoch:  3   step:  56   train loss:  3.906907081604004  val loss:  6.194996356964111  val L1 loss:  6.6772\n",
      "min_val_loss_print 6.194996356964111\n",
      "epoch:  3   step:  57   train loss:  5.770356178283691  val loss:  6.6199951171875  val L1 loss:  7.1072\n",
      "epoch:  3   step:  58   train loss:  4.280117034912109  val loss:  7.019606590270996  val L1 loss:  7.4834\n",
      "epoch:  3   step:  59   train loss:  6.670130729675293  val loss:  6.550361156463623  val L1 loss:  7.0215\n",
      "epoch:  3   step:  60   train loss:  5.9452972412109375  val loss:  6.099128723144531  val L1 loss:  6.5934\n",
      "min_val_loss_print 6.099128723144531\n",
      "epoch:  3   step:  61   train loss:  6.696993350982666  val loss:  6.218185901641846  val L1 loss:  6.713\n",
      "epoch:  3   step:  62   train loss:  7.716499328613281  val loss:  6.926161766052246  val L1 loss:  7.4186\n",
      "epoch:  3   step:  63   train loss:  6.901272773742676  val loss:  7.523447036743164  val L1 loss:  8.0212\n",
      "epoch:  3   step:  64   train loss:  5.295317649841309  val loss:  7.9314751625061035  val L1 loss:  8.424\n",
      "epoch:  3   step:  65   train loss:  4.840445518493652  val loss:  7.718888759613037  val L1 loss:  8.2091\n",
      "epoch:  3   step:  66   train loss:  5.412656784057617  val loss:  7.646122455596924  val L1 loss:  8.1327\n",
      "epoch:  3   step:  67   train loss:  4.9738616943359375  val loss:  7.195082664489746  val L1 loss:  7.6777\n",
      "epoch:  3   step:  68   train loss:  7.112090110778809  val loss:  7.067954063415527  val L1 loss:  7.5576\n",
      "epoch:  3   step:  69   train loss:  5.16884708404541  val loss:  6.964776039123535  val L1 loss:  7.4497\n",
      "epoch:  3   step:  70   train loss:  7.577875137329102  val loss:  6.836514949798584  val L1 loss:  7.3067\n",
      "epoch:  3   step:  71   train loss:  5.225305080413818  val loss:  6.66018533706665  val L1 loss:  7.1445\n",
      "epoch:  3   step:  72   train loss:  5.995865821838379  val loss:  6.52623176574707  val L1 loss:  7.0167\n",
      "epoch:  3   step:  73   train loss:  5.360726356506348  val loss:  6.406674861907959  val L1 loss:  6.8898\n",
      "epoch:  3   step:  74   train loss:  4.37496280670166  val loss:  6.254181385040283  val L1 loss:  6.7338\n",
      "epoch:  3   step:  75   train loss:  5.6207733154296875  val loss:  6.300704002380371  val L1 loss:  6.7769\n",
      "epoch:  3   step:  76   train loss:  5.587890625  val loss:  6.33135986328125  val L1 loss:  6.8122\n",
      "epoch:  3   step:  77   train loss:  6.22908353805542  val loss:  6.317677021026611  val L1 loss:  6.7995\n",
      "epoch:  3   step:  78   train loss:  3.7646474838256836  val loss:  6.26017427444458  val L1 loss:  6.7359\n",
      "epoch:  3   step:  79   train loss:  6.850545883178711  val loss:  6.259268760681152  val L1 loss:  6.7339\n",
      "epoch:  3   step:  80   train loss:  6.473106384277344  val loss:  6.314298152923584  val L1 loss:  6.8046\n",
      "epoch:  3   step:  81   train loss:  4.901031494140625  val loss:  6.408419609069824  val L1 loss:  6.896\n",
      "epoch:  3   step:  82   train loss:  5.8000078201293945  val loss:  6.595325946807861  val L1 loss:  7.0731\n",
      "epoch:  3   step:  83   train loss:  5.540696144104004  val loss:  6.978759765625  val L1 loss:  7.4586\n",
      "epoch:  3   step:  84   train loss:  8.269083023071289  val loss:  7.616725921630859  val L1 loss:  8.1156\n",
      "epoch:  3   step:  85   train loss:  6.093415260314941  val loss:  7.892242431640625  val L1 loss:  8.3876\n",
      "epoch:  3   step:  86   train loss:  5.495816230773926  val loss:  7.527698993682861  val L1 loss:  8.0133\n",
      "epoch:  3   step:  87   train loss:  7.047840595245361  val loss:  7.172488689422607  val L1 loss:  7.6683\n",
      "epoch:  3   step:  88   train loss:  5.811901569366455  val loss:  6.92064094543457  val L1 loss:  7.3898\n",
      "epoch:  3   step:  89   train loss:  5.621326446533203  val loss:  6.752352237701416  val L1 loss:  7.2236\n",
      "epoch:  3   step:  90   train loss:  6.481235027313232  val loss:  6.52741813659668  val L1 loss:  7.016\n",
      "epoch:  3   step:  91   train loss:  6.1712470054626465  val loss:  6.394280910491943  val L1 loss:  6.885\n",
      "epoch:  3   step:  92   train loss:  5.240464210510254  val loss:  6.278166770935059  val L1 loss:  6.7652\n",
      "epoch:  3   step:  93   train loss:  6.402054786682129  val loss:  6.184091091156006  val L1 loss:  6.6733\n",
      "epoch:  3   step:  94   train loss:  4.630637168884277  val loss:  6.16137170791626  val L1 loss:  6.6478\n",
      "epoch:  3   step:  95   train loss:  4.671897888183594  val loss:  6.213955402374268  val L1 loss:  6.6949\n",
      "epoch:  3   step:  96   train loss:  4.775747299194336  val loss:  6.244255065917969  val L1 loss:  6.7145\n",
      "epoch:  3   step:  97   train loss:  3.585869312286377  val loss:  6.265294075012207  val L1 loss:  6.7571\n",
      "epoch:  3   step:  98   train loss:  6.086880683898926  val loss:  6.510007381439209  val L1 loss:  6.9837\n",
      "epoch:  3   step:  99   train loss:  6.9088521003723145  val loss:  6.716021537780762  val L1 loss:  7.1903\n",
      "epoch:  3   step:  100   train loss:  5.688459873199463  val loss:  6.708963394165039  val L1 loss:  7.1873\n",
      "epoch:  3   step:  101   train loss:  3.3474934101104736  val loss:  6.5799560546875  val L1 loss:  7.0772\n",
      "epoch:  3   step:  102   train loss:  5.446771621704102  val loss:  6.511690139770508  val L1 loss:  6.9856\n",
      "epoch:  3   step:  103   train loss:  5.538975238800049  val loss:  6.512667655944824  val L1 loss:  6.9926\n",
      "epoch:  3   step:  104   train loss:  3.8766398429870605  val loss:  6.502615928649902  val L1 loss:  6.9866\n",
      "epoch:  3   step:  105   train loss:  3.7920103073120117  val loss:  6.435784816741943  val L1 loss:  6.9187\n",
      "epoch:  3   step:  106   train loss:  3.5189807415008545  val loss:  6.427056789398193  val L1 loss:  6.8898\n",
      "epoch:  3   step:  107   train loss:  4.8538031578063965  val loss:  6.438637733459473  val L1 loss:  6.903\n",
      "epoch:  3   step:  108   train loss:  3.71437931060791  val loss:  6.4646525382995605  val L1 loss:  6.9398\n",
      "epoch:  3   step:  109   train loss:  4.441789150238037  val loss:  6.523035526275635  val L1 loss:  7.0078\n",
      "epoch:  3   step:  110   train loss:  3.808936595916748  val loss:  6.553966522216797  val L1 loss:  7.0343\n",
      "epoch:  3   step:  111   train loss:  7.719024658203125  val loss:  6.476061820983887  val L1 loss:  6.9645\n",
      "epoch:  3   step:  112   train loss:  4.690537452697754  val loss:  6.45698881149292  val L1 loss:  6.9326\n",
      "epoch:  3   step:  113   train loss:  6.751108169555664  val loss:  6.45164680480957  val L1 loss:  6.9272\n",
      "epoch:  3   step:  114   train loss:  4.360841274261475  val loss:  6.449985027313232  val L1 loss:  6.9261\n",
      "epoch:  3   step:  115   train loss:  4.897914886474609  val loss:  6.4398980140686035  val L1 loss:  6.9256\n",
      "epoch:  3   step:  116   train loss:  3.7473411560058594  val loss:  6.420462608337402  val L1 loss:  6.9038\n",
      "epoch:  3   step:  117   train loss:  5.2159929275512695  val loss:  6.454993724822998  val L1 loss:  6.9435\n",
      "epoch:  3   step:  118   train loss:  4.7082839012146  val loss:  6.415956974029541  val L1 loss:  6.9058\n",
      "epoch:  3   step:  119   train loss:  5.023140907287598  val loss:  6.4080119132995605  val L1 loss:  6.8932\n",
      "epoch:  3   step:  120   train loss:  5.102874279022217  val loss:  6.40017557144165  val L1 loss:  6.8865\n",
      "epoch:  3   step:  121   train loss:  3.2380781173706055  val loss:  6.370264053344727  val L1 loss:  6.8485\n",
      "epoch:  3   step:  122   train loss:  4.537214279174805  val loss:  6.282398700714111  val L1 loss:  6.7584\n",
      "epoch:  3   step:  123   train loss:  3.1410675048828125  val loss:  6.292695045471191  val L1 loss:  6.7709\n",
      "epoch:  3   step:  124   train loss:  4.943517684936523  val loss:  6.271126747131348  val L1 loss:  6.7381\n",
      "epoch:  3   step:  125   train loss:  4.38067102432251  val loss:  6.293310642242432  val L1 loss:  6.7782\n",
      "epoch:  3   step:  126   train loss:  6.801592826843262  val loss:  6.579457759857178  val L1 loss:  7.0502\n",
      "epoch:  3   step:  127   train loss:  5.694525718688965  val loss:  7.020493030548096  val L1 loss:  7.5074\n",
      "epoch:  3   step:  128   train loss:  5.472250461578369  val loss:  7.21279239654541  val L1 loss:  7.7\n",
      "epoch:  3   step:  129   train loss:  6.092845916748047  val loss:  7.225369930267334  val L1 loss:  7.7148\n",
      "epoch:  3   step:  130   train loss:  5.718218803405762  val loss:  6.7019195556640625  val L1 loss:  7.19\n",
      "epoch:  3   step:  131   train loss:  5.048752784729004  val loss:  6.51525354385376  val L1 loss:  6.9995\n",
      "epoch:  3   step:  132   train loss:  3.7312874794006348  val loss:  6.830601215362549  val L1 loss:  7.3147\n",
      "epoch:  3   step:  133   train loss:  4.857212543487549  val loss:  7.249726295471191  val L1 loss:  7.7385\n",
      "epoch:  3   step:  134   train loss:  9.754688262939453  val loss:  7.365281581878662  val L1 loss:  7.844\n",
      "epoch:  3   step:  135   train loss:  6.493260383605957  val loss:  7.179779052734375  val L1 loss:  7.6666\n",
      "epoch:  3   step:  136   train loss:  4.973402976989746  val loss:  6.800085067749023  val L1 loss:  7.2783\n",
      "epoch:  3   step:  137   train loss:  3.818103313446045  val loss:  6.428318977355957  val L1 loss:  6.9138\n",
      "epoch:  3   step:  138   train loss:  4.6873626708984375  val loss:  6.362565040588379  val L1 loss:  6.8485\n",
      "epoch:  3   step:  139   train loss:  5.238669395446777  val loss:  6.646660804748535  val L1 loss:  7.131\n",
      "epoch:  3   step:  140   train loss:  5.652308464050293  val loss:  6.885308742523193  val L1 loss:  7.3742\n",
      "epoch:  3   step:  141   train loss:  7.7693047523498535  val loss:  6.800115585327148  val L1 loss:  7.2881\n",
      "epoch:  3   step:  142   train loss:  4.576303958892822  val loss:  6.505406379699707  val L1 loss:  6.9812\n",
      "epoch:  3   step:  143   train loss:  5.2950439453125  val loss:  6.479682445526123  val L1 loss:  6.9549\n",
      "epoch:  3   step:  144   train loss:  4.433815956115723  val loss:  7.06945276260376  val L1 loss:  7.5344\n",
      "epoch:  3   step:  145   train loss:  6.080938816070557  val loss:  7.429131984710693  val L1 loss:  7.9111\n",
      "epoch:  3   step:  146   train loss:  9.268369674682617  val loss:  7.344892501831055  val L1 loss:  7.8253\n",
      "epoch:  3   step:  147   train loss:  7.623106479644775  val loss:  6.849390506744385  val L1 loss:  7.3264\n",
      "epoch:  3   step:  148   train loss:  4.194787979125977  val loss:  6.370499134063721  val L1 loss:  6.8615\n",
      "epoch:  3   step:  149   train loss:  6.8536152839660645  val loss:  6.306671142578125  val L1 loss:  6.7924\n",
      "epoch:  3   step:  150   train loss:  5.586850643157959  val loss:  6.675296306610107  val L1 loss:  7.1632\n",
      "epoch:  3   step:  151   train loss:  4.348499298095703  val loss:  7.152256965637207  val L1 loss:  7.6239\n",
      "epoch:  3   step:  152   train loss:  7.568647384643555  val loss:  7.142800331115723  val L1 loss:  7.6234\n",
      "epoch:  3   step:  153   train loss:  6.857138633728027  val loss:  6.65981912612915  val L1 loss:  7.1217\n",
      "epoch:  3   step:  154   train loss:  4.656357765197754  val loss:  6.341774940490723  val L1 loss:  6.8282\n",
      "epoch:  3   step:  155   train loss:  5.108487129211426  val loss:  6.248332500457764  val L1 loss:  6.7148\n",
      "epoch:  3   step:  156   train loss:  8.569256782531738  val loss:  6.278989791870117  val L1 loss:  6.764\n",
      "epoch:  3   step:  157   train loss:  5.343230247497559  val loss:  6.2742180824279785  val L1 loss:  6.7457\n",
      "epoch:  3   step:  158   train loss:  4.2603349685668945  val loss:  6.4143967628479  val L1 loss:  6.8913\n",
      "epoch:  3   step:  159   train loss:  5.914522647857666  val loss:  6.671618461608887  val L1 loss:  7.1532\n",
      "epoch:  3   step:  160   train loss:  5.095981597900391  val loss:  6.935905456542969  val L1 loss:  7.4294\n",
      "epoch:  3   step:  161   train loss:  5.137331008911133  val loss:  6.904818058013916  val L1 loss:  7.3926\n",
      "epoch:  3   step:  162   train loss:  6.908580780029297  val loss:  6.586339473724365  val L1 loss:  7.0694\n",
      "epoch:  3   step:  163   train loss:  5.554318904876709  val loss:  6.252193450927734  val L1 loss:  6.7348\n",
      "epoch:  3   step:  164   train loss:  5.196126461029053  val loss:  6.1697869300842285  val L1 loss:  6.6411\n",
      "epoch:  3   step:  165   train loss:  5.582250118255615  val loss:  6.35917329788208  val L1 loss:  6.8362\n",
      "epoch:  3   step:  166   train loss:  4.371528625488281  val loss:  6.428253650665283  val L1 loss:  6.9022\n",
      "epoch:  3   step:  167   train loss:  3.2481539249420166  val loss:  6.421374320983887  val L1 loss:  6.8992\n",
      "epoch:  3   step:  168   train loss:  7.340468406677246  val loss:  6.393115043640137  val L1 loss:  6.8829\n",
      "epoch:  3   step:  169   train loss:  5.914660930633545  val loss:  6.330045700073242  val L1 loss:  6.8179\n",
      "epoch:  3   step:  170   train loss:  5.908924102783203  val loss:  6.293553352355957  val L1 loss:  6.7727\n",
      "epoch:  3   step:  171   train loss:  8.604948043823242  val loss:  6.239696979522705  val L1 loss:  6.7152\n",
      "epoch:  3   step:  172   train loss:  6.193232536315918  val loss:  6.25564432144165  val L1 loss:  6.7493\n",
      "epoch:  3   step:  173   train loss:  4.136135101318359  val loss:  6.308371543884277  val L1 loss:  6.8023\n",
      "epoch:  3   step:  174   train loss:  6.426547050476074  val loss:  6.402457237243652  val L1 loss:  6.8898\n",
      "epoch:  3   step:  175   train loss:  3.6663031578063965  val loss:  6.332902908325195  val L1 loss:  6.8227\n",
      "epoch:  3   step:  176   train loss:  4.884793758392334  val loss:  6.181571960449219  val L1 loss:  6.6787\n",
      "epoch:  3   step:  177   train loss:  4.14827823638916  val loss:  6.083404541015625  val L1 loss:  6.5623\n",
      "min_val_loss_print 6.083404541015625\n",
      "epoch:  3   step:  178   train loss:  6.919595718383789  val loss:  6.049707889556885  val L1 loss:  6.5222\n",
      "min_val_loss_print 6.049707889556885\n",
      "epoch:  3   step:  179   train loss:  7.039144039154053  val loss:  6.021282196044922  val L1 loss:  6.4922\n",
      "min_val_loss_print 6.021282196044922\n",
      "epoch:  3   step:  180   train loss:  6.426458358764648  val loss:  5.991893291473389  val L1 loss:  6.4664\n",
      "min_val_loss_print 5.991893291473389\n",
      "epoch:  3   step:  181   train loss:  4.599429607391357  val loss:  5.983786582946777  val L1 loss:  6.4532\n",
      "min_val_loss_print 5.983786582946777\n",
      "epoch:  3   step:  182   train loss:  3.6787214279174805  val loss:  5.942004203796387  val L1 loss:  6.4155\n",
      "min_val_loss_print 5.942004203796387\n",
      "epoch:  3   step:  183   train loss:  4.968835830688477  val loss:  5.912557125091553  val L1 loss:  6.3832\n",
      "min_val_loss_print 5.912557125091553\n",
      "epoch:  3   step:  184   train loss:  5.498291492462158  val loss:  5.8772454261779785  val L1 loss:  6.3459\n",
      "min_val_loss_print 5.8772454261779785\n",
      "epoch:  3   step:  185   train loss:  5.90543794631958  val loss:  5.982911109924316  val L1 loss:  6.4385\n",
      "epoch:  3   step:  186   train loss:  3.503465414047241  val loss:  6.119487762451172  val L1 loss:  6.5796\n",
      "epoch:  3   step:  187   train loss:  3.5919904708862305  val loss:  6.331438064575195  val L1 loss:  6.8061\n",
      "epoch:  3   step:  188   train loss:  6.533616065979004  val loss:  6.570955753326416  val L1 loss:  7.0536\n",
      "epoch:  3   step:  189   train loss:  6.132033348083496  val loss:  6.701320648193359  val L1 loss:  7.1802\n",
      "epoch:  3   step:  190   train loss:  7.916485786437988  val loss:  6.539244651794434  val L1 loss:  7.0046\n",
      "epoch:  3   step:  191   train loss:  8.995953559875488  val loss:  6.320915222167969  val L1 loss:  6.7995\n",
      "epoch:  3   step:  192   train loss:  6.63462495803833  val loss:  6.287105560302734  val L1 loss:  6.7641\n",
      "epoch:  3   step:  193   train loss:  4.684282302856445  val loss:  6.298714637756348  val L1 loss:  6.7753\n",
      "epoch:  3   step:  194   train loss:  7.142535209655762  val loss:  6.205052375793457  val L1 loss:  6.6924\n",
      "epoch:  3   step:  195   train loss:  4.545696258544922  val loss:  6.177614212036133  val L1 loss:  6.6591\n",
      "epoch:  3   step:  196   train loss:  6.482883453369141  val loss:  6.163096904754639  val L1 loss:  6.6371\n",
      "epoch:  3   step:  197   train loss:  6.941859245300293  val loss:  6.145029544830322  val L1 loss:  6.6414\n",
      "epoch:  3   step:  198   train loss:  5.535796165466309  val loss:  6.230381965637207  val L1 loss:  6.7147\n",
      "epoch:  3   step:  199   train loss:  4.440835952758789  val loss:  6.405114650726318  val L1 loss:  6.8897\n",
      "epoch:  3   step:  200   train loss:  7.551901817321777  val loss:  6.552282333374023  val L1 loss:  7.0369\n",
      "epoch:  3   step:  201   train loss:  5.881683349609375  val loss:  6.361883640289307  val L1 loss:  6.8508\n",
      "epoch:  3   step:  202   train loss:  5.46346378326416  val loss:  6.365076065063477  val L1 loss:  6.8433\n",
      "epoch:  3   step:  203   train loss:  4.461797714233398  val loss:  6.387349605560303  val L1 loss:  6.8591\n",
      "epoch:  3   step:  204   train loss:  5.278347969055176  val loss:  6.424316883087158  val L1 loss:  6.9047\n",
      "epoch:  3   step:  205   train loss:  4.111514091491699  val loss:  6.36505126953125  val L1 loss:  6.8314\n",
      "epoch:  3   step:  206   train loss:  3.31831955909729  val loss:  6.237830638885498  val L1 loss:  6.7103\n",
      "epoch:  3   step:  207   train loss:  3.3630857467651367  val loss:  6.289943695068359  val L1 loss:  6.7647\n",
      "epoch:  3   step:  208   train loss:  6.081785678863525  val loss:  6.267539024353027  val L1 loss:  6.7436\n",
      "epoch:  4   step:  0   train loss:  4.903848648071289  val loss:  6.175803184509277  val L1 loss:  6.6434\n",
      "epoch:  4   step:  1   train loss:  5.259647846221924  val loss:  6.149554252624512  val L1 loss:  6.63\n",
      "epoch:  4   step:  2   train loss:  5.321772575378418  val loss:  6.2942023277282715  val L1 loss:  6.7787\n",
      "epoch:  4   step:  3   train loss:  4.539838790893555  val loss:  6.5729265213012695  val L1 loss:  7.0711\n",
      "epoch:  4   step:  4   train loss:  4.906501293182373  val loss:  6.620654582977295  val L1 loss:  7.1095\n",
      "epoch:  4   step:  5   train loss:  3.500204086303711  val loss:  6.660525798797607  val L1 loss:  7.1404\n",
      "epoch:  4   step:  6   train loss:  5.08772611618042  val loss:  6.4732489585876465  val L1 loss:  6.9463\n",
      "epoch:  4   step:  7   train loss:  7.124709606170654  val loss:  6.500436782836914  val L1 loss:  6.9642\n",
      "epoch:  4   step:  8   train loss:  5.348430633544922  val loss:  6.6822404861450195  val L1 loss:  7.1622\n",
      "epoch:  4   step:  9   train loss:  4.705974578857422  val loss:  6.935105323791504  val L1 loss:  7.4255\n",
      "epoch:  4   step:  10   train loss:  6.746059417724609  val loss:  7.285333633422852  val L1 loss:  7.7677\n",
      "epoch:  4   step:  11   train loss:  6.482738494873047  val loss:  7.22206449508667  val L1 loss:  7.7037\n",
      "epoch:  4   step:  12   train loss:  5.696015357971191  val loss:  6.636610984802246  val L1 loss:  7.1058\n",
      "epoch:  4   step:  13   train loss:  5.413352012634277  val loss:  6.003483772277832  val L1 loss:  6.4892\n",
      "epoch:  4   step:  14   train loss:  4.070215702056885  val loss:  5.774340629577637  val L1 loss:  6.2584\n",
      "min_val_loss_print 5.774340629577637\n",
      "epoch:  4   step:  15   train loss:  5.351823806762695  val loss:  5.947080135345459  val L1 loss:  6.4194\n",
      "epoch:  4   step:  16   train loss:  6.103550434112549  val loss:  5.98533821105957  val L1 loss:  6.4754\n",
      "epoch:  4   step:  17   train loss:  5.7310991287231445  val loss:  5.8788676261901855  val L1 loss:  6.3576\n",
      "epoch:  4   step:  18   train loss:  5.374368667602539  val loss:  5.944786071777344  val L1 loss:  6.423\n",
      "epoch:  4   step:  19   train loss:  5.592862606048584  val loss:  6.116935729980469  val L1 loss:  6.5839\n",
      "epoch:  4   step:  20   train loss:  7.70998477935791  val loss:  6.504704475402832  val L1 loss:  6.995\n",
      "epoch:  4   step:  21   train loss:  7.088939666748047  val loss:  6.852548599243164  val L1 loss:  7.3282\n",
      "epoch:  4   step:  22   train loss:  5.064781665802002  val loss:  6.727770805358887  val L1 loss:  7.2144\n",
      "epoch:  4   step:  23   train loss:  4.050330638885498  val loss:  6.587470054626465  val L1 loss:  7.0484\n",
      "epoch:  4   step:  24   train loss:  6.132818698883057  val loss:  6.517362117767334  val L1 loss:  6.9786\n",
      "epoch:  4   step:  25   train loss:  6.3965888023376465  val loss:  6.310108661651611  val L1 loss:  6.7895\n",
      "epoch:  4   step:  26   train loss:  3.8015873432159424  val loss:  6.006922245025635  val L1 loss:  6.4796\n",
      "epoch:  4   step:  27   train loss:  8.57784652709961  val loss:  5.806408405303955  val L1 loss:  6.2872\n",
      "epoch:  4   step:  28   train loss:  3.7377986907958984  val loss:  5.709509372711182  val L1 loss:  6.1953\n",
      "min_val_loss_print 5.709509372711182\n",
      "epoch:  4   step:  29   train loss:  4.96863317489624  val loss:  5.778338432312012  val L1 loss:  6.2488\n",
      "epoch:  4   step:  30   train loss:  6.242595195770264  val loss:  5.9034423828125  val L1 loss:  6.3862\n",
      "epoch:  4   step:  31   train loss:  8.072988510131836  val loss:  5.945977210998535  val L1 loss:  6.4282\n",
      "epoch:  4   step:  32   train loss:  5.047336578369141  val loss:  5.800264358520508  val L1 loss:  6.2521\n",
      "epoch:  4   step:  33   train loss:  6.272524833679199  val loss:  5.852548599243164  val L1 loss:  6.3246\n",
      "epoch:  4   step:  34   train loss:  6.124650478363037  val loss:  5.883674144744873  val L1 loss:  6.3541\n",
      "epoch:  4   step:  35   train loss:  4.926773548126221  val loss:  5.865602970123291  val L1 loss:  6.335\n",
      "epoch:  4   step:  36   train loss:  8.589447021484375  val loss:  6.007742404937744  val L1 loss:  6.4755\n",
      "epoch:  4   step:  37   train loss:  4.108189105987549  val loss:  6.413837909698486  val L1 loss:  6.9041\n",
      "epoch:  4   step:  38   train loss:  5.292938232421875  val loss:  6.730339050292969  val L1 loss:  7.2028\n",
      "epoch:  4   step:  39   train loss:  7.787283420562744  val loss:  6.860764026641846  val L1 loss:  7.3441\n",
      "epoch:  4   step:  40   train loss:  4.473166465759277  val loss:  6.438984394073486  val L1 loss:  6.9062\n",
      "epoch:  4   step:  41   train loss:  4.342413902282715  val loss:  5.9691996574401855  val L1 loss:  6.4381\n",
      "epoch:  4   step:  42   train loss:  4.376579761505127  val loss:  5.967684268951416  val L1 loss:  6.4324\n",
      "epoch:  4   step:  43   train loss:  5.212625503540039  val loss:  6.191308498382568  val L1 loss:  6.6594\n",
      "epoch:  4   step:  44   train loss:  4.297948837280273  val loss:  6.265836238861084  val L1 loss:  6.7505\n",
      "epoch:  4   step:  45   train loss:  4.308591842651367  val loss:  6.005126953125  val L1 loss:  6.482\n",
      "epoch:  4   step:  46   train loss:  6.773734092712402  val loss:  5.898976802825928  val L1 loss:  6.3632\n",
      "epoch:  4   step:  47   train loss:  6.674774169921875  val loss:  6.007015228271484  val L1 loss:  6.4883\n",
      "epoch:  4   step:  48   train loss:  4.860831260681152  val loss:  6.138668060302734  val L1 loss:  6.6254\n",
      "epoch:  4   step:  49   train loss:  6.08578634262085  val loss:  6.148908615112305  val L1 loss:  6.6313\n",
      "epoch:  4   step:  50   train loss:  3.708904266357422  val loss:  5.979450702667236  val L1 loss:  6.4512\n",
      "epoch:  4   step:  51   train loss:  4.00832462310791  val loss:  6.135271072387695  val L1 loss:  6.6158\n",
      "epoch:  4   step:  52   train loss:  5.098842620849609  val loss:  6.404539585113525  val L1 loss:  6.8971\n",
      "epoch:  4   step:  53   train loss:  4.664641380310059  val loss:  6.48318338394165  val L1 loss:  6.9699\n",
      "epoch:  4   step:  54   train loss:  4.805541038513184  val loss:  6.228917121887207  val L1 loss:  6.7114\n",
      "epoch:  4   step:  55   train loss:  3.827601432800293  val loss:  6.09580135345459  val L1 loss:  6.5769\n",
      "epoch:  4   step:  56   train loss:  4.269743919372559  val loss:  6.215755462646484  val L1 loss:  6.6699\n",
      "epoch:  4   step:  57   train loss:  4.103224277496338  val loss:  6.413870811462402  val L1 loss:  6.9014\n",
      "epoch:  4   step:  58   train loss:  5.874393463134766  val loss:  6.45111083984375  val L1 loss:  6.9397\n",
      "epoch:  4   step:  59   train loss:  4.321962356567383  val loss:  6.301544189453125  val L1 loss:  6.7628\n",
      "epoch:  4   step:  60   train loss:  4.202390670776367  val loss:  6.249884128570557  val L1 loss:  6.7144\n",
      "epoch:  4   step:  61   train loss:  6.9868693351745605  val loss:  6.265246868133545  val L1 loss:  6.7282\n",
      "epoch:  4   step:  62   train loss:  5.967048645019531  val loss:  6.1884894371032715  val L1 loss:  6.6501\n",
      "epoch:  4   step:  63   train loss:  5.300640106201172  val loss:  6.135533332824707  val L1 loss:  6.6003\n",
      "epoch:  4   step:  64   train loss:  2.6805410385131836  val loss:  6.13297700881958  val L1 loss:  6.5931\n",
      "epoch:  4   step:  65   train loss:  5.056310176849365  val loss:  6.137546062469482  val L1 loss:  6.5979\n",
      "epoch:  4   step:  66   train loss:  6.12697696685791  val loss:  6.138095378875732  val L1 loss:  6.6021\n",
      "epoch:  4   step:  67   train loss:  4.163822174072266  val loss:  6.15331506729126  val L1 loss:  6.6212\n",
      "epoch:  4   step:  68   train loss:  3.153395175933838  val loss:  6.139066696166992  val L1 loss:  6.6029\n",
      "epoch:  4   step:  69   train loss:  4.262790203094482  val loss:  6.133539199829102  val L1 loss:  6.6084\n",
      "epoch:  4   step:  70   train loss:  3.551914691925049  val loss:  6.153876304626465  val L1 loss:  6.6248\n",
      "epoch:  4   step:  71   train loss:  5.168785572052002  val loss:  6.225932598114014  val L1 loss:  6.7022\n",
      "epoch:  4   step:  72   train loss:  4.775885581970215  val loss:  6.441986083984375  val L1 loss:  6.9283\n",
      "epoch:  4   step:  73   train loss:  4.555866241455078  val loss:  6.577322959899902  val L1 loss:  7.064\n",
      "epoch:  4   step:  74   train loss:  6.580001354217529  val loss:  6.4716410636901855  val L1 loss:  6.968\n",
      "epoch:  4   step:  75   train loss:  3.440201997756958  val loss:  6.350861549377441  val L1 loss:  6.8478\n",
      "epoch:  4   step:  76   train loss:  5.091723918914795  val loss:  6.250040054321289  val L1 loss:  6.7266\n",
      "epoch:  4   step:  77   train loss:  5.888613224029541  val loss:  6.127525329589844  val L1 loss:  6.6165\n",
      "epoch:  4   step:  78   train loss:  5.401989936828613  val loss:  6.119228363037109  val L1 loss:  6.6003\n",
      "epoch:  4   step:  79   train loss:  6.246762752532959  val loss:  6.092529296875  val L1 loss:  6.572\n",
      "epoch:  4   step:  80   train loss:  3.8636181354522705  val loss:  6.1593918800354  val L1 loss:  6.6365\n",
      "epoch:  4   step:  81   train loss:  8.858613967895508  val loss:  6.227542400360107  val L1 loss:  6.6959\n",
      "epoch:  4   step:  82   train loss:  3.958578109741211  val loss:  6.346120357513428  val L1 loss:  6.83\n",
      "epoch:  4   step:  83   train loss:  2.7153382301330566  val loss:  6.584756851196289  val L1 loss:  7.0671\n",
      "epoch:  4   step:  84   train loss:  8.46363639831543  val loss:  6.525711536407471  val L1 loss:  7.014\n",
      "epoch:  4   step:  85   train loss:  4.28896427154541  val loss:  6.416996955871582  val L1 loss:  6.8822\n",
      "epoch:  4   step:  86   train loss:  5.1138105392456055  val loss:  6.364777565002441  val L1 loss:  6.8449\n",
      "epoch:  4   step:  87   train loss:  4.4276251792907715  val loss:  6.35999870300293  val L1 loss:  6.8427\n",
      "epoch:  4   step:  88   train loss:  4.5633721351623535  val loss:  6.509783744812012  val L1 loss:  6.9938\n",
      "epoch:  4   step:  89   train loss:  3.8267359733581543  val loss:  6.495267391204834  val L1 loss:  6.976\n",
      "epoch:  4   step:  90   train loss:  8.138548851013184  val loss:  6.4608683586120605  val L1 loss:  6.9519\n",
      "epoch:  4   step:  91   train loss:  6.162274360656738  val loss:  6.434825420379639  val L1 loss:  6.9252\n",
      "epoch:  4   step:  92   train loss:  4.334626197814941  val loss:  6.343382358551025  val L1 loss:  6.8284\n",
      "epoch:  4   step:  93   train loss:  5.023712158203125  val loss:  6.313581466674805  val L1 loss:  6.7834\n",
      "epoch:  4   step:  94   train loss:  4.502182960510254  val loss:  6.389816761016846  val L1 loss:  6.868\n",
      "epoch:  4   step:  95   train loss:  8.298688888549805  val loss:  6.321476936340332  val L1 loss:  6.7873\n",
      "epoch:  4   step:  96   train loss:  4.9390106201171875  val loss:  6.224497318267822  val L1 loss:  6.6872\n",
      "epoch:  4   step:  97   train loss:  9.475801467895508  val loss:  6.281160354614258  val L1 loss:  6.7581\n",
      "epoch:  4   step:  98   train loss:  7.485918998718262  val loss:  6.331401348114014  val L1 loss:  6.7984\n",
      "epoch:  4   step:  99   train loss:  3.6572203636169434  val loss:  6.285060882568359  val L1 loss:  6.7592\n",
      "epoch:  4   step:  100   train loss:  3.92155385017395  val loss:  6.220380783081055  val L1 loss:  6.7015\n",
      "epoch:  4   step:  101   train loss:  4.97833776473999  val loss:  6.424441337585449  val L1 loss:  6.9075\n",
      "epoch:  4   step:  102   train loss:  3.3683769702911377  val loss:  6.884297847747803  val L1 loss:  7.3738\n",
      "epoch:  4   step:  103   train loss:  6.812607765197754  val loss:  6.821616172790527  val L1 loss:  7.304\n",
      "epoch:  4   step:  104   train loss:  3.4492273330688477  val loss:  6.714696884155273  val L1 loss:  7.2043\n",
      "epoch:  4   step:  105   train loss:  3.976027011871338  val loss:  6.6650519371032715  val L1 loss:  7.1487\n",
      "epoch:  4   step:  106   train loss:  5.83121395111084  val loss:  6.700567245483398  val L1 loss:  7.1863\n",
      "epoch:  4   step:  107   train loss:  5.036540985107422  val loss:  6.627040863037109  val L1 loss:  7.1126\n",
      "epoch:  4   step:  108   train loss:  4.883416175842285  val loss:  6.474594593048096  val L1 loss:  6.95\n",
      "epoch:  4   step:  109   train loss:  3.342618465423584  val loss:  6.355818271636963  val L1 loss:  6.8246\n",
      "epoch:  4   step:  110   train loss:  4.400415420532227  val loss:  6.270925045013428  val L1 loss:  6.7372\n",
      "epoch:  4   step:  111   train loss:  4.507637977600098  val loss:  6.204431056976318  val L1 loss:  6.6731\n",
      "epoch:  4   step:  112   train loss:  3.72699236869812  val loss:  6.143533229827881  val L1 loss:  6.6115\n",
      "epoch:  4   step:  113   train loss:  3.099731922149658  val loss:  6.052957057952881  val L1 loss:  6.5051\n",
      "epoch:  4   step:  114   train loss:  5.546431064605713  val loss:  6.056064128875732  val L1 loss:  6.5342\n",
      "epoch:  4   step:  115   train loss:  6.480866432189941  val loss:  6.061103343963623  val L1 loss:  6.5377\n",
      "epoch:  4   step:  116   train loss:  3.5632951259613037  val loss:  6.056880474090576  val L1 loss:  6.5419\n",
      "epoch:  4   step:  117   train loss:  4.259396553039551  val loss:  6.109665393829346  val L1 loss:  6.5996\n",
      "epoch:  4   step:  118   train loss:  5.9450883865356445  val loss:  6.387767314910889  val L1 loss:  6.8692\n",
      "epoch:  4   step:  119   train loss:  4.626512050628662  val loss:  6.803491592407227  val L1 loss:  7.2895\n",
      "epoch:  4   step:  120   train loss:  3.48878812789917  val loss:  6.909914493560791  val L1 loss:  7.3954\n",
      "epoch:  4   step:  121   train loss:  7.578289985656738  val loss:  6.5691752433776855  val L1 loss:  7.0594\n",
      "epoch:  4   step:  122   train loss:  6.088624954223633  val loss:  6.235640525817871  val L1 loss:  6.7239\n",
      "epoch:  4   step:  123   train loss:  6.032111167907715  val loss:  6.09258508682251  val L1 loss:  6.5442\n",
      "epoch:  4   step:  124   train loss:  2.934990406036377  val loss:  6.771056175231934  val L1 loss:  7.2642\n",
      "epoch:  4   step:  125   train loss:  8.874479293823242  val loss:  7.032711029052734  val L1 loss:  7.5193\n",
      "epoch:  4   step:  126   train loss:  6.191489219665527  val loss:  6.973243236541748  val L1 loss:  7.4623\n",
      "epoch:  4   step:  127   train loss:  6.135720252990723  val loss:  6.442689895629883  val L1 loss:  6.9236\n",
      "epoch:  4   step:  128   train loss:  5.523428440093994  val loss:  6.103671550750732  val L1 loss:  6.5733\n",
      "epoch:  4   step:  129   train loss:  6.912351608276367  val loss:  5.923000335693359  val L1 loss:  6.401\n",
      "epoch:  4   step:  130   train loss:  7.860167980194092  val loss:  6.060049533843994  val L1 loss:  6.5392\n",
      "epoch:  4   step:  131   train loss:  5.165761947631836  val loss:  6.056446552276611  val L1 loss:  6.5326\n",
      "epoch:  4   step:  132   train loss:  5.36902379989624  val loss:  6.065406322479248  val L1 loss:  6.543\n",
      "epoch:  4   step:  133   train loss:  8.414654731750488  val loss:  6.379813194274902  val L1 loss:  6.8457\n",
      "epoch:  4   step:  134   train loss:  3.8980560302734375  val loss:  6.950586318969727  val L1 loss:  7.4443\n",
      "epoch:  4   step:  135   train loss:  5.266295433044434  val loss:  7.2454681396484375  val L1 loss:  7.7248\n",
      "epoch:  4   step:  136   train loss:  5.666101932525635  val loss:  7.078958511352539  val L1 loss:  7.5681\n",
      "epoch:  4   step:  137   train loss:  7.4330291748046875  val loss:  6.527544975280762  val L1 loss:  7.0068\n",
      "epoch:  4   step:  138   train loss:  6.138324737548828  val loss:  6.255993366241455  val L1 loss:  6.7438\n",
      "epoch:  4   step:  139   train loss:  6.265325546264648  val loss:  6.146766662597656  val L1 loss:  6.6215\n",
      "epoch:  4   step:  140   train loss:  4.938757419586182  val loss:  6.147758483886719  val L1 loss:  6.6347\n",
      "epoch:  4   step:  141   train loss:  4.798511505126953  val loss:  6.144618511199951  val L1 loss:  6.6257\n",
      "epoch:  4   step:  142   train loss:  4.590716361999512  val loss:  6.201593399047852  val L1 loss:  6.6828\n",
      "epoch:  4   step:  143   train loss:  5.009181022644043  val loss:  6.559146404266357  val L1 loss:  7.0413\n",
      "epoch:  4   step:  144   train loss:  6.660106658935547  val loss:  7.013313293457031  val L1 loss:  7.4947\n",
      "epoch:  4   step:  145   train loss:  4.5168633460998535  val loss:  6.94380521774292  val L1 loss:  7.4226\n",
      "epoch:  4   step:  146   train loss:  6.966370582580566  val loss:  6.544652938842773  val L1 loss:  7.0264\n",
      "epoch:  4   step:  147   train loss:  4.22639274597168  val loss:  6.131357192993164  val L1 loss:  6.6113\n",
      "epoch:  4   step:  148   train loss:  5.049203395843506  val loss:  6.048695087432861  val L1 loss:  6.5235\n",
      "epoch:  4   step:  149   train loss:  5.957403182983398  val loss:  6.3405890464782715  val L1 loss:  6.8253\n",
      "epoch:  4   step:  150   train loss:  6.80728006362915  val loss:  6.45740270614624  val L1 loss:  6.9451\n",
      "epoch:  4   step:  151   train loss:  5.815032958984375  val loss:  6.232459545135498  val L1 loss:  6.7162\n",
      "epoch:  4   step:  152   train loss:  5.127109527587891  val loss:  5.979151248931885  val L1 loss:  6.4576\n",
      "epoch:  4   step:  153   train loss:  4.28619909286499  val loss:  5.824732303619385  val L1 loss:  6.3027\n",
      "epoch:  4   step:  154   train loss:  6.439062595367432  val loss:  5.842841625213623  val L1 loss:  6.3215\n",
      "epoch:  4   step:  155   train loss:  4.3589396476745605  val loss:  6.01662015914917  val L1 loss:  6.4945\n",
      "epoch:  4   step:  156   train loss:  6.6682024002075195  val loss:  6.007192134857178  val L1 loss:  6.4924\n",
      "epoch:  4   step:  157   train loss:  3.0114173889160156  val loss:  5.911627769470215  val L1 loss:  6.384\n",
      "epoch:  4   step:  158   train loss:  4.29932165145874  val loss:  5.898560523986816  val L1 loss:  6.3646\n",
      "epoch:  4   step:  159   train loss:  6.37639856338501  val loss:  5.969736099243164  val L1 loss:  6.4428\n",
      "epoch:  4   step:  160   train loss:  5.563409328460693  val loss:  6.130393981933594  val L1 loss:  6.611\n",
      "epoch:  4   step:  161   train loss:  4.435019493103027  val loss:  6.088686466217041  val L1 loss:  6.5698\n",
      "epoch:  4   step:  162   train loss:  5.051387786865234  val loss:  6.114262580871582  val L1 loss:  6.6\n",
      "epoch:  4   step:  163   train loss:  5.684937477111816  val loss:  5.979902267456055  val L1 loss:  6.4538\n",
      "epoch:  4   step:  164   train loss:  6.622194290161133  val loss:  5.85518217086792  val L1 loss:  6.3234\n",
      "epoch:  4   step:  165   train loss:  4.445601940155029  val loss:  5.892345905303955  val L1 loss:  6.3707\n",
      "epoch:  4   step:  166   train loss:  5.6908111572265625  val loss:  5.981711387634277  val L1 loss:  6.4669\n",
      "epoch:  4   step:  167   train loss:  6.304717540740967  val loss:  6.05103874206543  val L1 loss:  6.5375\n",
      "epoch:  4   step:  168   train loss:  6.560142993927002  val loss:  5.9924468994140625  val L1 loss:  6.4738\n",
      "epoch:  4   step:  169   train loss:  4.695034503936768  val loss:  5.941761493682861  val L1 loss:  6.4164\n",
      "epoch:  4   step:  170   train loss:  6.167263031005859  val loss:  5.9644012451171875  val L1 loss:  6.4451\n",
      "epoch:  4   step:  171   train loss:  5.911031723022461  val loss:  6.035222053527832  val L1 loss:  6.523\n",
      "epoch:  4   step:  172   train loss:  7.164838790893555  val loss:  6.225566387176514  val L1 loss:  6.6988\n",
      "epoch:  4   step:  173   train loss:  5.748517036437988  val loss:  6.39454460144043  val L1 loss:  6.8818\n",
      "epoch:  4   step:  174   train loss:  6.704929351806641  val loss:  6.279652118682861  val L1 loss:  6.7717\n",
      "epoch:  4   step:  175   train loss:  4.690276145935059  val loss:  6.106547832489014  val L1 loss:  6.5905\n",
      "epoch:  4   step:  176   train loss:  6.238877296447754  val loss:  6.166788101196289  val L1 loss:  6.6448\n",
      "epoch:  4   step:  177   train loss:  5.8095221519470215  val loss:  6.498355388641357  val L1 loss:  6.9857\n",
      "epoch:  4   step:  178   train loss:  4.971966743469238  val loss:  6.617081165313721  val L1 loss:  7.1145\n",
      "epoch:  4   step:  179   train loss:  6.127409934997559  val loss:  6.367690086364746  val L1 loss:  6.8468\n",
      "epoch:  4   step:  180   train loss:  6.243612766265869  val loss:  6.07998514175415  val L1 loss:  6.5567\n",
      "epoch:  4   step:  181   train loss:  5.02637243270874  val loss:  5.978402137756348  val L1 loss:  6.4581\n",
      "epoch:  4   step:  182   train loss:  3.343632221221924  val loss:  6.252276420593262  val L1 loss:  6.7263\n",
      "epoch:  4   step:  183   train loss:  4.298870086669922  val loss:  6.363577842712402  val L1 loss:  6.8391\n",
      "epoch:  4   step:  184   train loss:  6.7377824783325195  val loss:  6.174745559692383  val L1 loss:  6.658\n",
      "epoch:  4   step:  185   train loss:  4.036352634429932  val loss:  6.049558639526367  val L1 loss:  6.5358\n",
      "epoch:  4   step:  186   train loss:  3.395524024963379  val loss:  6.231727600097656  val L1 loss:  6.7118\n",
      "epoch:  4   step:  187   train loss:  5.535519599914551  val loss:  6.942394256591797  val L1 loss:  7.4185\n",
      "epoch:  4   step:  188   train loss:  4.769464492797852  val loss:  7.503725528717041  val L1 loss:  7.9948\n",
      "epoch:  4   step:  189   train loss:  5.614524841308594  val loss:  7.299369812011719  val L1 loss:  7.7863\n",
      "epoch:  4   step:  190   train loss:  7.7166643142700195  val loss:  6.693968296051025  val L1 loss:  7.1726\n",
      "epoch:  4   step:  191   train loss:  6.121959686279297  val loss:  6.204168796539307  val L1 loss:  6.6782\n",
      "epoch:  4   step:  192   train loss:  5.2730584144592285  val loss:  6.133615493774414  val L1 loss:  6.626\n",
      "epoch:  4   step:  193   train loss:  5.386634826660156  val loss:  6.144173622131348  val L1 loss:  6.6151\n",
      "epoch:  4   step:  194   train loss:  4.4918413162231445  val loss:  6.165014743804932  val L1 loss:  6.6336\n",
      "epoch:  4   step:  195   train loss:  6.197646617889404  val loss:  6.097932815551758  val L1 loss:  6.5834\n",
      "epoch:  4   step:  196   train loss:  5.714986801147461  val loss:  6.063287258148193  val L1 loss:  6.5329\n",
      "epoch:  4   step:  197   train loss:  3.1649136543273926  val loss:  6.185990333557129  val L1 loss:  6.6715\n",
      "epoch:  4   step:  198   train loss:  6.345147609710693  val loss:  6.306392669677734  val L1 loss:  6.8027\n",
      "epoch:  4   step:  199   train loss:  4.437952041625977  val loss:  6.361030101776123  val L1 loss:  6.8577\n",
      "epoch:  4   step:  200   train loss:  7.076415061950684  val loss:  6.263918399810791  val L1 loss:  6.7535\n",
      "epoch:  4   step:  201   train loss:  4.5714006423950195  val loss:  6.195583343505859  val L1 loss:  6.681\n",
      "epoch:  4   step:  202   train loss:  4.931116580963135  val loss:  6.140104293823242  val L1 loss:  6.6301\n",
      "epoch:  4   step:  203   train loss:  4.858022689819336  val loss:  6.168309211730957  val L1 loss:  6.6431\n",
      "epoch:  4   step:  204   train loss:  5.2579426765441895  val loss:  6.130938529968262  val L1 loss:  6.6113\n",
      "epoch:  4   step:  205   train loss:  5.358386993408203  val loss:  6.02739143371582  val L1 loss:  6.5189\n",
      "epoch:  4   step:  206   train loss:  3.079988718032837  val loss:  5.967897891998291  val L1 loss:  6.455\n",
      "epoch:  4   step:  207   train loss:  5.149990558624268  val loss:  6.028558254241943  val L1 loss:  6.5133\n",
      "epoch:  4   step:  208   train loss:  3.8536508083343506  val loss:  5.994505405426025  val L1 loss:  6.4755\n",
      "epoch:  5   step:  0   train loss:  5.285849094390869  val loss:  5.94956636428833  val L1 loss:  6.4216\n",
      "epoch:  5   step:  1   train loss:  7.086050987243652  val loss:  5.94807243347168  val L1 loss:  6.4399\n",
      "epoch:  5   step:  2   train loss:  4.534721851348877  val loss:  6.249145984649658  val L1 loss:  6.7382\n",
      "epoch:  5   step:  3   train loss:  3.8351478576660156  val loss:  6.769183158874512  val L1 loss:  7.2348\n",
      "epoch:  5   step:  4   train loss:  5.032806396484375  val loss:  7.119971752166748  val L1 loss:  7.5946\n",
      "epoch:  5   step:  5   train loss:  4.108613014221191  val loss:  6.941196918487549  val L1 loss:  7.4252\n",
      "epoch:  5   step:  6   train loss:  5.9189605712890625  val loss:  6.7490739822387695  val L1 loss:  7.2334\n",
      "epoch:  5   step:  7   train loss:  4.312812805175781  val loss:  6.401156902313232  val L1 loss:  6.88\n",
      "epoch:  5   step:  8   train loss:  4.540073871612549  val loss:  6.212753772735596  val L1 loss:  6.6859\n",
      "epoch:  5   step:  9   train loss:  6.916582107543945  val loss:  6.1603546142578125  val L1 loss:  6.6474\n",
      "epoch:  5   step:  10   train loss:  6.736087322235107  val loss:  6.035627365112305  val L1 loss:  6.5187\n",
      "epoch:  5   step:  11   train loss:  3.6720166206359863  val loss:  5.89385986328125  val L1 loss:  6.3698\n",
      "epoch:  5   step:  12   train loss:  5.016286849975586  val loss:  5.849252700805664  val L1 loss:  6.3172\n",
      "epoch:  5   step:  13   train loss:  4.853812217712402  val loss:  5.919617176055908  val L1 loss:  6.3983\n",
      "epoch:  5   step:  14   train loss:  4.17032527923584  val loss:  6.040438652038574  val L1 loss:  6.5122\n",
      "epoch:  5   step:  15   train loss:  5.176337242126465  val loss:  6.12159538269043  val L1 loss:  6.5947\n",
      "epoch:  5   step:  16   train loss:  3.8480262756347656  val loss:  6.262726783752441  val L1 loss:  6.7435\n",
      "epoch:  5   step:  17   train loss:  6.375679969787598  val loss:  6.341406345367432  val L1 loss:  6.8171\n",
      "epoch:  5   step:  18   train loss:  4.710819721221924  val loss:  6.274386405944824  val L1 loss:  6.7599\n",
      "epoch:  5   step:  19   train loss:  5.172325134277344  val loss:  6.0742106437683105  val L1 loss:  6.5492\n",
      "epoch:  5   step:  20   train loss:  6.736532211303711  val loss:  5.958539962768555  val L1 loss:  6.4337\n",
      "epoch:  5   step:  21   train loss:  4.663120746612549  val loss:  5.872791767120361  val L1 loss:  6.364\n",
      "epoch:  5   step:  22   train loss:  3.2845985889434814  val loss:  5.832766056060791  val L1 loss:  6.3216\n",
      "epoch:  5   step:  23   train loss:  6.967863082885742  val loss:  5.69079065322876  val L1 loss:  6.1601\n",
      "min_val_loss_print 5.69079065322876\n",
      "epoch:  5   step:  24   train loss:  2.811805248260498  val loss:  5.81654691696167  val L1 loss:  6.2942\n",
      "epoch:  5   step:  25   train loss:  4.906635761260986  val loss:  6.054989337921143  val L1 loss:  6.5267\n",
      "epoch:  5   step:  26   train loss:  6.622452259063721  val loss:  6.398179054260254  val L1 loss:  6.8563\n",
      "epoch:  5   step:  27   train loss:  6.193716049194336  val loss:  6.698403835296631  val L1 loss:  7.1564\n",
      "epoch:  5   step:  28   train loss:  4.357976913452148  val loss:  6.71226167678833  val L1 loss:  7.1641\n",
      "epoch:  5   step:  29   train loss:  5.909003257751465  val loss:  6.374359130859375  val L1 loss:  6.8318\n",
      "epoch:  5   step:  30   train loss:  4.425738334655762  val loss:  6.09580659866333  val L1 loss:  6.5767\n",
      "epoch:  5   step:  31   train loss:  2.725266695022583  val loss:  5.860180377960205  val L1 loss:  6.3407\n",
      "epoch:  5   step:  32   train loss:  3.009822368621826  val loss:  5.904272556304932  val L1 loss:  6.3951\n",
      "epoch:  5   step:  33   train loss:  4.655101776123047  val loss:  5.99468469619751  val L1 loss:  6.4838\n",
      "epoch:  5   step:  34   train loss:  5.121930122375488  val loss:  6.0139288902282715  val L1 loss:  6.5084\n",
      "epoch:  5   step:  35   train loss:  3.9446702003479004  val loss:  6.004000186920166  val L1 loss:  6.4931\n",
      "epoch:  5   step:  36   train loss:  8.044541358947754  val loss:  6.152669429779053  val L1 loss:  6.6265\n",
      "epoch:  5   step:  37   train loss:  4.497932434082031  val loss:  6.31863260269165  val L1 loss:  6.8063\n",
      "epoch:  5   step:  38   train loss:  6.8911333084106445  val loss:  6.338850021362305  val L1 loss:  6.817\n",
      "epoch:  5   step:  39   train loss:  7.275881290435791  val loss:  6.060554504394531  val L1 loss:  6.5082\n",
      "epoch:  5   step:  40   train loss:  5.1538190841674805  val loss:  5.944498062133789  val L1 loss:  6.4193\n",
      "epoch:  5   step:  41   train loss:  4.55627965927124  val loss:  6.0505218505859375  val L1 loss:  6.5023\n",
      "epoch:  5   step:  42   train loss:  5.217586040496826  val loss:  6.202051162719727  val L1 loss:  6.6862\n",
      "epoch:  5   step:  43   train loss:  6.027774810791016  val loss:  6.214118480682373  val L1 loss:  6.6815\n",
      "epoch:  5   step:  44   train loss:  7.6677632331848145  val loss:  6.0920329093933105  val L1 loss:  6.5684\n",
      "epoch:  5   step:  45   train loss:  4.883322238922119  val loss:  6.194986820220947  val L1 loss:  6.6519\n",
      "epoch:  5   step:  46   train loss:  4.5640153884887695  val loss:  6.414284706115723  val L1 loss:  6.8846\n",
      "epoch:  5   step:  47   train loss:  4.929571151733398  val loss:  6.238286018371582  val L1 loss:  6.7201\n",
      "epoch:  5   step:  48   train loss:  7.268392086029053  val loss:  5.802124500274658  val L1 loss:  6.2703\n",
      "epoch:  5   step:  49   train loss:  3.8650307655334473  val loss:  6.001139163970947  val L1 loss:  6.467\n",
      "epoch:  5   step:  50   train loss:  4.577374458312988  val loss:  6.506765365600586  val L1 loss:  6.9889\n",
      "epoch:  5   step:  51   train loss:  4.003969192504883  val loss:  7.026749134063721  val L1 loss:  7.5149\n",
      "epoch:  5   step:  52   train loss:  4.011689186096191  val loss:  7.250738143920898  val L1 loss:  7.7376\n",
      "epoch:  5   step:  53   train loss:  4.959078788757324  val loss:  7.025300025939941  val L1 loss:  7.5078\n",
      "epoch:  5   step:  54   train loss:  8.45891284942627  val loss:  6.537874698638916  val L1 loss:  7.0049\n",
      "epoch:  5   step:  55   train loss:  8.001199722290039  val loss:  6.104331970214844  val L1 loss:  6.5964\n",
      "epoch:  5   step:  56   train loss:  3.8763773441314697  val loss:  5.861932277679443  val L1 loss:  6.3247\n",
      "epoch:  5   step:  57   train loss:  4.774054527282715  val loss:  5.879835605621338  val L1 loss:  6.358\n",
      "epoch:  5   step:  58   train loss:  4.420299530029297  val loss:  5.844368934631348  val L1 loss:  6.3188\n",
      "epoch:  5   step:  59   train loss:  5.842866897583008  val loss:  5.734809875488281  val L1 loss:  6.2148\n",
      "epoch:  5   step:  60   train loss:  4.015746116638184  val loss:  5.692006587982178  val L1 loss:  6.1907\n",
      "epoch:  5   step:  61   train loss:  2.9707489013671875  val loss:  5.650362968444824  val L1 loss:  6.1227\n",
      "min_val_loss_print 5.650362968444824\n",
      "epoch:  5   step:  62   train loss:  7.496654033660889  val loss:  5.659759998321533  val L1 loss:  6.1336\n",
      "epoch:  5   step:  63   train loss:  5.251994609832764  val loss:  5.698134422302246  val L1 loss:  6.1724\n",
      "epoch:  5   step:  64   train loss:  5.032717704772949  val loss:  5.704729080200195  val L1 loss:  6.1807\n",
      "epoch:  5   step:  65   train loss:  3.78855562210083  val loss:  5.785340309143066  val L1 loss:  6.2618\n",
      "epoch:  5   step:  66   train loss:  5.644512176513672  val loss:  5.817132472991943  val L1 loss:  6.2955\n",
      "epoch:  5   step:  67   train loss:  4.152681350708008  val loss:  5.864404201507568  val L1 loss:  6.3478\n",
      "epoch:  5   step:  68   train loss:  6.103244781494141  val loss:  5.868624210357666  val L1 loss:  6.3561\n",
      "epoch:  5   step:  69   train loss:  5.440316200256348  val loss:  5.772993087768555  val L1 loss:  6.2303\n",
      "epoch:  5   step:  70   train loss:  6.576430320739746  val loss:  5.7967209815979  val L1 loss:  6.28\n",
      "epoch:  5   step:  71   train loss:  3.844940185546875  val loss:  5.942831516265869  val L1 loss:  6.416\n",
      "epoch:  5   step:  72   train loss:  5.048217296600342  val loss:  5.92994499206543  val L1 loss:  6.4078\n",
      "epoch:  5   step:  73   train loss:  6.219853401184082  val loss:  5.76404333114624  val L1 loss:  6.2353\n",
      "epoch:  5   step:  74   train loss:  3.1755247116088867  val loss:  5.645718097686768  val L1 loss:  6.1282\n",
      "min_val_loss_print 5.645718097686768\n",
      "epoch:  5   step:  75   train loss:  4.414560794830322  val loss:  5.578205585479736  val L1 loss:  6.069\n",
      "min_val_loss_print 5.578205585479736\n",
      "epoch:  5   step:  76   train loss:  5.960946559906006  val loss:  5.569978713989258  val L1 loss:  6.0467\n",
      "min_val_loss_print 5.569978713989258\n",
      "epoch:  5   step:  77   train loss:  2.93101167678833  val loss:  5.565685749053955  val L1 loss:  6.0393\n",
      "min_val_loss_print 5.565685749053955\n",
      "epoch:  5   step:  78   train loss:  5.529538154602051  val loss:  5.554172039031982  val L1 loss:  6.0298\n",
      "min_val_loss_print 5.554172039031982\n",
      "epoch:  5   step:  79   train loss:  4.548402309417725  val loss:  5.648708343505859  val L1 loss:  6.1203\n",
      "epoch:  5   step:  80   train loss:  3.6077795028686523  val loss:  5.768189430236816  val L1 loss:  6.2375\n",
      "epoch:  5   step:  81   train loss:  3.3148422241210938  val loss:  5.9366984367370605  val L1 loss:  6.4059\n",
      "epoch:  5   step:  82   train loss:  4.809226036071777  val loss:  5.80134391784668  val L1 loss:  6.2703\n",
      "epoch:  5   step:  83   train loss:  6.508532524108887  val loss:  5.553467750549316  val L1 loss:  6.0256\n",
      "min_val_loss_print 5.553467750549316\n",
      "epoch:  5   step:  84   train loss:  3.552654266357422  val loss:  5.490324020385742  val L1 loss:  5.9627\n",
      "min_val_loss_print 5.490324020385742\n",
      "epoch:  5   step:  85   train loss:  4.281116485595703  val loss:  5.546054363250732  val L1 loss:  6.0168\n",
      "epoch:  5   step:  86   train loss:  4.797658443450928  val loss:  5.743025779724121  val L1 loss:  6.236\n",
      "epoch:  5   step:  87   train loss:  5.282236099243164  val loss:  6.1966166496276855  val L1 loss:  6.6876\n",
      "epoch:  5   step:  88   train loss:  4.038450241088867  val loss:  6.64140510559082  val L1 loss:  7.1231\n",
      "epoch:  5   step:  89   train loss:  4.662380695343018  val loss:  6.753077030181885  val L1 loss:  7.2329\n",
      "epoch:  5   step:  90   train loss:  5.676456451416016  val loss:  6.857471942901611  val L1 loss:  7.337\n",
      "epoch:  5   step:  91   train loss:  3.716325521469116  val loss:  7.01597785949707  val L1 loss:  7.4871\n",
      "epoch:  5   step:  92   train loss:  3.665179967880249  val loss:  6.878562927246094  val L1 loss:  7.3525\n",
      "epoch:  5   step:  93   train loss:  10.132743835449219  val loss:  6.764710426330566  val L1 loss:  7.2418\n",
      "epoch:  5   step:  94   train loss:  4.200689315795898  val loss:  6.498405456542969  val L1 loss:  6.9839\n",
      "epoch:  5   step:  95   train loss:  4.368683815002441  val loss:  6.189091682434082  val L1 loss:  6.6709\n",
      "epoch:  5   step:  96   train loss:  3.5603201389312744  val loss:  6.05938196182251  val L1 loss:  6.533\n",
      "epoch:  5   step:  97   train loss:  4.241851329803467  val loss:  5.934346675872803  val L1 loss:  6.397\n",
      "epoch:  5   step:  98   train loss:  4.909597396850586  val loss:  5.925520896911621  val L1 loss:  6.4047\n",
      "epoch:  5   step:  99   train loss:  3.8727028369903564  val loss:  5.958326816558838  val L1 loss:  6.4459\n",
      "epoch:  5   step:  100   train loss:  4.913437366485596  val loss:  6.077913284301758  val L1 loss:  6.5488\n",
      "epoch:  5   step:  101   train loss:  4.709622383117676  val loss:  6.282315731048584  val L1 loss:  6.7692\n",
      "epoch:  5   step:  102   train loss:  4.294986248016357  val loss:  6.512535095214844  val L1 loss:  6.9908\n",
      "epoch:  5   step:  103   train loss:  4.97088623046875  val loss:  6.627466678619385  val L1 loss:  7.114\n",
      "epoch:  5   step:  104   train loss:  7.003889083862305  val loss:  6.215001106262207  val L1 loss:  6.6804\n",
      "epoch:  5   step:  105   train loss:  4.874108791351318  val loss:  5.912370681762695  val L1 loss:  6.3748\n",
      "epoch:  5   step:  106   train loss:  4.966170310974121  val loss:  5.727619647979736  val L1 loss:  6.2225\n",
      "epoch:  5   step:  107   train loss:  5.572457790374756  val loss:  5.81596565246582  val L1 loss:  6.3033\n",
      "epoch:  5   step:  108   train loss:  6.085382461547852  val loss:  5.830092430114746  val L1 loss:  6.3228\n",
      "epoch:  5   step:  109   train loss:  5.188839912414551  val loss:  5.908023357391357  val L1 loss:  6.3985\n",
      "epoch:  5   step:  110   train loss:  2.669437885284424  val loss:  5.89011812210083  val L1 loss:  6.3837\n",
      "epoch:  5   step:  111   train loss:  3.951228380203247  val loss:  5.932507038116455  val L1 loss:  6.4278\n",
      "epoch:  5   step:  112   train loss:  6.284050941467285  val loss:  5.882148742675781  val L1 loss:  6.3805\n",
      "epoch:  5   step:  113   train loss:  5.503949165344238  val loss:  5.8188157081604  val L1 loss:  6.311\n",
      "epoch:  5   step:  114   train loss:  4.808637619018555  val loss:  5.695273399353027  val L1 loss:  6.1681\n",
      "epoch:  5   step:  115   train loss:  3.1302103996276855  val loss:  5.689850330352783  val L1 loss:  6.1626\n",
      "epoch:  5   step:  116   train loss:  5.550211429595947  val loss:  5.675466060638428  val L1 loss:  6.146\n",
      "epoch:  5   step:  117   train loss:  5.2113776206970215  val loss:  5.639039993286133  val L1 loss:  6.0968\n",
      "epoch:  5   step:  118   train loss:  5.150558948516846  val loss:  5.632303237915039  val L1 loss:  6.0954\n",
      "epoch:  5   step:  119   train loss:  4.141385555267334  val loss:  5.699769020080566  val L1 loss:  6.1773\n",
      "epoch:  5   step:  120   train loss:  4.043984413146973  val loss:  5.860282897949219  val L1 loss:  6.3327\n",
      "epoch:  5   step:  121   train loss:  4.841025352478027  val loss:  6.002206802368164  val L1 loss:  6.4817\n",
      "epoch:  5   step:  122   train loss:  6.083643913269043  val loss:  5.970080375671387  val L1 loss:  6.4506\n",
      "epoch:  5   step:  123   train loss:  6.491903781890869  val loss:  5.875907897949219  val L1 loss:  6.3659\n",
      "epoch:  5   step:  124   train loss:  4.998549461364746  val loss:  5.83344841003418  val L1 loss:  6.3095\n",
      "epoch:  5   step:  125   train loss:  3.7262039184570312  val loss:  5.909358978271484  val L1 loss:  6.3699\n",
      "epoch:  5   step:  126   train loss:  5.576941013336182  val loss:  6.045183181762695  val L1 loss:  6.5212\n",
      "epoch:  5   step:  127   train loss:  9.56518840789795  val loss:  6.23051118850708  val L1 loss:  6.6883\n",
      "epoch:  5   step:  128   train loss:  5.12093448638916  val loss:  5.95944881439209  val L1 loss:  6.4277\n",
      "epoch:  5   step:  129   train loss:  5.0938215255737305  val loss:  5.665549278259277  val L1 loss:  6.1201\n",
      "epoch:  5   step:  130   train loss:  4.432172775268555  val loss:  5.785302639007568  val L1 loss:  6.2724\n",
      "epoch:  5   step:  131   train loss:  5.221292018890381  val loss:  6.380663871765137  val L1 loss:  6.86\n",
      "epoch:  5   step:  132   train loss:  4.332766056060791  val loss:  7.19763708114624  val L1 loss:  7.6863\n",
      "epoch:  5   step:  133   train loss:  5.676872253417969  val loss:  7.578615665435791  val L1 loss:  8.0644\n",
      "epoch:  5   step:  134   train loss:  5.2350616455078125  val loss:  7.486207008361816  val L1 loss:  7.9746\n",
      "epoch:  5   step:  135   train loss:  6.957381248474121  val loss:  7.155109882354736  val L1 loss:  7.644\n",
      "epoch:  5   step:  136   train loss:  6.8299880027771  val loss:  6.441025733947754  val L1 loss:  6.9282\n",
      "epoch:  5   step:  137   train loss:  6.8994059562683105  val loss:  6.1031622886657715  val L1 loss:  6.5754\n",
      "epoch:  5   step:  138   train loss:  5.389664649963379  val loss:  5.956761360168457  val L1 loss:  6.4433\n",
      "epoch:  5   step:  139   train loss:  4.520204544067383  val loss:  5.996086120605469  val L1 loss:  6.4702\n",
      "epoch:  5   step:  140   train loss:  4.762451171875  val loss:  5.945766448974609  val L1 loss:  6.4233\n",
      "epoch:  5   step:  141   train loss:  5.042621612548828  val loss:  5.8416900634765625  val L1 loss:  6.316\n",
      "epoch:  5   step:  142   train loss:  3.077955722808838  val loss:  5.876009464263916  val L1 loss:  6.3526\n",
      "epoch:  5   step:  143   train loss:  3.386759042739868  val loss:  6.05891227722168  val L1 loss:  6.5392\n",
      "epoch:  5   step:  144   train loss:  4.444746017456055  val loss:  6.402891635894775  val L1 loss:  6.8872\n",
      "epoch:  5   step:  145   train loss:  4.676762580871582  val loss:  6.350141525268555  val L1 loss:  6.8274\n",
      "epoch:  5   step:  146   train loss:  11.780323028564453  val loss:  6.193539142608643  val L1 loss:  6.6634\n",
      "epoch:  5   step:  147   train loss:  8.777358055114746  val loss:  5.99613618850708  val L1 loss:  6.4636\n",
      "epoch:  5   step:  148   train loss:  4.61518669128418  val loss:  5.97669792175293  val L1 loss:  6.4559\n",
      "epoch:  5   step:  149   train loss:  5.82011604309082  val loss:  6.007052898406982  val L1 loss:  6.4592\n",
      "epoch:  5   step:  150   train loss:  5.893438339233398  val loss:  6.127764701843262  val L1 loss:  6.5746\n",
      "epoch:  5   step:  151   train loss:  4.09177303314209  val loss:  6.3393707275390625  val L1 loss:  6.7821\n",
      "epoch:  5   step:  152   train loss:  4.671928405761719  val loss:  6.404842853546143  val L1 loss:  6.8506\n",
      "epoch:  5   step:  153   train loss:  3.9972035884857178  val loss:  6.405833721160889  val L1 loss:  6.886\n",
      "epoch:  5   step:  154   train loss:  4.9030351638793945  val loss:  6.516225337982178  val L1 loss:  6.9982\n",
      "epoch:  5   step:  155   train loss:  5.361763954162598  val loss:  6.321134090423584  val L1 loss:  6.8069\n",
      "epoch:  5   step:  156   train loss:  3.5311810970306396  val loss:  6.341855525970459  val L1 loss:  6.8317\n",
      "epoch:  5   step:  157   train loss:  6.685585975646973  val loss:  5.991987705230713  val L1 loss:  6.4781\n",
      "epoch:  5   step:  158   train loss:  4.598594665527344  val loss:  5.670166015625  val L1 loss:  6.1643\n",
      "epoch:  5   step:  159   train loss:  5.383021831512451  val loss:  5.52310037612915  val L1 loss:  5.9942\n",
      "epoch:  5   step:  160   train loss:  2.77504301071167  val loss:  5.68078088760376  val L1 loss:  6.1391\n",
      "epoch:  5   step:  161   train loss:  6.394312858581543  val loss:  5.816579341888428  val L1 loss:  6.2869\n",
      "epoch:  5   step:  162   train loss:  2.710143804550171  val loss:  5.786990642547607  val L1 loss:  6.2667\n",
      "epoch:  5   step:  163   train loss:  6.176688194274902  val loss:  5.874790668487549  val L1 loss:  6.3541\n",
      "epoch:  5   step:  164   train loss:  3.7289600372314453  val loss:  6.034980297088623  val L1 loss:  6.5192\n",
      "epoch:  5   step:  165   train loss:  5.256953239440918  val loss:  6.158543586730957  val L1 loss:  6.6455\n",
      "epoch:  5   step:  166   train loss:  3.7429373264312744  val loss:  6.153028964996338  val L1 loss:  6.633\n",
      "epoch:  5   step:  167   train loss:  5.563488006591797  val loss:  6.190829753875732  val L1 loss:  6.679\n",
      "epoch:  5   step:  168   train loss:  3.0985405445098877  val loss:  6.232670783996582  val L1 loss:  6.719\n",
      "epoch:  5   step:  169   train loss:  3.1514291763305664  val loss:  6.410173416137695  val L1 loss:  6.8906\n",
      "epoch:  5   step:  170   train loss:  5.611427307128906  val loss:  6.458554744720459  val L1 loss:  6.9317\n",
      "epoch:  5   step:  171   train loss:  8.799995422363281  val loss:  6.509430885314941  val L1 loss:  6.9782\n",
      "epoch:  5   step:  172   train loss:  3.9128775596618652  val loss:  6.745507717132568  val L1 loss:  7.2296\n",
      "epoch:  5   step:  173   train loss:  6.109478950500488  val loss:  7.125187873840332  val L1 loss:  7.619\n",
      "epoch:  5   step:  174   train loss:  5.454912185668945  val loss:  7.168980598449707  val L1 loss:  7.6637\n",
      "epoch:  5   step:  175   train loss:  4.551772117614746  val loss:  6.783796310424805  val L1 loss:  7.2696\n",
      "epoch:  5   step:  176   train loss:  5.563685417175293  val loss:  6.50006103515625  val L1 loss:  6.9704\n",
      "epoch:  5   step:  177   train loss:  4.061701774597168  val loss:  6.42750358581543  val L1 loss:  6.9095\n",
      "epoch:  5   step:  178   train loss:  4.960905075073242  val loss:  6.351373672485352  val L1 loss:  6.824\n",
      "epoch:  5   step:  179   train loss:  5.272371292114258  val loss:  6.385962009429932  val L1 loss:  6.8587\n",
      "epoch:  5   step:  180   train loss:  4.661264419555664  val loss:  6.403573989868164  val L1 loss:  6.8907\n",
      "epoch:  5   step:  181   train loss:  3.9383513927459717  val loss:  6.39132022857666  val L1 loss:  6.8641\n",
      "epoch:  5   step:  182   train loss:  4.110602378845215  val loss:  6.499118328094482  val L1 loss:  6.9926\n",
      "epoch:  5   step:  183   train loss:  4.869746208190918  val loss:  6.475255012512207  val L1 loss:  6.9677\n",
      "epoch:  5   step:  184   train loss:  4.799676418304443  val loss:  6.311039924621582  val L1 loss:  6.7875\n",
      "epoch:  5   step:  185   train loss:  6.966884613037109  val loss:  6.203400611877441  val L1 loss:  6.6831\n",
      "epoch:  5   step:  186   train loss:  2.8561511039733887  val loss:  6.063914775848389  val L1 loss:  6.5399\n",
      "epoch:  5   step:  187   train loss:  4.699419975280762  val loss:  6.04469108581543  val L1 loss:  6.5063\n",
      "epoch:  5   step:  188   train loss:  8.230772018432617  val loss:  6.111551284790039  val L1 loss:  6.5937\n",
      "epoch:  5   step:  189   train loss:  2.705886125564575  val loss:  6.209749698638916  val L1 loss:  6.6955\n",
      "epoch:  5   step:  190   train loss:  5.571505069732666  val loss:  6.359279632568359  val L1 loss:  6.8356\n",
      "epoch:  5   step:  191   train loss:  5.634442329406738  val loss:  6.507835388183594  val L1 loss:  6.9813\n",
      "epoch:  5   step:  192   train loss:  5.428174018859863  val loss:  6.43125581741333  val L1 loss:  6.9039\n",
      "epoch:  5   step:  193   train loss:  5.309146881103516  val loss:  6.275460243225098  val L1 loss:  6.7503\n",
      "epoch:  5   step:  194   train loss:  5.016602993011475  val loss:  6.235402584075928  val L1 loss:  6.7161\n",
      "epoch:  5   step:  195   train loss:  7.046629428863525  val loss:  6.27108097076416  val L1 loss:  6.7556\n",
      "epoch:  5   step:  196   train loss:  4.728550910949707  val loss:  6.212291240692139  val L1 loss:  6.6806\n",
      "epoch:  5   step:  197   train loss:  5.0610551834106445  val loss:  6.177298545837402  val L1 loss:  6.657\n",
      "epoch:  5   step:  198   train loss:  4.525076866149902  val loss:  6.359163761138916  val L1 loss:  6.8489\n",
      "epoch:  5   step:  199   train loss:  3.8026561737060547  val loss:  6.747774600982666  val L1 loss:  7.2374\n",
      "epoch:  5   step:  200   train loss:  5.272985935211182  val loss:  7.1294846534729  val L1 loss:  7.6055\n",
      "epoch:  5   step:  201   train loss:  4.874074935913086  val loss:  7.055335521697998  val L1 loss:  7.54\n",
      "epoch:  5   step:  202   train loss:  4.361227512359619  val loss:  6.811691761016846  val L1 loss:  7.2972\n",
      "epoch:  5   step:  203   train loss:  4.22122859954834  val loss:  6.251778602600098  val L1 loss:  6.7305\n",
      "epoch:  5   step:  204   train loss:  5.9919514656066895  val loss:  5.996951580047607  val L1 loss:  6.4655\n",
      "epoch:  5   step:  205   train loss:  6.239941120147705  val loss:  5.981229305267334  val L1 loss:  6.4614\n",
      "epoch:  5   step:  206   train loss:  5.432745933532715  val loss:  6.047250747680664  val L1 loss:  6.5384\n",
      "epoch:  5   step:  207   train loss:  4.032809257507324  val loss:  6.0089616775512695  val L1 loss:  6.5008\n",
      "epoch:  5   step:  208   train loss:  3.1500368118286133  val loss:  5.915083408355713  val L1 loss:  6.3902\n",
      "epoch:  6   step:  0   train loss:  4.280040264129639  val loss:  6.025513648986816  val L1 loss:  6.4984\n",
      "epoch:  6   step:  1   train loss:  3.1048996448516846  val loss:  6.39698600769043  val L1 loss:  6.885\n",
      "epoch:  6   step:  2   train loss:  5.370048522949219  val loss:  6.382662296295166  val L1 loss:  6.8751\n",
      "epoch:  6   step:  3   train loss:  4.66501522064209  val loss:  6.14736270904541  val L1 loss:  6.6364\n",
      "epoch:  6   step:  4   train loss:  5.272087097167969  val loss:  5.805169105529785  val L1 loss:  6.2756\n",
      "epoch:  6   step:  5   train loss:  4.642391204833984  val loss:  5.661248683929443  val L1 loss:  6.1238\n",
      "epoch:  6   step:  6   train loss:  5.290238380432129  val loss:  5.726660251617432  val L1 loss:  6.201\n",
      "epoch:  6   step:  7   train loss:  4.427928924560547  val loss:  5.884068965911865  val L1 loss:  6.3711\n",
      "epoch:  6   step:  8   train loss:  6.143835067749023  val loss:  5.975368499755859  val L1 loss:  6.4557\n",
      "epoch:  6   step:  9   train loss:  4.035547256469727  val loss:  5.757937908172607  val L1 loss:  6.2322\n",
      "epoch:  6   step:  10   train loss:  7.363509178161621  val loss:  5.472696304321289  val L1 loss:  5.9469\n",
      "min_val_loss_print 5.472696304321289\n",
      "epoch:  6   step:  11   train loss:  6.112192153930664  val loss:  5.516905307769775  val L1 loss:  6.0041\n",
      "epoch:  6   step:  12   train loss:  3.693720579147339  val loss:  5.691618919372559  val L1 loss:  6.1881\n",
      "epoch:  6   step:  13   train loss:  3.787015199661255  val loss:  5.693856716156006  val L1 loss:  6.1887\n",
      "epoch:  6   step:  14   train loss:  5.677406311035156  val loss:  5.548030853271484  val L1 loss:  6.0442\n",
      "epoch:  6   step:  15   train loss:  4.5519890785217285  val loss:  5.475094318389893  val L1 loss:  5.9509\n",
      "epoch:  6   step:  16   train loss:  3.9240951538085938  val loss:  5.616071701049805  val L1 loss:  6.0821\n",
      "epoch:  6   step:  17   train loss:  3.8750224113464355  val loss:  5.954657077789307  val L1 loss:  6.4434\n",
      "epoch:  6   step:  18   train loss:  4.347846984863281  val loss:  6.09791374206543  val L1 loss:  6.5856\n",
      "epoch:  6   step:  19   train loss:  5.620157241821289  val loss:  6.175174236297607  val L1 loss:  6.6529\n",
      "epoch:  6   step:  20   train loss:  3.601494550704956  val loss:  6.231298923492432  val L1 loss:  6.6948\n",
      "epoch:  6   step:  21   train loss:  5.203911304473877  val loss:  5.951503753662109  val L1 loss:  6.4217\n",
      "epoch:  6   step:  22   train loss:  4.216327667236328  val loss:  5.667388916015625  val L1 loss:  6.1549\n",
      "epoch:  6   step:  23   train loss:  4.041330814361572  val loss:  5.669747829437256  val L1 loss:  6.1312\n",
      "epoch:  6   step:  24   train loss:  3.954120397567749  val loss:  5.711938381195068  val L1 loss:  6.1843\n",
      "epoch:  6   step:  25   train loss:  4.083398818969727  val loss:  5.691362380981445  val L1 loss:  6.174\n",
      "epoch:  6   step:  26   train loss:  4.252422332763672  val loss:  5.788561820983887  val L1 loss:  6.2831\n",
      "epoch:  6   step:  27   train loss:  4.469354152679443  val loss:  5.835115909576416  val L1 loss:  6.3127\n",
      "epoch:  6   step:  28   train loss:  6.693886756896973  val loss:  5.807637691497803  val L1 loss:  6.2735\n",
      "epoch:  6   step:  29   train loss:  3.0774519443511963  val loss:  5.769701957702637  val L1 loss:  6.2579\n",
      "epoch:  6   step:  30   train loss:  3.30946946144104  val loss:  5.930252552032471  val L1 loss:  6.4142\n",
      "epoch:  6   step:  31   train loss:  3.241692543029785  val loss:  6.20245361328125  val L1 loss:  6.6977\n",
      "epoch:  6   step:  32   train loss:  6.634305000305176  val loss:  6.504201889038086  val L1 loss:  6.9962\n",
      "epoch:  6   step:  33   train loss:  4.796202182769775  val loss:  6.419599533081055  val L1 loss:  6.9143\n",
      "epoch:  6   step:  34   train loss:  3.8249473571777344  val loss:  6.294691562652588  val L1 loss:  6.7788\n",
      "epoch:  6   step:  35   train loss:  6.15980863571167  val loss:  6.28895378112793  val L1 loss:  6.7644\n",
      "epoch:  6   step:  36   train loss:  3.743530750274658  val loss:  6.212538719177246  val L1 loss:  6.6903\n",
      "epoch:  6   step:  37   train loss:  4.601715087890625  val loss:  6.074295997619629  val L1 loss:  6.5647\n",
      "epoch:  6   step:  38   train loss:  5.398865222930908  val loss:  5.966414451599121  val L1 loss:  6.4312\n",
      "epoch:  6   step:  39   train loss:  7.911831855773926  val loss:  5.944116115570068  val L1 loss:  6.4091\n",
      "epoch:  6   step:  40   train loss:  7.093243598937988  val loss:  6.094073295593262  val L1 loss:  6.561\n",
      "epoch:  6   step:  41   train loss:  4.600970268249512  val loss:  6.209543228149414  val L1 loss:  6.6901\n",
      "epoch:  6   step:  42   train loss:  4.717811584472656  val loss:  6.209460258483887  val L1 loss:  6.6941\n",
      "epoch:  6   step:  43   train loss:  6.5745439529418945  val loss:  6.130059719085693  val L1 loss:  6.6051\n",
      "epoch:  6   step:  44   train loss:  6.311589241027832  val loss:  6.044946670532227  val L1 loss:  6.5247\n",
      "epoch:  6   step:  45   train loss:  4.210851669311523  val loss:  6.044564247131348  val L1 loss:  6.5243\n",
      "epoch:  6   step:  46   train loss:  5.783628940582275  val loss:  6.078009128570557  val L1 loss:  6.5556\n",
      "epoch:  6   step:  47   train loss:  3.580399513244629  val loss:  6.136196136474609  val L1 loss:  6.6045\n",
      "epoch:  6   step:  48   train loss:  5.010148048400879  val loss:  6.161072731018066  val L1 loss:  6.6438\n",
      "epoch:  6   step:  49   train loss:  2.639224052429199  val loss:  6.179854393005371  val L1 loss:  6.6615\n",
      "epoch:  6   step:  50   train loss:  4.497832298278809  val loss:  6.279445648193359  val L1 loss:  6.7626\n",
      "epoch:  6   step:  51   train loss:  4.805415153503418  val loss:  6.414352893829346  val L1 loss:  6.9092\n",
      "epoch:  6   step:  52   train loss:  5.009446620941162  val loss:  6.459238052368164  val L1 loss:  6.9585\n",
      "epoch:  6   step:  53   train loss:  6.059205055236816  val loss:  6.37214994430542  val L1 loss:  6.8718\n",
      "epoch:  6   step:  54   train loss:  2.625178337097168  val loss:  6.182682991027832  val L1 loss:  6.6584\n",
      "epoch:  6   step:  55   train loss:  4.079971790313721  val loss:  6.042558670043945  val L1 loss:  6.518\n",
      "epoch:  6   step:  56   train loss:  4.267533302307129  val loss:  5.943563938140869  val L1 loss:  6.4263\n",
      "epoch:  6   step:  57   train loss:  6.310851097106934  val loss:  5.831445693969727  val L1 loss:  6.3034\n",
      "epoch:  6   step:  58   train loss:  3.086378812789917  val loss:  5.779160022735596  val L1 loss:  6.25\n",
      "epoch:  6   step:  59   train loss:  3.70925235748291  val loss:  5.746698379516602  val L1 loss:  6.2298\n",
      "epoch:  6   step:  60   train loss:  4.341767311096191  val loss:  5.845573425292969  val L1 loss:  6.3281\n",
      "epoch:  6   step:  61   train loss:  5.037581443786621  val loss:  6.064432144165039  val L1 loss:  6.5374\n",
      "epoch:  6   step:  62   train loss:  3.1183621883392334  val loss:  6.139430999755859  val L1 loss:  6.6189\n",
      "epoch:  6   step:  63   train loss:  3.785827159881592  val loss:  6.033576965332031  val L1 loss:  6.5101\n",
      "epoch:  6   step:  64   train loss:  5.095765113830566  val loss:  5.997509479522705  val L1 loss:  6.4633\n",
      "epoch:  6   step:  65   train loss:  4.026762962341309  val loss:  5.989870548248291  val L1 loss:  6.4729\n",
      "epoch:  6   step:  66   train loss:  4.814573287963867  val loss:  6.012477874755859  val L1 loss:  6.4859\n",
      "epoch:  6   step:  67   train loss:  3.2875006198883057  val loss:  6.073714256286621  val L1 loss:  6.5616\n",
      "epoch:  6   step:  68   train loss:  3.7645039558410645  val loss:  6.317493915557861  val L1 loss:  6.7764\n",
      "epoch:  6   step:  69   train loss:  4.549709796905518  val loss:  6.440682411193848  val L1 loss:  6.9095\n",
      "epoch:  6   step:  70   train loss:  5.202683448791504  val loss:  6.277740478515625  val L1 loss:  6.7424\n",
      "epoch:  6   step:  71   train loss:  4.575797080993652  val loss:  6.151801586151123  val L1 loss:  6.6178\n",
      "epoch:  6   step:  72   train loss:  5.811690330505371  val loss:  6.233394622802734  val L1 loss:  6.7333\n",
      "epoch:  6   step:  73   train loss:  5.063616752624512  val loss:  6.323523998260498  val L1 loss:  6.8194\n",
      "epoch:  6   step:  74   train loss:  4.893839359283447  val loss:  6.199212551116943  val L1 loss:  6.6985\n",
      "epoch:  6   step:  75   train loss:  4.811433792114258  val loss:  5.979155540466309  val L1 loss:  6.4712\n",
      "epoch:  6   step:  76   train loss:  4.150399208068848  val loss:  5.88537073135376  val L1 loss:  6.3561\n",
      "epoch:  6   step:  77   train loss:  4.539210319519043  val loss:  5.910583972930908  val L1 loss:  6.3897\n",
      "epoch:  6   step:  78   train loss:  5.629795074462891  val loss:  5.838541030883789  val L1 loss:  6.3106\n",
      "epoch:  6   step:  79   train loss:  2.784416913986206  val loss:  5.756468296051025  val L1 loss:  6.2299\n",
      "epoch:  6   step:  80   train loss:  6.478240489959717  val loss:  5.6902666091918945  val L1 loss:  6.162\n",
      "epoch:  6   step:  81   train loss:  7.102236270904541  val loss:  5.740945339202881  val L1 loss:  6.226\n",
      "epoch:  6   step:  82   train loss:  3.4397482872009277  val loss:  5.862254619598389  val L1 loss:  6.3559\n",
      "epoch:  6   step:  83   train loss:  4.976309299468994  val loss:  6.148106098175049  val L1 loss:  6.6366\n",
      "epoch:  6   step:  84   train loss:  4.054098129272461  val loss:  6.226187705993652  val L1 loss:  6.7201\n",
      "epoch:  6   step:  85   train loss:  3.646115779876709  val loss:  6.274142742156982  val L1 loss:  6.773\n",
      "epoch:  6   step:  86   train loss:  4.2576375007629395  val loss:  6.327797889709473  val L1 loss:  6.8204\n",
      "epoch:  6   step:  87   train loss:  4.896172046661377  val loss:  6.3072404861450195  val L1 loss:  6.796\n",
      "epoch:  6   step:  88   train loss:  4.513119697570801  val loss:  6.3350725173950195  val L1 loss:  6.8272\n",
      "epoch:  6   step:  89   train loss:  5.552143096923828  val loss:  6.53495979309082  val L1 loss:  7.0286\n",
      "epoch:  6   step:  90   train loss:  2.704326629638672  val loss:  6.549278259277344  val L1 loss:  7.0437\n",
      "epoch:  6   step:  91   train loss:  3.354116439819336  val loss:  6.32377815246582  val L1 loss:  6.8124\n",
      "epoch:  6   step:  92   train loss:  3.5535542964935303  val loss:  5.793298721313477  val L1 loss:  6.2823\n",
      "epoch:  6   step:  93   train loss:  4.737204074859619  val loss:  5.451267719268799  val L1 loss:  5.9237\n",
      "min_val_loss_print 5.451267719268799\n",
      "epoch:  6   step:  94   train loss:  4.2230682373046875  val loss:  5.436429977416992  val L1 loss:  5.9071\n",
      "min_val_loss_print 5.436429977416992\n",
      "epoch:  6   step:  95   train loss:  3.9045825004577637  val loss:  5.488642692565918  val L1 loss:  5.9777\n",
      "epoch:  6   step:  96   train loss:  5.973199367523193  val loss:  5.425894737243652  val L1 loss:  5.8986\n",
      "min_val_loss_print 5.425894737243652\n",
      "epoch:  6   step:  97   train loss:  3.7582712173461914  val loss:  5.420307636260986  val L1 loss:  5.9076\n",
      "min_val_loss_print 5.420307636260986\n",
      "epoch:  6   step:  98   train loss:  8.040699005126953  val loss:  5.4879584312438965  val L1 loss:  5.9637\n",
      "epoch:  6   step:  99   train loss:  5.178041458129883  val loss:  5.655328273773193  val L1 loss:  6.1186\n",
      "epoch:  6   step:  100   train loss:  6.527738571166992  val loss:  6.012353420257568  val L1 loss:  6.4949\n",
      "epoch:  6   step:  101   train loss:  5.095468521118164  val loss:  6.162304401397705  val L1 loss:  6.6388\n",
      "epoch:  6   step:  102   train loss:  5.765802383422852  val loss:  6.142698764801025  val L1 loss:  6.6216\n",
      "epoch:  6   step:  103   train loss:  4.156554698944092  val loss:  5.845554828643799  val L1 loss:  6.322\n",
      "epoch:  6   step:  104   train loss:  5.959619998931885  val loss:  5.898835182189941  val L1 loss:  6.3774\n",
      "epoch:  6   step:  105   train loss:  5.532858848571777  val loss:  5.922343730926514  val L1 loss:  6.4093\n",
      "epoch:  6   step:  106   train loss:  5.16900634765625  val loss:  5.896371841430664  val L1 loss:  6.3822\n",
      "epoch:  6   step:  107   train loss:  4.845686435699463  val loss:  5.832520484924316  val L1 loss:  6.3122\n",
      "epoch:  6   step:  108   train loss:  4.596743583679199  val loss:  5.7038254737854  val L1 loss:  6.1917\n",
      "epoch:  6   step:  109   train loss:  7.106645584106445  val loss:  5.638590335845947  val L1 loss:  6.119\n",
      "epoch:  6   step:  110   train loss:  6.2813920974731445  val loss:  5.524680137634277  val L1 loss:  6.0058\n",
      "epoch:  6   step:  111   train loss:  4.555974006652832  val loss:  5.348408222198486  val L1 loss:  5.8145\n",
      "min_val_loss_print 5.348408222198486\n",
      "epoch:  6   step:  112   train loss:  5.021728992462158  val loss:  5.16895866394043  val L1 loss:  5.6181\n",
      "min_val_loss_print 5.16895866394043\n",
      "epoch:  6   step:  113   train loss:  6.442464828491211  val loss:  5.235539436340332  val L1 loss:  5.7033\n",
      "epoch:  6   step:  114   train loss:  6.341473579406738  val loss:  5.254052639007568  val L1 loss:  5.7264\n",
      "epoch:  6   step:  115   train loss:  3.4371211528778076  val loss:  5.251696586608887  val L1 loss:  5.7206\n",
      "epoch:  6   step:  116   train loss:  5.266509532928467  val loss:  5.249963760375977  val L1 loss:  5.7187\n",
      "epoch:  6   step:  117   train loss:  5.462150573730469  val loss:  5.121674537658691  val L1 loss:  5.5787\n",
      "min_val_loss_print 5.121674537658691\n",
      "epoch:  6   step:  118   train loss:  3.6394710540771484  val loss:  5.092501640319824  val L1 loss:  5.5524\n",
      "min_val_loss_print 5.092501640319824\n",
      "epoch:  6   step:  119   train loss:  2.6595540046691895  val loss:  5.097926139831543  val L1 loss:  5.5609\n",
      "epoch:  6   step:  120   train loss:  5.53558349609375  val loss:  5.108036518096924  val L1 loss:  5.5697\n",
      "epoch:  6   step:  121   train loss:  6.43874979019165  val loss:  5.106263637542725  val L1 loss:  5.5586\n",
      "epoch:  6   step:  122   train loss:  5.2033305168151855  val loss:  5.145761013031006  val L1 loss:  5.6103\n",
      "epoch:  6   step:  123   train loss:  4.038804531097412  val loss:  5.170619487762451  val L1 loss:  5.6373\n",
      "epoch:  6   step:  124   train loss:  5.940842628479004  val loss:  5.395715236663818  val L1 loss:  5.8581\n",
      "epoch:  6   step:  125   train loss:  6.567456245422363  val loss:  5.636528015136719  val L1 loss:  6.0861\n",
      "epoch:  6   step:  126   train loss:  7.00156307220459  val loss:  5.639293193817139  val L1 loss:  6.091\n",
      "epoch:  6   step:  127   train loss:  3.5181150436401367  val loss:  5.447932720184326  val L1 loss:  5.913\n",
      "epoch:  6   step:  128   train loss:  5.094844818115234  val loss:  5.316525936126709  val L1 loss:  5.7621\n",
      "epoch:  6   step:  129   train loss:  4.661629676818848  val loss:  5.3567609786987305  val L1 loss:  5.817\n",
      "epoch:  6   step:  130   train loss:  3.907904863357544  val loss:  5.460071086883545  val L1 loss:  5.9325\n",
      "epoch:  6   step:  131   train loss:  6.700727462768555  val loss:  5.500450134277344  val L1 loss:  5.9792\n",
      "epoch:  6   step:  132   train loss:  2.514904022216797  val loss:  5.424962043762207  val L1 loss:  5.8959\n",
      "epoch:  6   step:  133   train loss:  4.373703479766846  val loss:  5.346568584442139  val L1 loss:  5.8283\n",
      "epoch:  6   step:  134   train loss:  4.311282157897949  val loss:  5.328794479370117  val L1 loss:  5.8069\n",
      "epoch:  6   step:  135   train loss:  4.498522758483887  val loss:  5.265520095825195  val L1 loss:  5.7248\n",
      "epoch:  6   step:  136   train loss:  5.2066569328308105  val loss:  5.487491130828857  val L1 loss:  5.9695\n",
      "epoch:  6   step:  137   train loss:  1.4931793212890625  val loss:  5.652782917022705  val L1 loss:  6.1286\n",
      "epoch:  6   step:  138   train loss:  5.78647518157959  val loss:  5.936118125915527  val L1 loss:  6.4273\n",
      "epoch:  6   step:  139   train loss:  4.870221138000488  val loss:  6.154411315917969  val L1 loss:  6.64\n",
      "epoch:  6   step:  140   train loss:  2.978147029876709  val loss:  6.204725742340088  val L1 loss:  6.6749\n",
      "epoch:  6   step:  141   train loss:  7.476466178894043  val loss:  6.461785793304443  val L1 loss:  6.9258\n",
      "epoch:  6   step:  142   train loss:  3.2705540657043457  val loss:  6.639821529388428  val L1 loss:  7.1144\n",
      "epoch:  6   step:  143   train loss:  5.267151832580566  val loss:  6.75589656829834  val L1 loss:  7.2239\n",
      "epoch:  6   step:  144   train loss:  7.184876441955566  val loss:  7.032294750213623  val L1 loss:  7.5201\n",
      "epoch:  6   step:  145   train loss:  5.13411808013916  val loss:  6.753342151641846  val L1 loss:  7.2358\n",
      "epoch:  6   step:  146   train loss:  3.8390355110168457  val loss:  6.276397705078125  val L1 loss:  6.7514\n",
      "epoch:  6   step:  147   train loss:  3.052427053451538  val loss:  5.873890399932861  val L1 loss:  6.3471\n",
      "epoch:  6   step:  148   train loss:  4.846584320068359  val loss:  5.673689842224121  val L1 loss:  6.1656\n",
      "epoch:  6   step:  149   train loss:  5.431354522705078  val loss:  5.500222206115723  val L1 loss:  5.9737\n",
      "epoch:  6   step:  150   train loss:  3.0091094970703125  val loss:  5.474891185760498  val L1 loss:  5.9451\n",
      "epoch:  6   step:  151   train loss:  4.282061576843262  val loss:  5.58028507232666  val L1 loss:  6.0539\n",
      "epoch:  6   step:  152   train loss:  4.174807548522949  val loss:  5.6501030921936035  val L1 loss:  6.1196\n",
      "epoch:  6   step:  153   train loss:  4.283806800842285  val loss:  5.72246789932251  val L1 loss:  6.1996\n",
      "epoch:  6   step:  154   train loss:  4.755167484283447  val loss:  5.7190070152282715  val L1 loss:  6.1936\n",
      "epoch:  6   step:  155   train loss:  4.47927188873291  val loss:  5.696278095245361  val L1 loss:  6.1821\n",
      "epoch:  6   step:  156   train loss:  4.450782775878906  val loss:  5.616952896118164  val L1 loss:  6.1016\n",
      "epoch:  6   step:  157   train loss:  4.415459632873535  val loss:  5.635659694671631  val L1 loss:  6.1074\n",
      "epoch:  6   step:  158   train loss:  4.360570907592773  val loss:  5.633093357086182  val L1 loss:  6.114\n",
      "epoch:  6   step:  159   train loss:  3.7622857093811035  val loss:  5.64616060256958  val L1 loss:  6.1288\n",
      "epoch:  6   step:  160   train loss:  6.878937721252441  val loss:  5.6846747398376465  val L1 loss:  6.1647\n",
      "epoch:  6   step:  161   train loss:  4.4610772132873535  val loss:  5.678781032562256  val L1 loss:  6.1599\n",
      "epoch:  6   step:  162   train loss:  6.261735916137695  val loss:  5.698026180267334  val L1 loss:  6.1951\n",
      "epoch:  6   step:  163   train loss:  2.9472086429595947  val loss:  5.813870906829834  val L1 loss:  6.2903\n",
      "epoch:  6   step:  164   train loss:  7.485687732696533  val loss:  5.837857723236084  val L1 loss:  6.3121\n",
      "epoch:  6   step:  165   train loss:  4.9526896476745605  val loss:  5.853018760681152  val L1 loss:  6.3221\n",
      "epoch:  6   step:  166   train loss:  6.531163215637207  val loss:  5.826688766479492  val L1 loss:  6.3094\n",
      "epoch:  6   step:  167   train loss:  3.642958641052246  val loss:  5.646933555603027  val L1 loss:  6.1171\n",
      "epoch:  6   step:  168   train loss:  3.738947868347168  val loss:  5.481350421905518  val L1 loss:  5.9607\n",
      "epoch:  6   step:  169   train loss:  2.7316009998321533  val loss:  5.525073051452637  val L1 loss:  5.9988\n",
      "epoch:  6   step:  170   train loss:  6.648530006408691  val loss:  5.55936336517334  val L1 loss:  6.0372\n",
      "epoch:  6   step:  171   train loss:  4.229403495788574  val loss:  5.58640718460083  val L1 loss:  6.0689\n",
      "epoch:  6   step:  172   train loss:  3.2577357292175293  val loss:  5.368210315704346  val L1 loss:  5.8506\n",
      "epoch:  6   step:  173   train loss:  3.219956398010254  val loss:  5.36267614364624  val L1 loss:  5.8414\n",
      "epoch:  6   step:  174   train loss:  6.239926338195801  val loss:  5.448530197143555  val L1 loss:  5.9396\n",
      "epoch:  6   step:  175   train loss:  4.090192794799805  val loss:  5.462131023406982  val L1 loss:  5.9466\n",
      "epoch:  6   step:  176   train loss:  3.419858932495117  val loss:  5.435933589935303  val L1 loss:  5.9197\n",
      "epoch:  6   step:  177   train loss:  2.7716312408447266  val loss:  5.4425225257873535  val L1 loss:  5.9173\n",
      "epoch:  6   step:  178   train loss:  5.297410011291504  val loss:  5.4988694190979  val L1 loss:  5.971\n",
      "epoch:  6   step:  179   train loss:  4.666013717651367  val loss:  5.587237358093262  val L1 loss:  6.0626\n",
      "epoch:  6   step:  180   train loss:  5.82597541809082  val loss:  5.694586753845215  val L1 loss:  6.1778\n",
      "epoch:  6   step:  181   train loss:  6.319768905639648  val loss:  5.736066818237305  val L1 loss:  6.2163\n",
      "epoch:  6   step:  182   train loss:  4.172397136688232  val loss:  5.72004508972168  val L1 loss:  6.2029\n",
      "epoch:  6   step:  183   train loss:  5.057948589324951  val loss:  5.920072078704834  val L1 loss:  6.3912\n",
      "epoch:  6   step:  184   train loss:  4.510669231414795  val loss:  6.582840442657471  val L1 loss:  7.0706\n",
      "epoch:  6   step:  185   train loss:  6.261481761932373  val loss:  7.015053749084473  val L1 loss:  7.4961\n",
      "epoch:  6   step:  186   train loss:  4.895603179931641  val loss:  6.9770660400390625  val L1 loss:  7.4673\n",
      "epoch:  6   step:  187   train loss:  5.1921796798706055  val loss:  6.436159133911133  val L1 loss:  6.929\n",
      "epoch:  6   step:  188   train loss:  4.742829322814941  val loss:  6.069417476654053  val L1 loss:  6.5499\n",
      "epoch:  6   step:  189   train loss:  3.774083137512207  val loss:  6.055093765258789  val L1 loss:  6.5349\n",
      "epoch:  6   step:  190   train loss:  5.396988868713379  val loss:  5.919992446899414  val L1 loss:  6.399\n",
      "epoch:  6   step:  191   train loss:  6.668731212615967  val loss:  5.628294467926025  val L1 loss:  6.1081\n",
      "epoch:  6   step:  192   train loss:  8.679666519165039  val loss:  5.645848274230957  val L1 loss:  6.119\n",
      "epoch:  6   step:  193   train loss:  4.648521900177002  val loss:  6.131606101989746  val L1 loss:  6.6047\n",
      "epoch:  6   step:  194   train loss:  4.96660041809082  val loss:  6.86702299118042  val L1 loss:  7.3517\n",
      "epoch:  6   step:  195   train loss:  5.790412425994873  val loss:  7.003493309020996  val L1 loss:  7.4918\n",
      "epoch:  6   step:  196   train loss:  4.887500762939453  val loss:  6.6578240394592285  val L1 loss:  7.1462\n",
      "epoch:  6   step:  197   train loss:  6.496092796325684  val loss:  5.811094284057617  val L1 loss:  6.2803\n",
      "epoch:  6   step:  198   train loss:  5.379854202270508  val loss:  5.470293045043945  val L1 loss:  5.9439\n",
      "epoch:  6   step:  199   train loss:  2.8294034004211426  val loss:  5.621130466461182  val L1 loss:  6.092\n",
      "epoch:  6   step:  200   train loss:  6.513958930969238  val loss:  5.701948165893555  val L1 loss:  6.1681\n",
      "epoch:  6   step:  201   train loss:  6.315776824951172  val loss:  5.549897193908691  val L1 loss:  6.039\n",
      "epoch:  6   step:  202   train loss:  8.17201042175293  val loss:  5.727564334869385  val L1 loss:  6.2074\n",
      "epoch:  6   step:  203   train loss:  4.715325832366943  val loss:  6.498296737670898  val L1 loss:  6.9847\n",
      "epoch:  6   step:  204   train loss:  5.987882614135742  val loss:  6.903397083282471  val L1 loss:  7.4014\n",
      "epoch:  6   step:  205   train loss:  4.758478164672852  val loss:  6.841435432434082  val L1 loss:  7.3346\n",
      "epoch:  6   step:  206   train loss:  5.625672340393066  val loss:  6.298835277557373  val L1 loss:  6.7825\n",
      "epoch:  6   step:  207   train loss:  3.563577175140381  val loss:  5.714813232421875  val L1 loss:  6.1896\n",
      "epoch:  6   step:  208   train loss:  4.434987545013428  val loss:  5.599838733673096  val L1 loss:  6.0677\n",
      "epoch:  7   step:  0   train loss:  4.6546220779418945  val loss:  5.641940116882324  val L1 loss:  6.1191\n",
      "epoch:  7   step:  1   train loss:  5.494813442230225  val loss:  5.591497898101807  val L1 loss:  6.0556\n",
      "epoch:  7   step:  2   train loss:  4.956711292266846  val loss:  5.531562328338623  val L1 loss:  6.001\n",
      "epoch:  7   step:  3   train loss:  5.545914649963379  val loss:  5.545598030090332  val L1 loss:  6.0289\n",
      "epoch:  7   step:  4   train loss:  4.925725936889648  val loss:  5.668864727020264  val L1 loss:  6.1376\n",
      "epoch:  7   step:  5   train loss:  3.4643633365631104  val loss:  5.783052921295166  val L1 loss:  6.2377\n",
      "epoch:  7   step:  6   train loss:  5.840260982513428  val loss:  5.7940354347229  val L1 loss:  6.2597\n",
      "epoch:  7   step:  7   train loss:  3.9619674682617188  val loss:  5.780667781829834  val L1 loss:  6.2425\n",
      "epoch:  7   step:  8   train loss:  4.175461292266846  val loss:  5.685786724090576  val L1 loss:  6.1391\n",
      "epoch:  7   step:  9   train loss:  6.643770217895508  val loss:  5.694018363952637  val L1 loss:  6.1648\n",
      "epoch:  7   step:  10   train loss:  5.557353496551514  val loss:  5.699009895324707  val L1 loss:  6.1772\n",
      "epoch:  7   step:  11   train loss:  3.529916286468506  val loss:  5.719955921173096  val L1 loss:  6.2155\n",
      "epoch:  7   step:  12   train loss:  7.52161979675293  val loss:  5.686651706695557  val L1 loss:  6.1811\n",
      "epoch:  7   step:  13   train loss:  7.758048057556152  val loss:  5.632649898529053  val L1 loss:  6.1295\n",
      "epoch:  7   step:  14   train loss:  4.8400726318359375  val loss:  5.556535720825195  val L1 loss:  6.0528\n",
      "epoch:  7   step:  15   train loss:  3.1298670768737793  val loss:  5.454580783843994  val L1 loss:  5.9515\n",
      "epoch:  7   step:  16   train loss:  3.279162645339966  val loss:  5.331912040710449  val L1 loss:  5.8231\n",
      "epoch:  7   step:  17   train loss:  3.02252197265625  val loss:  5.261584281921387  val L1 loss:  5.748\n",
      "epoch:  7   step:  18   train loss:  4.44322395324707  val loss:  5.284385681152344  val L1 loss:  5.7674\n",
      "epoch:  7   step:  19   train loss:  4.000209331512451  val loss:  5.558747291564941  val L1 loss:  6.024\n",
      "epoch:  7   step:  20   train loss:  4.2221479415893555  val loss:  6.00480318069458  val L1 loss:  6.4954\n",
      "epoch:  7   step:  21   train loss:  6.013363838195801  val loss:  6.45683479309082  val L1 loss:  6.9414\n",
      "epoch:  7   step:  22   train loss:  4.780254364013672  val loss:  6.291680335998535  val L1 loss:  6.7735\n",
      "epoch:  7   step:  23   train loss:  4.0666422843933105  val loss:  5.941603183746338  val L1 loss:  6.4238\n",
      "epoch:  7   step:  24   train loss:  6.964646339416504  val loss:  5.768008708953857  val L1 loss:  6.2439\n",
      "epoch:  7   step:  25   train loss:  5.085274696350098  val loss:  5.567256927490234  val L1 loss:  6.0476\n",
      "epoch:  7   step:  26   train loss:  5.138218402862549  val loss:  5.409446716308594  val L1 loss:  5.8752\n",
      "epoch:  7   step:  27   train loss:  2.9656805992126465  val loss:  5.288278579711914  val L1 loss:  5.7399\n",
      "epoch:  7   step:  28   train loss:  2.7281455993652344  val loss:  5.229198455810547  val L1 loss:  5.7079\n",
      "epoch:  7   step:  29   train loss:  3.6513609886169434  val loss:  5.237431526184082  val L1 loss:  5.708\n",
      "epoch:  7   step:  30   train loss:  3.5505735874176025  val loss:  5.318118572235107  val L1 loss:  5.7959\n",
      "epoch:  7   step:  31   train loss:  2.9855127334594727  val loss:  5.432346820831299  val L1 loss:  5.9117\n",
      "epoch:  7   step:  32   train loss:  4.575164794921875  val loss:  5.515375137329102  val L1 loss:  5.9971\n",
      "epoch:  7   step:  33   train loss:  4.963427543640137  val loss:  5.477811336517334  val L1 loss:  5.9579\n",
      "epoch:  7   step:  34   train loss:  4.917583465576172  val loss:  5.58161735534668  val L1 loss:  6.0676\n",
      "epoch:  7   step:  35   train loss:  6.489253044128418  val loss:  5.997819900512695  val L1 loss:  6.481\n",
      "epoch:  7   step:  36   train loss:  4.563040256500244  val loss:  6.207687854766846  val L1 loss:  6.6757\n",
      "epoch:  7   step:  37   train loss:  3.230276107788086  val loss:  6.180888652801514  val L1 loss:  6.6647\n",
      "epoch:  7   step:  38   train loss:  5.851357460021973  val loss:  6.138458728790283  val L1 loss:  6.6161\n",
      "epoch:  7   step:  39   train loss:  4.829868316650391  val loss:  6.083488464355469  val L1 loss:  6.5511\n",
      "epoch:  7   step:  40   train loss:  3.653707504272461  val loss:  6.116676330566406  val L1 loss:  6.5873\n",
      "epoch:  7   step:  41   train loss:  3.5352396965026855  val loss:  6.080440521240234  val L1 loss:  6.5503\n",
      "epoch:  7   step:  42   train loss:  2.5196480751037598  val loss:  5.833716869354248  val L1 loss:  6.2963\n",
      "epoch:  7   step:  43   train loss:  3.2024521827697754  val loss:  5.756048679351807  val L1 loss:  6.2272\n",
      "epoch:  7   step:  44   train loss:  7.491186141967773  val loss:  5.810812950134277  val L1 loss:  6.3018\n",
      "epoch:  7   step:  45   train loss:  3.8509817123413086  val loss:  5.911414623260498  val L1 loss:  6.3829\n",
      "epoch:  7   step:  46   train loss:  6.3526177406311035  val loss:  5.9528489112854  val L1 loss:  6.4343\n",
      "epoch:  7   step:  47   train loss:  5.9470367431640625  val loss:  6.00063943862915  val L1 loss:  6.4725\n",
      "epoch:  7   step:  48   train loss:  4.516840934753418  val loss:  5.977967262268066  val L1 loss:  6.4622\n",
      "epoch:  7   step:  49   train loss:  4.657370567321777  val loss:  5.831199645996094  val L1 loss:  6.3167\n",
      "epoch:  7   step:  50   train loss:  3.9511189460754395  val loss:  5.815079212188721  val L1 loss:  6.2983\n",
      "epoch:  7   step:  51   train loss:  4.271130084991455  val loss:  5.699018478393555  val L1 loss:  6.1926\n",
      "epoch:  7   step:  52   train loss:  2.2938780784606934  val loss:  5.537348747253418  val L1 loss:  6.0175\n",
      "epoch:  7   step:  53   train loss:  4.28653621673584  val loss:  5.506788730621338  val L1 loss:  5.9765\n",
      "epoch:  7   step:  54   train loss:  6.338364124298096  val loss:  5.813260555267334  val L1 loss:  6.2969\n",
      "epoch:  7   step:  55   train loss:  3.8675899505615234  val loss:  6.02004861831665  val L1 loss:  6.5006\n",
      "epoch:  7   step:  56   train loss:  2.9009904861450195  val loss:  6.057675838470459  val L1 loss:  6.5382\n",
      "epoch:  7   step:  57   train loss:  5.9124436378479  val loss:  5.750176429748535  val L1 loss:  6.2359\n",
      "epoch:  7   step:  58   train loss:  4.223641395568848  val loss:  5.539243221282959  val L1 loss:  6.0264\n",
      "epoch:  7   step:  59   train loss:  5.195491313934326  val loss:  5.566929340362549  val L1 loss:  6.0442\n",
      "epoch:  7   step:  60   train loss:  3.6037065982818604  val loss:  5.587703704833984  val L1 loss:  6.062\n",
      "epoch:  7   step:  61   train loss:  5.446980953216553  val loss:  5.666449069976807  val L1 loss:  6.1515\n",
      "epoch:  7   step:  62   train loss:  3.277104377746582  val loss:  5.7311248779296875  val L1 loss:  6.2086\n",
      "epoch:  7   step:  63   train loss:  3.173767566680908  val loss:  5.671166896820068  val L1 loss:  6.1569\n",
      "epoch:  7   step:  64   train loss:  5.062272548675537  val loss:  5.640584468841553  val L1 loss:  6.1193\n",
      "epoch:  7   step:  65   train loss:  4.5953450202941895  val loss:  5.69903039932251  val L1 loss:  6.1811\n",
      "epoch:  7   step:  66   train loss:  4.351524353027344  val loss:  5.7637786865234375  val L1 loss:  6.236\n",
      "epoch:  7   step:  67   train loss:  4.466246604919434  val loss:  5.645231246948242  val L1 loss:  6.1183\n",
      "epoch:  7   step:  68   train loss:  4.868521690368652  val loss:  5.511765003204346  val L1 loss:  5.9857\n",
      "epoch:  7   step:  69   train loss:  3.801222562789917  val loss:  5.676318645477295  val L1 loss:  6.1646\n",
      "epoch:  7   step:  70   train loss:  3.102640151977539  val loss:  5.853069305419922  val L1 loss:  6.31\n",
      "epoch:  7   step:  71   train loss:  6.989956855773926  val loss:  5.988437175750732  val L1 loss:  6.4474\n",
      "epoch:  7   step:  72   train loss:  3.706047534942627  val loss:  5.919485569000244  val L1 loss:  6.3755\n",
      "epoch:  7   step:  73   train loss:  4.524195194244385  val loss:  5.783927917480469  val L1 loss:  6.2597\n",
      "epoch:  7   step:  74   train loss:  6.543185234069824  val loss:  5.734312534332275  val L1 loss:  6.2062\n",
      "epoch:  7   step:  75   train loss:  3.4679160118103027  val loss:  5.807833194732666  val L1 loss:  6.282\n",
      "epoch:  7   step:  76   train loss:  5.185151100158691  val loss:  5.7562994956970215  val L1 loss:  6.2262\n",
      "epoch:  7   step:  77   train loss:  5.344566822052002  val loss:  5.633969783782959  val L1 loss:  6.1066\n",
      "epoch:  7   step:  78   train loss:  5.613863945007324  val loss:  5.789167881011963  val L1 loss:  6.2663\n",
      "epoch:  7   step:  79   train loss:  6.207772731781006  val loss:  5.93450403213501  val L1 loss:  6.4166\n",
      "epoch:  7   step:  80   train loss:  7.540374279022217  val loss:  5.874487400054932  val L1 loss:  6.3556\n",
      "epoch:  7   step:  81   train loss:  5.750336170196533  val loss:  5.621360778808594  val L1 loss:  6.0928\n",
      "epoch:  7   step:  82   train loss:  5.641689300537109  val loss:  5.485390663146973  val L1 loss:  5.962\n",
      "epoch:  7   step:  83   train loss:  3.812429428100586  val loss:  5.469698429107666  val L1 loss:  5.9569\n",
      "epoch:  7   step:  84   train loss:  3.8789587020874023  val loss:  5.483332633972168  val L1 loss:  5.9403\n",
      "epoch:  7   step:  85   train loss:  3.949845552444458  val loss:  5.729450702667236  val L1 loss:  6.2187\n",
      "epoch:  7   step:  86   train loss:  2.6902453899383545  val loss:  6.058106422424316  val L1 loss:  6.5428\n",
      "epoch:  7   step:  87   train loss:  4.593250274658203  val loss:  6.240389347076416  val L1 loss:  6.724\n",
      "epoch:  7   step:  88   train loss:  9.275747299194336  val loss:  5.869940280914307  val L1 loss:  6.337\n",
      "epoch:  7   step:  89   train loss:  4.08495569229126  val loss:  5.484943866729736  val L1 loss:  5.9641\n",
      "epoch:  7   step:  90   train loss:  5.870652675628662  val loss:  5.368322849273682  val L1 loss:  5.8506\n",
      "epoch:  7   step:  91   train loss:  5.269780158996582  val loss:  5.435483455657959  val L1 loss:  5.9036\n",
      "epoch:  7   step:  92   train loss:  3.586496114730835  val loss:  5.631527423858643  val L1 loss:  6.1023\n",
      "epoch:  7   step:  93   train loss:  5.070697784423828  val loss:  5.821180820465088  val L1 loss:  6.3065\n",
      "epoch:  7   step:  94   train loss:  3.5296988487243652  val loss:  5.922300815582275  val L1 loss:  6.4039\n",
      "epoch:  7   step:  95   train loss:  5.8026556968688965  val loss:  5.635786533355713  val L1 loss:  6.1167\n",
      "epoch:  7   step:  96   train loss:  5.87659215927124  val loss:  5.5721435546875  val L1 loss:  6.0373\n",
      "epoch:  7   step:  97   train loss:  5.813182830810547  val loss:  5.8093791007995605  val L1 loss:  6.2812\n",
      "epoch:  7   step:  98   train loss:  4.232698440551758  val loss:  5.921491622924805  val L1 loss:  6.4025\n",
      "epoch:  7   step:  99   train loss:  5.9047651290893555  val loss:  5.761483192443848  val L1 loss:  6.2205\n",
      "epoch:  7   step:  100   train loss:  5.052211761474609  val loss:  5.533625602722168  val L1 loss:  6.0106\n",
      "epoch:  7   step:  101   train loss:  1.885819911956787  val loss:  5.425379753112793  val L1 loss:  5.9006\n",
      "epoch:  7   step:  102   train loss:  4.740765571594238  val loss:  5.447030067443848  val L1 loss:  5.9353\n",
      "epoch:  7   step:  103   train loss:  4.9537787437438965  val loss:  5.491006374359131  val L1 loss:  5.9871\n",
      "epoch:  7   step:  104   train loss:  3.601956844329834  val loss:  5.482043266296387  val L1 loss:  5.9678\n",
      "epoch:  7   step:  105   train loss:  5.429490089416504  val loss:  5.494515419006348  val L1 loss:  5.9922\n",
      "epoch:  7   step:  106   train loss:  4.561635494232178  val loss:  5.520473003387451  val L1 loss:  6.0048\n",
      "epoch:  7   step:  107   train loss:  3.1177639961242676  val loss:  5.497405052185059  val L1 loss:  5.9733\n",
      "epoch:  7   step:  108   train loss:  2.735567092895508  val loss:  5.47327995300293  val L1 loss:  5.9496\n",
      "epoch:  7   step:  109   train loss:  3.9345340728759766  val loss:  5.489962100982666  val L1 loss:  5.9757\n",
      "epoch:  7   step:  110   train loss:  3.718358039855957  val loss:  5.526019096374512  val L1 loss:  6.0068\n",
      "epoch:  7   step:  111   train loss:  4.892020225524902  val loss:  5.709578514099121  val L1 loss:  6.1821\n",
      "epoch:  7   step:  112   train loss:  6.262377738952637  val loss:  5.732112884521484  val L1 loss:  6.2155\n",
      "epoch:  7   step:  113   train loss:  5.4297051429748535  val loss:  5.51399040222168  val L1 loss:  6.0054\n",
      "epoch:  7   step:  114   train loss:  2.7313413619995117  val loss:  5.20643424987793  val L1 loss:  5.6853\n",
      "epoch:  7   step:  115   train loss:  6.288917541503906  val loss:  5.100025653839111  val L1 loss:  5.5723\n",
      "epoch:  7   step:  116   train loss:  3.535261631011963  val loss:  5.156346321105957  val L1 loss:  5.6313\n",
      "epoch:  7   step:  117   train loss:  4.332756519317627  val loss:  5.216734409332275  val L1 loss:  5.6797\n",
      "epoch:  7   step:  118   train loss:  2.979057788848877  val loss:  5.25398588180542  val L1 loss:  5.7076\n",
      "epoch:  7   step:  119   train loss:  2.771723985671997  val loss:  5.30943489074707  val L1 loss:  5.7921\n",
      "epoch:  7   step:  120   train loss:  4.426672458648682  val loss:  5.38765811920166  val L1 loss:  5.876\n",
      "epoch:  7   step:  121   train loss:  3.7842822074890137  val loss:  5.452415466308594  val L1 loss:  5.9368\n",
      "epoch:  7   step:  122   train loss:  5.4538774490356445  val loss:  5.469743728637695  val L1 loss:  5.9562\n",
      "epoch:  7   step:  123   train loss:  6.666747093200684  val loss:  5.423579216003418  val L1 loss:  5.9165\n",
      "epoch:  7   step:  124   train loss:  3.9262118339538574  val loss:  5.388415336608887  val L1 loss:  5.8565\n",
      "epoch:  7   step:  125   train loss:  3.975863456726074  val loss:  5.397027492523193  val L1 loss:  5.8818\n",
      "epoch:  7   step:  126   train loss:  3.709780693054199  val loss:  5.3241472244262695  val L1 loss:  5.7892\n",
      "epoch:  7   step:  127   train loss:  4.01777458190918  val loss:  5.35230827331543  val L1 loss:  5.8322\n",
      "epoch:  7   step:  128   train loss:  3.2616255283355713  val loss:  5.371528625488281  val L1 loss:  5.8521\n",
      "epoch:  7   step:  129   train loss:  6.618128776550293  val loss:  5.352225303649902  val L1 loss:  5.8236\n",
      "epoch:  7   step:  130   train loss:  4.591595649719238  val loss:  5.522535800933838  val L1 loss:  5.9827\n",
      "epoch:  7   step:  131   train loss:  6.510461807250977  val loss:  5.976579666137695  val L1 loss:  6.4571\n",
      "epoch:  7   step:  132   train loss:  5.080014228820801  val loss:  6.115784645080566  val L1 loss:  6.5835\n",
      "epoch:  7   step:  133   train loss:  5.246087074279785  val loss:  6.0677666664123535  val L1 loss:  6.5415\n",
      "epoch:  7   step:  134   train loss:  4.796087741851807  val loss:  5.66875696182251  val L1 loss:  6.1473\n",
      "epoch:  7   step:  135   train loss:  2.8305139541625977  val loss:  5.471756458282471  val L1 loss:  5.9496\n",
      "epoch:  7   step:  136   train loss:  3.8871188163757324  val loss:  5.5755743980407715  val L1 loss:  6.0551\n",
      "epoch:  7   step:  137   train loss:  6.096917629241943  val loss:  5.476964473724365  val L1 loss:  5.9544\n",
      "epoch:  7   step:  138   train loss:  4.010612487792969  val loss:  5.367790222167969  val L1 loss:  5.8042\n",
      "epoch:  7   step:  139   train loss:  3.621837615966797  val loss:  5.572957515716553  val L1 loss:  6.0489\n",
      "epoch:  7   step:  140   train loss:  3.677921772003174  val loss:  5.821027755737305  val L1 loss:  6.3123\n",
      "epoch:  7   step:  141   train loss:  4.091127395629883  val loss:  5.618741512298584  val L1 loss:  6.093\n",
      "epoch:  7   step:  142   train loss:  3.955087661743164  val loss:  5.281862258911133  val L1 loss:  5.768\n",
      "epoch:  7   step:  143   train loss:  4.172540187835693  val loss:  5.429420471191406  val L1 loss:  5.8973\n",
      "epoch:  7   step:  144   train loss:  3.4386050701141357  val loss:  5.990129470825195  val L1 loss:  6.4657\n",
      "epoch:  7   step:  145   train loss:  3.6936140060424805  val loss:  6.23869514465332  val L1 loss:  6.7278\n",
      "epoch:  7   step:  146   train loss:  4.931346893310547  val loss:  5.8357133865356445  val L1 loss:  6.3193\n",
      "epoch:  7   step:  147   train loss:  7.134843826293945  val loss:  5.261070251464844  val L1 loss:  5.7213\n",
      "epoch:  7   step:  148   train loss:  5.229002952575684  val loss:  5.333082675933838  val L1 loss:  5.8116\n",
      "epoch:  7   step:  149   train loss:  3.2173166275024414  val loss:  5.85060453414917  val L1 loss:  6.3242\n",
      "epoch:  7   step:  150   train loss:  5.41444206237793  val loss:  5.948451995849609  val L1 loss:  6.4268\n",
      "epoch:  7   step:  151   train loss:  4.62384557723999  val loss:  5.693808078765869  val L1 loss:  6.1676\n",
      "epoch:  7   step:  152   train loss:  3.562839984893799  val loss:  5.352958679199219  val L1 loss:  5.8292\n",
      "epoch:  7   step:  153   train loss:  4.064295291900635  val loss:  5.271363258361816  val L1 loss:  5.7525\n",
      "epoch:  7   step:  154   train loss:  7.736677169799805  val loss:  5.351320743560791  val L1 loss:  5.8262\n",
      "epoch:  7   step:  155   train loss:  7.619176864624023  val loss:  5.282958984375  val L1 loss:  5.7517\n",
      "epoch:  7   step:  156   train loss:  4.5396528244018555  val loss:  5.0952558517456055  val L1 loss:  5.5642\n",
      "epoch:  7   step:  157   train loss:  3.8239235877990723  val loss:  5.077396869659424  val L1 loss:  5.5454\n",
      "min_val_loss_print 5.077396869659424\n",
      "epoch:  7   step:  158   train loss:  7.515368461608887  val loss:  5.111504077911377  val L1 loss:  5.5993\n",
      "epoch:  7   step:  159   train loss:  4.597323417663574  val loss:  5.164786338806152  val L1 loss:  5.6607\n",
      "epoch:  7   step:  160   train loss:  5.653322696685791  val loss:  5.235983848571777  val L1 loss:  5.7139\n",
      "epoch:  7   step:  161   train loss:  4.2991108894348145  val loss:  5.35490608215332  val L1 loss:  5.8435\n",
      "epoch:  7   step:  162   train loss:  5.876291751861572  val loss:  5.45198917388916  val L1 loss:  5.9394\n",
      "epoch:  7   step:  163   train loss:  5.531008720397949  val loss:  5.513373851776123  val L1 loss:  6.0031\n",
      "epoch:  7   step:  164   train loss:  3.520073890686035  val loss:  5.554841041564941  val L1 loss:  6.0374\n",
      "epoch:  7   step:  165   train loss:  6.0449419021606445  val loss:  5.558838844299316  val L1 loss:  6.0364\n",
      "epoch:  7   step:  166   train loss:  3.187800884246826  val loss:  5.552027225494385  val L1 loss:  6.0286\n",
      "epoch:  7   step:  167   train loss:  8.16909408569336  val loss:  5.552825927734375  val L1 loss:  6.0368\n",
      "epoch:  7   step:  168   train loss:  5.705073356628418  val loss:  5.550842761993408  val L1 loss:  6.0429\n",
      "epoch:  7   step:  169   train loss:  5.782791614532471  val loss:  5.526401042938232  val L1 loss:  6.0023\n",
      "epoch:  7   step:  170   train loss:  3.695364475250244  val loss:  5.476576328277588  val L1 loss:  5.965\n",
      "epoch:  7   step:  171   train loss:  4.4121294021606445  val loss:  5.46156644821167  val L1 loss:  5.9451\n",
      "epoch:  7   step:  172   train loss:  4.038158416748047  val loss:  5.429629325866699  val L1 loss:  5.9127\n",
      "epoch:  7   step:  173   train loss:  5.453174591064453  val loss:  5.427390098571777  val L1 loss:  5.9112\n",
      "epoch:  7   step:  174   train loss:  3.106477737426758  val loss:  5.429642677307129  val L1 loss:  5.9097\n",
      "epoch:  7   step:  175   train loss:  8.482511520385742  val loss:  5.467045783996582  val L1 loss:  5.9581\n",
      "epoch:  7   step:  176   train loss:  1.8972840309143066  val loss:  5.501932144165039  val L1 loss:  5.9839\n",
      "epoch:  7   step:  177   train loss:  4.666475296020508  val loss:  5.468973159790039  val L1 loss:  5.9523\n",
      "epoch:  7   step:  178   train loss:  7.270541191101074  val loss:  5.390041351318359  val L1 loss:  5.8711\n",
      "epoch:  7   step:  179   train loss:  4.558212757110596  val loss:  5.4665985107421875  val L1 loss:  5.9424\n",
      "epoch:  7   step:  180   train loss:  7.475381374359131  val loss:  5.737032413482666  val L1 loss:  6.2207\n",
      "epoch:  7   step:  181   train loss:  5.151182651519775  val loss:  5.830145359039307  val L1 loss:  6.2896\n",
      "epoch:  7   step:  182   train loss:  4.660654067993164  val loss:  5.789463996887207  val L1 loss:  6.231\n",
      "epoch:  7   step:  183   train loss:  3.05725359916687  val loss:  5.702065944671631  val L1 loss:  6.1598\n",
      "epoch:  7   step:  184   train loss:  4.388631343841553  val loss:  5.614703178405762  val L1 loss:  6.0751\n",
      "epoch:  7   step:  185   train loss:  2.5828757286071777  val loss:  5.556994438171387  val L1 loss:  6.0292\n",
      "epoch:  7   step:  186   train loss:  4.957751274108887  val loss:  5.49797248840332  val L1 loss:  5.9776\n",
      "epoch:  7   step:  187   train loss:  4.80341100692749  val loss:  5.447022438049316  val L1 loss:  5.9133\n",
      "epoch:  7   step:  188   train loss:  4.845614433288574  val loss:  5.505288124084473  val L1 loss:  5.9881\n",
      "epoch:  7   step:  189   train loss:  3.470461845397949  val loss:  5.6245622634887695  val L1 loss:  6.102\n",
      "epoch:  7   step:  190   train loss:  6.013594150543213  val loss:  5.7769293785095215  val L1 loss:  6.2588\n",
      "epoch:  7   step:  191   train loss:  3.671393871307373  val loss:  5.962348937988281  val L1 loss:  6.4561\n",
      "epoch:  7   step:  192   train loss:  4.7877397537231445  val loss:  5.807959079742432  val L1 loss:  6.2974\n",
      "epoch:  7   step:  193   train loss:  4.418285369873047  val loss:  5.386740684509277  val L1 loss:  5.8658\n",
      "epoch:  7   step:  194   train loss:  5.111782073974609  val loss:  5.229677200317383  val L1 loss:  5.7098\n",
      "epoch:  7   step:  195   train loss:  5.083946228027344  val loss:  5.225222110748291  val L1 loss:  5.6842\n",
      "epoch:  7   step:  196   train loss:  5.463449954986572  val loss:  5.303455829620361  val L1 loss:  5.7714\n",
      "epoch:  7   step:  197   train loss:  2.8191699981689453  val loss:  5.267648696899414  val L1 loss:  5.722\n",
      "epoch:  7   step:  198   train loss:  2.437613010406494  val loss:  5.238526344299316  val L1 loss:  5.7005\n",
      "epoch:  7   step:  199   train loss:  5.138487815856934  val loss:  5.25469970703125  val L1 loss:  5.7307\n",
      "epoch:  7   step:  200   train loss:  4.805053234100342  val loss:  5.396713733673096  val L1 loss:  5.8736\n",
      "epoch:  7   step:  201   train loss:  7.116510391235352  val loss:  5.408229827880859  val L1 loss:  5.8903\n",
      "epoch:  7   step:  202   train loss:  4.833721160888672  val loss:  5.259952068328857  val L1 loss:  5.7304\n",
      "epoch:  7   step:  203   train loss:  8.075084686279297  val loss:  5.108944416046143  val L1 loss:  5.5595\n",
      "epoch:  7   step:  204   train loss:  4.205636024475098  val loss:  5.010104656219482  val L1 loss:  5.4643\n",
      "min_val_loss_print 5.010104656219482\n",
      "epoch:  7   step:  205   train loss:  4.411921977996826  val loss:  4.995598793029785  val L1 loss:  5.4739\n",
      "min_val_loss_print 4.995598793029785\n",
      "epoch:  7   step:  206   train loss:  4.191109657287598  val loss:  5.003492832183838  val L1 loss:  5.4809\n",
      "epoch:  7   step:  207   train loss:  4.115954399108887  val loss:  5.023731708526611  val L1 loss:  5.4969\n",
      "epoch:  7   step:  208   train loss:  4.627356052398682  val loss:  5.017522811889648  val L1 loss:  5.4779\n",
      "epoch:  8   step:  0   train loss:  4.3664116859436035  val loss:  5.053657531738281  val L1 loss:  5.5259\n",
      "epoch:  8   step:  1   train loss:  2.631619930267334  val loss:  5.043872356414795  val L1 loss:  5.5203\n",
      "epoch:  8   step:  2   train loss:  3.70644211769104  val loss:  4.973386287689209  val L1 loss:  5.445\n",
      "min_val_loss_print 4.973386287689209\n",
      "epoch:  8   step:  3   train loss:  2.5960118770599365  val loss:  4.902602195739746  val L1 loss:  5.3676\n",
      "min_val_loss_print 4.902602195739746\n",
      "epoch:  8   step:  4   train loss:  5.980241775512695  val loss:  4.8764142990112305  val L1 loss:  5.3397\n",
      "min_val_loss_print 4.8764142990112305\n",
      "epoch:  8   step:  5   train loss:  5.13903284072876  val loss:  4.872712135314941  val L1 loss:  5.3452\n",
      "min_val_loss_print 4.872712135314941\n",
      "epoch:  8   step:  6   train loss:  4.205366134643555  val loss:  4.835649490356445  val L1 loss:  5.3069\n",
      "min_val_loss_print 4.835649490356445\n",
      "epoch:  8   step:  7   train loss:  3.410604238510132  val loss:  4.926215171813965  val L1 loss:  5.396\n",
      "epoch:  8   step:  8   train loss:  3.510965347290039  val loss:  4.974438667297363  val L1 loss:  5.4341\n",
      "epoch:  8   step:  9   train loss:  5.167611122131348  val loss:  5.094734191894531  val L1 loss:  5.5643\n",
      "epoch:  8   step:  10   train loss:  5.360526084899902  val loss:  5.09145450592041  val L1 loss:  5.5783\n",
      "epoch:  8   step:  11   train loss:  3.591197967529297  val loss:  5.122781276702881  val L1 loss:  5.6108\n",
      "epoch:  8   step:  12   train loss:  5.237537860870361  val loss:  5.1952714920043945  val L1 loss:  5.6836\n",
      "epoch:  8   step:  13   train loss:  4.422387599945068  val loss:  5.3103251457214355  val L1 loss:  5.809\n",
      "epoch:  8   step:  14   train loss:  3.3607637882232666  val loss:  5.392662048339844  val L1 loss:  5.8872\n",
      "epoch:  8   step:  15   train loss:  2.867133617401123  val loss:  5.395257472991943  val L1 loss:  5.8886\n",
      "epoch:  8   step:  16   train loss:  5.056862831115723  val loss:  5.320738792419434  val L1 loss:  5.8123\n",
      "epoch:  8   step:  17   train loss:  6.134356498718262  val loss:  5.466625690460205  val L1 loss:  5.9323\n",
      "epoch:  8   step:  18   train loss:  4.113871097564697  val loss:  5.630681037902832  val L1 loss:  6.1005\n",
      "epoch:  8   step:  19   train loss:  4.246049880981445  val loss:  5.777454853057861  val L1 loss:  6.2535\n",
      "epoch:  8   step:  20   train loss:  5.153674602508545  val loss:  6.041754722595215  val L1 loss:  6.5285\n",
      "epoch:  8   step:  21   train loss:  5.634546279907227  val loss:  6.107334613800049  val L1 loss:  6.5785\n",
      "epoch:  8   step:  22   train loss:  3.6167516708374023  val loss:  5.831758975982666  val L1 loss:  6.3185\n",
      "epoch:  8   step:  23   train loss:  4.671578407287598  val loss:  5.765925884246826  val L1 loss:  6.2428\n",
      "epoch:  8   step:  24   train loss:  3.7340807914733887  val loss:  5.570950984954834  val L1 loss:  6.0487\n",
      "epoch:  8   step:  25   train loss:  4.9413275718688965  val loss:  5.585415363311768  val L1 loss:  6.079\n",
      "epoch:  8   step:  26   train loss:  3.318310022354126  val loss:  5.820953369140625  val L1 loss:  6.3145\n",
      "epoch:  8   step:  27   train loss:  7.239315986633301  val loss:  6.4897332191467285  val L1 loss:  6.9634\n",
      "epoch:  8   step:  28   train loss:  4.827462196350098  val loss:  6.8723368644714355  val L1 loss:  7.3417\n",
      "epoch:  8   step:  29   train loss:  4.209210395812988  val loss:  6.496979713439941  val L1 loss:  6.9949\n",
      "epoch:  8   step:  30   train loss:  4.755778789520264  val loss:  5.709239482879639  val L1 loss:  6.1931\n",
      "epoch:  8   step:  31   train loss:  3.4037060737609863  val loss:  5.408014297485352  val L1 loss:  5.8685\n",
      "epoch:  8   step:  32   train loss:  4.769630432128906  val loss:  5.424246311187744  val L1 loss:  5.8826\n",
      "epoch:  8   step:  33   train loss:  4.10606575012207  val loss:  5.416777610778809  val L1 loss:  5.9009\n",
      "epoch:  8   step:  34   train loss:  4.1589508056640625  val loss:  5.518816947937012  val L1 loss:  6.0127\n",
      "epoch:  8   step:  35   train loss:  5.766481399536133  val loss:  5.704280376434326  val L1 loss:  6.1997\n",
      "epoch:  8   step:  36   train loss:  4.553217887878418  val loss:  5.863530158996582  val L1 loss:  6.3464\n",
      "epoch:  8   step:  37   train loss:  5.420781135559082  val loss:  5.755042552947998  val L1 loss:  6.2448\n",
      "epoch:  8   step:  38   train loss:  4.263510227203369  val loss:  5.622949600219727  val L1 loss:  6.1093\n",
      "epoch:  8   step:  39   train loss:  5.972367286682129  val loss:  5.50971794128418  val L1 loss:  5.9891\n",
      "epoch:  8   step:  40   train loss:  4.4075117111206055  val loss:  5.469577312469482  val L1 loss:  5.9354\n",
      "epoch:  8   step:  41   train loss:  2.592428207397461  val loss:  5.435909271240234  val L1 loss:  5.9139\n",
      "epoch:  8   step:  42   train loss:  5.294846534729004  val loss:  5.365036964416504  val L1 loss:  5.8495\n",
      "epoch:  8   step:  43   train loss:  5.82677698135376  val loss:  5.370983600616455  val L1 loss:  5.8503\n",
      "epoch:  8   step:  44   train loss:  4.957764148712158  val loss:  5.507875919342041  val L1 loss:  5.9685\n",
      "epoch:  8   step:  45   train loss:  3.2446725368499756  val loss:  5.4213385581970215  val L1 loss:  5.9018\n",
      "epoch:  8   step:  46   train loss:  5.002527236938477  val loss:  5.236491680145264  val L1 loss:  5.7137\n",
      "epoch:  8   step:  47   train loss:  5.186054229736328  val loss:  5.199936389923096  val L1 loss:  5.6836\n",
      "epoch:  8   step:  48   train loss:  4.122126579284668  val loss:  5.311737060546875  val L1 loss:  5.7898\n",
      "epoch:  8   step:  49   train loss:  4.306789398193359  val loss:  5.394047737121582  val L1 loss:  5.8692\n",
      "epoch:  8   step:  50   train loss:  5.72246789932251  val loss:  5.356647968292236  val L1 loss:  5.847\n",
      "epoch:  8   step:  51   train loss:  4.118132591247559  val loss:  5.399484634399414  val L1 loss:  5.8862\n",
      "epoch:  8   step:  52   train loss:  2.9531497955322266  val loss:  5.3800859451293945  val L1 loss:  5.8698\n",
      "epoch:  8   step:  53   train loss:  4.933862686157227  val loss:  5.374054908752441  val L1 loss:  5.836\n",
      "epoch:  8   step:  54   train loss:  3.155320405960083  val loss:  5.45026969909668  val L1 loss:  5.9181\n",
      "epoch:  8   step:  55   train loss:  3.3143529891967773  val loss:  5.40061092376709  val L1 loss:  5.8732\n",
      "epoch:  8   step:  56   train loss:  5.389334678649902  val loss:  5.377933979034424  val L1 loss:  5.8547\n",
      "epoch:  8   step:  57   train loss:  3.837553024291992  val loss:  5.432229995727539  val L1 loss:  5.9015\n",
      "epoch:  8   step:  58   train loss:  7.268949031829834  val loss:  5.498837947845459  val L1 loss:  5.984\n",
      "epoch:  8   step:  59   train loss:  5.089690685272217  val loss:  5.783525466918945  val L1 loss:  6.2638\n",
      "epoch:  8   step:  60   train loss:  4.439602851867676  val loss:  6.0713677406311035  val L1 loss:  6.5622\n",
      "epoch:  8   step:  61   train loss:  4.939763069152832  val loss:  5.937252044677734  val L1 loss:  6.4244\n",
      "epoch:  8   step:  62   train loss:  2.741939067840576  val loss:  5.462378978729248  val L1 loss:  5.9386\n",
      "epoch:  8   step:  63   train loss:  4.9314165115356445  val loss:  5.25381326675415  val L1 loss:  5.7258\n",
      "epoch:  8   step:  64   train loss:  3.612003803253174  val loss:  5.1851396560668945  val L1 loss:  5.6324\n",
      "epoch:  8   step:  65   train loss:  3.225321054458618  val loss:  5.289859294891357  val L1 loss:  5.7477\n",
      "epoch:  8   step:  66   train loss:  2.9787416458129883  val loss:  5.316432476043701  val L1 loss:  5.768\n",
      "epoch:  8   step:  67   train loss:  3.679396629333496  val loss:  5.2715744972229  val L1 loss:  5.7306\n",
      "epoch:  8   step:  68   train loss:  4.537878036499023  val loss:  5.341683387756348  val L1 loss:  5.8107\n",
      "epoch:  8   step:  69   train loss:  4.973783493041992  val loss:  5.572564601898193  val L1 loss:  6.0539\n",
      "epoch:  8   step:  70   train loss:  3.017698287963867  val loss:  5.73777437210083  val L1 loss:  6.2185\n",
      "epoch:  8   step:  71   train loss:  5.427857398986816  val loss:  5.754339694976807  val L1 loss:  6.2229\n",
      "epoch:  8   step:  72   train loss:  4.823988914489746  val loss:  5.803926467895508  val L1 loss:  6.2956\n",
      "epoch:  8   step:  73   train loss:  4.923264503479004  val loss:  5.809203147888184  val L1 loss:  6.292\n",
      "epoch:  8   step:  74   train loss:  3.6974635124206543  val loss:  5.784335613250732  val L1 loss:  6.2574\n",
      "epoch:  8   step:  75   train loss:  3.4889276027679443  val loss:  5.854424953460693  val L1 loss:  6.3341\n",
      "epoch:  8   step:  76   train loss:  7.209435939788818  val loss:  5.898218631744385  val L1 loss:  6.3843\n",
      "epoch:  8   step:  77   train loss:  4.885432243347168  val loss:  6.013343334197998  val L1 loss:  6.4998\n",
      "epoch:  8   step:  78   train loss:  4.842621803283691  val loss:  5.905538082122803  val L1 loss:  6.3903\n",
      "epoch:  8   step:  79   train loss:  3.890533447265625  val loss:  5.736308574676514  val L1 loss:  6.2128\n",
      "epoch:  8   step:  80   train loss:  4.07691764831543  val loss:  5.5836381912231445  val L1 loss:  6.0592\n",
      "epoch:  8   step:  81   train loss:  4.407589912414551  val loss:  5.572021961212158  val L1 loss:  6.0476\n",
      "epoch:  8   step:  82   train loss:  7.134203910827637  val loss:  5.559274673461914  val L1 loss:  6.0272\n",
      "epoch:  8   step:  83   train loss:  4.829936504364014  val loss:  5.622890472412109  val L1 loss:  6.0944\n",
      "epoch:  8   step:  84   train loss:  5.195030212402344  val loss:  5.706291198730469  val L1 loss:  6.176\n",
      "epoch:  8   step:  85   train loss:  2.7335634231567383  val loss:  5.879640579223633  val L1 loss:  6.3412\n",
      "epoch:  8   step:  86   train loss:  3.677053928375244  val loss:  6.184540271759033  val L1 loss:  6.67\n",
      "epoch:  8   step:  87   train loss:  4.086172103881836  val loss:  6.330409526824951  val L1 loss:  6.8202\n",
      "epoch:  8   step:  88   train loss:  5.4759368896484375  val loss:  6.374488353729248  val L1 loss:  6.864\n",
      "epoch:  8   step:  89   train loss:  4.272465229034424  val loss:  6.07489013671875  val L1 loss:  6.5607\n",
      "epoch:  8   step:  90   train loss:  4.401110649108887  val loss:  5.919191360473633  val L1 loss:  6.3946\n",
      "epoch:  8   step:  91   train loss:  2.8350470066070557  val loss:  5.828517913818359  val L1 loss:  6.2967\n",
      "epoch:  8   step:  92   train loss:  3.911421537399292  val loss:  5.8281402587890625  val L1 loss:  6.3066\n",
      "epoch:  8   step:  93   train loss:  4.097633361816406  val loss:  5.809140682220459  val L1 loss:  6.2975\n",
      "epoch:  8   step:  94   train loss:  5.452154159545898  val loss:  5.719545364379883  val L1 loss:  6.192\n",
      "epoch:  8   step:  95   train loss:  4.093957424163818  val loss:  5.682555198669434  val L1 loss:  6.1512\n",
      "epoch:  8   step:  96   train loss:  3.729482889175415  val loss:  5.677429676055908  val L1 loss:  6.1577\n",
      "epoch:  8   step:  97   train loss:  4.751848220825195  val loss:  5.709798336029053  val L1 loss:  6.1991\n",
      "epoch:  8   step:  98   train loss:  4.557765960693359  val loss:  5.581820487976074  val L1 loss:  6.0774\n",
      "epoch:  8   step:  99   train loss:  4.309117317199707  val loss:  5.524458885192871  val L1 loss:  6.0152\n",
      "epoch:  8   step:  100   train loss:  5.613687515258789  val loss:  5.432236194610596  val L1 loss:  5.9037\n",
      "epoch:  8   step:  101   train loss:  5.372040748596191  val loss:  5.337197303771973  val L1 loss:  5.8045\n",
      "epoch:  8   step:  102   train loss:  5.429470062255859  val loss:  5.2778801918029785  val L1 loss:  5.7558\n",
      "epoch:  8   step:  103   train loss:  4.873446464538574  val loss:  5.183306694030762  val L1 loss:  5.6571\n",
      "epoch:  8   step:  104   train loss:  7.242863178253174  val loss:  5.141822338104248  val L1 loss:  5.6143\n",
      "epoch:  8   step:  105   train loss:  4.4100189208984375  val loss:  5.059023380279541  val L1 loss:  5.5318\n",
      "epoch:  8   step:  106   train loss:  5.7625508308410645  val loss:  4.9913787841796875  val L1 loss:  5.4633\n",
      "epoch:  8   step:  107   train loss:  3.245229721069336  val loss:  4.923460960388184  val L1 loss:  5.3943\n",
      "epoch:  8   step:  108   train loss:  5.710965633392334  val loss:  4.9189043045043945  val L1 loss:  5.3726\n",
      "epoch:  8   step:  109   train loss:  8.258340835571289  val loss:  4.942333698272705  val L1 loss:  5.3929\n",
      "epoch:  8   step:  110   train loss:  4.031957626342773  val loss:  4.935431003570557  val L1 loss:  5.4083\n",
      "epoch:  8   step:  111   train loss:  3.92950177192688  val loss:  4.916967868804932  val L1 loss:  5.3972\n",
      "epoch:  8   step:  112   train loss:  2.4074716567993164  val loss:  4.841434001922607  val L1 loss:  5.325\n",
      "epoch:  8   step:  113   train loss:  2.1471593379974365  val loss:  4.833949565887451  val L1 loss:  5.3178\n",
      "min_val_loss_print 4.833949565887451\n",
      "epoch:  8   step:  114   train loss:  4.1333160400390625  val loss:  4.77445650100708  val L1 loss:  5.2624\n",
      "min_val_loss_print 4.77445650100708\n",
      "epoch:  8   step:  115   train loss:  4.12896203994751  val loss:  4.699256896972656  val L1 loss:  5.1605\n",
      "min_val_loss_print 4.699256896972656\n",
      "epoch:  8   step:  116   train loss:  3.6711201667785645  val loss:  4.73833703994751  val L1 loss:  5.2092\n",
      "epoch:  8   step:  117   train loss:  5.065441131591797  val loss:  4.811352252960205  val L1 loss:  5.2876\n",
      "epoch:  8   step:  118   train loss:  5.426102161407471  val loss:  4.7268967628479  val L1 loss:  5.2081\n",
      "epoch:  8   step:  119   train loss:  4.585060119628906  val loss:  4.808117389678955  val L1 loss:  5.2833\n",
      "epoch:  8   step:  120   train loss:  3.7368130683898926  val loss:  4.7744364738464355  val L1 loss:  5.2503\n",
      "epoch:  8   step:  121   train loss:  4.148670196533203  val loss:  4.819323539733887  val L1 loss:  5.2953\n",
      "epoch:  8   step:  122   train loss:  3.9122910499572754  val loss:  4.73446798324585  val L1 loss:  5.2095\n",
      "epoch:  8   step:  123   train loss:  3.54742431640625  val loss:  4.734966278076172  val L1 loss:  5.2107\n",
      "epoch:  8   step:  124   train loss:  5.361970901489258  val loss:  4.798837661743164  val L1 loss:  5.263\n",
      "epoch:  8   step:  125   train loss:  5.384640693664551  val loss:  5.04742431640625  val L1 loss:  5.5194\n",
      "epoch:  8   step:  126   train loss:  2.548337459564209  val loss:  5.4736247062683105  val L1 loss:  5.9486\n",
      "epoch:  8   step:  127   train loss:  5.30197811126709  val loss:  5.558536052703857  val L1 loss:  6.0431\n",
      "epoch:  8   step:  128   train loss:  2.668672800064087  val loss:  5.464442253112793  val L1 loss:  5.9562\n",
      "epoch:  8   step:  129   train loss:  6.038596153259277  val loss:  4.998514175415039  val L1 loss:  5.4541\n",
      "epoch:  8   step:  130   train loss:  3.168764352798462  val loss:  5.014843940734863  val L1 loss:  5.4806\n",
      "epoch:  8   step:  131   train loss:  4.111949920654297  val loss:  5.107622146606445  val L1 loss:  5.5719\n",
      "epoch:  8   step:  132   train loss:  4.6155266761779785  val loss:  4.999484539031982  val L1 loss:  5.4639\n",
      "epoch:  8   step:  133   train loss:  4.814419269561768  val loss:  5.019838333129883  val L1 loss:  5.4771\n",
      "epoch:  8   step:  134   train loss:  4.629419326782227  val loss:  5.345821380615234  val L1 loss:  5.8303\n",
      "epoch:  8   step:  135   train loss:  3.1336605548858643  val loss:  5.275875091552734  val L1 loss:  5.7525\n",
      "epoch:  8   step:  136   train loss:  3.0590336322784424  val loss:  5.085330009460449  val L1 loss:  5.5483\n",
      "epoch:  8   step:  137   train loss:  2.7537503242492676  val loss:  5.19627046585083  val L1 loss:  5.652\n",
      "epoch:  8   step:  138   train loss:  2.7654638290405273  val loss:  5.444447040557861  val L1 loss:  5.9226\n",
      "epoch:  8   step:  139   train loss:  6.011285781860352  val loss:  5.367555618286133  val L1 loss:  5.8208\n",
      "epoch:  8   step:  140   train loss:  5.017193794250488  val loss:  5.354928493499756  val L1 loss:  5.831\n",
      "epoch:  8   step:  141   train loss:  3.6678881645202637  val loss:  5.473657131195068  val L1 loss:  5.9642\n",
      "epoch:  8   step:  142   train loss:  4.785116195678711  val loss:  5.638567924499512  val L1 loss:  6.1177\n",
      "epoch:  8   step:  143   train loss:  4.450761795043945  val loss:  5.710572242736816  val L1 loss:  6.1876\n",
      "epoch:  8   step:  144   train loss:  3.659562587738037  val loss:  5.563223838806152  val L1 loss:  6.0418\n",
      "epoch:  8   step:  145   train loss:  3.2301342487335205  val loss:  5.436424255371094  val L1 loss:  5.9118\n",
      "epoch:  8   step:  146   train loss:  5.684396743774414  val loss:  5.464336395263672  val L1 loss:  5.9473\n",
      "epoch:  8   step:  147   train loss:  5.4801716804504395  val loss:  5.486390113830566  val L1 loss:  5.9656\n",
      "epoch:  8   step:  148   train loss:  2.654862880706787  val loss:  5.659449100494385  val L1 loss:  6.1414\n",
      "epoch:  8   step:  149   train loss:  3.9316771030426025  val loss:  5.890488147735596  val L1 loss:  6.3756\n",
      "epoch:  8   step:  150   train loss:  4.869680404663086  val loss:  5.891383647918701  val L1 loss:  6.3724\n",
      "epoch:  8   step:  151   train loss:  3.924175500869751  val loss:  5.518002986907959  val L1 loss:  6.0001\n",
      "epoch:  8   step:  152   train loss:  4.169225692749023  val loss:  5.283780574798584  val L1 loss:  5.7355\n",
      "epoch:  8   step:  153   train loss:  3.770598888397217  val loss:  5.221536159515381  val L1 loss:  5.666\n",
      "epoch:  8   step:  154   train loss:  2.9718494415283203  val loss:  5.263129711151123  val L1 loss:  5.7295\n",
      "epoch:  8   step:  155   train loss:  5.316644668579102  val loss:  5.361147403717041  val L1 loss:  5.8247\n",
      "epoch:  8   step:  156   train loss:  5.176578521728516  val loss:  5.306710243225098  val L1 loss:  5.7774\n",
      "epoch:  8   step:  157   train loss:  3.5295894145965576  val loss:  5.193512916564941  val L1 loss:  5.6617\n",
      "epoch:  8   step:  158   train loss:  3.5249104499816895  val loss:  5.14240026473999  val L1 loss:  5.6206\n",
      "epoch:  8   step:  159   train loss:  6.3305253982543945  val loss:  5.135441780090332  val L1 loss:  5.608\n",
      "epoch:  8   step:  160   train loss:  2.8031630516052246  val loss:  5.089252948760986  val L1 loss:  5.5637\n",
      "epoch:  8   step:  161   train loss:  3.6072802543640137  val loss:  5.061948299407959  val L1 loss:  5.5235\n",
      "epoch:  8   step:  162   train loss:  4.785379409790039  val loss:  5.0171661376953125  val L1 loss:  5.4954\n",
      "epoch:  8   step:  163   train loss:  5.314103126525879  val loss:  5.045695781707764  val L1 loss:  5.5212\n",
      "epoch:  8   step:  164   train loss:  3.7403128147125244  val loss:  5.150922775268555  val L1 loss:  5.6181\n",
      "epoch:  8   step:  165   train loss:  3.4275405406951904  val loss:  5.305859088897705  val L1 loss:  5.7681\n",
      "epoch:  8   step:  166   train loss:  3.1105880737304688  val loss:  5.386921405792236  val L1 loss:  5.8504\n",
      "epoch:  8   step:  167   train loss:  3.9090397357940674  val loss:  5.322790622711182  val L1 loss:  5.7804\n",
      "epoch:  8   step:  168   train loss:  4.103806495666504  val loss:  5.227621555328369  val L1 loss:  5.6946\n",
      "epoch:  8   step:  169   train loss:  3.4378719329833984  val loss:  4.9749603271484375  val L1 loss:  5.4444\n",
      "epoch:  8   step:  170   train loss:  5.324443817138672  val loss:  4.899656295776367  val L1 loss:  5.3785\n",
      "epoch:  8   step:  171   train loss:  4.407825946807861  val loss:  5.167382717132568  val L1 loss:  5.6342\n",
      "epoch:  8   step:  172   train loss:  3.2784042358398438  val loss:  5.506385326385498  val L1 loss:  5.9601\n",
      "epoch:  8   step:  173   train loss:  5.81108283996582  val loss:  5.398534297943115  val L1 loss:  5.8738\n",
      "epoch:  8   step:  174   train loss:  4.641471862792969  val loss:  5.2471208572387695  val L1 loss:  5.7164\n",
      "epoch:  8   step:  175   train loss:  5.03586483001709  val loss:  5.370632171630859  val L1 loss:  5.845\n",
      "epoch:  8   step:  176   train loss:  5.3266472816467285  val loss:  5.829423904418945  val L1 loss:  6.326\n",
      "epoch:  8   step:  177   train loss:  5.0817155838012695  val loss:  6.2759904861450195  val L1 loss:  6.7533\n",
      "epoch:  8   step:  178   train loss:  6.330340385437012  val loss:  6.272773742675781  val L1 loss:  6.7548\n",
      "epoch:  8   step:  179   train loss:  6.511916160583496  val loss:  6.011629581451416  val L1 loss:  6.508\n",
      "epoch:  8   step:  180   train loss:  4.754538536071777  val loss:  5.735203742980957  val L1 loss:  6.2072\n",
      "epoch:  8   step:  181   train loss:  3.7574315071105957  val loss:  5.977415561676025  val L1 loss:  6.4508\n",
      "epoch:  8   step:  182   train loss:  3.997037887573242  val loss:  6.3027167320251465  val L1 loss:  6.7838\n",
      "epoch:  8   step:  183   train loss:  6.173125267028809  val loss:  6.211940765380859  val L1 loss:  6.6916\n",
      "epoch:  8   step:  184   train loss:  7.157071113586426  val loss:  5.800940990447998  val L1 loss:  6.2758\n",
      "epoch:  8   step:  185   train loss:  4.094094276428223  val loss:  6.0081305503845215  val L1 loss:  6.492\n",
      "epoch:  8   step:  186   train loss:  6.694270133972168  val loss:  6.951571941375732  val L1 loss:  7.4424\n",
      "epoch:  8   step:  187   train loss:  5.920624256134033  val loss:  7.913655757904053  val L1 loss:  8.4052\n",
      "epoch:  8   step:  188   train loss:  7.547031402587891  val loss:  8.569534301757812  val L1 loss:  9.056\n",
      "epoch:  8   step:  189   train loss:  6.761867046356201  val loss:  8.221244812011719  val L1 loss:  8.7095\n",
      "epoch:  8   step:  190   train loss:  8.066082000732422  val loss:  7.431509971618652  val L1 loss:  7.9008\n",
      "epoch:  8   step:  191   train loss:  4.815657615661621  val loss:  6.4842658042907715  val L1 loss:  6.964\n",
      "epoch:  8   step:  192   train loss:  2.8891682624816895  val loss:  6.264671325683594  val L1 loss:  6.7387\n",
      "epoch:  8   step:  193   train loss:  5.024946212768555  val loss:  6.180750370025635  val L1 loss:  6.6627\n",
      "epoch:  8   step:  194   train loss:  4.133065700531006  val loss:  6.014389514923096  val L1 loss:  6.4936\n",
      "epoch:  8   step:  195   train loss:  3.7014055252075195  val loss:  5.695001125335693  val L1 loss:  6.1907\n",
      "epoch:  8   step:  196   train loss:  6.467884063720703  val loss:  5.486076354980469  val L1 loss:  5.9542\n",
      "epoch:  8   step:  197   train loss:  6.741670608520508  val loss:  5.85416316986084  val L1 loss:  6.3357\n",
      "epoch:  8   step:  198   train loss:  3.1430599689483643  val loss:  6.120038986206055  val L1 loss:  6.5991\n",
      "epoch:  8   step:  199   train loss:  5.780080795288086  val loss:  6.128071308135986  val L1 loss:  6.5925\n",
      "epoch:  8   step:  200   train loss:  3.752758264541626  val loss:  5.887244701385498  val L1 loss:  6.3807\n",
      "epoch:  8   step:  201   train loss:  4.860116481781006  val loss:  5.598108291625977  val L1 loss:  6.0854\n",
      "epoch:  8   step:  202   train loss:  3.4164984226226807  val loss:  5.661996841430664  val L1 loss:  6.1265\n",
      "epoch:  8   step:  203   train loss:  5.815177917480469  val loss:  5.9963908195495605  val L1 loss:  6.4883\n",
      "epoch:  8   step:  204   train loss:  5.642034530639648  val loss:  6.129053115844727  val L1 loss:  6.6172\n",
      "epoch:  8   step:  205   train loss:  5.3698859214782715  val loss:  6.071386814117432  val L1 loss:  6.5561\n",
      "epoch:  8   step:  206   train loss:  7.6055006980896  val loss:  5.678622722625732  val L1 loss:  6.1579\n",
      "epoch:  8   step:  207   train loss:  6.421979904174805  val loss:  5.664803981781006  val L1 loss:  6.1647\n",
      "epoch:  8   step:  208   train loss:  6.231937885284424  val loss:  6.002848148345947  val L1 loss:  6.4921\n",
      "epoch:  9   step:  0   train loss:  6.9504289627075195  val loss:  6.26961612701416  val L1 loss:  6.7448\n",
      "epoch:  9   step:  1   train loss:  5.380046844482422  val loss:  6.034707069396973  val L1 loss:  6.5117\n",
      "epoch:  9   step:  2   train loss:  5.274839401245117  val loss:  5.601929664611816  val L1 loss:  6.0677\n",
      "epoch:  9   step:  3   train loss:  4.250676155090332  val loss:  5.475766658782959  val L1 loss:  5.9565\n",
      "epoch:  9   step:  4   train loss:  3.358823776245117  val loss:  5.671380519866943  val L1 loss:  6.1534\n",
      "epoch:  9   step:  5   train loss:  5.522187232971191  val loss:  5.74819278717041  val L1 loss:  6.2244\n",
      "epoch:  9   step:  6   train loss:  5.342510223388672  val loss:  5.547950267791748  val L1 loss:  6.0309\n",
      "epoch:  9   step:  7   train loss:  2.604325294494629  val loss:  5.401003360748291  val L1 loss:  5.8751\n",
      "epoch:  9   step:  8   train loss:  4.391666412353516  val loss:  5.430647850036621  val L1 loss:  5.919\n",
      "epoch:  9   step:  9   train loss:  4.540609359741211  val loss:  5.512242317199707  val L1 loss:  5.9957\n",
      "epoch:  9   step:  10   train loss:  3.9943795204162598  val loss:  5.639707565307617  val L1 loss:  6.1269\n",
      "epoch:  9   step:  11   train loss:  4.1777143478393555  val loss:  5.574195384979248  val L1 loss:  6.0583\n",
      "epoch:  9   step:  12   train loss:  5.410242080688477  val loss:  5.395251750946045  val L1 loss:  5.8782\n",
      "epoch:  9   step:  13   train loss:  4.30517578125  val loss:  5.33558988571167  val L1 loss:  5.8066\n",
      "epoch:  9   step:  14   train loss:  7.380270004272461  val loss:  5.348278045654297  val L1 loss:  5.8239\n",
      "epoch:  9   step:  15   train loss:  2.8545103073120117  val loss:  5.285007953643799  val L1 loss:  5.7621\n",
      "epoch:  9   step:  16   train loss:  4.255685806274414  val loss:  5.24609375  val L1 loss:  5.7188\n",
      "epoch:  9   step:  17   train loss:  4.656680107116699  val loss:  5.205385208129883  val L1 loss:  5.67\n",
      "epoch:  9   step:  18   train loss:  3.5148696899414062  val loss:  5.154891014099121  val L1 loss:  5.6188\n",
      "epoch:  9   step:  19   train loss:  3.9062111377716064  val loss:  5.089348793029785  val L1 loss:  5.5462\n",
      "epoch:  9   step:  20   train loss:  3.7048254013061523  val loss:  5.098372459411621  val L1 loss:  5.572\n",
      "epoch:  9   step:  21   train loss:  6.569060802459717  val loss:  5.131692886352539  val L1 loss:  5.6135\n",
      "epoch:  9   step:  22   train loss:  2.363877773284912  val loss:  5.168605327606201  val L1 loss:  5.6489\n",
      "epoch:  9   step:  23   train loss:  4.29677677154541  val loss:  5.21079683303833  val L1 loss:  5.6887\n",
      "epoch:  9   step:  24   train loss:  3.6068029403686523  val loss:  5.21616792678833  val L1 loss:  5.6919\n",
      "epoch:  9   step:  25   train loss:  2.564033031463623  val loss:  5.146913528442383  val L1 loss:  5.6078\n",
      "epoch:  9   step:  26   train loss:  5.469244003295898  val loss:  5.10024356842041  val L1 loss:  5.5569\n",
      "epoch:  9   step:  27   train loss:  3.153754234313965  val loss:  5.013160705566406  val L1 loss:  5.4721\n",
      "epoch:  9   step:  28   train loss:  4.152341842651367  val loss:  4.982396125793457  val L1 loss:  5.4415\n",
      "epoch:  9   step:  29   train loss:  3.2996973991394043  val loss:  4.953685283660889  val L1 loss:  5.424\n",
      "epoch:  9   step:  30   train loss:  3.009721517562866  val loss:  4.925535202026367  val L1 loss:  5.3872\n",
      "epoch:  9   step:  31   train loss:  5.224387168884277  val loss:  4.942988872528076  val L1 loss:  5.4141\n",
      "epoch:  9   step:  32   train loss:  6.202899932861328  val loss:  4.901312351226807  val L1 loss:  5.3748\n",
      "epoch:  9   step:  33   train loss:  3.07688045501709  val loss:  4.922440528869629  val L1 loss:  5.3992\n",
      "epoch:  9   step:  34   train loss:  2.506654739379883  val loss:  5.027278423309326  val L1 loss:  5.5026\n",
      "epoch:  9   step:  35   train loss:  2.623459577560425  val loss:  5.094862937927246  val L1 loss:  5.5775\n",
      "epoch:  9   step:  36   train loss:  3.87558650970459  val loss:  5.14622163772583  val L1 loss:  5.6228\n",
      "epoch:  9   step:  37   train loss:  3.4552078247070312  val loss:  5.216980934143066  val L1 loss:  5.7026\n",
      "epoch:  9   step:  38   train loss:  3.441112518310547  val loss:  5.239506721496582  val L1 loss:  5.7308\n",
      "epoch:  9   step:  39   train loss:  5.423598289489746  val loss:  5.25157356262207  val L1 loss:  5.7433\n",
      "epoch:  9   step:  40   train loss:  4.520026206970215  val loss:  5.270760536193848  val L1 loss:  5.7634\n",
      "epoch:  9   step:  41   train loss:  5.001982688903809  val loss:  5.272969722747803  val L1 loss:  5.7485\n",
      "epoch:  9   step:  42   train loss:  4.127353668212891  val loss:  5.291994571685791  val L1 loss:  5.775\n",
      "epoch:  9   step:  43   train loss:  3.2150890827178955  val loss:  5.307154655456543  val L1 loss:  5.7955\n",
      "epoch:  9   step:  44   train loss:  3.1174890995025635  val loss:  5.350935459136963  val L1 loss:  5.8496\n",
      "epoch:  9   step:  45   train loss:  3.346942663192749  val loss:  5.368358135223389  val L1 loss:  5.8644\n",
      "epoch:  9   step:  46   train loss:  6.244407653808594  val loss:  5.3395676612854  val L1 loss:  5.8251\n",
      "epoch:  9   step:  47   train loss:  4.340388774871826  val loss:  5.311596393585205  val L1 loss:  5.7997\n",
      "epoch:  9   step:  48   train loss:  2.925368309020996  val loss:  5.3252363204956055  val L1 loss:  5.8136\n",
      "epoch:  9   step:  49   train loss:  3.6225967407226562  val loss:  5.302858829498291  val L1 loss:  5.7737\n",
      "epoch:  9   step:  50   train loss:  5.766903877258301  val loss:  5.337257385253906  val L1 loss:  5.7928\n",
      "epoch:  9   step:  51   train loss:  3.3945157527923584  val loss:  5.429744720458984  val L1 loss:  5.9037\n",
      "epoch:  9   step:  52   train loss:  3.3535289764404297  val loss:  5.391273021697998  val L1 loss:  5.8473\n",
      "epoch:  9   step:  53   train loss:  4.301782608032227  val loss:  5.399335861206055  val L1 loss:  5.8514\n",
      "epoch:  9   step:  54   train loss:  3.9111790657043457  val loss:  5.363081932067871  val L1 loss:  5.8347\n",
      "epoch:  9   step:  55   train loss:  2.700946807861328  val loss:  5.292381286621094  val L1 loss:  5.774\n",
      "epoch:  9   step:  56   train loss:  4.290588855743408  val loss:  5.170292854309082  val L1 loss:  5.626\n",
      "epoch:  9   step:  57   train loss:  4.958459854125977  val loss:  5.264069080352783  val L1 loss:  5.7374\n",
      "epoch:  9   step:  58   train loss:  5.07848596572876  val loss:  5.384328365325928  val L1 loss:  5.8728\n",
      "epoch:  9   step:  59   train loss:  3.7039308547973633  val loss:  5.455307960510254  val L1 loss:  5.948\n",
      "epoch:  9   step:  60   train loss:  5.433570861816406  val loss:  5.294529438018799  val L1 loss:  5.7681\n",
      "epoch:  9   step:  61   train loss:  3.53456974029541  val loss:  5.275930881500244  val L1 loss:  5.7635\n",
      "epoch:  9   step:  62   train loss:  2.850513458251953  val loss:  5.367349624633789  val L1 loss:  5.8435\n",
      "epoch:  9   step:  63   train loss:  3.695469856262207  val loss:  5.399825096130371  val L1 loss:  5.8769\n",
      "epoch:  9   step:  64   train loss:  4.4926323890686035  val loss:  5.266560077667236  val L1 loss:  5.7548\n",
      "epoch:  9   step:  65   train loss:  2.063344955444336  val loss:  5.23060417175293  val L1 loss:  5.6976\n",
      "epoch:  9   step:  66   train loss:  6.12324333190918  val loss:  5.567635536193848  val L1 loss:  6.0422\n",
      "epoch:  9   step:  67   train loss:  3.630945920944214  val loss:  6.000634670257568  val L1 loss:  6.4896\n",
      "epoch:  9   step:  68   train loss:  3.5256807804107666  val loss:  5.856821060180664  val L1 loss:  6.3348\n",
      "epoch:  9   step:  69   train loss:  2.425182819366455  val loss:  5.636791706085205  val L1 loss:  6.1063\n",
      "epoch:  9   step:  70   train loss:  3.27850341796875  val loss:  5.415400505065918  val L1 loss:  5.882\n",
      "epoch:  9   step:  71   train loss:  3.9586358070373535  val loss:  5.159949779510498  val L1 loss:  5.6487\n",
      "epoch:  9   step:  72   train loss:  2.3727827072143555  val loss:  5.208537578582764  val L1 loss:  5.6813\n",
      "epoch:  9   step:  73   train loss:  4.418054580688477  val loss:  5.404864311218262  val L1 loss:  5.8804\n",
      "epoch:  9   step:  74   train loss:  4.497488021850586  val loss:  5.285621643066406  val L1 loss:  5.772\n",
      "epoch:  9   step:  75   train loss:  4.1623992919921875  val loss:  5.312973976135254  val L1 loss:  5.7856\n",
      "epoch:  9   step:  76   train loss:  4.958609580993652  val loss:  5.473402976989746  val L1 loss:  5.9511\n",
      "epoch:  9   step:  77   train loss:  7.457189083099365  val loss:  5.614943504333496  val L1 loss:  6.0939\n",
      "epoch:  9   step:  78   train loss:  4.611132621765137  val loss:  5.490412712097168  val L1 loss:  5.9675\n",
      "epoch:  9   step:  79   train loss:  3.835012435913086  val loss:  5.25701904296875  val L1 loss:  5.7303\n",
      "epoch:  9   step:  80   train loss:  4.843623638153076  val loss:  5.321103572845459  val L1 loss:  5.8108\n",
      "epoch:  9   step:  81   train loss:  3.615981340408325  val loss:  5.569214820861816  val L1 loss:  6.0373\n",
      "epoch:  9   step:  82   train loss:  3.0898265838623047  val loss:  5.578058242797852  val L1 loss:  6.0508\n",
      "epoch:  9   step:  83   train loss:  3.709986925125122  val loss:  5.55773401260376  val L1 loss:  6.0398\n",
      "epoch:  9   step:  84   train loss:  5.141380310058594  val loss:  5.471169471740723  val L1 loss:  5.9579\n",
      "epoch:  9   step:  85   train loss:  3.1845192909240723  val loss:  5.455683708190918  val L1 loss:  5.927\n",
      "epoch:  9   step:  86   train loss:  4.474912643432617  val loss:  5.523242473602295  val L1 loss:  5.9964\n",
      "epoch:  9   step:  87   train loss:  3.8123064041137695  val loss:  5.464747428894043  val L1 loss:  5.942\n",
      "epoch:  9   step:  88   train loss:  3.584798574447632  val loss:  5.450173377990723  val L1 loss:  5.9356\n",
      "epoch:  9   step:  89   train loss:  3.6826300621032715  val loss:  5.490302562713623  val L1 loss:  5.9665\n",
      "epoch:  9   step:  90   train loss:  2.8977174758911133  val loss:  5.521657466888428  val L1 loss:  6.0035\n",
      "epoch:  9   step:  91   train loss:  5.468080520629883  val loss:  5.583057880401611  val L1 loss:  6.0705\n",
      "epoch:  9   step:  92   train loss:  3.8765652179718018  val loss:  5.584802627563477  val L1 loss:  6.0562\n",
      "epoch:  9   step:  93   train loss:  5.244681358337402  val loss:  5.631900310516357  val L1 loss:  6.0965\n",
      "epoch:  9   step:  94   train loss:  3.0969810485839844  val loss:  5.584868431091309  val L1 loss:  6.0573\n",
      "epoch:  9   step:  95   train loss:  3.2282888889312744  val loss:  5.561842918395996  val L1 loss:  6.0396\n",
      "epoch:  9   step:  96   train loss:  4.520974159240723  val loss:  5.4722819328308105  val L1 loss:  5.9536\n",
      "epoch:  9   step:  97   train loss:  2.752474784851074  val loss:  5.371718406677246  val L1 loss:  5.8456\n",
      "epoch:  9   step:  98   train loss:  7.058468818664551  val loss:  5.332597732543945  val L1 loss:  5.8142\n",
      "epoch:  9   step:  99   train loss:  6.250173091888428  val loss:  5.3210906982421875  val L1 loss:  5.806\n",
      "epoch:  9   step:  100   train loss:  6.207357883453369  val loss:  5.295276165008545  val L1 loss:  5.7761\n",
      "epoch:  9   step:  101   train loss:  4.643866539001465  val loss:  5.2404465675354  val L1 loss:  5.7194\n",
      "epoch:  9   step:  102   train loss:  3.3754100799560547  val loss:  5.270011901855469  val L1 loss:  5.7358\n",
      "epoch:  9   step:  103   train loss:  5.905141830444336  val loss:  5.29602575302124  val L1 loss:  5.762\n",
      "epoch:  9   step:  104   train loss:  7.155211448669434  val loss:  5.198176383972168  val L1 loss:  5.68\n",
      "epoch:  9   step:  105   train loss:  5.285221099853516  val loss:  5.17518424987793  val L1 loss:  5.6408\n",
      "epoch:  9   step:  106   train loss:  3.601104974746704  val loss:  5.1552839279174805  val L1 loss:  5.6387\n",
      "epoch:  9   step:  107   train loss:  3.706202745437622  val loss:  5.181609153747559  val L1 loss:  5.6591\n",
      "epoch:  9   step:  108   train loss:  4.027792930603027  val loss:  5.276415824890137  val L1 loss:  5.743\n",
      "epoch:  9   step:  109   train loss:  5.497422695159912  val loss:  5.1611127853393555  val L1 loss:  5.6364\n",
      "epoch:  9   step:  110   train loss:  4.3990254402160645  val loss:  5.0470781326293945  val L1 loss:  5.5024\n",
      "epoch:  9   step:  111   train loss:  3.6360745429992676  val loss:  5.011443614959717  val L1 loss:  5.5005\n",
      "epoch:  9   step:  112   train loss:  3.979675054550171  val loss:  5.113959789276123  val L1 loss:  5.5861\n",
      "epoch:  9   step:  113   train loss:  2.819319248199463  val loss:  5.507771968841553  val L1 loss:  5.98\n",
      "epoch:  9   step:  114   train loss:  3.620967388153076  val loss:  5.483178615570068  val L1 loss:  5.9449\n",
      "epoch:  9   step:  115   train loss:  4.852902889251709  val loss:  5.44004487991333  val L1 loss:  5.9036\n",
      "epoch:  9   step:  116   train loss:  3.215322971343994  val loss:  5.328855037689209  val L1 loss:  5.7843\n",
      "epoch:  9   step:  117   train loss:  3.955648183822632  val loss:  5.2082953453063965  val L1 loss:  5.6645\n",
      "epoch:  9   step:  118   train loss:  5.51833438873291  val loss:  5.185595512390137  val L1 loss:  5.6471\n",
      "epoch:  9   step:  119   train loss:  4.085719108581543  val loss:  5.1359357833862305  val L1 loss:  5.6017\n",
      "epoch:  9   step:  120   train loss:  5.068849563598633  val loss:  5.052402019500732  val L1 loss:  5.5135\n",
      "epoch:  9   step:  121   train loss:  4.379406452178955  val loss:  4.9259467124938965  val L1 loss:  5.3749\n",
      "epoch:  9   step:  122   train loss:  3.3026316165924072  val loss:  4.895495891571045  val L1 loss:  5.3463\n",
      "epoch:  9   step:  123   train loss:  4.550278663635254  val loss:  4.94429349899292  val L1 loss:  5.4007\n",
      "epoch:  9   step:  124   train loss:  6.644043922424316  val loss:  5.18025541305542  val L1 loss:  5.6416\n",
      "epoch:  9   step:  125   train loss:  2.940300941467285  val loss:  5.640294551849365  val L1 loss:  6.1374\n",
      "epoch:  9   step:  126   train loss:  4.469241619110107  val loss:  6.108684062957764  val L1 loss:  6.6071\n",
      "epoch:  9   step:  127   train loss:  3.928394317626953  val loss:  6.205291748046875  val L1 loss:  6.7045\n",
      "epoch:  9   step:  128   train loss:  4.112725257873535  val loss:  5.97547721862793  val L1 loss:  6.461\n",
      "epoch:  9   step:  129   train loss:  2.6811530590057373  val loss:  5.784348011016846  val L1 loss:  6.263\n",
      "epoch:  9   step:  130   train loss:  4.467534065246582  val loss:  5.765047073364258  val L1 loss:  6.2389\n",
      "epoch:  9   step:  131   train loss:  5.492460250854492  val loss:  5.893265724182129  val L1 loss:  6.3661\n",
      "epoch:  9   step:  132   train loss:  3.8861899375915527  val loss:  5.96373176574707  val L1 loss:  6.4311\n",
      "epoch:  9   step:  133   train loss:  3.3773417472839355  val loss:  6.191368103027344  val L1 loss:  6.6832\n",
      "epoch:  9   step:  134   train loss:  7.179125785827637  val loss:  6.185338020324707  val L1 loss:  6.6757\n",
      "epoch:  9   step:  135   train loss:  3.5828752517700195  val loss:  5.730669975280762  val L1 loss:  6.194\n",
      "epoch:  9   step:  136   train loss:  6.099310398101807  val loss:  5.395338535308838  val L1 loss:  5.8726\n",
      "epoch:  9   step:  137   train loss:  4.761993408203125  val loss:  5.184572219848633  val L1 loss:  5.6646\n",
      "epoch:  9   step:  138   train loss:  4.409911155700684  val loss:  5.0903191566467285  val L1 loss:  5.5428\n",
      "epoch:  9   step:  139   train loss:  3.236215591430664  val loss:  5.395366191864014  val L1 loss:  5.8698\n",
      "epoch:  9   step:  140   train loss:  4.867691993713379  val loss:  5.893653392791748  val L1 loss:  6.359\n",
      "epoch:  9   step:  141   train loss:  3.583071708679199  val loss:  5.985044956207275  val L1 loss:  6.4676\n",
      "epoch:  9   step:  142   train loss:  6.471587181091309  val loss:  5.621242523193359  val L1 loss:  6.104\n",
      "epoch:  9   step:  143   train loss:  2.8632125854492188  val loss:  5.493844509124756  val L1 loss:  5.9614\n",
      "epoch:  9   step:  144   train loss:  2.343136787414551  val loss:  5.298043251037598  val L1 loss:  5.7603\n",
      "epoch:  9   step:  145   train loss:  5.38966178894043  val loss:  5.118897914886475  val L1 loss:  5.5924\n",
      "epoch:  9   step:  146   train loss:  5.540169715881348  val loss:  5.0723466873168945  val L1 loss:  5.564\n",
      "epoch:  9   step:  147   train loss:  4.841951370239258  val loss:  5.1698408126831055  val L1 loss:  5.6378\n",
      "epoch:  9   step:  148   train loss:  4.410457611083984  val loss:  5.918149471282959  val L1 loss:  6.4113\n",
      "epoch:  9   step:  149   train loss:  5.613971710205078  val loss:  6.923234939575195  val L1 loss:  7.4024\n",
      "epoch:  9   step:  150   train loss:  3.5684471130371094  val loss:  7.453011989593506  val L1 loss:  7.9369\n",
      "epoch:  9   step:  151   train loss:  4.86448335647583  val loss:  7.381580829620361  val L1 loss:  7.8679\n",
      "epoch:  9   step:  152   train loss:  5.646655082702637  val loss:  7.192523002624512  val L1 loss:  7.6818\n",
      "epoch:  9   step:  153   train loss:  7.150231838226318  val loss:  7.019079208374023  val L1 loss:  7.4927\n",
      "epoch:  9   step:  154   train loss:  5.142632007598877  val loss:  6.75631046295166  val L1 loss:  7.2461\n",
      "epoch:  9   step:  155   train loss:  4.812826156616211  val loss:  6.220284461975098  val L1 loss:  6.695\n",
      "epoch:  9   step:  156   train loss:  3.809206962585449  val loss:  5.879152297973633  val L1 loss:  6.345\n",
      "epoch:  9   step:  157   train loss:  4.773305892944336  val loss:  5.781766414642334  val L1 loss:  6.2412\n",
      "epoch:  9   step:  158   train loss:  5.574916839599609  val loss:  5.977861404418945  val L1 loss:  6.4355\n",
      "epoch:  9   step:  159   train loss:  5.9631853103637695  val loss:  6.123173236846924  val L1 loss:  6.596\n",
      "epoch:  9   step:  160   train loss:  4.8122100830078125  val loss:  6.28070068359375  val L1 loss:  6.7531\n",
      "epoch:  9   step:  161   train loss:  2.661912679672241  val loss:  6.545175075531006  val L1 loss:  7.0366\n",
      "epoch:  9   step:  162   train loss:  4.85268497467041  val loss:  6.909153461456299  val L1 loss:  7.3904\n",
      "epoch:  9   step:  163   train loss:  3.829798698425293  val loss:  7.49340295791626  val L1 loss:  7.973\n",
      "epoch:  9   step:  164   train loss:  5.006735324859619  val loss:  7.628359317779541  val L1 loss:  8.1215\n",
      "epoch:  9   step:  165   train loss:  5.232699871063232  val loss:  7.048521995544434  val L1 loss:  7.5216\n",
      "epoch:  9   step:  166   train loss:  5.393524646759033  val loss:  6.285372734069824  val L1 loss:  6.7617\n",
      "epoch:  9   step:  167   train loss:  4.019896507263184  val loss:  5.785467624664307  val L1 loss:  6.2592\n",
      "epoch:  9   step:  168   train loss:  3.7913990020751953  val loss:  5.604176998138428  val L1 loss:  6.0693\n",
      "epoch:  9   step:  169   train loss:  4.42095947265625  val loss:  5.538422584533691  val L1 loss:  6.0075\n",
      "epoch:  9   step:  170   train loss:  2.2654802799224854  val loss:  5.466714859008789  val L1 loss:  5.9329\n",
      "epoch:  9   step:  171   train loss:  3.1684162616729736  val loss:  5.413650989532471  val L1 loss:  5.8743\n",
      "epoch:  9   step:  172   train loss:  3.345815896987915  val loss:  5.42155647277832  val L1 loss:  5.8783\n",
      "epoch:  9   step:  173   train loss:  3.3013744354248047  val loss:  5.46579647064209  val L1 loss:  5.9337\n",
      "epoch:  9   step:  174   train loss:  4.969132423400879  val loss:  5.334292888641357  val L1 loss:  5.8061\n",
      "epoch:  9   step:  175   train loss:  7.200665473937988  val loss:  5.308199405670166  val L1 loss:  5.7667\n",
      "epoch:  9   step:  176   train loss:  3.7518503665924072  val loss:  5.321207046508789  val L1 loss:  5.7729\n",
      "epoch:  9   step:  177   train loss:  3.28220272064209  val loss:  5.414263725280762  val L1 loss:  5.8749\n",
      "epoch:  9   step:  178   train loss:  3.6736607551574707  val loss:  5.328264236450195  val L1 loss:  5.7845\n",
      "epoch:  9   step:  179   train loss:  3.6529700756073  val loss:  5.459667682647705  val L1 loss:  5.9304\n",
      "epoch:  9   step:  180   train loss:  2.71989107131958  val loss:  5.6458635330200195  val L1 loss:  6.108\n",
      "epoch:  9   step:  181   train loss:  3.8420255184173584  val loss:  5.514389514923096  val L1 loss:  5.9725\n",
      "epoch:  9   step:  182   train loss:  6.176179885864258  val loss:  5.0640058517456055  val L1 loss:  5.5227\n",
      "epoch:  9   step:  183   train loss:  5.400781631469727  val loss:  4.908297538757324  val L1 loss:  5.3745\n",
      "epoch:  9   step:  184   train loss:  7.212037563323975  val loss:  5.038222312927246  val L1 loss:  5.5175\n",
      "epoch:  9   step:  185   train loss:  2.7146873474121094  val loss:  5.171000957489014  val L1 loss:  5.6565\n",
      "epoch:  9   step:  186   train loss:  4.95610237121582  val loss:  5.064633846282959  val L1 loss:  5.532\n",
      "epoch:  9   step:  187   train loss:  3.418806791305542  val loss:  4.98281717300415  val L1 loss:  5.4457\n",
      "epoch:  9   step:  188   train loss:  4.919125556945801  val loss:  5.037179470062256  val L1 loss:  5.4937\n",
      "epoch:  9   step:  189   train loss:  3.700991630554199  val loss:  5.096508026123047  val L1 loss:  5.5546\n",
      "epoch:  9   step:  190   train loss:  5.936639785766602  val loss:  5.097095012664795  val L1 loss:  5.5711\n",
      "epoch:  9   step:  191   train loss:  3.318202018737793  val loss:  5.093503475189209  val L1 loss:  5.5794\n",
      "epoch:  9   step:  192   train loss:  7.645881652832031  val loss:  5.198371410369873  val L1 loss:  5.6562\n",
      "epoch:  9   step:  193   train loss:  3.1235599517822266  val loss:  5.3018798828125  val L1 loss:  5.7747\n",
      "epoch:  9   step:  194   train loss:  3.4838433265686035  val loss:  5.427017688751221  val L1 loss:  5.8874\n",
      "epoch:  9   step:  195   train loss:  4.466383457183838  val loss:  5.544150352478027  val L1 loss:  6.0057\n",
      "epoch:  9   step:  196   train loss:  6.156300067901611  val loss:  5.514099597930908  val L1 loss:  5.9886\n",
      "epoch:  9   step:  197   train loss:  2.4993176460266113  val loss:  5.357837200164795  val L1 loss:  5.8269\n",
      "epoch:  9   step:  198   train loss:  4.141651153564453  val loss:  5.388272762298584  val L1 loss:  5.8837\n",
      "epoch:  9   step:  199   train loss:  3.9838976860046387  val loss:  5.425617694854736  val L1 loss:  5.9231\n",
      "epoch:  9   step:  200   train loss:  5.697622299194336  val loss:  5.401149749755859  val L1 loss:  5.8804\n",
      "epoch:  9   step:  201   train loss:  3.561408519744873  val loss:  5.5504679679870605  val L1 loss:  6.0289\n",
      "epoch:  9   step:  202   train loss:  3.1911964416503906  val loss:  5.749684810638428  val L1 loss:  6.2297\n",
      "epoch:  9   step:  203   train loss:  4.375250816345215  val loss:  6.207430362701416  val L1 loss:  6.6906\n",
      "epoch:  9   step:  204   train loss:  4.593356132507324  val loss:  6.312946796417236  val L1 loss:  6.8039\n",
      "epoch:  9   step:  205   train loss:  5.5241899490356445  val loss:  6.104632377624512  val L1 loss:  6.5958\n",
      "epoch:  9   step:  206   train loss:  4.151913642883301  val loss:  5.812540054321289  val L1 loss:  6.2977\n",
      "epoch:  9   step:  207   train loss:  4.345017433166504  val loss:  5.591753959655762  val L1 loss:  6.0593\n",
      "epoch:  9   step:  208   train loss:  3.33809494972229  val loss:  5.48250150680542  val L1 loss:  5.9631\n",
      "epoch:  10   step:  0   train loss:  3.494103193283081  val loss:  5.566832542419434  val L1 loss:  6.0233\n",
      "epoch:  10   step:  1   train loss:  4.334683418273926  val loss:  5.512185096740723  val L1 loss:  5.9804\n",
      "epoch:  10   step:  2   train loss:  4.61000919342041  val loss:  5.399137020111084  val L1 loss:  5.8511\n",
      "epoch:  10   step:  3   train loss:  3.1432666778564453  val loss:  5.561364650726318  val L1 loss:  6.0377\n",
      "epoch:  10   step:  4   train loss:  4.5540947914123535  val loss:  5.775683403015137  val L1 loss:  6.2624\n",
      "epoch:  10   step:  5   train loss:  4.5203399658203125  val loss:  5.844862937927246  val L1 loss:  6.321\n",
      "epoch:  10   step:  6   train loss:  4.887684345245361  val loss:  5.771177768707275  val L1 loss:  6.2541\n",
      "epoch:  10   step:  7   train loss:  3.4939687252044678  val loss:  5.649699687957764  val L1 loss:  6.1332\n",
      "epoch:  10   step:  8   train loss:  4.066471099853516  val loss:  5.481922626495361  val L1 loss:  5.9443\n",
      "epoch:  10   step:  9   train loss:  3.840824604034424  val loss:  5.452110290527344  val L1 loss:  5.9251\n",
      "epoch:  10   step:  10   train loss:  5.227525234222412  val loss:  5.521223545074463  val L1 loss:  5.9683\n",
      "epoch:  10   step:  11   train loss:  2.5758140087127686  val loss:  5.708853244781494  val L1 loss:  6.1693\n",
      "epoch:  10   step:  12   train loss:  5.080031394958496  val loss:  5.80199670791626  val L1 loss:  6.2804\n",
      "epoch:  10   step:  13   train loss:  3.6992545127868652  val loss:  5.677138328552246  val L1 loss:  6.1456\n",
      "epoch:  10   step:  14   train loss:  2.3396873474121094  val loss:  5.474499225616455  val L1 loss:  5.9208\n",
      "epoch:  10   step:  15   train loss:  2.921417713165283  val loss:  5.284384727478027  val L1 loss:  5.7649\n",
      "epoch:  10   step:  16   train loss:  4.620245933532715  val loss:  5.153994083404541  val L1 loss:  5.634\n",
      "epoch:  10   step:  17   train loss:  5.463958740234375  val loss:  5.22773551940918  val L1 loss:  5.7043\n",
      "epoch:  10   step:  18   train loss:  3.6575698852539062  val loss:  5.325304985046387  val L1 loss:  5.8001\n",
      "epoch:  10   step:  19   train loss:  4.085986137390137  val loss:  5.395678520202637  val L1 loss:  5.8771\n",
      "epoch:  10   step:  20   train loss:  2.922879695892334  val loss:  5.291328430175781  val L1 loss:  5.7534\n",
      "epoch:  10   step:  21   train loss:  3.4469199180603027  val loss:  5.202962398529053  val L1 loss:  5.6838\n",
      "epoch:  10   step:  22   train loss:  3.2081491947174072  val loss:  5.106263637542725  val L1 loss:  5.5827\n",
      "epoch:  10   step:  23   train loss:  3.5954999923706055  val loss:  5.054862022399902  val L1 loss:  5.5236\n",
      "epoch:  10   step:  24   train loss:  4.408071517944336  val loss:  5.019653797149658  val L1 loss:  5.4925\n",
      "epoch:  10   step:  25   train loss:  2.4995760917663574  val loss:  5.188406944274902  val L1 loss:  5.6602\n",
      "epoch:  10   step:  26   train loss:  3.851585865020752  val loss:  5.263408660888672  val L1 loss:  5.7418\n",
      "epoch:  10   step:  27   train loss:  3.69334077835083  val loss:  5.199246883392334  val L1 loss:  5.6751\n",
      "epoch:  10   step:  28   train loss:  2.8288826942443848  val loss:  5.0123748779296875  val L1 loss:  5.4828\n",
      "epoch:  10   step:  29   train loss:  4.1830573081970215  val loss:  4.996470928192139  val L1 loss:  5.4732\n",
      "epoch:  10   step:  30   train loss:  3.462897300720215  val loss:  5.13099479675293  val L1 loss:  5.5929\n",
      "epoch:  10   step:  31   train loss:  3.333045244216919  val loss:  5.251290321350098  val L1 loss:  5.7243\n",
      "epoch:  10   step:  32   train loss:  2.2702949047088623  val loss:  5.249858379364014  val L1 loss:  5.7228\n",
      "epoch:  10   step:  33   train loss:  5.258526802062988  val loss:  5.181123733520508  val L1 loss:  5.6535\n",
      "epoch:  10   step:  34   train loss:  3.9206106662750244  val loss:  5.046822547912598  val L1 loss:  5.5182\n",
      "epoch:  10   step:  35   train loss:  3.601785182952881  val loss:  5.0401201248168945  val L1 loss:  5.5045\n",
      "epoch:  10   step:  36   train loss:  4.598408222198486  val loss:  5.018552303314209  val L1 loss:  5.4944\n",
      "epoch:  10   step:  37   train loss:  2.6332592964172363  val loss:  4.962002277374268  val L1 loss:  5.4304\n",
      "epoch:  10   step:  38   train loss:  7.289741039276123  val loss:  4.994134902954102  val L1 loss:  5.4624\n",
      "epoch:  10   step:  39   train loss:  4.877682685852051  val loss:  5.0105390548706055  val L1 loss:  5.4852\n",
      "epoch:  10   step:  40   train loss:  4.445013523101807  val loss:  4.973294734954834  val L1 loss:  5.4385\n",
      "epoch:  10   step:  41   train loss:  2.15347957611084  val loss:  4.919878005981445  val L1 loss:  5.386\n",
      "epoch:  10   step:  42   train loss:  3.039853096008301  val loss:  4.926016330718994  val L1 loss:  5.4044\n",
      "epoch:  10   step:  43   train loss:  3.7037107944488525  val loss:  5.002142429351807  val L1 loss:  5.4843\n",
      "epoch:  10   step:  44   train loss:  3.2750277519226074  val loss:  5.107316493988037  val L1 loss:  5.5743\n",
      "epoch:  10   step:  45   train loss:  3.236870288848877  val loss:  5.19461727142334  val L1 loss:  5.6669\n",
      "epoch:  10   step:  46   train loss:  2.5520730018615723  val loss:  5.219470500946045  val L1 loss:  5.6961\n",
      "epoch:  10   step:  47   train loss:  5.449716567993164  val loss:  4.949741840362549  val L1 loss:  5.4185\n",
      "epoch:  10   step:  48   train loss:  4.608405590057373  val loss:  4.884319305419922  val L1 loss:  5.363\n",
      "epoch:  10   step:  49   train loss:  4.44183349609375  val loss:  5.101217746734619  val L1 loss:  5.5611\n",
      "epoch:  10   step:  50   train loss:  4.828814506530762  val loss:  5.165892124176025  val L1 loss:  5.644\n",
      "epoch:  10   step:  51   train loss:  3.406404495239258  val loss:  4.831803321838379  val L1 loss:  5.2852\n",
      "epoch:  10   step:  52   train loss:  5.20570707321167  val loss:  4.834386825561523  val L1 loss:  5.3065\n",
      "epoch:  10   step:  53   train loss:  5.938603401184082  val loss:  5.064924240112305  val L1 loss:  5.5483\n",
      "epoch:  10   step:  54   train loss:  3.0007917881011963  val loss:  5.068759918212891  val L1 loss:  5.5515\n",
      "epoch:  10   step:  55   train loss:  5.392590522766113  val loss:  4.854550838470459  val L1 loss:  5.3173\n",
      "epoch:  10   step:  56   train loss:  3.303682804107666  val loss:  4.897669792175293  val L1 loss:  5.3624\n",
      "epoch:  10   step:  57   train loss:  6.068950653076172  val loss:  5.082924842834473  val L1 loss:  5.5563\n",
      "epoch:  10   step:  58   train loss:  3.050133228302002  val loss:  4.8973188400268555  val L1 loss:  5.3818\n",
      "epoch:  10   step:  59   train loss:  4.037575721740723  val loss:  4.593111038208008  val L1 loss:  5.0604\n",
      "min_val_loss_print 4.593111038208008\n",
      "epoch:  10   step:  60   train loss:  3.4105982780456543  val loss:  4.519710063934326  val L1 loss:  4.9689\n",
      "min_val_loss_print 4.519710063934326\n",
      "epoch:  10   step:  61   train loss:  8.073993682861328  val loss:  4.969400405883789  val L1 loss:  5.4534\n",
      "epoch:  10   step:  62   train loss:  4.291541576385498  val loss:  5.589964389801025  val L1 loss:  6.0897\n",
      "epoch:  10   step:  63   train loss:  4.894880294799805  val loss:  5.516415119171143  val L1 loss:  5.9901\n",
      "epoch:  10   step:  64   train loss:  5.330711841583252  val loss:  5.102082252502441  val L1 loss:  5.5815\n",
      "epoch:  10   step:  65   train loss:  1.6204874515533447  val loss:  4.889945983886719  val L1 loss:  5.3766\n",
      "epoch:  10   step:  66   train loss:  4.173162937164307  val loss:  4.595559120178223  val L1 loss:  5.0633\n",
      "epoch:  10   step:  67   train loss:  4.501859664916992  val loss:  4.485513210296631  val L1 loss:  4.9314\n",
      "min_val_loss_print 4.485513210296631\n",
      "epoch:  10   step:  68   train loss:  2.906465530395508  val loss:  4.815852642059326  val L1 loss:  5.2887\n",
      "epoch:  10   step:  69   train loss:  3.240159034729004  val loss:  5.144415378570557  val L1 loss:  5.6088\n",
      "epoch:  10   step:  70   train loss:  4.617651462554932  val loss:  5.505162239074707  val L1 loss:  5.968\n",
      "epoch:  10   step:  71   train loss:  2.8377108573913574  val loss:  5.708217144012451  val L1 loss:  6.1912\n",
      "epoch:  10   step:  72   train loss:  4.632952690124512  val loss:  5.645503520965576  val L1 loss:  6.1167\n",
      "epoch:  10   step:  73   train loss:  4.46157169342041  val loss:  5.616045951843262  val L1 loss:  6.0987\n",
      "epoch:  10   step:  74   train loss:  4.162162780761719  val loss:  5.389454364776611  val L1 loss:  5.8754\n",
      "epoch:  10   step:  75   train loss:  3.608189821243286  val loss:  4.984068393707275  val L1 loss:  5.4588\n",
      "epoch:  10   step:  76   train loss:  3.9639129638671875  val loss:  4.999841690063477  val L1 loss:  5.4647\n",
      "epoch:  10   step:  77   train loss:  7.179098129272461  val loss:  5.5047125816345215  val L1 loss:  5.9901\n",
      "epoch:  10   step:  78   train loss:  4.283212661743164  val loss:  5.370326995849609  val L1 loss:  5.8571\n",
      "epoch:  10   step:  79   train loss:  3.1916255950927734  val loss:  4.874137878417969  val L1 loss:  5.3507\n",
      "epoch:  10   step:  80   train loss:  5.06790828704834  val loss:  4.845152378082275  val L1 loss:  5.2943\n",
      "epoch:  10   step:  81   train loss:  4.9442973136901855  val loss:  5.232454776763916  val L1 loss:  5.696\n",
      "epoch:  10   step:  82   train loss:  4.47130823135376  val loss:  5.279768943786621  val L1 loss:  5.7477\n",
      "epoch:  10   step:  83   train loss:  6.316583633422852  val loss:  5.0709381103515625  val L1 loss:  5.5481\n",
      "epoch:  10   step:  84   train loss:  4.837669849395752  val loss:  5.03315544128418  val L1 loss:  5.5081\n",
      "epoch:  10   step:  85   train loss:  3.894962787628174  val loss:  5.067002296447754  val L1 loss:  5.5504\n",
      "epoch:  10   step:  86   train loss:  3.1946284770965576  val loss:  5.053190231323242  val L1 loss:  5.5149\n",
      "epoch:  10   step:  87   train loss:  3.839017868041992  val loss:  4.991662502288818  val L1 loss:  5.453\n",
      "epoch:  10   step:  88   train loss:  4.951101303100586  val loss:  4.95815896987915  val L1 loss:  5.4296\n",
      "epoch:  10   step:  89   train loss:  4.989177227020264  val loss:  4.944159030914307  val L1 loss:  5.4287\n",
      "epoch:  10   step:  90   train loss:  4.873354911804199  val loss:  4.874319076538086  val L1 loss:  5.3571\n",
      "epoch:  10   step:  91   train loss:  3.6215310096740723  val loss:  4.819351673126221  val L1 loss:  5.3034\n",
      "epoch:  10   step:  92   train loss:  2.7418384552001953  val loss:  4.794793128967285  val L1 loss:  5.2719\n",
      "epoch:  10   step:  93   train loss:  4.392600059509277  val loss:  4.7936530113220215  val L1 loss:  5.2718\n",
      "epoch:  10   step:  94   train loss:  4.2293853759765625  val loss:  4.73470401763916  val L1 loss:  5.207\n",
      "epoch:  10   step:  95   train loss:  4.203432083129883  val loss:  4.677970886230469  val L1 loss:  5.1531\n",
      "epoch:  10   step:  96   train loss:  3.416321277618408  val loss:  4.712151527404785  val L1 loss:  5.1644\n",
      "epoch:  10   step:  97   train loss:  3.894294023513794  val loss:  4.821229934692383  val L1 loss:  5.287\n",
      "epoch:  10   step:  98   train loss:  4.5406951904296875  val loss:  4.89957332611084  val L1 loss:  5.3591\n",
      "epoch:  10   step:  99   train loss:  3.1546154022216797  val loss:  4.9351301193237305  val L1 loss:  5.429\n",
      "epoch:  10   step:  100   train loss:  3.283052444458008  val loss:  5.030720233917236  val L1 loss:  5.5168\n",
      "epoch:  10   step:  101   train loss:  3.3975863456726074  val loss:  5.059213638305664  val L1 loss:  5.5278\n",
      "epoch:  10   step:  102   train loss:  2.807164192199707  val loss:  5.12678337097168  val L1 loss:  5.5984\n",
      "epoch:  10   step:  103   train loss:  2.6771960258483887  val loss:  5.274192810058594  val L1 loss:  5.7268\n",
      "epoch:  10   step:  104   train loss:  4.26963472366333  val loss:  5.4423346519470215  val L1 loss:  5.9086\n",
      "epoch:  10   step:  105   train loss:  2.7723450660705566  val loss:  5.62573766708374  val L1 loss:  6.0971\n",
      "epoch:  10   step:  106   train loss:  2.233569383621216  val loss:  5.607857704162598  val L1 loss:  6.0779\n",
      "epoch:  10   step:  107   train loss:  3.546234130859375  val loss:  5.650645732879639  val L1 loss:  6.1163\n",
      "epoch:  10   step:  108   train loss:  2.5811314582824707  val loss:  5.545591831207275  val L1 loss:  6.0111\n",
      "epoch:  10   step:  109   train loss:  5.032934188842773  val loss:  5.511070251464844  val L1 loss:  5.9901\n",
      "epoch:  10   step:  110   train loss:  3.2428441047668457  val loss:  5.580452919006348  val L1 loss:  6.0402\n",
      "epoch:  10   step:  111   train loss:  4.696164131164551  val loss:  5.96963357925415  val L1 loss:  6.4427\n",
      "epoch:  10   step:  112   train loss:  7.09018611907959  val loss:  6.212291717529297  val L1 loss:  6.6951\n",
      "epoch:  10   step:  113   train loss:  3.4958584308624268  val loss:  6.180536270141602  val L1 loss:  6.6455\n",
      "epoch:  10   step:  114   train loss:  6.992119312286377  val loss:  6.223852634429932  val L1 loss:  6.6914\n",
      "epoch:  10   step:  115   train loss:  4.990438461303711  val loss:  6.369093894958496  val L1 loss:  6.8218\n",
      "epoch:  10   step:  116   train loss:  4.758304119110107  val loss:  6.321404933929443  val L1 loss:  6.7969\n",
      "epoch:  10   step:  117   train loss:  4.402802467346191  val loss:  5.618001461029053  val L1 loss:  6.1044\n",
      "epoch:  10   step:  118   train loss:  4.139784336090088  val loss:  5.2202863693237305  val L1 loss:  5.6909\n",
      "epoch:  10   step:  119   train loss:  6.320583343505859  val loss:  5.1639227867126465  val L1 loss:  5.6422\n",
      "epoch:  10   step:  120   train loss:  3.704991579055786  val loss:  5.144317626953125  val L1 loss:  5.6088\n",
      "epoch:  10   step:  121   train loss:  4.226982593536377  val loss:  5.155452728271484  val L1 loss:  5.631\n",
      "epoch:  10   step:  122   train loss:  5.139313697814941  val loss:  5.002103328704834  val L1 loss:  5.4601\n",
      "epoch:  10   step:  123   train loss:  3.7509002685546875  val loss:  5.16563606262207  val L1 loss:  5.6558\n",
      "epoch:  10   step:  124   train loss:  5.067174911499023  val loss:  5.917973041534424  val L1 loss:  6.4085\n",
      "epoch:  10   step:  125   train loss:  3.1685097217559814  val loss:  6.516992568969727  val L1 loss:  7.0125\n",
      "epoch:  10   step:  126   train loss:  3.356693744659424  val loss:  6.390477657318115  val L1 loss:  6.8849\n",
      "epoch:  10   step:  127   train loss:  7.2219085693359375  val loss:  5.837884426116943  val L1 loss:  6.3379\n",
      "epoch:  10   step:  128   train loss:  2.6794533729553223  val loss:  5.153332233428955  val L1 loss:  5.5936\n",
      "epoch:  10   step:  129   train loss:  2.3296737670898438  val loss:  5.121316432952881  val L1 loss:  5.591\n",
      "epoch:  10   step:  130   train loss:  2.8545141220092773  val loss:  5.271371364593506  val L1 loss:  5.7629\n",
      "epoch:  10   step:  131   train loss:  3.4257192611694336  val loss:  5.369297981262207  val L1 loss:  5.8529\n",
      "epoch:  10   step:  132   train loss:  3.701141357421875  val loss:  5.033099174499512  val L1 loss:  5.5222\n",
      "epoch:  10   step:  133   train loss:  10.245539665222168  val loss:  5.028229713439941  val L1 loss:  5.5109\n",
      "epoch:  10   step:  134   train loss:  2.914440631866455  val loss:  5.600129127502441  val L1 loss:  6.0769\n",
      "epoch:  10   step:  135   train loss:  3.809438705444336  val loss:  5.936545372009277  val L1 loss:  6.4288\n",
      "epoch:  10   step:  136   train loss:  3.785855293273926  val loss:  5.681520462036133  val L1 loss:  6.1615\n",
      "epoch:  10   step:  137   train loss:  6.12431526184082  val loss:  5.393874168395996  val L1 loss:  5.86\n",
      "epoch:  10   step:  138   train loss:  3.4552130699157715  val loss:  5.035017013549805  val L1 loss:  5.5275\n",
      "epoch:  10   step:  139   train loss:  6.402606964111328  val loss:  4.937800884246826  val L1 loss:  5.4083\n",
      "epoch:  10   step:  140   train loss:  2.7172322273254395  val loss:  4.900984764099121  val L1 loss:  5.3556\n",
      "epoch:  10   step:  141   train loss:  8.677217483520508  val loss:  4.901549339294434  val L1 loss:  5.3562\n",
      "epoch:  10   step:  142   train loss:  4.563852310180664  val loss:  4.935475826263428  val L1 loss:  5.3903\n",
      "epoch:  10   step:  143   train loss:  2.3521041870117188  val loss:  5.145905017852783  val L1 loss:  5.6419\n",
      "epoch:  10   step:  144   train loss:  4.258549213409424  val loss:  5.303984642028809  val L1 loss:  5.7879\n",
      "epoch:  10   step:  145   train loss:  3.8584208488464355  val loss:  5.341350078582764  val L1 loss:  5.8303\n",
      "epoch:  10   step:  146   train loss:  5.195878028869629  val loss:  5.185921669006348  val L1 loss:  5.6828\n",
      "epoch:  10   step:  147   train loss:  4.725760459899902  val loss:  5.066799640655518  val L1 loss:  5.5339\n",
      "epoch:  10   step:  148   train loss:  3.99479341506958  val loss:  5.037478923797607  val L1 loss:  5.4836\n",
      "epoch:  10   step:  149   train loss:  4.4485015869140625  val loss:  5.14496374130249  val L1 loss:  5.61\n",
      "epoch:  10   step:  150   train loss:  5.330296039581299  val loss:  5.4493536949157715  val L1 loss:  5.9314\n",
      "epoch:  10   step:  151   train loss:  4.227740287780762  val loss:  5.47652006149292  val L1 loss:  5.951\n",
      "epoch:  10   step:  152   train loss:  4.31339168548584  val loss:  5.167616367340088  val L1 loss:  5.6329\n",
      "epoch:  10   step:  153   train loss:  3.3983981609344482  val loss:  5.025999546051025  val L1 loss:  5.4879\n",
      "epoch:  10   step:  154   train loss:  2.920154094696045  val loss:  5.1038432121276855  val L1 loss:  5.5756\n",
      "epoch:  10   step:  155   train loss:  4.758691787719727  val loss:  5.24934196472168  val L1 loss:  5.7309\n",
      "epoch:  10   step:  156   train loss:  4.616171836853027  val loss:  5.115132808685303  val L1 loss:  5.5812\n",
      "epoch:  10   step:  157   train loss:  4.379354476928711  val loss:  4.909296989440918  val L1 loss:  5.3706\n",
      "epoch:  10   step:  158   train loss:  2.2259397506713867  val loss:  5.23683500289917  val L1 loss:  5.724\n",
      "epoch:  10   step:  159   train loss:  4.790764808654785  val loss:  5.519677639007568  val L1 loss:  5.9909\n",
      "epoch:  10   step:  160   train loss:  4.661516189575195  val loss:  5.33680534362793  val L1 loss:  5.8138\n",
      "epoch:  10   step:  161   train loss:  3.392596483230591  val loss:  4.962438106536865  val L1 loss:  5.4175\n",
      "epoch:  10   step:  162   train loss:  6.108915328979492  val loss:  4.855774402618408  val L1 loss:  5.3392\n",
      "epoch:  10   step:  163   train loss:  5.4133620262146  val loss:  5.110555171966553  val L1 loss:  5.5976\n",
      "epoch:  10   step:  164   train loss:  3.1336686611175537  val loss:  5.091756820678711  val L1 loss:  5.5773\n",
      "epoch:  10   step:  165   train loss:  5.556282997131348  val loss:  5.1351423263549805  val L1 loss:  5.6248\n",
      "epoch:  10   step:  166   train loss:  3.2882580757141113  val loss:  4.945994853973389  val L1 loss:  5.4311\n",
      "epoch:  10   step:  167   train loss:  4.888271331787109  val loss:  4.7899885177612305  val L1 loss:  5.2683\n",
      "epoch:  10   step:  168   train loss:  4.111917972564697  val loss:  4.737459182739258  val L1 loss:  5.1846\n",
      "epoch:  10   step:  169   train loss:  6.462111473083496  val loss:  4.68654203414917  val L1 loss:  5.147\n",
      "epoch:  10   step:  170   train loss:  4.712852954864502  val loss:  4.685846328735352  val L1 loss:  5.1688\n",
      "epoch:  10   step:  171   train loss:  3.471654176712036  val loss:  4.75513219833374  val L1 loss:  5.2335\n",
      "epoch:  10   step:  172   train loss:  5.9973368644714355  val loss:  4.914701461791992  val L1 loss:  5.3966\n",
      "epoch:  10   step:  173   train loss:  5.479032039642334  val loss:  5.387975215911865  val L1 loss:  5.8772\n",
      "epoch:  10   step:  174   train loss:  4.455365180969238  val loss:  5.578444480895996  val L1 loss:  6.0664\n",
      "epoch:  10   step:  175   train loss:  4.233839988708496  val loss:  5.289294242858887  val L1 loss:  5.7766\n",
      "epoch:  10   step:  176   train loss:  4.674751281738281  val loss:  4.923405170440674  val L1 loss:  5.4105\n",
      "epoch:  10   step:  177   train loss:  3.0151572227478027  val loss:  4.678550720214844  val L1 loss:  5.1182\n",
      "epoch:  10   step:  178   train loss:  4.820789813995361  val loss:  4.82296085357666  val L1 loss:  5.2866\n",
      "epoch:  10   step:  179   train loss:  5.608987808227539  val loss:  4.8325982093811035  val L1 loss:  5.2889\n",
      "epoch:  10   step:  180   train loss:  3.9191641807556152  val loss:  4.715151786804199  val L1 loss:  5.1654\n",
      "epoch:  10   step:  181   train loss:  3.9173004627227783  val loss:  4.622701168060303  val L1 loss:  5.0688\n",
      "epoch:  10   step:  182   train loss:  5.958439826965332  val loss:  4.733084201812744  val L1 loss:  5.199\n",
      "epoch:  10   step:  183   train loss:  3.9434702396392822  val loss:  4.887466907501221  val L1 loss:  5.3525\n",
      "epoch:  10   step:  184   train loss:  5.179804801940918  val loss:  4.949620723724365  val L1 loss:  5.432\n",
      "epoch:  10   step:  185   train loss:  4.584704875946045  val loss:  4.949610710144043  val L1 loss:  5.4195\n",
      "epoch:  10   step:  186   train loss:  5.631752014160156  val loss:  5.248265266418457  val L1 loss:  5.7264\n",
      "epoch:  10   step:  187   train loss:  4.34343957901001  val loss:  5.718283653259277  val L1 loss:  6.2072\n",
      "epoch:  10   step:  188   train loss:  7.997452735900879  val loss:  5.696374416351318  val L1 loss:  6.1821\n",
      "epoch:  10   step:  189   train loss:  3.756021738052368  val loss:  5.495574951171875  val L1 loss:  5.9795\n",
      "epoch:  10   step:  190   train loss:  4.994640350341797  val loss:  4.9848785400390625  val L1 loss:  5.4525\n",
      "epoch:  10   step:  191   train loss:  3.517921209335327  val loss:  5.0578789710998535  val L1 loss:  5.5324\n",
      "epoch:  10   step:  192   train loss:  3.0055758953094482  val loss:  5.693665504455566  val L1 loss:  6.1745\n",
      "epoch:  10   step:  193   train loss:  4.000271797180176  val loss:  6.139064311981201  val L1 loss:  6.6217\n",
      "epoch:  10   step:  194   train loss:  8.060952186584473  val loss:  6.095803737640381  val L1 loss:  6.5751\n",
      "epoch:  10   step:  195   train loss:  3.755156993865967  val loss:  5.410245418548584  val L1 loss:  5.8863\n",
      "epoch:  10   step:  196   train loss:  6.1529645919799805  val loss:  5.084758758544922  val L1 loss:  5.5595\n",
      "epoch:  10   step:  197   train loss:  4.633969783782959  val loss:  5.040503978729248  val L1 loss:  5.5082\n",
      "epoch:  10   step:  198   train loss:  3.9828224182128906  val loss:  5.035309791564941  val L1 loss:  5.4916\n",
      "epoch:  10   step:  199   train loss:  5.7358245849609375  val loss:  5.224413871765137  val L1 loss:  5.6923\n",
      "epoch:  10   step:  200   train loss:  3.4122262001037598  val loss:  5.391811847686768  val L1 loss:  5.8752\n",
      "epoch:  10   step:  201   train loss:  4.373664855957031  val loss:  5.38880729675293  val L1 loss:  5.8829\n",
      "epoch:  10   step:  202   train loss:  4.525623798370361  val loss:  5.634888648986816  val L1 loss:  6.1335\n",
      "epoch:  10   step:  203   train loss:  3.1292319297790527  val loss:  5.67270565032959  val L1 loss:  6.1678\n",
      "epoch:  10   step:  204   train loss:  4.586170196533203  val loss:  5.796979904174805  val L1 loss:  6.282\n",
      "epoch:  10   step:  205   train loss:  4.497756481170654  val loss:  5.806431293487549  val L1 loss:  6.2961\n",
      "epoch:  10   step:  206   train loss:  2.188661575317383  val loss:  5.729621410369873  val L1 loss:  6.1929\n",
      "epoch:  10   step:  207   train loss:  4.264184474945068  val loss:  5.631368160247803  val L1 loss:  6.1021\n",
      "epoch:  10   step:  208   train loss:  5.792527675628662  val loss:  5.531965255737305  val L1 loss:  6.0051\n",
      "epoch:  11   step:  0   train loss:  5.2394208908081055  val loss:  5.588087558746338  val L1 loss:  6.0459\n",
      "epoch:  11   step:  1   train loss:  5.87207555770874  val loss:  5.563834190368652  val L1 loss:  6.0282\n",
      "epoch:  11   step:  2   train loss:  4.309607982635498  val loss:  5.446780681610107  val L1 loss:  5.9032\n",
      "epoch:  11   step:  3   train loss:  6.056262493133545  val loss:  5.430168151855469  val L1 loss:  5.9183\n",
      "epoch:  11   step:  4   train loss:  2.2733991146087646  val loss:  5.667839050292969  val L1 loss:  6.1398\n",
      "epoch:  11   step:  5   train loss:  4.484615325927734  val loss:  5.631227970123291  val L1 loss:  6.1041\n",
      "epoch:  11   step:  6   train loss:  6.331967353820801  val loss:  5.627219200134277  val L1 loss:  6.1106\n",
      "epoch:  11   step:  7   train loss:  3.5426902770996094  val loss:  5.671749591827393  val L1 loss:  6.1567\n",
      "epoch:  11   step:  8   train loss:  1.8071727752685547  val loss:  5.53924560546875  val L1 loss:  6.0208\n",
      "epoch:  11   step:  9   train loss:  2.8767147064208984  val loss:  5.321839332580566  val L1 loss:  5.7974\n",
      "epoch:  11   step:  10   train loss:  3.5227601528167725  val loss:  5.0470476150512695  val L1 loss:  5.4986\n",
      "epoch:  11   step:  11   train loss:  4.595855712890625  val loss:  5.2435431480407715  val L1 loss:  5.7251\n",
      "epoch:  11   step:  12   train loss:  4.336234092712402  val loss:  5.320175647735596  val L1 loss:  5.8015\n",
      "epoch:  11   step:  13   train loss:  5.66437292098999  val loss:  5.094396591186523  val L1 loss:  5.5574\n",
      "epoch:  11   step:  14   train loss:  4.126518249511719  val loss:  5.22651481628418  val L1 loss:  5.7082\n",
      "epoch:  11   step:  15   train loss:  3.4910998344421387  val loss:  5.625319480895996  val L1 loss:  6.1126\n",
      "epoch:  11   step:  16   train loss:  3.7278192043304443  val loss:  5.863292217254639  val L1 loss:  6.3475\n",
      "epoch:  11   step:  17   train loss:  3.4557507038116455  val loss:  5.878780841827393  val L1 loss:  6.3551\n",
      "epoch:  11   step:  18   train loss:  2.7605831623077393  val loss:  5.483071327209473  val L1 loss:  5.9742\n",
      "epoch:  11   step:  19   train loss:  2.075058698654175  val loss:  5.257911205291748  val L1 loss:  5.7466\n",
      "epoch:  11   step:  20   train loss:  5.564205169677734  val loss:  5.284956932067871  val L1 loss:  5.7743\n",
      "epoch:  11   step:  21   train loss:  3.2363638877868652  val loss:  5.286564350128174  val L1 loss:  5.7692\n",
      "epoch:  11   step:  22   train loss:  5.627190113067627  val loss:  5.4973649978637695  val L1 loss:  5.9867\n",
      "epoch:  11   step:  23   train loss:  4.941285610198975  val loss:  5.692400932312012  val L1 loss:  6.1822\n",
      "epoch:  11   step:  24   train loss:  3.114290237426758  val loss:  5.812593936920166  val L1 loss:  6.2974\n",
      "epoch:  11   step:  25   train loss:  3.8564000129699707  val loss:  5.949672222137451  val L1 loss:  6.4382\n",
      "epoch:  11   step:  26   train loss:  2.7802271842956543  val loss:  6.094969272613525  val L1 loss:  6.5742\n",
      "epoch:  11   step:  27   train loss:  4.747471809387207  val loss:  5.751600742340088  val L1 loss:  6.2433\n",
      "epoch:  11   step:  28   train loss:  5.466808795928955  val loss:  5.418512344360352  val L1 loss:  5.904\n",
      "epoch:  11   step:  29   train loss:  3.1752541065216064  val loss:  5.06438684463501  val L1 loss:  5.5379\n",
      "epoch:  11   step:  30   train loss:  3.3853261470794678  val loss:  4.9271697998046875  val L1 loss:  5.3886\n",
      "epoch:  11   step:  31   train loss:  3.0332117080688477  val loss:  4.8855671882629395  val L1 loss:  5.3544\n",
      "epoch:  11   step:  32   train loss:  3.954058885574341  val loss:  4.888560771942139  val L1 loss:  5.3586\n",
      "epoch:  11   step:  33   train loss:  3.146549940109253  val loss:  5.05845308303833  val L1 loss:  5.5118\n",
      "epoch:  11   step:  34   train loss:  3.160162925720215  val loss:  5.33858060836792  val L1 loss:  5.7962\n",
      "epoch:  11   step:  35   train loss:  2.422166347503662  val loss:  5.657126426696777  val L1 loss:  6.1275\n",
      "epoch:  11   step:  36   train loss:  4.4271111488342285  val loss:  5.530441761016846  val L1 loss:  6.0227\n",
      "epoch:  11   step:  37   train loss:  3.026488780975342  val loss:  5.529172897338867  val L1 loss:  6.0128\n",
      "epoch:  11   step:  38   train loss:  5.368391990661621  val loss:  5.767534255981445  val L1 loss:  6.2522\n",
      "epoch:  11   step:  39   train loss:  2.8466765880584717  val loss:  6.073182106018066  val L1 loss:  6.5398\n",
      "epoch:  11   step:  40   train loss:  3.743882179260254  val loss:  6.19610071182251  val L1 loss:  6.6615\n",
      "epoch:  11   step:  41   train loss:  5.150478363037109  val loss:  5.943480014801025  val L1 loss:  6.4015\n",
      "epoch:  11   step:  42   train loss:  7.365109443664551  val loss:  5.380563259124756  val L1 loss:  5.8637\n",
      "epoch:  11   step:  43   train loss:  3.760145664215088  val loss:  4.90390682220459  val L1 loss:  5.3819\n",
      "epoch:  11   step:  44   train loss:  4.356327533721924  val loss:  4.960521221160889  val L1 loss:  5.4392\n",
      "epoch:  11   step:  45   train loss:  5.141629219055176  val loss:  5.116901874542236  val L1 loss:  5.6024\n",
      "epoch:  11   step:  46   train loss:  4.415239334106445  val loss:  4.9572625160217285  val L1 loss:  5.4214\n",
      "epoch:  11   step:  47   train loss:  3.1931874752044678  val loss:  4.92206335067749  val L1 loss:  5.3893\n",
      "epoch:  11   step:  48   train loss:  4.052366256713867  val loss:  4.915650367736816  val L1 loss:  5.3786\n",
      "epoch:  11   step:  49   train loss:  4.711401462554932  val loss:  4.840577602386475  val L1 loss:  5.2976\n",
      "epoch:  11   step:  50   train loss:  5.321378707885742  val loss:  4.770511150360107  val L1 loss:  5.2135\n",
      "epoch:  11   step:  51   train loss:  3.4176745414733887  val loss:  4.7741546630859375  val L1 loss:  5.2177\n",
      "epoch:  11   step:  52   train loss:  3.5613365173339844  val loss:  4.803642272949219  val L1 loss:  5.2646\n",
      "epoch:  11   step:  53   train loss:  2.850048542022705  val loss:  4.815937519073486  val L1 loss:  5.2885\n",
      "epoch:  11   step:  54   train loss:  3.71157169342041  val loss:  4.802372932434082  val L1 loss:  5.2829\n",
      "epoch:  11   step:  55   train loss:  4.173666954040527  val loss:  4.900966167449951  val L1 loss:  5.3784\n",
      "epoch:  11   step:  56   train loss:  3.1520280838012695  val loss:  5.138034343719482  val L1 loss:  5.6356\n",
      "epoch:  11   step:  57   train loss:  3.7992446422576904  val loss:  4.902010917663574  val L1 loss:  5.3912\n",
      "epoch:  11   step:  58   train loss:  2.487165927886963  val loss:  4.754134178161621  val L1 loss:  5.2335\n",
      "epoch:  11   step:  59   train loss:  4.425022602081299  val loss:  4.7581305503845215  val L1 loss:  5.2447\n",
      "epoch:  11   step:  60   train loss:  3.477764129638672  val loss:  4.7269816398620605  val L1 loss:  5.2106\n",
      "epoch:  11   step:  61   train loss:  5.373970985412598  val loss:  4.800568580627441  val L1 loss:  5.2727\n",
      "epoch:  11   step:  62   train loss:  3.704925060272217  val loss:  5.162753582000732  val L1 loss:  5.6434\n",
      "epoch:  11   step:  63   train loss:  2.4019999504089355  val loss:  5.91908597946167  val L1 loss:  6.3905\n",
      "epoch:  11   step:  64   train loss:  4.849218368530273  val loss:  6.40196418762207  val L1 loss:  6.8944\n",
      "epoch:  11   step:  65   train loss:  5.393301486968994  val loss:  6.388222694396973  val L1 loss:  6.8777\n",
      "epoch:  11   step:  66   train loss:  5.948208808898926  val loss:  5.887904644012451  val L1 loss:  6.3657\n",
      "epoch:  11   step:  67   train loss:  2.588094472885132  val loss:  5.238925457000732  val L1 loss:  5.7129\n",
      "epoch:  11   step:  68   train loss:  3.6665287017822266  val loss:  4.895069599151611  val L1 loss:  5.3664\n",
      "epoch:  11   step:  69   train loss:  3.5809450149536133  val loss:  4.84423828125  val L1 loss:  5.3177\n",
      "epoch:  11   step:  70   train loss:  4.497964382171631  val loss:  4.8766889572143555  val L1 loss:  5.3152\n",
      "epoch:  11   step:  71   train loss:  4.853584289550781  val loss:  4.931837558746338  val L1 loss:  5.3927\n",
      "epoch:  11   step:  72   train loss:  2.11190128326416  val loss:  4.966433525085449  val L1 loss:  5.4273\n",
      "epoch:  11   step:  73   train loss:  5.161557197570801  val loss:  5.006869792938232  val L1 loss:  5.4799\n",
      "epoch:  11   step:  74   train loss:  5.651491165161133  val loss:  5.0626420974731445  val L1 loss:  5.5438\n",
      "epoch:  11   step:  75   train loss:  5.603697299957275  val loss:  5.015233516693115  val L1 loss:  5.4997\n",
      "epoch:  11   step:  76   train loss:  1.9030786752700806  val loss:  4.978696823120117  val L1 loss:  5.4665\n",
      "epoch:  11   step:  77   train loss:  3.185397148132324  val loss:  4.75552225112915  val L1 loss:  5.2283\n",
      "epoch:  11   step:  78   train loss:  5.771443843841553  val loss:  4.630599498748779  val L1 loss:  5.0922\n",
      "epoch:  11   step:  79   train loss:  5.224262714385986  val loss:  4.649511337280273  val L1 loss:  5.1129\n",
      "epoch:  11   step:  80   train loss:  3.64743971824646  val loss:  4.618439674377441  val L1 loss:  5.0949\n",
      "epoch:  11   step:  81   train loss:  2.660770893096924  val loss:  4.502938270568848  val L1 loss:  4.9641\n",
      "epoch:  11   step:  82   train loss:  5.188090801239014  val loss:  4.543019771575928  val L1 loss:  4.9992\n",
      "epoch:  11   step:  83   train loss:  3.086186170578003  val loss:  4.565855503082275  val L1 loss:  5.0287\n",
      "epoch:  11   step:  84   train loss:  2.15493106842041  val loss:  4.440417766571045  val L1 loss:  4.9085\n",
      "min_val_loss_print 4.440417766571045\n",
      "epoch:  11   step:  85   train loss:  3.517357587814331  val loss:  4.292741775512695  val L1 loss:  4.7607\n",
      "min_val_loss_print 4.292741775512695\n",
      "epoch:  11   step:  86   train loss:  2.641209125518799  val loss:  4.253595352172852  val L1 loss:  4.7244\n",
      "min_val_loss_print 4.253595352172852\n",
      "epoch:  11   step:  87   train loss:  2.4040334224700928  val loss:  4.2153215408325195  val L1 loss:  4.6959\n",
      "min_val_loss_print 4.2153215408325195\n",
      "epoch:  11   step:  88   train loss:  3.709902286529541  val loss:  4.2547783851623535  val L1 loss:  4.7331\n",
      "epoch:  11   step:  89   train loss:  3.6767215728759766  val loss:  4.2288055419921875  val L1 loss:  4.7168\n",
      "epoch:  11   step:  90   train loss:  3.7774646282196045  val loss:  4.278223991394043  val L1 loss:  4.7658\n",
      "epoch:  11   step:  91   train loss:  3.814194440841675  val loss:  4.185962200164795  val L1 loss:  4.6584\n",
      "min_val_loss_print 4.185962200164795\n",
      "epoch:  11   step:  92   train loss:  3.083400011062622  val loss:  4.095638275146484  val L1 loss:  4.5655\n",
      "min_val_loss_print 4.095638275146484\n",
      "epoch:  11   step:  93   train loss:  2.084717035293579  val loss:  4.0651445388793945  val L1 loss:  4.5183\n",
      "min_val_loss_print 4.0651445388793945\n",
      "epoch:  11   step:  94   train loss:  4.417016983032227  val loss:  4.127657413482666  val L1 loss:  4.6029\n",
      "epoch:  11   step:  95   train loss:  4.829357147216797  val loss:  4.250164985656738  val L1 loss:  4.7253\n",
      "epoch:  11   step:  96   train loss:  6.594028949737549  val loss:  4.434649467468262  val L1 loss:  4.9073\n",
      "epoch:  11   step:  97   train loss:  4.977097034454346  val loss:  4.837484836578369  val L1 loss:  5.3119\n",
      "epoch:  11   step:  98   train loss:  3.3577661514282227  val loss:  5.403623104095459  val L1 loss:  5.8839\n",
      "epoch:  11   step:  99   train loss:  4.857810974121094  val loss:  5.781820774078369  val L1 loss:  6.2643\n",
      "epoch:  11   step:  100   train loss:  3.701809883117676  val loss:  5.92384147644043  val L1 loss:  6.4111\n",
      "epoch:  11   step:  101   train loss:  4.009864330291748  val loss:  5.628946304321289  val L1 loss:  6.1086\n",
      "epoch:  11   step:  102   train loss:  3.2573533058166504  val loss:  5.2381768226623535  val L1 loss:  5.7204\n",
      "epoch:  11   step:  103   train loss:  2.851670503616333  val loss:  4.80637788772583  val L1 loss:  5.2782\n",
      "epoch:  11   step:  104   train loss:  4.267848014831543  val loss:  4.788549900054932  val L1 loss:  5.2659\n",
      "epoch:  11   step:  105   train loss:  4.918347358703613  val loss:  4.926198482513428  val L1 loss:  5.3884\n",
      "epoch:  11   step:  106   train loss:  3.9799704551696777  val loss:  4.931053161621094  val L1 loss:  5.3991\n",
      "epoch:  11   step:  107   train loss:  3.0361037254333496  val loss:  4.784445285797119  val L1 loss:  5.2509\n",
      "epoch:  11   step:  108   train loss:  5.852405548095703  val loss:  4.673516273498535  val L1 loss:  5.1617\n",
      "epoch:  11   step:  109   train loss:  5.105217933654785  val loss:  4.761075496673584  val L1 loss:  5.23\n",
      "epoch:  11   step:  110   train loss:  2.1354119777679443  val loss:  4.823368549346924  val L1 loss:  5.2924\n",
      "epoch:  11   step:  111   train loss:  3.2197299003601074  val loss:  4.901243686676025  val L1 loss:  5.37\n",
      "epoch:  11   step:  112   train loss:  5.056402683258057  val loss:  4.9432501792907715  val L1 loss:  5.4168\n",
      "epoch:  11   step:  113   train loss:  3.6632838249206543  val loss:  4.969144344329834  val L1 loss:  5.4352\n",
      "epoch:  11   step:  114   train loss:  7.0232133865356445  val loss:  4.968738079071045  val L1 loss:  5.4401\n",
      "epoch:  11   step:  115   train loss:  3.032136917114258  val loss:  4.9979352951049805  val L1 loss:  5.4704\n",
      "epoch:  11   step:  116   train loss:  2.317744016647339  val loss:  5.035533428192139  val L1 loss:  5.5042\n",
      "epoch:  11   step:  117   train loss:  3.1238045692443848  val loss:  5.045595645904541  val L1 loss:  5.5253\n",
      "epoch:  11   step:  118   train loss:  2.466947317123413  val loss:  5.072878360748291  val L1 loss:  5.5521\n",
      "epoch:  11   step:  119   train loss:  2.347278594970703  val loss:  5.0734453201293945  val L1 loss:  5.551\n",
      "epoch:  11   step:  120   train loss:  4.1081624031066895  val loss:  4.962866306304932  val L1 loss:  5.4304\n",
      "epoch:  11   step:  121   train loss:  2.890517473220825  val loss:  4.900728702545166  val L1 loss:  5.3691\n",
      "epoch:  11   step:  122   train loss:  3.5250980854034424  val loss:  5.11130428314209  val L1 loss:  5.6006\n",
      "epoch:  11   step:  123   train loss:  2.488907814025879  val loss:  5.270565032958984  val L1 loss:  5.7456\n",
      "epoch:  11   step:  124   train loss:  3.900974750518799  val loss:  5.167590141296387  val L1 loss:  5.6502\n",
      "epoch:  11   step:  125   train loss:  3.53645658493042  val loss:  4.763984680175781  val L1 loss:  5.2408\n",
      "epoch:  11   step:  126   train loss:  4.542142868041992  val loss:  4.567615985870361  val L1 loss:  5.0347\n",
      "epoch:  11   step:  127   train loss:  3.6952309608459473  val loss:  4.497483253479004  val L1 loss:  4.9514\n",
      "epoch:  11   step:  128   train loss:  5.9123921394348145  val loss:  4.490835666656494  val L1 loss:  4.9549\n",
      "epoch:  11   step:  129   train loss:  5.515746116638184  val loss:  4.419841289520264  val L1 loss:  4.8907\n",
      "epoch:  11   step:  130   train loss:  4.7602128982543945  val loss:  4.415400981903076  val L1 loss:  4.8777\n",
      "epoch:  11   step:  131   train loss:  3.759761095046997  val loss:  4.588990211486816  val L1 loss:  5.0654\n",
      "epoch:  11   step:  132   train loss:  5.540923118591309  val loss:  4.932699203491211  val L1 loss:  5.4017\n",
      "epoch:  11   step:  133   train loss:  4.224985122680664  val loss:  5.162169456481934  val L1 loss:  5.6433\n",
      "epoch:  11   step:  134   train loss:  3.713524103164673  val loss:  4.991143703460693  val L1 loss:  5.4619\n",
      "epoch:  11   step:  135   train loss:  2.7861719131469727  val loss:  4.880633354187012  val L1 loss:  5.3424\n",
      "epoch:  11   step:  136   train loss:  2.865391969680786  val loss:  4.906869411468506  val L1 loss:  5.3679\n",
      "epoch:  11   step:  137   train loss:  5.478549480438232  val loss:  5.0622382164001465  val L1 loss:  5.5223\n",
      "epoch:  11   step:  138   train loss:  3.393195152282715  val loss:  5.234647750854492  val L1 loss:  5.7237\n",
      "epoch:  11   step:  139   train loss:  4.084087371826172  val loss:  5.115588188171387  val L1 loss:  5.6015\n",
      "epoch:  11   step:  140   train loss:  3.0295655727386475  val loss:  4.881801128387451  val L1 loss:  5.3652\n",
      "epoch:  11   step:  141   train loss:  4.390763282775879  val loss:  4.74049711227417  val L1 loss:  5.2152\n",
      "epoch:  11   step:  142   train loss:  3.922159194946289  val loss:  4.950547218322754  val L1 loss:  5.4218\n",
      "epoch:  11   step:  143   train loss:  2.5144786834716797  val loss:  5.0547590255737305  val L1 loss:  5.5348\n",
      "epoch:  11   step:  144   train loss:  3.3313589096069336  val loss:  4.883549213409424  val L1 loss:  5.3448\n",
      "epoch:  11   step:  145   train loss:  4.140756607055664  val loss:  4.810297012329102  val L1 loss:  5.2789\n",
      "epoch:  11   step:  146   train loss:  3.9795265197753906  val loss:  4.636378765106201  val L1 loss:  5.1042\n",
      "epoch:  11   step:  147   train loss:  2.9132742881774902  val loss:  4.549584865570068  val L1 loss:  4.9842\n",
      "epoch:  11   step:  148   train loss:  2.61698317527771  val loss:  4.763300895690918  val L1 loss:  5.2399\n",
      "epoch:  11   step:  149   train loss:  3.7440524101257324  val loss:  4.869410514831543  val L1 loss:  5.3471\n",
      "epoch:  11   step:  150   train loss:  2.4703783988952637  val loss:  4.760155200958252  val L1 loss:  5.2176\n",
      "epoch:  11   step:  151   train loss:  3.558708429336548  val loss:  4.723599910736084  val L1 loss:  5.1953\n",
      "epoch:  11   step:  152   train loss:  3.6979713439941406  val loss:  4.831273078918457  val L1 loss:  5.302\n",
      "epoch:  11   step:  153   train loss:  3.552762985229492  val loss:  4.867558479309082  val L1 loss:  5.3461\n",
      "epoch:  11   step:  154   train loss:  4.35858678817749  val loss:  4.979888916015625  val L1 loss:  5.4663\n",
      "epoch:  11   step:  155   train loss:  3.845540761947632  val loss:  5.036476135253906  val L1 loss:  5.5228\n",
      "epoch:  11   step:  156   train loss:  4.207420349121094  val loss:  4.89984655380249  val L1 loss:  5.3696\n",
      "epoch:  11   step:  157   train loss:  3.748837471008301  val loss:  4.888300895690918  val L1 loss:  5.3603\n",
      "epoch:  11   step:  158   train loss:  2.9138574600219727  val loss:  4.958084583282471  val L1 loss:  5.4264\n",
      "epoch:  11   step:  159   train loss:  4.891638278961182  val loss:  4.889826774597168  val L1 loss:  5.3499\n",
      "epoch:  11   step:  160   train loss:  4.862316608428955  val loss:  4.786940574645996  val L1 loss:  5.2577\n",
      "epoch:  11   step:  161   train loss:  2.9629251956939697  val loss:  4.856173992156982  val L1 loss:  5.3074\n",
      "epoch:  11   step:  162   train loss:  3.4076433181762695  val loss:  4.841002464294434  val L1 loss:  5.3092\n",
      "epoch:  11   step:  163   train loss:  3.5101375579833984  val loss:  4.867334365844727  val L1 loss:  5.3534\n",
      "epoch:  11   step:  164   train loss:  3.89530611038208  val loss:  5.159454345703125  val L1 loss:  5.6419\n",
      "epoch:  11   step:  165   train loss:  3.7123565673828125  val loss:  5.464771747589111  val L1 loss:  5.9264\n",
      "epoch:  11   step:  166   train loss:  2.9216573238372803  val loss:  5.525644302368164  val L1 loss:  5.9978\n",
      "epoch:  11   step:  167   train loss:  6.130213260650635  val loss:  5.34818696975708  val L1 loss:  5.8371\n",
      "epoch:  11   step:  168   train loss:  4.78834342956543  val loss:  4.959282875061035  val L1 loss:  5.4207\n",
      "epoch:  11   step:  169   train loss:  4.735228061676025  val loss:  4.816159725189209  val L1 loss:  5.2758\n",
      "epoch:  11   step:  170   train loss:  5.394704818725586  val loss:  4.703212261199951  val L1 loss:  5.1699\n",
      "epoch:  11   step:  171   train loss:  2.226024627685547  val loss:  4.561929225921631  val L1 loss:  5.013\n",
      "epoch:  11   step:  172   train loss:  3.904416799545288  val loss:  4.466801166534424  val L1 loss:  4.9165\n",
      "epoch:  11   step:  173   train loss:  3.543877601623535  val loss:  4.355109214782715  val L1 loss:  4.7947\n",
      "epoch:  11   step:  174   train loss:  2.8342230319976807  val loss:  4.282257556915283  val L1 loss:  4.7488\n",
      "epoch:  11   step:  175   train loss:  4.997237205505371  val loss:  4.2581892013549805  val L1 loss:  4.7181\n",
      "epoch:  11   step:  176   train loss:  2.9219460487365723  val loss:  4.211704730987549  val L1 loss:  4.6647\n",
      "epoch:  11   step:  177   train loss:  3.192464590072632  val loss:  4.124669075012207  val L1 loss:  4.5714\n",
      "epoch:  11   step:  178   train loss:  3.0990896224975586  val loss:  4.08803653717041  val L1 loss:  4.5426\n",
      "epoch:  11   step:  179   train loss:  6.422895431518555  val loss:  4.112736225128174  val L1 loss:  4.5658\n",
      "epoch:  11   step:  180   train loss:  4.8221564292907715  val loss:  4.109436988830566  val L1 loss:  4.5752\n",
      "epoch:  11   step:  181   train loss:  3.436863422393799  val loss:  4.28043794631958  val L1 loss:  4.7237\n",
      "epoch:  11   step:  182   train loss:  2.592656373977661  val loss:  4.859957695007324  val L1 loss:  5.3329\n",
      "epoch:  11   step:  183   train loss:  3.4161627292633057  val loss:  5.3401594161987305  val L1 loss:  5.8206\n",
      "epoch:  11   step:  184   train loss:  6.467885494232178  val loss:  5.130947113037109  val L1 loss:  5.5997\n",
      "epoch:  11   step:  185   train loss:  4.9105024337768555  val loss:  4.66989803314209  val L1 loss:  5.1417\n",
      "epoch:  11   step:  186   train loss:  3.7888293266296387  val loss:  4.454931735992432  val L1 loss:  4.9215\n",
      "epoch:  11   step:  187   train loss:  3.986268997192383  val loss:  4.466457366943359  val L1 loss:  4.9459\n",
      "epoch:  11   step:  188   train loss:  3.124673843383789  val loss:  4.537558555603027  val L1 loss:  5.0178\n",
      "epoch:  11   step:  189   train loss:  4.22208309173584  val loss:  4.575324058532715  val L1 loss:  5.0572\n",
      "epoch:  11   step:  190   train loss:  3.8332386016845703  val loss:  4.613956928253174  val L1 loss:  5.0926\n",
      "epoch:  11   step:  191   train loss:  4.459514617919922  val loss:  4.733363151550293  val L1 loss:  5.2067\n",
      "epoch:  11   step:  192   train loss:  1.8793617486953735  val loss:  4.805213928222656  val L1 loss:  5.2885\n",
      "epoch:  11   step:  193   train loss:  4.952873706817627  val loss:  4.841065406799316  val L1 loss:  5.3182\n",
      "epoch:  11   step:  194   train loss:  3.6473259925842285  val loss:  4.954355716705322  val L1 loss:  5.4112\n",
      "epoch:  11   step:  195   train loss:  5.32192325592041  val loss:  5.20777702331543  val L1 loss:  5.6719\n",
      "epoch:  11   step:  196   train loss:  4.268101692199707  val loss:  5.475789546966553  val L1 loss:  5.9388\n",
      "epoch:  11   step:  197   train loss:  3.1200921535491943  val loss:  6.071822166442871  val L1 loss:  6.5523\n",
      "epoch:  11   step:  198   train loss:  6.171085834503174  val loss:  6.173907279968262  val L1 loss:  6.6572\n",
      "epoch:  11   step:  199   train loss:  5.049047470092773  val loss:  5.604750633239746  val L1 loss:  6.0541\n",
      "epoch:  11   step:  200   train loss:  3.3919928073883057  val loss:  4.721274375915527  val L1 loss:  5.1952\n",
      "epoch:  11   step:  201   train loss:  4.2733635902404785  val loss:  4.483765602111816  val L1 loss:  4.9548\n",
      "epoch:  11   step:  202   train loss:  3.6212308406829834  val loss:  4.603371620178223  val L1 loss:  5.0826\n",
      "epoch:  11   step:  203   train loss:  5.604414463043213  val loss:  4.690284252166748  val L1 loss:  5.1657\n",
      "epoch:  11   step:  204   train loss:  4.242722511291504  val loss:  4.592316627502441  val L1 loss:  5.0663\n",
      "epoch:  11   step:  205   train loss:  3.8442788124084473  val loss:  4.534758567810059  val L1 loss:  5.0129\n",
      "epoch:  11   step:  206   train loss:  5.554391860961914  val loss:  4.577656269073486  val L1 loss:  5.0346\n",
      "epoch:  11   step:  207   train loss:  4.780245304107666  val loss:  4.704354286193848  val L1 loss:  5.1403\n",
      "epoch:  11   step:  208   train loss:  4.865565776824951  val loss:  4.859407424926758  val L1 loss:  5.3175\n",
      "epoch:  12   step:  0   train loss:  2.9405789375305176  val loss:  4.853038311004639  val L1 loss:  5.3219\n",
      "epoch:  12   step:  1   train loss:  2.931058883666992  val loss:  4.7971038818359375  val L1 loss:  5.2787\n",
      "epoch:  12   step:  2   train loss:  3.736356735229492  val loss:  4.763339519500732  val L1 loss:  5.2177\n",
      "epoch:  12   step:  3   train loss:  4.984425067901611  val loss:  4.796245098114014  val L1 loss:  5.2518\n",
      "epoch:  12   step:  4   train loss:  4.862617492675781  val loss:  4.824028491973877  val L1 loss:  5.2892\n",
      "epoch:  12   step:  5   train loss:  2.425457715988159  val loss:  4.788358211517334  val L1 loss:  5.2698\n",
      "epoch:  12   step:  6   train loss:  3.586033344268799  val loss:  4.849567890167236  val L1 loss:  5.3291\n",
      "epoch:  12   step:  7   train loss:  2.5488176345825195  val loss:  4.99526309967041  val L1 loss:  5.4763\n",
      "epoch:  12   step:  8   train loss:  3.5536892414093018  val loss:  5.065186977386475  val L1 loss:  5.5324\n",
      "epoch:  12   step:  9   train loss:  2.922037124633789  val loss:  5.148445129394531  val L1 loss:  5.6135\n",
      "epoch:  12   step:  10   train loss:  2.7123517990112305  val loss:  5.364721775054932  val L1 loss:  5.8438\n",
      "epoch:  12   step:  11   train loss:  4.569411754608154  val loss:  5.605103492736816  val L1 loss:  6.0768\n",
      "epoch:  12   step:  12   train loss:  4.196619510650635  val loss:  5.570184230804443  val L1 loss:  6.0348\n",
      "epoch:  12   step:  13   train loss:  4.018598556518555  val loss:  5.538558006286621  val L1 loss:  6.0005\n",
      "epoch:  12   step:  14   train loss:  3.171971321105957  val loss:  5.395412445068359  val L1 loss:  5.861\n",
      "epoch:  12   step:  15   train loss:  6.183605670928955  val loss:  5.2091064453125  val L1 loss:  5.6734\n",
      "epoch:  12   step:  16   train loss:  2.5401782989501953  val loss:  5.118936538696289  val L1 loss:  5.5874\n",
      "epoch:  12   step:  17   train loss:  2.293856143951416  val loss:  5.112358093261719  val L1 loss:  5.6002\n",
      "epoch:  12   step:  18   train loss:  4.328273773193359  val loss:  5.174202919006348  val L1 loss:  5.663\n",
      "epoch:  12   step:  19   train loss:  4.155597686767578  val loss:  5.07394552230835  val L1 loss:  5.5551\n",
      "epoch:  12   step:  20   train loss:  4.37058162689209  val loss:  4.984781265258789  val L1 loss:  5.471\n",
      "epoch:  12   step:  21   train loss:  3.736008405685425  val loss:  5.008192539215088  val L1 loss:  5.4769\n",
      "epoch:  12   step:  22   train loss:  2.500861167907715  val loss:  4.971573352813721  val L1 loss:  5.4422\n",
      "epoch:  12   step:  23   train loss:  3.417665481567383  val loss:  4.925935745239258  val L1 loss:  5.3827\n",
      "epoch:  12   step:  24   train loss:  5.098466396331787  val loss:  4.951852321624756  val L1 loss:  5.4167\n",
      "epoch:  12   step:  25   train loss:  3.8865740299224854  val loss:  5.062473773956299  val L1 loss:  5.521\n",
      "epoch:  12   step:  26   train loss:  3.2191038131713867  val loss:  5.387137413024902  val L1 loss:  5.8525\n",
      "epoch:  12   step:  27   train loss:  2.8970556259155273  val loss:  5.700186729431152  val L1 loss:  6.1838\n",
      "epoch:  12   step:  28   train loss:  4.598400115966797  val loss:  6.592990398406982  val L1 loss:  7.0906\n",
      "epoch:  12   step:  29   train loss:  3.6331727504730225  val loss:  7.3561272621154785  val L1 loss:  7.8488\n",
      "epoch:  12   step:  30   train loss:  5.799976348876953  val loss:  7.140366554260254  val L1 loss:  7.6255\n",
      "epoch:  12   step:  31   train loss:  5.126818656921387  val loss:  6.135546684265137  val L1 loss:  6.6335\n",
      "epoch:  12   step:  32   train loss:  3.3930420875549316  val loss:  4.777243614196777  val L1 loss:  5.2679\n",
      "epoch:  12   step:  33   train loss:  1.9107341766357422  val loss:  4.548185348510742  val L1 loss:  5.0215\n",
      "epoch:  12   step:  34   train loss:  2.390174388885498  val loss:  5.419469356536865  val L1 loss:  5.9089\n",
      "epoch:  12   step:  35   train loss:  7.762813568115234  val loss:  5.736494541168213  val L1 loss:  6.219\n",
      "epoch:  12   step:  36   train loss:  4.475777626037598  val loss:  5.141270637512207  val L1 loss:  5.6143\n",
      "epoch:  12   step:  37   train loss:  4.644375801086426  val loss:  4.357507705688477  val L1 loss:  4.8119\n",
      "epoch:  12   step:  38   train loss:  2.912778377532959  val loss:  5.040575981140137  val L1 loss:  5.5315\n",
      "epoch:  12   step:  39   train loss:  3.3727641105651855  val loss:  5.6628241539001465  val L1 loss:  6.1518\n",
      "epoch:  12   step:  40   train loss:  4.848660469055176  val loss:  5.674787521362305  val L1 loss:  6.1658\n",
      "epoch:  12   step:  41   train loss:  3.8660736083984375  val loss:  5.198652744293213  val L1 loss:  5.6845\n",
      "epoch:  12   step:  42   train loss:  4.5976338386535645  val loss:  4.560657978057861  val L1 loss:  5.0345\n",
      "epoch:  12   step:  43   train loss:  3.1391682624816895  val loss:  4.4883036613464355  val L1 loss:  4.9322\n",
      "epoch:  12   step:  44   train loss:  6.812368392944336  val loss:  4.810666561126709  val L1 loss:  5.2799\n",
      "epoch:  12   step:  45   train loss:  4.432013511657715  val loss:  4.825900077819824  val L1 loss:  5.293\n",
      "epoch:  12   step:  46   train loss:  5.412014007568359  val loss:  4.557918548583984  val L1 loss:  5.0194\n",
      "epoch:  12   step:  47   train loss:  3.9835853576660156  val loss:  4.805843353271484  val L1 loss:  5.2815\n",
      "epoch:  12   step:  48   train loss:  4.145761013031006  val loss:  5.789076328277588  val L1 loss:  6.289\n",
      "epoch:  12   step:  49   train loss:  2.7566301822662354  val loss:  6.63912296295166  val L1 loss:  7.1381\n",
      "epoch:  12   step:  50   train loss:  3.963200569152832  val loss:  6.934272289276123  val L1 loss:  7.4334\n",
      "epoch:  12   step:  51   train loss:  4.506684303283691  val loss:  6.942336082458496  val L1 loss:  7.4411\n",
      "epoch:  12   step:  52   train loss:  6.816163539886475  val loss:  6.434537887573242  val L1 loss:  6.9277\n",
      "epoch:  12   step:  53   train loss:  5.28550910949707  val loss:  5.614951133728027  val L1 loss:  6.1102\n",
      "epoch:  12   step:  54   train loss:  3.629094123840332  val loss:  4.771814346313477  val L1 loss:  5.2368\n",
      "epoch:  12   step:  55   train loss:  3.6897530555725098  val loss:  4.731802463531494  val L1 loss:  5.2041\n",
      "epoch:  12   step:  56   train loss:  3.947028636932373  val loss:  4.81803035736084  val L1 loss:  5.2954\n",
      "epoch:  12   step:  57   train loss:  4.212069511413574  val loss:  4.757144927978516  val L1 loss:  5.239\n",
      "epoch:  12   step:  58   train loss:  3.769866466522217  val loss:  4.678525924682617  val L1 loss:  5.1514\n",
      "epoch:  12   step:  59   train loss:  3.402251958847046  val loss:  5.017350196838379  val L1 loss:  5.4942\n",
      "epoch:  12   step:  60   train loss:  3.7047243118286133  val loss:  5.714020729064941  val L1 loss:  6.2058\n",
      "epoch:  12   step:  61   train loss:  3.7857131958007812  val loss:  6.154911994934082  val L1 loss:  6.6475\n",
      "epoch:  12   step:  62   train loss:  3.9397122859954834  val loss:  6.173013210296631  val L1 loss:  6.6583\n",
      "epoch:  12   step:  63   train loss:  4.379884719848633  val loss:  5.640420436859131  val L1 loss:  6.1296\n",
      "epoch:  12   step:  64   train loss:  4.9712934494018555  val loss:  5.190839767456055  val L1 loss:  5.6674\n",
      "epoch:  12   step:  65   train loss:  4.331199645996094  val loss:  4.959230422973633  val L1 loss:  5.436\n",
      "epoch:  12   step:  66   train loss:  6.449037551879883  val loss:  4.752616882324219  val L1 loss:  5.2239\n",
      "epoch:  12   step:  67   train loss:  3.1531105041503906  val loss:  4.580860614776611  val L1 loss:  5.0299\n",
      "epoch:  12   step:  68   train loss:  3.2661194801330566  val loss:  4.501779079437256  val L1 loss:  4.9546\n",
      "epoch:  12   step:  69   train loss:  5.644996643066406  val loss:  4.397266387939453  val L1 loss:  4.8562\n",
      "epoch:  12   step:  70   train loss:  2.8410730361938477  val loss:  4.389543533325195  val L1 loss:  4.8588\n",
      "epoch:  12   step:  71   train loss:  3.0186009407043457  val loss:  4.37355899810791  val L1 loss:  4.8236\n",
      "epoch:  12   step:  72   train loss:  3.5100643634796143  val loss:  4.384298801422119  val L1 loss:  4.8434\n",
      "epoch:  12   step:  73   train loss:  3.393037796020508  val loss:  4.343588352203369  val L1 loss:  4.8094\n",
      "epoch:  12   step:  74   train loss:  4.22465181350708  val loss:  4.325933933258057  val L1 loss:  4.7728\n",
      "epoch:  12   step:  75   train loss:  3.823430061340332  val loss:  4.310798168182373  val L1 loss:  4.7587\n",
      "epoch:  12   step:  76   train loss:  4.8333635330200195  val loss:  4.494999408721924  val L1 loss:  4.9669\n",
      "epoch:  12   step:  77   train loss:  2.9772520065307617  val loss:  4.751582145690918  val L1 loss:  5.2236\n",
      "epoch:  12   step:  78   train loss:  4.062194347381592  val loss:  4.977001190185547  val L1 loss:  5.4391\n",
      "epoch:  12   step:  79   train loss:  4.468903064727783  val loss:  5.091109275817871  val L1 loss:  5.5555\n",
      "epoch:  12   step:  80   train loss:  3.4232630729675293  val loss:  5.424936294555664  val L1 loss:  5.8976\n",
      "epoch:  12   step:  81   train loss:  3.262199878692627  val loss:  5.239430904388428  val L1 loss:  5.7201\n",
      "epoch:  12   step:  82   train loss:  3.9568657875061035  val loss:  4.8296098709106445  val L1 loss:  5.2929\n",
      "epoch:  12   step:  83   train loss:  7.445362091064453  val loss:  4.776022911071777  val L1 loss:  5.2623\n",
      "epoch:  12   step:  84   train loss:  3.6380653381347656  val loss:  4.677250385284424  val L1 loss:  5.1399\n",
      "epoch:  12   step:  85   train loss:  4.491927146911621  val loss:  4.583523273468018  val L1 loss:  5.0447\n",
      "epoch:  12   step:  86   train loss:  2.349008560180664  val loss:  4.506768703460693  val L1 loss:  4.9621\n",
      "epoch:  12   step:  87   train loss:  3.594116687774658  val loss:  4.433834075927734  val L1 loss:  4.8727\n",
      "epoch:  12   step:  88   train loss:  3.283000946044922  val loss:  4.4712653160095215  val L1 loss:  4.9199\n",
      "epoch:  12   step:  89   train loss:  3.834547996520996  val loss:  4.5104169845581055  val L1 loss:  4.9558\n",
      "epoch:  12   step:  90   train loss:  2.5442633628845215  val loss:  4.71244478225708  val L1 loss:  5.1821\n",
      "epoch:  12   step:  91   train loss:  7.3938751220703125  val loss:  4.522128582000732  val L1 loss:  4.98\n",
      "epoch:  12   step:  92   train loss:  3.0628418922424316  val loss:  4.364931583404541  val L1 loss:  4.8246\n",
      "epoch:  12   step:  93   train loss:  3.897230386734009  val loss:  4.34364652633667  val L1 loss:  4.7803\n",
      "epoch:  12   step:  94   train loss:  3.959684133529663  val loss:  4.503037452697754  val L1 loss:  4.9491\n",
      "epoch:  12   step:  95   train loss:  4.8401618003845215  val loss:  4.875232219696045  val L1 loss:  5.3478\n",
      "epoch:  12   step:  96   train loss:  5.005035877227783  val loss:  5.346275329589844  val L1 loss:  5.8267\n",
      "epoch:  12   step:  97   train loss:  3.596039295196533  val loss:  5.426390171051025  val L1 loss:  5.9109\n",
      "epoch:  12   step:  98   train loss:  3.392061233520508  val loss:  5.039950370788574  val L1 loss:  5.4988\n",
      "epoch:  12   step:  99   train loss:  4.840506553649902  val loss:  5.022839069366455  val L1 loss:  5.5022\n",
      "epoch:  12   step:  100   train loss:  3.3127129077911377  val loss:  4.847733974456787  val L1 loss:  5.3225\n",
      "epoch:  12   step:  101   train loss:  3.3335258960723877  val loss:  4.641870975494385  val L1 loss:  5.0816\n",
      "epoch:  12   step:  102   train loss:  3.9570436477661133  val loss:  4.497004508972168  val L1 loss:  4.9463\n",
      "epoch:  12   step:  103   train loss:  2.9405593872070312  val loss:  4.492602825164795  val L1 loss:  4.9521\n",
      "epoch:  12   step:  104   train loss:  4.688238620758057  val loss:  4.648209571838379  val L1 loss:  5.1297\n",
      "epoch:  12   step:  105   train loss:  5.706263065338135  val loss:  4.666941165924072  val L1 loss:  5.1242\n",
      "epoch:  12   step:  106   train loss:  5.134434223175049  val loss:  4.5811848640441895  val L1 loss:  5.052\n",
      "epoch:  12   step:  107   train loss:  3.2662978172302246  val loss:  4.503335952758789  val L1 loss:  4.9807\n",
      "epoch:  12   step:  108   train loss:  4.331076622009277  val loss:  4.4550371170043945  val L1 loss:  4.9324\n",
      "epoch:  12   step:  109   train loss:  2.056854724884033  val loss:  4.492618560791016  val L1 loss:  4.9646\n",
      "epoch:  12   step:  110   train loss:  4.710515975952148  val loss:  4.53997802734375  val L1 loss:  5.0006\n",
      "epoch:  12   step:  111   train loss:  5.12691593170166  val loss:  4.408334732055664  val L1 loss:  4.8627\n",
      "epoch:  12   step:  112   train loss:  4.284517288208008  val loss:  4.367680549621582  val L1 loss:  4.8242\n",
      "epoch:  12   step:  113   train loss:  4.755537033081055  val loss:  4.391479969024658  val L1 loss:  4.8621\n",
      "epoch:  12   step:  114   train loss:  3.9925241470336914  val loss:  4.442341327667236  val L1 loss:  4.8988\n",
      "epoch:  12   step:  115   train loss:  2.987802505493164  val loss:  4.544407367706299  val L1 loss:  5.0115\n",
      "epoch:  12   step:  116   train loss:  3.477652072906494  val loss:  4.66549015045166  val L1 loss:  5.1119\n",
      "epoch:  12   step:  117   train loss:  4.142589569091797  val loss:  4.754365921020508  val L1 loss:  5.1985\n",
      "epoch:  12   step:  118   train loss:  6.968227386474609  val loss:  4.729655742645264  val L1 loss:  5.1816\n",
      "epoch:  12   step:  119   train loss:  4.086080551147461  val loss:  4.512977123260498  val L1 loss:  4.9693\n",
      "epoch:  12   step:  120   train loss:  4.147129535675049  val loss:  4.604541301727295  val L1 loss:  5.065\n",
      "epoch:  12   step:  121   train loss:  3.3302602767944336  val loss:  4.753563404083252  val L1 loss:  5.2042\n",
      "epoch:  12   step:  122   train loss:  4.700926780700684  val loss:  4.547763824462891  val L1 loss:  5.0078\n",
      "epoch:  12   step:  123   train loss:  3.337350606918335  val loss:  4.483724117279053  val L1 loss:  4.9424\n",
      "epoch:  12   step:  124   train loss:  3.019442081451416  val loss:  4.907075881958008  val L1 loss:  5.4031\n",
      "epoch:  12   step:  125   train loss:  2.430736541748047  val loss:  5.094711780548096  val L1 loss:  5.5896\n",
      "epoch:  12   step:  126   train loss:  3.9492111206054688  val loss:  5.300026893615723  val L1 loss:  5.7892\n",
      "epoch:  12   step:  127   train loss:  4.35487699508667  val loss:  5.139092922210693  val L1 loss:  5.6247\n",
      "epoch:  12   step:  128   train loss:  2.433889389038086  val loss:  4.714804172515869  val L1 loss:  5.1881\n",
      "epoch:  12   step:  129   train loss:  3.35880184173584  val loss:  4.471579074859619  val L1 loss:  4.9421\n",
      "epoch:  12   step:  130   train loss:  4.174948215484619  val loss:  4.701785087585449  val L1 loss:  5.159\n",
      "epoch:  12   step:  131   train loss:  2.528205633163452  val loss:  4.796730995178223  val L1 loss:  5.2586\n",
      "epoch:  12   step:  132   train loss:  4.262813091278076  val loss:  4.603622913360596  val L1 loss:  5.0686\n",
      "epoch:  12   step:  133   train loss:  2.8373043537139893  val loss:  4.569259166717529  val L1 loss:  5.0213\n",
      "epoch:  12   step:  134   train loss:  2.6870222091674805  val loss:  5.34812068939209  val L1 loss:  5.8358\n",
      "epoch:  12   step:  135   train loss:  2.8093252182006836  val loss:  6.236869812011719  val L1 loss:  6.7289\n",
      "epoch:  12   step:  136   train loss:  5.075115203857422  val loss:  6.648987770080566  val L1 loss:  7.136\n",
      "epoch:  12   step:  137   train loss:  4.881903171539307  val loss:  6.886290073394775  val L1 loss:  7.3802\n",
      "epoch:  12   step:  138   train loss:  4.71880578994751  val loss:  6.487710952758789  val L1 loss:  6.9725\n",
      "epoch:  12   step:  139   train loss:  4.6741485595703125  val loss:  6.076268196105957  val L1 loss:  6.5478\n",
      "epoch:  12   step:  140   train loss:  5.528095245361328  val loss:  5.6073150634765625  val L1 loss:  6.0786\n",
      "epoch:  12   step:  141   train loss:  7.928083419799805  val loss:  5.661909580230713  val L1 loss:  6.1466\n",
      "epoch:  12   step:  142   train loss:  4.217530727386475  val loss:  5.520484447479248  val L1 loss:  5.9978\n",
      "epoch:  12   step:  143   train loss:  5.613126754760742  val loss:  5.417128562927246  val L1 loss:  5.8889\n",
      "epoch:  12   step:  144   train loss:  6.610426902770996  val loss:  5.268206596374512  val L1 loss:  5.7298\n",
      "epoch:  12   step:  145   train loss:  2.3611912727355957  val loss:  5.429520130157471  val L1 loss:  5.9041\n",
      "epoch:  12   step:  146   train loss:  3.0008132457733154  val loss:  5.512861251831055  val L1 loss:  5.9884\n",
      "epoch:  12   step:  147   train loss:  4.266940593719482  val loss:  5.481040000915527  val L1 loss:  5.9725\n",
      "epoch:  12   step:  148   train loss:  4.9726881980896  val loss:  5.179770469665527  val L1 loss:  5.65\n",
      "epoch:  12   step:  149   train loss:  5.335982322692871  val loss:  4.97326135635376  val L1 loss:  5.4346\n",
      "epoch:  12   step:  150   train loss:  2.7867989540100098  val loss:  4.734721660614014  val L1 loss:  5.2244\n",
      "epoch:  12   step:  151   train loss:  3.290897846221924  val loss:  4.7160420417785645  val L1 loss:  5.1748\n",
      "epoch:  12   step:  152   train loss:  4.7824506759643555  val loss:  5.213812828063965  val L1 loss:  5.6895\n",
      "epoch:  12   step:  153   train loss:  6.614789009094238  val loss:  5.551926136016846  val L1 loss:  6.0456\n",
      "epoch:  12   step:  154   train loss:  5.1260786056518555  val loss:  5.364607810974121  val L1 loss:  5.8512\n",
      "epoch:  12   step:  155   train loss:  5.274700164794922  val loss:  4.728999137878418  val L1 loss:  5.1751\n",
      "epoch:  12   step:  156   train loss:  5.152588844299316  val loss:  4.463590145111084  val L1 loss:  4.9344\n",
      "epoch:  12   step:  157   train loss:  3.7230582237243652  val loss:  4.607236862182617  val L1 loss:  5.0829\n",
      "epoch:  12   step:  158   train loss:  3.740715503692627  val loss:  4.5308661460876465  val L1 loss:  4.9955\n",
      "epoch:  12   step:  159   train loss:  4.290045738220215  val loss:  4.270068168640137  val L1 loss:  4.7315\n",
      "epoch:  12   step:  160   train loss:  4.083414077758789  val loss:  3.981680154800415  val L1 loss:  4.4204\n",
      "min_val_loss_print 3.981680154800415\n",
      "epoch:  12   step:  161   train loss:  2.491316795349121  val loss:  3.9604671001434326  val L1 loss:  4.4193\n",
      "min_val_loss_print 3.9604671001434326\n",
      "epoch:  12   step:  162   train loss:  3.6695971488952637  val loss:  4.358489513397217  val L1 loss:  4.8515\n",
      "epoch:  12   step:  163   train loss:  4.323614597320557  val loss:  4.3372721672058105  val L1 loss:  4.8274\n",
      "epoch:  12   step:  164   train loss:  11.338680267333984  val loss:  3.7549612522125244  val L1 loss:  4.191\n",
      "min_val_loss_print 3.7549612522125244\n",
      "epoch:  12   step:  165   train loss:  2.348926067352295  val loss:  3.770132303237915  val L1 loss:  4.2475\n",
      "epoch:  12   step:  166   train loss:  2.1518354415893555  val loss:  4.175144672393799  val L1 loss:  4.6476\n",
      "epoch:  12   step:  167   train loss:  2.363800048828125  val loss:  4.3802103996276855  val L1 loss:  4.8649\n",
      "epoch:  12   step:  168   train loss:  3.66313099861145  val loss:  4.280515193939209  val L1 loss:  4.765\n",
      "epoch:  12   step:  169   train loss:  3.0562944412231445  val loss:  4.130753993988037  val L1 loss:  4.5916\n",
      "epoch:  12   step:  170   train loss:  3.8980259895324707  val loss:  4.489186763763428  val L1 loss:  4.9645\n",
      "epoch:  12   step:  171   train loss:  3.9023146629333496  val loss:  4.961557388305664  val L1 loss:  5.4242\n",
      "epoch:  12   step:  172   train loss:  4.883859634399414  val loss:  5.183676242828369  val L1 loss:  5.6585\n",
      "epoch:  12   step:  173   train loss:  2.842526912689209  val loss:  5.430779933929443  val L1 loss:  5.9197\n",
      "epoch:  12   step:  174   train loss:  3.6254467964172363  val loss:  5.3553056716918945  val L1 loss:  5.842\n",
      "epoch:  12   step:  175   train loss:  5.352006912231445  val loss:  4.720755577087402  val L1 loss:  5.1897\n",
      "epoch:  12   step:  176   train loss:  2.1694955825805664  val loss:  4.266787052154541  val L1 loss:  4.7192\n",
      "epoch:  12   step:  177   train loss:  2.2190048694610596  val loss:  4.416740417480469  val L1 loss:  4.8924\n",
      "epoch:  12   step:  178   train loss:  3.8948614597320557  val loss:  4.506989479064941  val L1 loss:  4.9769\n",
      "epoch:  12   step:  179   train loss:  4.440640926361084  val loss:  4.513050556182861  val L1 loss:  4.988\n",
      "epoch:  12   step:  180   train loss:  4.506002426147461  val loss:  4.443565845489502  val L1 loss:  4.9153\n",
      "epoch:  12   step:  181   train loss:  6.804339408874512  val loss:  4.440390110015869  val L1 loss:  4.921\n",
      "epoch:  12   step:  182   train loss:  4.146413326263428  val loss:  4.478462219238281  val L1 loss:  4.9472\n",
      "epoch:  12   step:  183   train loss:  4.782286643981934  val loss:  4.551914691925049  val L1 loss:  5.0289\n",
      "epoch:  12   step:  184   train loss:  3.4433343410491943  val loss:  4.722867012023926  val L1 loss:  5.2127\n",
      "epoch:  12   step:  185   train loss:  3.737210750579834  val loss:  4.771860599517822  val L1 loss:  5.2496\n",
      "epoch:  12   step:  186   train loss:  4.207981586456299  val loss:  5.086383819580078  val L1 loss:  5.5772\n",
      "epoch:  12   step:  187   train loss:  3.458646774291992  val loss:  5.413229942321777  val L1 loss:  5.8917\n",
      "epoch:  12   step:  188   train loss:  3.468996047973633  val loss:  5.757872104644775  val L1 loss:  6.2372\n",
      "epoch:  12   step:  189   train loss:  4.099972724914551  val loss:  5.747366905212402  val L1 loss:  6.2279\n",
      "epoch:  12   step:  190   train loss:  5.161801338195801  val loss:  5.5692009925842285  val L1 loss:  6.0593\n",
      "epoch:  12   step:  191   train loss:  2.9946184158325195  val loss:  5.150251388549805  val L1 loss:  5.6369\n",
      "epoch:  12   step:  192   train loss:  3.331613540649414  val loss:  4.944944858551025  val L1 loss:  5.4268\n",
      "epoch:  12   step:  193   train loss:  3.6787779331207275  val loss:  4.785803318023682  val L1 loss:  5.2553\n",
      "epoch:  12   step:  194   train loss:  4.177160739898682  val loss:  4.789780139923096  val L1 loss:  5.2615\n",
      "epoch:  12   step:  195   train loss:  5.326280117034912  val loss:  4.894540786743164  val L1 loss:  5.3785\n",
      "epoch:  12   step:  196   train loss:  5.9484758377075195  val loss:  5.1272382736206055  val L1 loss:  5.6178\n",
      "epoch:  12   step:  197   train loss:  4.101975440979004  val loss:  5.312816143035889  val L1 loss:  5.7935\n",
      "epoch:  12   step:  198   train loss:  2.8729002475738525  val loss:  5.586827754974365  val L1 loss:  6.0729\n",
      "epoch:  12   step:  199   train loss:  3.140721321105957  val loss:  5.437009334564209  val L1 loss:  5.923\n",
      "epoch:  12   step:  200   train loss:  4.999004364013672  val loss:  5.419517517089844  val L1 loss:  5.8976\n",
      "epoch:  12   step:  201   train loss:  4.337743282318115  val loss:  5.169104099273682  val L1 loss:  5.6551\n",
      "epoch:  12   step:  202   train loss:  4.070069789886475  val loss:  4.941002368927002  val L1 loss:  5.4164\n",
      "epoch:  12   step:  203   train loss:  3.7858152389526367  val loss:  4.7860846519470215  val L1 loss:  5.2797\n",
      "epoch:  12   step:  204   train loss:  3.245479106903076  val loss:  4.782867908477783  val L1 loss:  5.2766\n",
      "epoch:  12   step:  205   train loss:  3.3587706089019775  val loss:  4.789549827575684  val L1 loss:  5.2631\n",
      "epoch:  12   step:  206   train loss:  3.8566081523895264  val loss:  4.768881320953369  val L1 loss:  5.2493\n",
      "epoch:  12   step:  207   train loss:  3.7961952686309814  val loss:  4.929020881652832  val L1 loss:  5.3916\n",
      "epoch:  12   step:  208   train loss:  2.8916938304901123  val loss:  5.173298358917236  val L1 loss:  5.6483\n",
      "epoch:  13   step:  0   train loss:  3.8764498233795166  val loss:  4.935590744018555  val L1 loss:  5.4041\n",
      "epoch:  13   step:  1   train loss:  3.455925941467285  val loss:  4.6707658767700195  val L1 loss:  5.1409\n",
      "epoch:  13   step:  2   train loss:  3.0714111328125  val loss:  4.63371467590332  val L1 loss:  5.1009\n",
      "epoch:  13   step:  3   train loss:  2.475545644760132  val loss:  4.877179145812988  val L1 loss:  5.3509\n",
      "epoch:  13   step:  4   train loss:  4.754004955291748  val loss:  4.8914594650268555  val L1 loss:  5.3748\n",
      "epoch:  13   step:  5   train loss:  2.4661784172058105  val loss:  4.787469387054443  val L1 loss:  5.2705\n",
      "epoch:  13   step:  6   train loss:  2.646453619003296  val loss:  4.637491226196289  val L1 loss:  5.1042\n",
      "epoch:  13   step:  7   train loss:  4.393141746520996  val loss:  4.658014297485352  val L1 loss:  5.1193\n",
      "epoch:  13   step:  8   train loss:  3.2075393199920654  val loss:  4.800239086151123  val L1 loss:  5.272\n",
      "epoch:  13   step:  9   train loss:  5.992547988891602  val loss:  4.925563335418701  val L1 loss:  5.3939\n",
      "epoch:  13   step:  10   train loss:  5.438877105712891  val loss:  4.8350372314453125  val L1 loss:  5.2912\n",
      "epoch:  13   step:  11   train loss:  3.1342275142669678  val loss:  4.682844161987305  val L1 loss:  5.1201\n",
      "epoch:  13   step:  12   train loss:  5.139715671539307  val loss:  4.643289566040039  val L1 loss:  5.0737\n",
      "epoch:  13   step:  13   train loss:  2.719587802886963  val loss:  4.588943004608154  val L1 loss:  5.0219\n",
      "epoch:  13   step:  14   train loss:  4.516760349273682  val loss:  4.725396156311035  val L1 loss:  5.1801\n",
      "epoch:  13   step:  15   train loss:  3.0813403129577637  val loss:  4.937269687652588  val L1 loss:  5.4164\n",
      "epoch:  13   step:  16   train loss:  2.052582263946533  val loss:  5.0874762535095215  val L1 loss:  5.5676\n",
      "epoch:  13   step:  17   train loss:  4.615355491638184  val loss:  5.157480239868164  val L1 loss:  5.6281\n",
      "epoch:  13   step:  18   train loss:  4.5141119956970215  val loss:  5.068271636962891  val L1 loss:  5.532\n",
      "epoch:  13   step:  19   train loss:  3.4205734729766846  val loss:  4.808835983276367  val L1 loss:  5.2795\n",
      "epoch:  13   step:  20   train loss:  4.113487243652344  val loss:  4.797776222229004  val L1 loss:  5.2692\n",
      "epoch:  13   step:  21   train loss:  2.457551956176758  val loss:  4.766510486602783  val L1 loss:  5.2353\n",
      "epoch:  13   step:  22   train loss:  3.0902156829833984  val loss:  4.541172504425049  val L1 loss:  5.0277\n",
      "epoch:  13   step:  23   train loss:  3.4842891693115234  val loss:  4.419231414794922  val L1 loss:  4.9114\n",
      "epoch:  13   step:  24   train loss:  3.039520263671875  val loss:  4.416013717651367  val L1 loss:  4.8831\n",
      "epoch:  13   step:  25   train loss:  5.1794843673706055  val loss:  4.441535472869873  val L1 loss:  4.9195\n",
      "epoch:  13   step:  26   train loss:  2.252542495727539  val loss:  4.432755470275879  val L1 loss:  4.8809\n",
      "epoch:  13   step:  27   train loss:  3.8779072761535645  val loss:  4.437576770782471  val L1 loss:  4.8868\n",
      "epoch:  13   step:  28   train loss:  4.102473258972168  val loss:  4.533055782318115  val L1 loss:  5.0178\n",
      "epoch:  13   step:  29   train loss:  4.062435626983643  val loss:  4.636530876159668  val L1 loss:  5.1075\n",
      "epoch:  13   step:  30   train loss:  4.029606342315674  val loss:  4.908627986907959  val L1 loss:  5.3904\n",
      "epoch:  13   step:  31   train loss:  3.084615707397461  val loss:  4.903074264526367  val L1 loss:  5.3918\n",
      "epoch:  13   step:  32   train loss:  2.9864282608032227  val loss:  4.751718521118164  val L1 loss:  5.2136\n",
      "epoch:  13   step:  33   train loss:  3.4032368659973145  val loss:  4.587197303771973  val L1 loss:  5.0338\n",
      "epoch:  13   step:  34   train loss:  3.1048712730407715  val loss:  4.781007766723633  val L1 loss:  5.2607\n",
      "epoch:  13   step:  35   train loss:  5.13179349899292  val loss:  5.066060543060303  val L1 loss:  5.5378\n",
      "epoch:  13   step:  36   train loss:  4.394435405731201  val loss:  4.914981842041016  val L1 loss:  5.3957\n",
      "epoch:  13   step:  37   train loss:  5.191815376281738  val loss:  4.723316669464111  val L1 loss:  5.19\n",
      "epoch:  13   step:  38   train loss:  3.696803092956543  val loss:  4.941559791564941  val L1 loss:  5.4101\n",
      "epoch:  13   step:  39   train loss:  2.489147186279297  val loss:  5.230954647064209  val L1 loss:  5.7162\n",
      "epoch:  13   step:  40   train loss:  5.6921586990356445  val loss:  5.505533218383789  val L1 loss:  5.9861\n",
      "epoch:  13   step:  41   train loss:  3.079946517944336  val loss:  5.641147136688232  val L1 loss:  6.1256\n",
      "epoch:  13   step:  42   train loss:  3.6277244091033936  val loss:  5.347131729125977  val L1 loss:  5.8204\n",
      "epoch:  13   step:  43   train loss:  3.986604690551758  val loss:  5.052174091339111  val L1 loss:  5.5325\n",
      "epoch:  13   step:  44   train loss:  3.973813056945801  val loss:  5.025477886199951  val L1 loss:  5.5098\n",
      "epoch:  13   step:  45   train loss:  4.2779436111450195  val loss:  4.998118877410889  val L1 loss:  5.4811\n",
      "epoch:  13   step:  46   train loss:  4.951554775238037  val loss:  4.932986259460449  val L1 loss:  5.4208\n",
      "epoch:  13   step:  47   train loss:  6.146970748901367  val loss:  4.918186664581299  val L1 loss:  5.3691\n",
      "epoch:  13   step:  48   train loss:  1.8908867835998535  val loss:  4.998733043670654  val L1 loss:  5.4758\n",
      "epoch:  13   step:  49   train loss:  1.8005708456039429  val loss:  4.999441146850586  val L1 loss:  5.4969\n",
      "epoch:  13   step:  50   train loss:  3.5297632217407227  val loss:  4.734584331512451  val L1 loss:  5.2009\n",
      "epoch:  13   step:  51   train loss:  3.0915772914886475  val loss:  4.533292770385742  val L1 loss:  4.9872\n",
      "epoch:  13   step:  52   train loss:  4.2676615715026855  val loss:  4.588068008422852  val L1 loss:  5.0562\n",
      "epoch:  13   step:  53   train loss:  3.865198850631714  val loss:  4.82983922958374  val L1 loss:  5.3175\n",
      "epoch:  13   step:  54   train loss:  3.525015354156494  val loss:  4.942001819610596  val L1 loss:  5.4219\n",
      "epoch:  13   step:  55   train loss:  3.0066308975219727  val loss:  4.919700622558594  val L1 loss:  5.408\n",
      "epoch:  13   step:  56   train loss:  4.684681415557861  val loss:  4.747583389282227  val L1 loss:  5.2333\n",
      "epoch:  13   step:  57   train loss:  2.6324374675750732  val loss:  4.735583782196045  val L1 loss:  5.222\n",
      "epoch:  13   step:  58   train loss:  4.3686909675598145  val loss:  4.56631326675415  val L1 loss:  5.0267\n",
      "epoch:  13   step:  59   train loss:  5.012339115142822  val loss:  4.613564968109131  val L1 loss:  5.0748\n",
      "epoch:  13   step:  60   train loss:  4.551156997680664  val loss:  4.611762046813965  val L1 loss:  5.0672\n",
      "epoch:  13   step:  61   train loss:  3.7104272842407227  val loss:  4.519595623016357  val L1 loss:  4.9831\n",
      "epoch:  13   step:  62   train loss:  3.748701572418213  val loss:  4.503056526184082  val L1 loss:  4.943\n",
      "epoch:  13   step:  63   train loss:  2.1044087409973145  val loss:  4.55270528793335  val L1 loss:  5.0216\n",
      "epoch:  13   step:  64   train loss:  3.6568260192871094  val loss:  4.620796203613281  val L1 loss:  5.0795\n",
      "epoch:  13   step:  65   train loss:  2.9414360523223877  val loss:  4.992468357086182  val L1 loss:  5.4619\n",
      "epoch:  13   step:  66   train loss:  3.317352533340454  val loss:  5.1620635986328125  val L1 loss:  5.6435\n",
      "epoch:  13   step:  67   train loss:  2.3987481594085693  val loss:  4.971358299255371  val L1 loss:  5.4375\n",
      "epoch:  13   step:  68   train loss:  2.212368965148926  val loss:  4.8242316246032715  val L1 loss:  5.2927\n",
      "epoch:  13   step:  69   train loss:  2.573507785797119  val loss:  4.944664478302002  val L1 loss:  5.4217\n",
      "epoch:  13   step:  70   train loss:  4.453470230102539  val loss:  4.943911075592041  val L1 loss:  5.4166\n",
      "epoch:  13   step:  71   train loss:  4.04191780090332  val loss:  4.83672571182251  val L1 loss:  5.3037\n",
      "epoch:  13   step:  72   train loss:  5.1007184982299805  val loss:  4.763843536376953  val L1 loss:  5.2307\n",
      "epoch:  13   step:  73   train loss:  4.471881866455078  val loss:  4.797518253326416  val L1 loss:  5.2704\n",
      "epoch:  13   step:  74   train loss:  3.580399990081787  val loss:  5.071507453918457  val L1 loss:  5.5471\n",
      "epoch:  13   step:  75   train loss:  2.226193428039551  val loss:  5.234843730926514  val L1 loss:  5.7005\n",
      "epoch:  13   step:  76   train loss:  2.0879077911376953  val loss:  5.578535079956055  val L1 loss:  6.0477\n",
      "epoch:  13   step:  77   train loss:  2.1751937866210938  val loss:  5.749536991119385  val L1 loss:  6.2291\n",
      "epoch:  13   step:  78   train loss:  3.0144083499908447  val loss:  5.528480529785156  val L1 loss:  5.9834\n",
      "epoch:  13   step:  79   train loss:  4.603487014770508  val loss:  5.137078285217285  val L1 loss:  5.6234\n",
      "epoch:  13   step:  80   train loss:  3.647049903869629  val loss:  5.065126895904541  val L1 loss:  5.547\n",
      "epoch:  13   step:  81   train loss:  4.485292434692383  val loss:  5.084341526031494  val L1 loss:  5.5677\n",
      "epoch:  13   step:  82   train loss:  2.621140956878662  val loss:  5.052534580230713  val L1 loss:  5.5325\n",
      "epoch:  13   step:  83   train loss:  3.3708815574645996  val loss:  5.161233901977539  val L1 loss:  5.6365\n",
      "epoch:  13   step:  84   train loss:  4.712630271911621  val loss:  5.137484073638916  val L1 loss:  5.6012\n",
      "epoch:  13   step:  85   train loss:  4.386302947998047  val loss:  4.697841167449951  val L1 loss:  5.1524\n",
      "epoch:  13   step:  86   train loss:  2.109095573425293  val loss:  4.35306978225708  val L1 loss:  4.7941\n",
      "epoch:  13   step:  87   train loss:  2.7622616291046143  val loss:  4.526820659637451  val L1 loss:  5.0151\n",
      "epoch:  13   step:  88   train loss:  3.166933298110962  val loss:  4.992394924163818  val L1 loss:  5.4843\n",
      "epoch:  13   step:  89   train loss:  3.4972829818725586  val loss:  5.371996879577637  val L1 loss:  5.8599\n",
      "epoch:  13   step:  90   train loss:  2.8258683681488037  val loss:  5.669074535369873  val L1 loss:  6.1532\n",
      "epoch:  13   step:  91   train loss:  4.045687675476074  val loss:  5.787632942199707  val L1 loss:  6.2728\n",
      "epoch:  13   step:  92   train loss:  3.082712173461914  val loss:  5.4069318771362305  val L1 loss:  5.8817\n",
      "epoch:  13   step:  93   train loss:  3.446601390838623  val loss:  5.136556148529053  val L1 loss:  5.6056\n",
      "epoch:  13   step:  94   train loss:  3.6860389709472656  val loss:  4.920282363891602  val L1 loss:  5.3866\n",
      "epoch:  13   step:  95   train loss:  3.2669453620910645  val loss:  4.85376501083374  val L1 loss:  5.3092\n",
      "epoch:  13   step:  96   train loss:  3.2049355506896973  val loss:  4.75779390335083  val L1 loss:  5.2105\n",
      "epoch:  13   step:  97   train loss:  3.5454344749450684  val loss:  4.697594165802002  val L1 loss:  5.1498\n",
      "epoch:  13   step:  98   train loss:  3.724332571029663  val loss:  4.695684909820557  val L1 loss:  5.1683\n",
      "epoch:  13   step:  99   train loss:  3.199512481689453  val loss:  4.715114116668701  val L1 loss:  5.1738\n",
      "epoch:  13   step:  100   train loss:  3.520826816558838  val loss:  4.765190124511719  val L1 loss:  5.2459\n",
      "epoch:  13   step:  101   train loss:  4.647393703460693  val loss:  4.804127216339111  val L1 loss:  5.2766\n",
      "epoch:  13   step:  102   train loss:  3.755262851715088  val loss:  4.783561706542969  val L1 loss:  5.2571\n",
      "epoch:  13   step:  103   train loss:  2.8837313652038574  val loss:  4.7421369552612305  val L1 loss:  5.2073\n",
      "epoch:  13   step:  104   train loss:  5.311586380004883  val loss:  4.944221496582031  val L1 loss:  5.4171\n",
      "epoch:  13   step:  105   train loss:  4.643342971801758  val loss:  5.541633129119873  val L1 loss:  6.0254\n",
      "epoch:  13   step:  106   train loss:  3.7652955055236816  val loss:  6.217500686645508  val L1 loss:  6.7112\n",
      "epoch:  13   step:  107   train loss:  3.015197515487671  val loss:  6.724252700805664  val L1 loss:  7.2107\n",
      "epoch:  13   step:  108   train loss:  3.3921713829040527  val loss:  6.875247001647949  val L1 loss:  7.3588\n",
      "epoch:  13   step:  109   train loss:  3.941046714782715  val loss:  6.541606426239014  val L1 loss:  7.0228\n",
      "epoch:  13   step:  110   train loss:  4.520734786987305  val loss:  5.75335693359375  val L1 loss:  6.2376\n",
      "epoch:  13   step:  111   train loss:  4.012165546417236  val loss:  5.258122444152832  val L1 loss:  5.7432\n",
      "epoch:  13   step:  112   train loss:  2.964317798614502  val loss:  4.890704154968262  val L1 loss:  5.3679\n",
      "epoch:  13   step:  113   train loss:  3.207642078399658  val loss:  4.779513835906982  val L1 loss:  5.2397\n",
      "epoch:  13   step:  114   train loss:  4.278027534484863  val loss:  4.828917980194092  val L1 loss:  5.2824\n",
      "epoch:  13   step:  115   train loss:  3.262802839279175  val loss:  4.84483003616333  val L1 loss:  5.3083\n",
      "epoch:  13   step:  116   train loss:  3.6402904987335205  val loss:  4.872161388397217  val L1 loss:  5.359\n",
      "epoch:  13   step:  117   train loss:  2.848681926727295  val loss:  4.908545017242432  val L1 loss:  5.3886\n",
      "epoch:  13   step:  118   train loss:  5.223552703857422  val loss:  5.100073337554932  val L1 loss:  5.5878\n",
      "epoch:  13   step:  119   train loss:  4.073556900024414  val loss:  5.110495567321777  val L1 loss:  5.5851\n",
      "epoch:  13   step:  120   train loss:  3.045452833175659  val loss:  4.992832660675049  val L1 loss:  5.4597\n",
      "epoch:  13   step:  121   train loss:  6.31416130065918  val loss:  5.049397945404053  val L1 loss:  5.4901\n",
      "epoch:  13   step:  122   train loss:  3.6343040466308594  val loss:  5.454462051391602  val L1 loss:  5.9234\n",
      "epoch:  13   step:  123   train loss:  4.784359931945801  val loss:  5.77925968170166  val L1 loss:  6.2397\n",
      "epoch:  13   step:  124   train loss:  3.45390248298645  val loss:  5.8988237380981445  val L1 loss:  6.3368\n",
      "epoch:  13   step:  125   train loss:  4.26411247253418  val loss:  5.895826816558838  val L1 loss:  6.332\n",
      "epoch:  13   step:  126   train loss:  3.914245843887329  val loss:  5.463566303253174  val L1 loss:  5.9102\n",
      "epoch:  13   step:  127   train loss:  2.3344664573669434  val loss:  5.133918762207031  val L1 loss:  5.6085\n",
      "epoch:  13   step:  128   train loss:  4.09998893737793  val loss:  5.004870414733887  val L1 loss:  5.4613\n",
      "epoch:  13   step:  129   train loss:  2.7050530910491943  val loss:  4.953433990478516  val L1 loss:  5.3972\n",
      "epoch:  13   step:  130   train loss:  3.130998134613037  val loss:  4.935258865356445  val L1 loss:  5.3866\n",
      "epoch:  13   step:  131   train loss:  3.565889596939087  val loss:  5.2334747314453125  val L1 loss:  5.7164\n",
      "epoch:  13   step:  132   train loss:  3.218790292739868  val loss:  5.202788352966309  val L1 loss:  5.6853\n",
      "epoch:  13   step:  133   train loss:  3.096437931060791  val loss:  4.99697732925415  val L1 loss:  5.4529\n",
      "epoch:  13   step:  134   train loss:  4.155571460723877  val loss:  4.933661460876465  val L1 loss:  5.401\n",
      "epoch:  13   step:  135   train loss:  3.789705276489258  val loss:  5.039145469665527  val L1 loss:  5.5038\n",
      "epoch:  13   step:  136   train loss:  2.3579914569854736  val loss:  5.049437046051025  val L1 loss:  5.5267\n",
      "epoch:  13   step:  137   train loss:  4.934730529785156  val loss:  4.979277610778809  val L1 loss:  5.4523\n",
      "epoch:  13   step:  138   train loss:  2.2216057777404785  val loss:  4.9364824295043945  val L1 loss:  5.4159\n",
      "epoch:  13   step:  139   train loss:  5.981998443603516  val loss:  4.944899559020996  val L1 loss:  5.4054\n",
      "epoch:  13   step:  140   train loss:  2.4983372688293457  val loss:  4.965115070343018  val L1 loss:  5.442\n",
      "epoch:  13   step:  141   train loss:  3.2697768211364746  val loss:  5.00918436050415  val L1 loss:  5.4839\n",
      "epoch:  13   step:  142   train loss:  3.506744384765625  val loss:  4.717789173126221  val L1 loss:  5.194\n",
      "epoch:  13   step:  143   train loss:  4.117498397827148  val loss:  4.435478210449219  val L1 loss:  4.8856\n",
      "epoch:  13   step:  144   train loss:  2.6483359336853027  val loss:  4.66331148147583  val L1 loss:  5.1221\n",
      "epoch:  13   step:  145   train loss:  3.382293939590454  val loss:  4.96855354309082  val L1 loss:  5.4404\n",
      "epoch:  13   step:  146   train loss:  6.098091125488281  val loss:  4.779809951782227  val L1 loss:  5.224\n",
      "epoch:  13   step:  147   train loss:  7.488471984863281  val loss:  4.704768180847168  val L1 loss:  5.1869\n",
      "epoch:  13   step:  148   train loss:  4.205348014831543  val loss:  5.119101047515869  val L1 loss:  5.5908\n",
      "epoch:  13   step:  149   train loss:  5.2105865478515625  val loss:  5.968140125274658  val L1 loss:  6.4623\n",
      "epoch:  13   step:  150   train loss:  3.9686295986175537  val loss:  6.505592346191406  val L1 loss:  7.0056\n",
      "epoch:  13   step:  151   train loss:  3.04487943649292  val loss:  6.592350959777832  val L1 loss:  7.0924\n",
      "epoch:  13   step:  152   train loss:  3.4257943630218506  val loss:  6.44183349609375  val L1 loss:  6.9338\n",
      "epoch:  13   step:  153   train loss:  3.549941062927246  val loss:  5.985006809234619  val L1 loss:  6.4706\n",
      "epoch:  13   step:  154   train loss:  3.922008991241455  val loss:  5.562264442443848  val L1 loss:  6.0297\n",
      "epoch:  13   step:  155   train loss:  3.429203510284424  val loss:  5.459804058074951  val L1 loss:  5.9387\n",
      "epoch:  13   step:  156   train loss:  3.2172963619232178  val loss:  5.303905487060547  val L1 loss:  5.7853\n",
      "epoch:  13   step:  157   train loss:  6.433919906616211  val loss:  4.977795600891113  val L1 loss:  5.457\n",
      "epoch:  13   step:  158   train loss:  3.6218600273132324  val loss:  4.759144306182861  val L1 loss:  5.2338\n",
      "epoch:  13   step:  159   train loss:  3.390289783477783  val loss:  4.644941806793213  val L1 loss:  5.1251\n",
      "epoch:  13   step:  160   train loss:  2.0667614936828613  val loss:  4.632861614227295  val L1 loss:  5.1125\n",
      "epoch:  13   step:  161   train loss:  2.4701879024505615  val loss:  4.525907039642334  val L1 loss:  5.0071\n",
      "epoch:  13   step:  162   train loss:  4.823343753814697  val loss:  4.33922004699707  val L1 loss:  4.8126\n",
      "epoch:  13   step:  163   train loss:  5.355040550231934  val loss:  4.199295997619629  val L1 loss:  4.6551\n",
      "epoch:  13   step:  164   train loss:  3.4075450897216797  val loss:  4.287391185760498  val L1 loss:  4.7646\n",
      "epoch:  13   step:  165   train loss:  4.257159233093262  val loss:  4.633841037750244  val L1 loss:  5.1123\n",
      "epoch:  13   step:  166   train loss:  3.2963953018188477  val loss:  5.06044340133667  val L1 loss:  5.5394\n",
      "epoch:  13   step:  167   train loss:  3.4682564735412598  val loss:  4.908863544464111  val L1 loss:  5.3962\n",
      "epoch:  13   step:  168   train loss:  5.370621681213379  val loss:  4.798623561859131  val L1 loss:  5.2807\n",
      "epoch:  13   step:  169   train loss:  5.266104221343994  val loss:  4.60858678817749  val L1 loss:  5.0868\n",
      "epoch:  13   step:  170   train loss:  3.3545749187469482  val loss:  4.3028435707092285  val L1 loss:  4.7658\n",
      "epoch:  13   step:  171   train loss:  3.769362449645996  val loss:  4.16921854019165  val L1 loss:  4.6288\n",
      "epoch:  13   step:  172   train loss:  3.6327755451202393  val loss:  4.2904534339904785  val L1 loss:  4.7505\n",
      "epoch:  13   step:  173   train loss:  2.2190732955932617  val loss:  4.4352593421936035  val L1 loss:  4.8963\n",
      "epoch:  13   step:  174   train loss:  6.58916711807251  val loss:  4.4388651847839355  val L1 loss:  4.9101\n",
      "epoch:  13   step:  175   train loss:  2.0921010971069336  val loss:  4.6182050704956055  val L1 loss:  5.0992\n",
      "epoch:  13   step:  176   train loss:  5.41347599029541  val loss:  4.776061058044434  val L1 loss:  5.2561\n",
      "epoch:  13   step:  177   train loss:  3.3487532138824463  val loss:  4.83734655380249  val L1 loss:  5.3216\n",
      "epoch:  13   step:  178   train loss:  2.277116298675537  val loss:  4.9727983474731445  val L1 loss:  5.4639\n",
      "epoch:  13   step:  179   train loss:  3.177820920944214  val loss:  4.950823783874512  val L1 loss:  5.4224\n",
      "epoch:  13   step:  180   train loss:  3.151784658432007  val loss:  4.8421854972839355  val L1 loss:  5.3026\n",
      "epoch:  13   step:  181   train loss:  3.3938961029052734  val loss:  4.807653903961182  val L1 loss:  5.2636\n",
      "epoch:  13   step:  182   train loss:  2.8945212364196777  val loss:  4.7607102394104  val L1 loss:  5.2311\n",
      "epoch:  13   step:  183   train loss:  4.575016975402832  val loss:  4.683245658874512  val L1 loss:  5.1616\n",
      "epoch:  13   step:  184   train loss:  5.282558441162109  val loss:  4.6416916847229  val L1 loss:  5.1134\n",
      "epoch:  13   step:  185   train loss:  4.247137069702148  val loss:  4.76682186126709  val L1 loss:  5.259\n",
      "epoch:  13   step:  186   train loss:  2.9444737434387207  val loss:  4.817220211029053  val L1 loss:  5.2947\n",
      "epoch:  13   step:  187   train loss:  4.311892509460449  val loss:  4.774062633514404  val L1 loss:  5.2481\n",
      "epoch:  13   step:  188   train loss:  4.485747337341309  val loss:  4.668724536895752  val L1 loss:  5.1426\n",
      "epoch:  13   step:  189   train loss:  5.6510772705078125  val loss:  4.932905673980713  val L1 loss:  5.4263\n",
      "epoch:  13   step:  190   train loss:  3.6570651531219482  val loss:  5.312613487243652  val L1 loss:  5.7974\n",
      "epoch:  13   step:  191   train loss:  2.7036569118499756  val loss:  5.311770439147949  val L1 loss:  5.7907\n",
      "epoch:  13   step:  192   train loss:  2.31364369392395  val loss:  5.144011497497559  val L1 loss:  5.62\n",
      "epoch:  13   step:  193   train loss:  3.2063684463500977  val loss:  5.082385540008545  val L1 loss:  5.5688\n",
      "epoch:  13   step:  194   train loss:  4.433769226074219  val loss:  5.046204566955566  val L1 loss:  5.5235\n",
      "epoch:  13   step:  195   train loss:  3.751652479171753  val loss:  4.744847297668457  val L1 loss:  5.2142\n",
      "epoch:  13   step:  196   train loss:  4.463000774383545  val loss:  4.385953426361084  val L1 loss:  4.8586\n",
      "epoch:  13   step:  197   train loss:  3.7345690727233887  val loss:  4.380922317504883  val L1 loss:  4.8458\n",
      "epoch:  13   step:  198   train loss:  3.00984787940979  val loss:  4.457363605499268  val L1 loss:  4.9229\n",
      "epoch:  13   step:  199   train loss:  3.4406604766845703  val loss:  4.42813777923584  val L1 loss:  4.8921\n",
      "epoch:  13   step:  200   train loss:  4.632743835449219  val loss:  4.260082244873047  val L1 loss:  4.7089\n",
      "epoch:  13   step:  201   train loss:  3.356893539428711  val loss:  4.210873603820801  val L1 loss:  4.6638\n",
      "epoch:  13   step:  202   train loss:  2.7091877460479736  val loss:  4.170867443084717  val L1 loss:  4.6267\n",
      "epoch:  13   step:  203   train loss:  5.080566883087158  val loss:  4.297369956970215  val L1 loss:  4.766\n",
      "epoch:  13   step:  204   train loss:  2.660153388977051  val loss:  4.827792167663574  val L1 loss:  5.321\n",
      "epoch:  13   step:  205   train loss:  4.147077560424805  val loss:  4.9957075119018555  val L1 loss:  5.4792\n",
      "epoch:  13   step:  206   train loss:  3.3376402854919434  val loss:  4.8863067626953125  val L1 loss:  5.3524\n",
      "epoch:  13   step:  207   train loss:  1.6466178894042969  val loss:  4.605360507965088  val L1 loss:  5.079\n",
      "epoch:  13   step:  208   train loss:  2.6761763095855713  val loss:  4.5800700187683105  val L1 loss:  5.0472\n",
      "epoch:  14   step:  0   train loss:  2.4565913677215576  val loss:  4.6168622970581055  val L1 loss:  5.0941\n",
      "epoch:  14   step:  1   train loss:  2.434210777282715  val loss:  4.680182456970215  val L1 loss:  5.1451\n",
      "epoch:  14   step:  2   train loss:  2.550949811935425  val loss:  4.478316783905029  val L1 loss:  4.9394\n",
      "epoch:  14   step:  3   train loss:  3.111680030822754  val loss:  4.270485877990723  val L1 loss:  4.7406\n",
      "epoch:  14   step:  4   train loss:  2.4967586994171143  val loss:  4.25308895111084  val L1 loss:  4.7066\n",
      "epoch:  14   step:  5   train loss:  2.732473373413086  val loss:  4.2542619705200195  val L1 loss:  4.685\n",
      "epoch:  14   step:  6   train loss:  4.425985813140869  val loss:  4.270541667938232  val L1 loss:  4.7223\n",
      "epoch:  14   step:  7   train loss:  4.135236740112305  val loss:  4.307420253753662  val L1 loss:  4.7657\n",
      "epoch:  14   step:  8   train loss:  2.646179676055908  val loss:  4.40068244934082  val L1 loss:  4.8553\n",
      "epoch:  14   step:  9   train loss:  5.350277900695801  val loss:  4.430898189544678  val L1 loss:  4.8866\n",
      "epoch:  14   step:  10   train loss:  4.6700439453125  val loss:  4.418315410614014  val L1 loss:  4.8807\n",
      "epoch:  14   step:  11   train loss:  2.489837169647217  val loss:  4.550392150878906  val L1 loss:  5.0199\n",
      "epoch:  14   step:  12   train loss:  6.022176742553711  val loss:  4.717486381530762  val L1 loss:  5.1922\n",
      "epoch:  14   step:  13   train loss:  3.068922996520996  val loss:  4.835330486297607  val L1 loss:  5.3021\n",
      "epoch:  14   step:  14   train loss:  3.128286361694336  val loss:  4.630337238311768  val L1 loss:  5.096\n",
      "epoch:  14   step:  15   train loss:  3.3265647888183594  val loss:  4.444563388824463  val L1 loss:  4.9085\n",
      "epoch:  14   step:  16   train loss:  5.667383670806885  val loss:  4.442982196807861  val L1 loss:  4.9022\n",
      "epoch:  14   step:  17   train loss:  3.018157958984375  val loss:  4.750901222229004  val L1 loss:  5.2291\n",
      "epoch:  14   step:  18   train loss:  4.995739936828613  val loss:  4.929516315460205  val L1 loss:  5.4201\n",
      "epoch:  14   step:  19   train loss:  3.9371986389160156  val loss:  4.766861915588379  val L1 loss:  5.2484\n",
      "epoch:  14   step:  20   train loss:  3.5589678287506104  val loss:  4.657783031463623  val L1 loss:  5.1163\n",
      "epoch:  14   step:  21   train loss:  3.2079591751098633  val loss:  4.660745620727539  val L1 loss:  5.1097\n",
      "epoch:  14   step:  22   train loss:  2.3023366928100586  val loss:  4.529271602630615  val L1 loss:  4.9691\n",
      "epoch:  14   step:  23   train loss:  2.288834810256958  val loss:  4.391047954559326  val L1 loss:  4.8549\n",
      "epoch:  14   step:  24   train loss:  3.4905083179473877  val loss:  4.384719371795654  val L1 loss:  4.8518\n",
      "epoch:  14   step:  25   train loss:  2.909278392791748  val loss:  4.75131893157959  val L1 loss:  5.2045\n",
      "epoch:  14   step:  26   train loss:  4.223215103149414  val loss:  5.530262470245361  val L1 loss:  6.0245\n",
      "epoch:  14   step:  27   train loss:  4.706713676452637  val loss:  5.981296062469482  val L1 loss:  6.4778\n",
      "epoch:  14   step:  28   train loss:  3.6597371101379395  val loss:  5.686324119567871  val L1 loss:  6.1814\n",
      "epoch:  14   step:  29   train loss:  6.395012855529785  val loss:  5.331512451171875  val L1 loss:  5.7997\n",
      "epoch:  14   step:  30   train loss:  4.461883544921875  val loss:  4.957894325256348  val L1 loss:  5.4511\n",
      "epoch:  14   step:  31   train loss:  3.271031618118286  val loss:  4.839892387390137  val L1 loss:  5.3278\n",
      "epoch:  14   step:  32   train loss:  3.7295730113983154  val loss:  4.666956901550293  val L1 loss:  5.1229\n",
      "epoch:  14   step:  33   train loss:  3.6823525428771973  val loss:  4.758720397949219  val L1 loss:  5.2359\n",
      "epoch:  14   step:  34   train loss:  5.21760892868042  val loss:  4.815260410308838  val L1 loss:  5.2924\n",
      "epoch:  14   step:  35   train loss:  2.6057474613189697  val loss:  4.958897590637207  val L1 loss:  5.4444\n",
      "epoch:  14   step:  36   train loss:  2.302394151687622  val loss:  5.259706497192383  val L1 loss:  5.735\n",
      "epoch:  14   step:  37   train loss:  2.700753927230835  val loss:  5.535662651062012  val L1 loss:  6.0141\n",
      "epoch:  14   step:  38   train loss:  2.7754993438720703  val loss:  5.738651752471924  val L1 loss:  6.2117\n",
      "epoch:  14   step:  39   train loss:  4.181103706359863  val loss:  5.535662651062012  val L1 loss:  6.0022\n",
      "epoch:  14   step:  40   train loss:  4.054872512817383  val loss:  5.191707611083984  val L1 loss:  5.6773\n",
      "epoch:  14   step:  41   train loss:  2.641803503036499  val loss:  5.078917980194092  val L1 loss:  5.5366\n",
      "epoch:  14   step:  42   train loss:  5.760933876037598  val loss:  5.156060695648193  val L1 loss:  5.5996\n",
      "epoch:  14   step:  43   train loss:  3.1167685985565186  val loss:  5.167322158813477  val L1 loss:  5.6337\n",
      "epoch:  14   step:  44   train loss:  4.82893705368042  val loss:  5.085786819458008  val L1 loss:  5.5477\n",
      "epoch:  14   step:  45   train loss:  4.489205360412598  val loss:  5.218343257904053  val L1 loss:  5.6932\n",
      "epoch:  14   step:  46   train loss:  2.0094189643859863  val loss:  5.419379711151123  val L1 loss:  5.9088\n",
      "epoch:  14   step:  47   train loss:  4.468433856964111  val loss:  5.536205291748047  val L1 loss:  6.0198\n",
      "epoch:  14   step:  48   train loss:  4.38300895690918  val loss:  5.289607048034668  val L1 loss:  5.7673\n",
      "epoch:  14   step:  49   train loss:  2.936143636703491  val loss:  5.093455791473389  val L1 loss:  5.5419\n",
      "epoch:  14   step:  50   train loss:  3.3769993782043457  val loss:  5.140851974487305  val L1 loss:  5.6229\n",
      "epoch:  14   step:  51   train loss:  3.2714455127716064  val loss:  5.2614336013793945  val L1 loss:  5.7575\n",
      "epoch:  14   step:  52   train loss:  4.6241350173950195  val loss:  5.169660568237305  val L1 loss:  5.6624\n",
      "epoch:  14   step:  53   train loss:  4.793778419494629  val loss:  4.959651470184326  val L1 loss:  5.4208\n",
      "epoch:  14   step:  54   train loss:  6.2221760749816895  val loss:  4.796927452087402  val L1 loss:  5.2849\n",
      "epoch:  14   step:  55   train loss:  3.1353631019592285  val loss:  4.763223648071289  val L1 loss:  5.2423\n",
      "epoch:  14   step:  56   train loss:  3.42582106590271  val loss:  4.720699310302734  val L1 loss:  5.2163\n",
      "epoch:  14   step:  57   train loss:  4.3707075119018555  val loss:  4.639963150024414  val L1 loss:  5.1289\n",
      "epoch:  14   step:  58   train loss:  3.8139092922210693  val loss:  4.575345993041992  val L1 loss:  5.0578\n",
      "epoch:  14   step:  59   train loss:  2.9278790950775146  val loss:  4.693185806274414  val L1 loss:  5.1534\n",
      "epoch:  14   step:  60   train loss:  3.5712599754333496  val loss:  4.777048110961914  val L1 loss:  5.2211\n",
      "epoch:  14   step:  61   train loss:  5.07924747467041  val loss:  4.930019378662109  val L1 loss:  5.3881\n",
      "epoch:  14   step:  62   train loss:  2.395775318145752  val loss:  5.188732624053955  val L1 loss:  5.6503\n",
      "epoch:  14   step:  63   train loss:  2.1989784240722656  val loss:  5.521653175354004  val L1 loss:  5.9961\n",
      "epoch:  14   step:  64   train loss:  2.666563034057617  val loss:  5.413002967834473  val L1 loss:  5.8872\n",
      "epoch:  14   step:  65   train loss:  5.139150619506836  val loss:  5.0505218505859375  val L1 loss:  5.5289\n",
      "epoch:  14   step:  66   train loss:  3.3966455459594727  val loss:  4.730316162109375  val L1 loss:  5.2138\n",
      "epoch:  14   step:  67   train loss:  4.351819038391113  val loss:  4.343624114990234  val L1 loss:  4.7958\n",
      "epoch:  14   step:  68   train loss:  4.024341106414795  val loss:  4.260446071624756  val L1 loss:  4.7072\n",
      "epoch:  14   step:  69   train loss:  2.716775894165039  val loss:  4.131147384643555  val L1 loss:  4.5588\n",
      "epoch:  14   step:  70   train loss:  3.4288644790649414  val loss:  4.211330413818359  val L1 loss:  4.6763\n",
      "epoch:  14   step:  71   train loss:  4.814485549926758  val loss:  4.407349586486816  val L1 loss:  4.8767\n",
      "epoch:  14   step:  72   train loss:  3.276885986328125  val loss:  4.361735820770264  val L1 loss:  4.8321\n",
      "epoch:  14   step:  73   train loss:  2.454878807067871  val loss:  4.245338439941406  val L1 loss:  4.6914\n",
      "epoch:  14   step:  74   train loss:  3.5478670597076416  val loss:  4.236934185028076  val L1 loss:  4.6974\n",
      "epoch:  14   step:  75   train loss:  2.889875650405884  val loss:  4.341486930847168  val L1 loss:  4.8054\n",
      "epoch:  14   step:  76   train loss:  2.7460408210754395  val loss:  4.399858474731445  val L1 loss:  4.865\n",
      "epoch:  14   step:  77   train loss:  2.5594842433929443  val loss:  4.387105464935303  val L1 loss:  4.8331\n",
      "epoch:  14   step:  78   train loss:  2.139007568359375  val loss:  4.2626447677612305  val L1 loss:  4.7152\n",
      "epoch:  14   step:  79   train loss:  3.939850091934204  val loss:  4.30864143371582  val L1 loss:  4.7717\n",
      "epoch:  14   step:  80   train loss:  3.1704845428466797  val loss:  4.64367151260376  val L1 loss:  5.1219\n",
      "epoch:  14   step:  81   train loss:  4.678447723388672  val loss:  5.024319171905518  val L1 loss:  5.5178\n",
      "epoch:  14   step:  82   train loss:  3.6453890800476074  val loss:  5.137063503265381  val L1 loss:  5.6316\n",
      "epoch:  14   step:  83   train loss:  2.405595541000366  val loss:  4.987254619598389  val L1 loss:  5.4627\n",
      "epoch:  14   step:  84   train loss:  2.0490188598632812  val loss:  4.569851398468018  val L1 loss:  5.0189\n",
      "epoch:  14   step:  85   train loss:  2.176809310913086  val loss:  4.377223491668701  val L1 loss:  4.817\n",
      "epoch:  14   step:  86   train loss:  3.813199520111084  val loss:  4.407803058624268  val L1 loss:  4.8693\n",
      "epoch:  14   step:  87   train loss:  2.9241232872009277  val loss:  4.324725151062012  val L1 loss:  4.7885\n",
      "epoch:  14   step:  88   train loss:  2.6912007331848145  val loss:  4.287395477294922  val L1 loss:  4.7381\n",
      "epoch:  14   step:  89   train loss:  2.0295302867889404  val loss:  4.289180278778076  val L1 loss:  4.7318\n",
      "epoch:  14   step:  90   train loss:  5.344976902008057  val loss:  4.345375061035156  val L1 loss:  4.8183\n",
      "epoch:  14   step:  91   train loss:  3.0212278366088867  val loss:  4.377462863922119  val L1 loss:  4.8379\n",
      "epoch:  14   step:  92   train loss:  2.4249072074890137  val loss:  4.404098987579346  val L1 loss:  4.8673\n",
      "epoch:  14   step:  93   train loss:  4.788370132446289  val loss:  4.398637771606445  val L1 loss:  4.8673\n",
      "epoch:  14   step:  94   train loss:  3.4297947883605957  val loss:  4.465856075286865  val L1 loss:  4.9469\n",
      "epoch:  14   step:  95   train loss:  3.922558546066284  val loss:  4.509487628936768  val L1 loss:  4.9871\n",
      "epoch:  14   step:  96   train loss:  3.0756988525390625  val loss:  4.581143379211426  val L1 loss:  5.058\n",
      "epoch:  14   step:  97   train loss:  2.349119186401367  val loss:  4.677927017211914  val L1 loss:  5.1454\n",
      "epoch:  14   step:  98   train loss:  2.4650213718414307  val loss:  4.816964626312256  val L1 loss:  5.2855\n",
      "epoch:  14   step:  99   train loss:  3.2168710231781006  val loss:  4.844295978546143  val L1 loss:  5.3207\n",
      "epoch:  14   step:  100   train loss:  2.8657190799713135  val loss:  4.777260780334473  val L1 loss:  5.2388\n",
      "epoch:  14   step:  101   train loss:  5.162721157073975  val loss:  4.747908115386963  val L1 loss:  5.2198\n",
      "epoch:  14   step:  102   train loss:  2.2749736309051514  val loss:  4.828974723815918  val L1 loss:  5.3126\n",
      "epoch:  14   step:  103   train loss:  4.514987468719482  val loss:  4.857097148895264  val L1 loss:  5.343\n",
      "epoch:  14   step:  104   train loss:  2.971252679824829  val loss:  4.803210735321045  val L1 loss:  5.2823\n",
      "epoch:  14   step:  105   train loss:  3.7457847595214844  val loss:  4.8855881690979  val L1 loss:  5.3485\n",
      "epoch:  14   step:  106   train loss:  3.5514297485351562  val loss:  5.051055431365967  val L1 loss:  5.5082\n",
      "epoch:  14   step:  107   train loss:  3.1109867095947266  val loss:  5.56404447555542  val L1 loss:  6.0495\n",
      "epoch:  14   step:  108   train loss:  3.494138717651367  val loss:  5.807831764221191  val L1 loss:  6.3015\n",
      "epoch:  14   step:  109   train loss:  3.595372200012207  val loss:  5.252062797546387  val L1 loss:  5.7138\n",
      "epoch:  14   step:  110   train loss:  2.485517978668213  val loss:  4.671766757965088  val L1 loss:  5.1348\n",
      "epoch:  14   step:  111   train loss:  3.1192033290863037  val loss:  4.691981792449951  val L1 loss:  5.1598\n",
      "epoch:  14   step:  112   train loss:  4.564760208129883  val loss:  4.738343238830566  val L1 loss:  5.2037\n",
      "epoch:  14   step:  113   train loss:  3.358593702316284  val loss:  4.777804374694824  val L1 loss:  5.2165\n",
      "epoch:  14   step:  114   train loss:  3.687763214111328  val loss:  4.892252445220947  val L1 loss:  5.3548\n",
      "epoch:  14   step:  115   train loss:  2.875563621520996  val loss:  5.183780193328857  val L1 loss:  5.6745\n",
      "epoch:  14   step:  116   train loss:  2.6653971672058105  val loss:  5.297214031219482  val L1 loss:  5.782\n",
      "epoch:  14   step:  117   train loss:  3.568073272705078  val loss:  5.21812629699707  val L1 loss:  5.7087\n",
      "epoch:  14   step:  118   train loss:  3.200254440307617  val loss:  4.906973838806152  val L1 loss:  5.3857\n",
      "epoch:  14   step:  119   train loss:  1.6404772996902466  val loss:  4.850811958312988  val L1 loss:  5.3033\n",
      "epoch:  14   step:  120   train loss:  2.425931215286255  val loss:  5.105327129364014  val L1 loss:  5.5568\n",
      "epoch:  14   step:  121   train loss:  3.5468242168426514  val loss:  5.272130012512207  val L1 loss:  5.7398\n",
      "epoch:  14   step:  122   train loss:  4.122753143310547  val loss:  5.107195854187012  val L1 loss:  5.5584\n",
      "epoch:  14   step:  123   train loss:  5.695184230804443  val loss:  4.840287208557129  val L1 loss:  5.3157\n",
      "epoch:  14   step:  124   train loss:  3.2592341899871826  val loss:  5.082440376281738  val L1 loss:  5.5677\n",
      "epoch:  14   step:  125   train loss:  5.043701171875  val loss:  5.410524845123291  val L1 loss:  5.8922\n",
      "epoch:  14   step:  126   train loss:  4.3840436935424805  val loss:  5.277853965759277  val L1 loss:  5.7603\n",
      "epoch:  14   step:  127   train loss:  3.5241832733154297  val loss:  4.9276227951049805  val L1 loss:  5.408\n",
      "epoch:  14   step:  128   train loss:  3.813152551651001  val loss:  4.704977512359619  val L1 loss:  5.1639\n",
      "epoch:  14   step:  129   train loss:  3.3459160327911377  val loss:  4.723757266998291  val L1 loss:  5.1842\n",
      "epoch:  14   step:  130   train loss:  4.558109283447266  val loss:  4.691221714019775  val L1 loss:  5.1403\n",
      "epoch:  14   step:  131   train loss:  3.6160049438476562  val loss:  4.711665153503418  val L1 loss:  5.1543\n",
      "epoch:  14   step:  132   train loss:  4.300212860107422  val loss:  4.902235984802246  val L1 loss:  5.3844\n",
      "epoch:  14   step:  133   train loss:  3.1532225608825684  val loss:  5.188072204589844  val L1 loss:  5.6782\n",
      "epoch:  14   step:  134   train loss:  2.9998106956481934  val loss:  5.470768928527832  val L1 loss:  5.9576\n",
      "epoch:  14   step:  135   train loss:  3.2852118015289307  val loss:  5.467169761657715  val L1 loss:  5.9488\n",
      "epoch:  14   step:  136   train loss:  4.134045600891113  val loss:  5.218056678771973  val L1 loss:  5.7011\n",
      "epoch:  14   step:  137   train loss:  3.1360530853271484  val loss:  4.921710014343262  val L1 loss:  5.4073\n",
      "epoch:  14   step:  138   train loss:  3.8900420665740967  val loss:  4.807738304138184  val L1 loss:  5.2701\n",
      "epoch:  14   step:  139   train loss:  5.475060939788818  val loss:  4.816444396972656  val L1 loss:  5.29\n",
      "epoch:  14   step:  140   train loss:  4.388413429260254  val loss:  4.8128180503845215  val L1 loss:  5.2954\n",
      "epoch:  14   step:  141   train loss:  5.1067728996276855  val loss:  5.010781764984131  val L1 loss:  5.4658\n",
      "epoch:  14   step:  142   train loss:  2.1340808868408203  val loss:  5.190421104431152  val L1 loss:  5.6675\n",
      "epoch:  14   step:  143   train loss:  4.038374900817871  val loss:  5.494279861450195  val L1 loss:  5.9658\n",
      "epoch:  14   step:  144   train loss:  3.8279190063476562  val loss:  5.363858699798584  val L1 loss:  5.8343\n",
      "epoch:  14   step:  145   train loss:  2.7274410724639893  val loss:  4.940243244171143  val L1 loss:  5.4254\n",
      "epoch:  14   step:  146   train loss:  5.7346673011779785  val loss:  4.560808181762695  val L1 loss:  5.0255\n",
      "epoch:  14   step:  147   train loss:  2.935872793197632  val loss:  4.544266223907471  val L1 loss:  4.9894\n",
      "epoch:  14   step:  148   train loss:  2.159865379333496  val loss:  4.6189093589782715  val L1 loss:  5.0936\n",
      "epoch:  14   step:  149   train loss:  2.5834174156188965  val loss:  4.952455520629883  val L1 loss:  5.4344\n",
      "epoch:  14   step:  150   train loss:  2.2669591903686523  val loss:  4.9875102043151855  val L1 loss:  5.4745\n",
      "epoch:  14   step:  151   train loss:  3.7887356281280518  val loss:  4.665317058563232  val L1 loss:  5.126\n",
      "epoch:  14   step:  152   train loss:  3.5391669273376465  val loss:  4.386040210723877  val L1 loss:  4.8565\n",
      "epoch:  14   step:  153   train loss:  3.5160717964172363  val loss:  4.381356239318848  val L1 loss:  4.8437\n",
      "epoch:  14   step:  154   train loss:  2.4981822967529297  val loss:  4.374516010284424  val L1 loss:  4.8113\n",
      "epoch:  14   step:  155   train loss:  2.9011435508728027  val loss:  4.311060905456543  val L1 loss:  4.7927\n",
      "epoch:  14   step:  156   train loss:  5.176737308502197  val loss:  4.446608066558838  val L1 loss:  4.8981\n",
      "epoch:  14   step:  157   train loss:  2.5542612075805664  val loss:  4.61702823638916  val L1 loss:  5.1046\n",
      "epoch:  14   step:  158   train loss:  3.088015079498291  val loss:  4.7077436447143555  val L1 loss:  5.1757\n",
      "epoch:  14   step:  159   train loss:  4.5106120109558105  val loss:  4.378144264221191  val L1 loss:  4.8582\n",
      "epoch:  14   step:  160   train loss:  2.526383876800537  val loss:  4.454648494720459  val L1 loss:  4.9105\n",
      "epoch:  14   step:  161   train loss:  4.445518493652344  val loss:  4.660661697387695  val L1 loss:  5.1302\n",
      "epoch:  14   step:  162   train loss:  4.231138229370117  val loss:  5.17408561706543  val L1 loss:  5.6147\n",
      "epoch:  14   step:  163   train loss:  4.637576580047607  val loss:  6.329555511474609  val L1 loss:  6.8167\n",
      "epoch:  14   step:  164   train loss:  3.4581491947174072  val loss:  6.970613956451416  val L1 loss:  7.4706\n",
      "epoch:  14   step:  165   train loss:  4.571769714355469  val loss:  6.706358432769775  val L1 loss:  7.1995\n",
      "epoch:  14   step:  166   train loss:  2.7700729370117188  val loss:  6.074599266052246  val L1 loss:  6.5232\n",
      "epoch:  14   step:  167   train loss:  4.459494590759277  val loss:  5.426066875457764  val L1 loss:  5.9046\n",
      "epoch:  14   step:  168   train loss:  2.458482265472412  val loss:  5.024049758911133  val L1 loss:  5.4702\n",
      "epoch:  14   step:  169   train loss:  5.043125152587891  val loss:  4.844849586486816  val L1 loss:  5.308\n",
      "epoch:  14   step:  170   train loss:  2.3750860691070557  val loss:  4.65974235534668  val L1 loss:  5.1387\n",
      "epoch:  14   step:  171   train loss:  6.6446685791015625  val loss:  4.585273742675781  val L1 loss:  5.0537\n",
      "epoch:  14   step:  172   train loss:  3.582327127456665  val loss:  4.683699131011963  val L1 loss:  5.1627\n",
      "epoch:  14   step:  173   train loss:  2.8210039138793945  val loss:  4.606386661529541  val L1 loss:  5.0875\n",
      "epoch:  14   step:  174   train loss:  5.995827674865723  val loss:  4.522464752197266  val L1 loss:  4.9992\n",
      "epoch:  14   step:  175   train loss:  4.504854679107666  val loss:  4.39825439453125  val L1 loss:  4.8642\n",
      "epoch:  14   step:  176   train loss:  2.1383771896362305  val loss:  4.2712321281433105  val L1 loss:  4.7194\n",
      "epoch:  14   step:  177   train loss:  6.076786518096924  val loss:  4.141821384429932  val L1 loss:  4.6023\n",
      "epoch:  14   step:  178   train loss:  4.105270862579346  val loss:  4.136965274810791  val L1 loss:  4.6132\n",
      "epoch:  14   step:  179   train loss:  2.8421430587768555  val loss:  4.0464396476745605  val L1 loss:  4.5182\n",
      "epoch:  14   step:  180   train loss:  3.5753560066223145  val loss:  4.048791408538818  val L1 loss:  4.5189\n",
      "epoch:  14   step:  181   train loss:  2.54447078704834  val loss:  4.2132673263549805  val L1 loss:  4.6883\n",
      "epoch:  14   step:  182   train loss:  2.4317593574523926  val loss:  4.158349990844727  val L1 loss:  4.6284\n",
      "epoch:  14   step:  183   train loss:  5.041622638702393  val loss:  4.161665439605713  val L1 loss:  4.6303\n",
      "epoch:  14   step:  184   train loss:  3.1262831687927246  val loss:  4.070306777954102  val L1 loss:  4.53\n",
      "epoch:  14   step:  185   train loss:  4.943875312805176  val loss:  4.071564674377441  val L1 loss:  4.5379\n",
      "epoch:  14   step:  186   train loss:  3.507333755493164  val loss:  4.13669490814209  val L1 loss:  4.6013\n",
      "epoch:  14   step:  187   train loss:  2.912998676300049  val loss:  4.189975261688232  val L1 loss:  4.6525\n",
      "epoch:  14   step:  188   train loss:  4.049314975738525  val loss:  4.226462364196777  val L1 loss:  4.6887\n",
      "epoch:  14   step:  189   train loss:  3.913785457611084  val loss:  4.287039279937744  val L1 loss:  4.7461\n",
      "epoch:  14   step:  190   train loss:  2.8735086917877197  val loss:  4.385274410247803  val L1 loss:  4.8452\n",
      "epoch:  14   step:  191   train loss:  4.793821334838867  val loss:  4.458224773406982  val L1 loss:  4.9237\n",
      "epoch:  14   step:  192   train loss:  1.6510182619094849  val loss:  4.519613265991211  val L1 loss:  4.9801\n",
      "epoch:  14   step:  193   train loss:  2.4554195404052734  val loss:  4.8196539878845215  val L1 loss:  5.2975\n",
      "epoch:  14   step:  194   train loss:  1.8400609493255615  val loss:  4.899167537689209  val L1 loss:  5.3823\n",
      "epoch:  14   step:  195   train loss:  5.338886737823486  val loss:  4.729581356048584  val L1 loss:  5.2069\n",
      "epoch:  14   step:  196   train loss:  3.1619105339050293  val loss:  4.675228595733643  val L1 loss:  5.1277\n",
      "epoch:  14   step:  197   train loss:  2.072042465209961  val loss:  4.809150695800781  val L1 loss:  5.2799\n",
      "epoch:  14   step:  198   train loss:  3.598933696746826  val loss:  4.805173397064209  val L1 loss:  5.2694\n",
      "epoch:  14   step:  199   train loss:  2.6420774459838867  val loss:  4.796672821044922  val L1 loss:  5.2533\n",
      "epoch:  14   step:  200   train loss:  3.281980514526367  val loss:  4.826221466064453  val L1 loss:  5.2989\n",
      "epoch:  14   step:  201   train loss:  1.6799511909484863  val loss:  4.779592990875244  val L1 loss:  5.2547\n",
      "epoch:  14   step:  202   train loss:  2.974792242050171  val loss:  4.733854293823242  val L1 loss:  5.2201\n",
      "epoch:  14   step:  203   train loss:  3.8275485038757324  val loss:  4.663302421569824  val L1 loss:  5.1353\n",
      "epoch:  14   step:  204   train loss:  2.104808807373047  val loss:  4.6209564208984375  val L1 loss:  5.0838\n",
      "epoch:  14   step:  205   train loss:  2.492723226547241  val loss:  4.525659561157227  val L1 loss:  4.9965\n",
      "epoch:  14   step:  206   train loss:  3.734135627746582  val loss:  4.4376959800720215  val L1 loss:  4.915\n",
      "epoch:  14   step:  207   train loss:  3.3717093467712402  val loss:  4.394683361053467  val L1 loss:  4.8562\n",
      "epoch:  14   step:  208   train loss:  4.4995832443237305  val loss:  4.346055030822754  val L1 loss:  4.8141\n",
      "epoch:  15   step:  0   train loss:  1.7990760803222656  val loss:  4.304245471954346  val L1 loss:  4.7802\n",
      "epoch:  15   step:  1   train loss:  4.054544448852539  val loss:  4.250072479248047  val L1 loss:  4.7127\n",
      "epoch:  15   step:  2   train loss:  5.439070701599121  val loss:  4.231429100036621  val L1 loss:  4.6917\n",
      "epoch:  15   step:  3   train loss:  2.218759298324585  val loss:  4.273296356201172  val L1 loss:  4.7452\n",
      "epoch:  15   step:  4   train loss:  4.468461513519287  val loss:  4.302214622497559  val L1 loss:  4.7711\n",
      "epoch:  15   step:  5   train loss:  1.7136142253875732  val loss:  4.346986770629883  val L1 loss:  4.802\n",
      "epoch:  15   step:  6   train loss:  2.1559722423553467  val loss:  4.30774450302124  val L1 loss:  4.7631\n",
      "epoch:  15   step:  7   train loss:  5.7416510581970215  val loss:  4.240830421447754  val L1 loss:  4.6939\n",
      "epoch:  15   step:  8   train loss:  4.757346153259277  val loss:  4.189237117767334  val L1 loss:  4.6473\n",
      "epoch:  15   step:  9   train loss:  3.111276388168335  val loss:  4.1190385818481445  val L1 loss:  4.5741\n",
      "epoch:  15   step:  10   train loss:  4.105379104614258  val loss:  4.099710464477539  val L1 loss:  4.5693\n",
      "epoch:  15   step:  11   train loss:  2.8556556701660156  val loss:  3.9694159030914307  val L1 loss:  4.4433\n",
      "epoch:  15   step:  12   train loss:  3.547316789627075  val loss:  4.052523612976074  val L1 loss:  4.5368\n",
      "epoch:  15   step:  13   train loss:  3.009361982345581  val loss:  4.32930850982666  val L1 loss:  4.7854\n",
      "epoch:  15   step:  14   train loss:  3.3709750175476074  val loss:  4.594656467437744  val L1 loss:  5.0579\n",
      "epoch:  15   step:  15   train loss:  3.3418989181518555  val loss:  4.649318695068359  val L1 loss:  5.1069\n",
      "epoch:  15   step:  16   train loss:  4.373750686645508  val loss:  4.434523582458496  val L1 loss:  4.8981\n",
      "epoch:  15   step:  17   train loss:  2.7388548851013184  val loss:  4.06782341003418  val L1 loss:  4.5416\n",
      "epoch:  15   step:  18   train loss:  2.8417768478393555  val loss:  4.146897315979004  val L1 loss:  4.591\n",
      "epoch:  15   step:  19   train loss:  2.5178515911102295  val loss:  4.668912410736084  val L1 loss:  5.1398\n",
      "epoch:  15   step:  20   train loss:  4.9479265213012695  val loss:  4.6721062660217285  val L1 loss:  5.1524\n",
      "epoch:  15   step:  21   train loss:  4.2428741455078125  val loss:  4.3725152015686035  val L1 loss:  4.8214\n",
      "epoch:  15   step:  22   train loss:  3.1661882400512695  val loss:  4.3153791427612305  val L1 loss:  4.7819\n",
      "epoch:  15   step:  23   train loss:  4.188176155090332  val loss:  4.457167148590088  val L1 loss:  4.9383\n",
      "epoch:  15   step:  24   train loss:  3.787564992904663  val loss:  4.479734897613525  val L1 loss:  4.9524\n",
      "epoch:  15   step:  25   train loss:  3.5720767974853516  val loss:  4.290900230407715  val L1 loss:  4.749\n",
      "epoch:  15   step:  26   train loss:  4.501028537750244  val loss:  4.218674659729004  val L1 loss:  4.6691\n",
      "epoch:  15   step:  27   train loss:  2.709502696990967  val loss:  4.250603199005127  val L1 loss:  4.7072\n",
      "epoch:  15   step:  28   train loss:  2.5460433959960938  val loss:  4.323376178741455  val L1 loss:  4.7872\n",
      "epoch:  15   step:  29   train loss:  2.7834889888763428  val loss:  4.438839435577393  val L1 loss:  4.9029\n",
      "epoch:  15   step:  30   train loss:  3.973475933074951  val loss:  4.4737067222595215  val L1 loss:  4.9384\n",
      "epoch:  15   step:  31   train loss:  5.142671585083008  val loss:  4.903166770935059  val L1 loss:  5.3739\n",
      "epoch:  15   step:  32   train loss:  3.211639165878296  val loss:  4.948719501495361  val L1 loss:  5.4257\n",
      "epoch:  15   step:  33   train loss:  3.3197991847991943  val loss:  4.726109981536865  val L1 loss:  5.1792\n",
      "epoch:  15   step:  34   train loss:  3.6443939208984375  val loss:  4.282822608947754  val L1 loss:  4.7454\n",
      "epoch:  15   step:  35   train loss:  2.268183708190918  val loss:  4.20074987411499  val L1 loss:  4.6563\n",
      "epoch:  15   step:  36   train loss:  3.9690842628479004  val loss:  4.295987606048584  val L1 loss:  4.7611\n",
      "epoch:  15   step:  37   train loss:  2.1643333435058594  val loss:  4.439983367919922  val L1 loss:  4.901\n",
      "epoch:  15   step:  38   train loss:  7.643675804138184  val loss:  4.4905595779418945  val L1 loss:  4.939\n",
      "epoch:  15   step:  39   train loss:  3.365851402282715  val loss:  4.373999118804932  val L1 loss:  4.8311\n",
      "epoch:  15   step:  40   train loss:  2.6230757236480713  val loss:  4.307356834411621  val L1 loss:  4.76\n",
      "epoch:  15   step:  41   train loss:  2.472525119781494  val loss:  4.330759048461914  val L1 loss:  4.7954\n",
      "epoch:  15   step:  42   train loss:  5.882518768310547  val loss:  4.299266338348389  val L1 loss:  4.7746\n",
      "epoch:  15   step:  43   train loss:  3.3765292167663574  val loss:  4.193894386291504  val L1 loss:  4.654\n",
      "epoch:  15   step:  44   train loss:  6.768461227416992  val loss:  4.150361061096191  val L1 loss:  4.5914\n",
      "epoch:  15   step:  45   train loss:  4.092897415161133  val loss:  4.102512836456299  val L1 loss:  4.566\n",
      "epoch:  15   step:  46   train loss:  1.7890965938568115  val loss:  4.162595748901367  val L1 loss:  4.6343\n",
      "epoch:  15   step:  47   train loss:  3.677560567855835  val loss:  4.230177402496338  val L1 loss:  4.7042\n",
      "epoch:  15   step:  48   train loss:  2.7000985145568848  val loss:  4.319637775421143  val L1 loss:  4.791\n",
      "epoch:  15   step:  49   train loss:  2.267674446105957  val loss:  4.280221939086914  val L1 loss:  4.7476\n",
      "epoch:  15   step:  50   train loss:  3.2582931518554688  val loss:  4.3389458656311035  val L1 loss:  4.7964\n",
      "epoch:  15   step:  51   train loss:  4.682864665985107  val loss:  4.338159084320068  val L1 loss:  4.7932\n",
      "epoch:  15   step:  52   train loss:  2.472198247909546  val loss:  4.3211493492126465  val L1 loss:  4.7745\n",
      "epoch:  15   step:  53   train loss:  2.641888380050659  val loss:  4.281919002532959  val L1 loss:  4.7395\n",
      "epoch:  15   step:  54   train loss:  3.225951910018921  val loss:  4.320648193359375  val L1 loss:  4.7829\n",
      "epoch:  15   step:  55   train loss:  4.114457607269287  val loss:  4.451305389404297  val L1 loss:  4.928\n",
      "epoch:  15   step:  56   train loss:  2.762686252593994  val loss:  4.639375686645508  val L1 loss:  5.0854\n",
      "epoch:  15   step:  57   train loss:  1.7864964008331299  val loss:  4.940024375915527  val L1 loss:  5.4219\n",
      "epoch:  15   step:  58   train loss:  4.521563529968262  val loss:  4.793412208557129  val L1 loss:  5.2677\n",
      "epoch:  15   step:  59   train loss:  4.932965278625488  val loss:  4.389805793762207  val L1 loss:  4.8593\n",
      "epoch:  15   step:  60   train loss:  3.3446128368377686  val loss:  4.258524417877197  val L1 loss:  4.711\n",
      "epoch:  15   step:  61   train loss:  2.0093722343444824  val loss:  4.709417819976807  val L1 loss:  5.1771\n",
      "epoch:  15   step:  62   train loss:  2.858719825744629  val loss:  4.717333793640137  val L1 loss:  5.1847\n",
      "epoch:  15   step:  63   train loss:  3.491243362426758  val loss:  4.40454626083374  val L1 loss:  4.8596\n",
      "epoch:  15   step:  64   train loss:  3.7849583625793457  val loss:  4.355364799499512  val L1 loss:  4.796\n",
      "epoch:  15   step:  65   train loss:  1.921771764755249  val loss:  4.364284038543701  val L1 loss:  4.8081\n",
      "epoch:  15   step:  66   train loss:  3.5529918670654297  val loss:  4.409692287445068  val L1 loss:  4.8801\n",
      "epoch:  15   step:  67   train loss:  2.659266471862793  val loss:  4.291782855987549  val L1 loss:  4.7452\n",
      "epoch:  15   step:  68   train loss:  5.109987735748291  val loss:  4.396636962890625  val L1 loss:  4.8431\n",
      "epoch:  15   step:  69   train loss:  2.7345080375671387  val loss:  4.771503925323486  val L1 loss:  5.243\n",
      "epoch:  15   step:  70   train loss:  3.5090293884277344  val loss:  4.8477325439453125  val L1 loss:  5.3135\n",
      "epoch:  15   step:  71   train loss:  2.808279037475586  val loss:  4.669366836547852  val L1 loss:  5.1535\n",
      "epoch:  15   step:  72   train loss:  3.421149730682373  val loss:  4.349483489990234  val L1 loss:  4.8339\n",
      "epoch:  15   step:  73   train loss:  4.643013954162598  val loss:  4.156169414520264  val L1 loss:  4.6176\n",
      "epoch:  15   step:  74   train loss:  2.2538790702819824  val loss:  4.183687686920166  val L1 loss:  4.6387\n",
      "epoch:  15   step:  75   train loss:  2.5324554443359375  val loss:  4.293979167938232  val L1 loss:  4.7523\n",
      "epoch:  15   step:  76   train loss:  4.741426467895508  val loss:  4.1923418045043945  val L1 loss:  4.6503\n",
      "epoch:  15   step:  77   train loss:  2.89133882522583  val loss:  4.166872978210449  val L1 loss:  4.6199\n",
      "epoch:  15   step:  78   train loss:  4.163974761962891  val loss:  4.522555828094482  val L1 loss:  5.0\n",
      "epoch:  15   step:  79   train loss:  3.127509593963623  val loss:  5.349438190460205  val L1 loss:  5.8259\n",
      "epoch:  15   step:  80   train loss:  4.195862770080566  val loss:  5.423079013824463  val L1 loss:  5.9015\n",
      "epoch:  15   step:  81   train loss:  3.959049701690674  val loss:  4.8445868492126465  val L1 loss:  5.3345\n",
      "epoch:  15   step:  82   train loss:  3.169682025909424  val loss:  4.427100658416748  val L1 loss:  4.8897\n",
      "epoch:  15   step:  83   train loss:  2.982924461364746  val loss:  4.350091934204102  val L1 loss:  4.7866\n",
      "epoch:  15   step:  84   train loss:  1.986565113067627  val loss:  4.752208709716797  val L1 loss:  5.2267\n",
      "epoch:  15   step:  85   train loss:  3.652897357940674  val loss:  4.775622844696045  val L1 loss:  5.2478\n",
      "epoch:  15   step:  86   train loss:  4.363635540008545  val loss:  4.542184352874756  val L1 loss:  4.9676\n",
      "epoch:  15   step:  87   train loss:  3.6439530849456787  val loss:  4.7378644943237305  val L1 loss:  5.2187\n",
      "epoch:  15   step:  88   train loss:  2.2944016456604004  val loss:  5.14871072769165  val L1 loss:  5.6287\n",
      "epoch:  15   step:  89   train loss:  4.1320624351501465  val loss:  5.446041107177734  val L1 loss:  5.9175\n",
      "epoch:  15   step:  90   train loss:  3.607720136642456  val loss:  5.378421306610107  val L1 loss:  5.8493\n",
      "epoch:  15   step:  91   train loss:  4.840950965881348  val loss:  4.866490364074707  val L1 loss:  5.3337\n",
      "epoch:  15   step:  92   train loss:  3.462829351425171  val loss:  4.6484599113464355  val L1 loss:  5.1136\n",
      "epoch:  15   step:  93   train loss:  5.092374801635742  val loss:  4.6423211097717285  val L1 loss:  5.1091\n",
      "epoch:  15   step:  94   train loss:  2.904859781265259  val loss:  4.714544773101807  val L1 loss:  5.1706\n",
      "epoch:  15   step:  95   train loss:  3.1843421459198  val loss:  4.830663204193115  val L1 loss:  5.3142\n",
      "epoch:  15   step:  96   train loss:  4.737448692321777  val loss:  4.767841339111328  val L1 loss:  5.2299\n",
      "epoch:  15   step:  97   train loss:  3.9681735038757324  val loss:  4.7746734619140625  val L1 loss:  5.2632\n",
      "epoch:  15   step:  98   train loss:  3.2104055881500244  val loss:  4.961765766143799  val L1 loss:  5.4395\n",
      "epoch:  15   step:  99   train loss:  2.8054065704345703  val loss:  4.9396491050720215  val L1 loss:  5.4052\n",
      "epoch:  15   step:  100   train loss:  3.0488510131835938  val loss:  4.798210144042969  val L1 loss:  5.2627\n",
      "epoch:  15   step:  101   train loss:  2.3283700942993164  val loss:  4.502829551696777  val L1 loss:  4.965\n",
      "epoch:  15   step:  102   train loss:  2.383345603942871  val loss:  4.7563605308532715  val L1 loss:  5.2174\n",
      "epoch:  15   step:  103   train loss:  2.5771031379699707  val loss:  4.777297019958496  val L1 loss:  5.2455\n",
      "epoch:  15   step:  104   train loss:  2.8765511512756348  val loss:  4.694493293762207  val L1 loss:  5.1716\n",
      "epoch:  15   step:  105   train loss:  2.243476390838623  val loss:  4.536645412445068  val L1 loss:  4.9957\n",
      "epoch:  15   step:  106   train loss:  5.137718677520752  val loss:  4.446558952331543  val L1 loss:  4.9184\n",
      "epoch:  15   step:  107   train loss:  2.6791131496429443  val loss:  4.420173168182373  val L1 loss:  4.8861\n",
      "epoch:  15   step:  108   train loss:  1.6987926959991455  val loss:  4.377696990966797  val L1 loss:  4.842\n",
      "epoch:  15   step:  109   train loss:  3.6319034099578857  val loss:  4.400477886199951  val L1 loss:  4.8621\n",
      "epoch:  15   step:  110   train loss:  3.985567808151245  val loss:  4.429248332977295  val L1 loss:  4.8955\n",
      "epoch:  15   step:  111   train loss:  2.825493335723877  val loss:  4.445937633514404  val L1 loss:  4.9035\n",
      "epoch:  15   step:  112   train loss:  2.879070281982422  val loss:  4.447910785675049  val L1 loss:  4.9051\n",
      "epoch:  15   step:  113   train loss:  4.785592079162598  val loss:  4.465506553649902  val L1 loss:  4.9262\n",
      "epoch:  15   step:  114   train loss:  5.393817901611328  val loss:  4.5846147537231445  val L1 loss:  5.0431\n",
      "epoch:  15   step:  115   train loss:  4.957536697387695  val loss:  4.692785263061523  val L1 loss:  5.1578\n",
      "epoch:  15   step:  116   train loss:  3.4281005859375  val loss:  4.794020652770996  val L1 loss:  5.2719\n",
      "epoch:  15   step:  117   train loss:  2.2937235832214355  val loss:  4.797338008880615  val L1 loss:  5.2735\n",
      "epoch:  15   step:  118   train loss:  3.098048210144043  val loss:  4.719951629638672  val L1 loss:  5.1784\n",
      "epoch:  15   step:  119   train loss:  3.1146178245544434  val loss:  4.7207183837890625  val L1 loss:  5.1844\n",
      "epoch:  15   step:  120   train loss:  5.209752082824707  val loss:  4.88373327255249  val L1 loss:  5.3313\n",
      "epoch:  15   step:  121   train loss:  3.5567097663879395  val loss:  5.28851318359375  val L1 loss:  5.7384\n",
      "epoch:  15   step:  122   train loss:  2.3144261837005615  val loss:  5.553477764129639  val L1 loss:  6.0154\n",
      "epoch:  15   step:  123   train loss:  4.000883102416992  val loss:  5.660235404968262  val L1 loss:  6.1299\n",
      "epoch:  15   step:  124   train loss:  1.8267842531204224  val loss:  5.796502590179443  val L1 loss:  6.2566\n",
      "epoch:  15   step:  125   train loss:  2.3895516395568848  val loss:  5.798752307891846  val L1 loss:  6.2597\n",
      "epoch:  15   step:  126   train loss:  3.8182647228240967  val loss:  5.85453462600708  val L1 loss:  6.326\n",
      "epoch:  15   step:  127   train loss:  3.1715946197509766  val loss:  5.87379789352417  val L1 loss:  6.3463\n",
      "epoch:  15   step:  128   train loss:  3.4203503131866455  val loss:  5.968321323394775  val L1 loss:  6.4511\n",
      "epoch:  15   step:  129   train loss:  1.415964126586914  val loss:  5.911871910095215  val L1 loss:  6.3985\n",
      "epoch:  15   step:  130   train loss:  1.9892977476119995  val loss:  5.679385662078857  val L1 loss:  6.165\n",
      "epoch:  15   step:  131   train loss:  3.4951257705688477  val loss:  5.456851482391357  val L1 loss:  5.9281\n",
      "epoch:  15   step:  132   train loss:  2.037649631500244  val loss:  5.292905807495117  val L1 loss:  5.7636\n",
      "epoch:  15   step:  133   train loss:  4.375673294067383  val loss:  5.109747409820557  val L1 loss:  5.5749\n",
      "epoch:  15   step:  134   train loss:  3.3443851470947266  val loss:  4.930785179138184  val L1 loss:  5.3968\n",
      "epoch:  15   step:  135   train loss:  3.0819034576416016  val loss:  4.842653274536133  val L1 loss:  5.3032\n",
      "epoch:  15   step:  136   train loss:  2.246126651763916  val loss:  4.77623176574707  val L1 loss:  5.2445\n",
      "epoch:  15   step:  137   train loss:  3.51912260055542  val loss:  4.7382097244262695  val L1 loss:  5.2101\n",
      "epoch:  15   step:  138   train loss:  4.75986909866333  val loss:  4.740972995758057  val L1 loss:  5.2097\n",
      "epoch:  15   step:  139   train loss:  2.6788511276245117  val loss:  4.642867565155029  val L1 loss:  5.106\n",
      "epoch:  15   step:  140   train loss:  3.2499451637268066  val loss:  4.44905948638916  val L1 loss:  4.9216\n",
      "epoch:  15   step:  141   train loss:  3.9706461429595947  val loss:  4.313388347625732  val L1 loss:  4.7659\n",
      "epoch:  15   step:  142   train loss:  1.8117563724517822  val loss:  4.25150203704834  val L1 loss:  4.6745\n",
      "epoch:  15   step:  143   train loss:  3.5220048427581787  val loss:  4.222165584564209  val L1 loss:  4.6665\n",
      "epoch:  15   step:  144   train loss:  1.427127718925476  val loss:  4.326236248016357  val L1 loss:  4.7813\n",
      "epoch:  15   step:  145   train loss:  4.164325714111328  val loss:  4.513462543487549  val L1 loss:  4.9629\n",
      "epoch:  15   step:  146   train loss:  3.697786808013916  val loss:  4.4212751388549805  val L1 loss:  4.861\n",
      "epoch:  15   step:  147   train loss:  3.087083339691162  val loss:  4.452469825744629  val L1 loss:  4.9056\n",
      "epoch:  15   step:  148   train loss:  1.3838574886322021  val loss:  4.478831768035889  val L1 loss:  4.9383\n",
      "epoch:  15   step:  149   train loss:  2.0926437377929688  val loss:  4.596543312072754  val L1 loss:  5.0753\n",
      "epoch:  15   step:  150   train loss:  4.1291327476501465  val loss:  4.796527862548828  val L1 loss:  5.2685\n",
      "epoch:  15   step:  151   train loss:  4.468511581420898  val loss:  4.856893062591553  val L1 loss:  5.3371\n",
      "epoch:  15   step:  152   train loss:  3.9856553077697754  val loss:  4.893653869628906  val L1 loss:  5.3824\n",
      "epoch:  15   step:  153   train loss:  2.87791109085083  val loss:  4.928047180175781  val L1 loss:  5.4009\n",
      "epoch:  15   step:  154   train loss:  2.489896774291992  val loss:  4.905610084533691  val L1 loss:  5.3682\n",
      "epoch:  15   step:  155   train loss:  2.511138916015625  val loss:  4.853135108947754  val L1 loss:  5.3255\n",
      "epoch:  15   step:  156   train loss:  4.484277248382568  val loss:  4.605931282043457  val L1 loss:  5.0775\n",
      "epoch:  15   step:  157   train loss:  3.3478317260742188  val loss:  4.340638160705566  val L1 loss:  4.7933\n",
      "epoch:  15   step:  158   train loss:  3.495326042175293  val loss:  4.328555583953857  val L1 loss:  4.817\n",
      "epoch:  15   step:  159   train loss:  2.095649003982544  val loss:  4.471514701843262  val L1 loss:  4.9333\n",
      "epoch:  15   step:  160   train loss:  3.7467753887176514  val loss:  4.649691581726074  val L1 loss:  5.1263\n",
      "epoch:  15   step:  161   train loss:  3.8512823581695557  val loss:  4.800601482391357  val L1 loss:  5.2824\n",
      "epoch:  15   step:  162   train loss:  3.867347002029419  val loss:  4.585633754730225  val L1 loss:  5.0492\n",
      "epoch:  15   step:  163   train loss:  2.40641450881958  val loss:  4.312362194061279  val L1 loss:  4.7684\n",
      "epoch:  15   step:  164   train loss:  4.6717529296875  val loss:  4.365388870239258  val L1 loss:  4.8407\n",
      "epoch:  15   step:  165   train loss:  4.061621189117432  val loss:  4.588512420654297  val L1 loss:  5.0583\n",
      "epoch:  15   step:  166   train loss:  2.282823324203491  val loss:  4.980532646179199  val L1 loss:  5.4463\n",
      "epoch:  15   step:  167   train loss:  3.114384174346924  val loss:  5.59022331237793  val L1 loss:  6.0756\n",
      "epoch:  15   step:  168   train loss:  5.069109916687012  val loss:  5.375519752502441  val L1 loss:  5.8677\n",
      "epoch:  15   step:  169   train loss:  2.762233018875122  val loss:  5.080358982086182  val L1 loss:  5.5522\n",
      "epoch:  15   step:  170   train loss:  2.360811948776245  val loss:  4.6490397453308105  val L1 loss:  5.1371\n",
      "epoch:  15   step:  171   train loss:  3.1596310138702393  val loss:  4.507710933685303  val L1 loss:  4.9497\n",
      "epoch:  15   step:  172   train loss:  2.6540801525115967  val loss:  4.760204792022705  val L1 loss:  5.2269\n",
      "epoch:  15   step:  173   train loss:  5.388999938964844  val loss:  4.892031192779541  val L1 loss:  5.3678\n",
      "epoch:  15   step:  174   train loss:  6.040947914123535  val loss:  4.778835773468018  val L1 loss:  5.2391\n",
      "epoch:  15   step:  175   train loss:  2.086801290512085  val loss:  4.744894981384277  val L1 loss:  5.2022\n",
      "epoch:  15   step:  176   train loss:  2.836270809173584  val loss:  4.726841449737549  val L1 loss:  5.1793\n",
      "epoch:  15   step:  177   train loss:  3.7712583541870117  val loss:  4.616560459136963  val L1 loss:  5.0796\n",
      "epoch:  15   step:  178   train loss:  3.835024833679199  val loss:  4.528956890106201  val L1 loss:  5.0107\n",
      "epoch:  15   step:  179   train loss:  4.571123123168945  val loss:  4.627031326293945  val L1 loss:  5.1081\n",
      "epoch:  15   step:  180   train loss:  4.757092475891113  val loss:  4.6654839515686035  val L1 loss:  5.1403\n",
      "epoch:  15   step:  181   train loss:  6.015279293060303  val loss:  4.915168762207031  val L1 loss:  5.3901\n",
      "epoch:  15   step:  182   train loss:  4.853832244873047  val loss:  5.454914569854736  val L1 loss:  5.9468\n",
      "epoch:  15   step:  183   train loss:  2.653634786605835  val loss:  5.74613618850708  val L1 loss:  6.2278\n",
      "epoch:  15   step:  184   train loss:  4.176661968231201  val loss:  5.306014060974121  val L1 loss:  5.7732\n",
      "epoch:  15   step:  185   train loss:  2.6248724460601807  val loss:  4.719218730926514  val L1 loss:  5.1975\n",
      "epoch:  15   step:  186   train loss:  2.2651681900024414  val loss:  4.557064056396484  val L1 loss:  5.0179\n",
      "epoch:  15   step:  187   train loss:  3.716845989227295  val loss:  4.49022912979126  val L1 loss:  4.9591\n",
      "epoch:  15   step:  188   train loss:  4.6013994216918945  val loss:  4.4401044845581055  val L1 loss:  4.9073\n",
      "epoch:  15   step:  189   train loss:  4.642351150512695  val loss:  4.648681640625  val L1 loss:  5.1205\n",
      "epoch:  15   step:  190   train loss:  3.1128334999084473  val loss:  5.181603908538818  val L1 loss:  5.6615\n",
      "epoch:  15   step:  191   train loss:  1.963829755783081  val loss:  5.729991436004639  val L1 loss:  6.2153\n",
      "epoch:  15   step:  192   train loss:  5.317391395568848  val loss:  6.257601261138916  val L1 loss:  6.7404\n",
      "epoch:  15   step:  193   train loss:  4.092740535736084  val loss:  6.090221405029297  val L1 loss:  6.5873\n",
      "epoch:  15   step:  194   train loss:  3.1806159019470215  val loss:  5.55735969543457  val L1 loss:  6.0465\n",
      "epoch:  15   step:  195   train loss:  2.5046796798706055  val loss:  4.991091251373291  val L1 loss:  5.4333\n",
      "epoch:  15   step:  196   train loss:  2.5862436294555664  val loss:  4.793385028839111  val L1 loss:  5.2509\n",
      "epoch:  15   step:  197   train loss:  3.806469202041626  val loss:  4.877680778503418  val L1 loss:  5.3384\n",
      "epoch:  15   step:  198   train loss:  4.091030120849609  val loss:  5.007754325866699  val L1 loss:  5.4845\n",
      "epoch:  15   step:  199   train loss:  3.9584901332855225  val loss:  5.543823719024658  val L1 loss:  6.0291\n",
      "epoch:  15   step:  200   train loss:  4.675331115722656  val loss:  6.0797648429870605  val L1 loss:  6.5725\n",
      "epoch:  15   step:  201   train loss:  4.402681827545166  val loss:  5.779393196105957  val L1 loss:  6.2682\n",
      "epoch:  15   step:  202   train loss:  2.945664882659912  val loss:  4.955272674560547  val L1 loss:  5.442\n",
      "epoch:  15   step:  203   train loss:  4.246391296386719  val loss:  4.294880390167236  val L1 loss:  4.7626\n",
      "epoch:  15   step:  204   train loss:  3.4176113605499268  val loss:  4.283300399780273  val L1 loss:  4.7423\n",
      "epoch:  15   step:  205   train loss:  2.750419855117798  val loss:  4.582364559173584  val L1 loss:  5.0414\n",
      "epoch:  15   step:  206   train loss:  2.0782060623168945  val loss:  4.887310981750488  val L1 loss:  5.3696\n",
      "epoch:  15   step:  207   train loss:  4.524806022644043  val loss:  4.7246856689453125  val L1 loss:  5.2013\n",
      "epoch:  15   step:  208   train loss:  1.5721255540847778  val loss:  4.544398307800293  val L1 loss:  5.0017\n",
      "epoch:  16   step:  0   train loss:  3.9167256355285645  val loss:  4.454004287719727  val L1 loss:  4.9337\n",
      "epoch:  16   step:  1   train loss:  3.609658718109131  val loss:  4.5557541847229  val L1 loss:  5.0213\n",
      "epoch:  16   step:  2   train loss:  5.742806434631348  val loss:  4.661322593688965  val L1 loss:  5.1198\n",
      "epoch:  16   step:  3   train loss:  4.325508117675781  val loss:  4.94425106048584  val L1 loss:  5.418\n",
      "epoch:  16   step:  4   train loss:  2.4958720207214355  val loss:  4.950951099395752  val L1 loss:  5.4194\n",
      "epoch:  16   step:  5   train loss:  2.6293256282806396  val loss:  4.877312660217285  val L1 loss:  5.3555\n",
      "epoch:  16   step:  6   train loss:  1.6476917266845703  val loss:  4.799074172973633  val L1 loss:  5.2812\n",
      "epoch:  16   step:  7   train loss:  3.1943376064300537  val loss:  4.645844459533691  val L1 loss:  5.1091\n",
      "epoch:  16   step:  8   train loss:  2.405270576477051  val loss:  4.389801025390625  val L1 loss:  4.8409\n",
      "epoch:  16   step:  9   train loss:  3.650026798248291  val loss:  4.33479642868042  val L1 loss:  4.7668\n",
      "epoch:  16   step:  10   train loss:  3.0744237899780273  val loss:  4.487320423126221  val L1 loss:  4.9512\n",
      "epoch:  16   step:  11   train loss:  3.973752737045288  val loss:  4.595896244049072  val L1 loss:  5.0605\n",
      "epoch:  16   step:  12   train loss:  4.077023506164551  val loss:  4.711885929107666  val L1 loss:  5.1717\n",
      "epoch:  16   step:  13   train loss:  2.443497896194458  val loss:  4.812288284301758  val L1 loss:  5.2832\n",
      "epoch:  16   step:  14   train loss:  3.43565034866333  val loss:  4.8729166984558105  val L1 loss:  5.337\n",
      "epoch:  16   step:  15   train loss:  2.979470729827881  val loss:  5.037380218505859  val L1 loss:  5.5045\n",
      "epoch:  16   step:  16   train loss:  2.85573148727417  val loss:  5.261667251586914  val L1 loss:  5.7235\n",
      "epoch:  16   step:  17   train loss:  3.0967185497283936  val loss:  5.47026252746582  val L1 loss:  5.9366\n",
      "epoch:  16   step:  18   train loss:  3.9285190105438232  val loss:  5.286229133605957  val L1 loss:  5.766\n",
      "epoch:  16   step:  19   train loss:  3.387277841567993  val loss:  4.878642559051514  val L1 loss:  5.3452\n",
      "epoch:  16   step:  20   train loss:  2.5408360958099365  val loss:  4.658512592315674  val L1 loss:  5.131\n",
      "epoch:  16   step:  21   train loss:  2.867380142211914  val loss:  4.58411169052124  val L1 loss:  5.0464\n",
      "epoch:  16   step:  22   train loss:  2.678907871246338  val loss:  4.586603164672852  val L1 loss:  5.0644\n",
      "epoch:  16   step:  23   train loss:  4.474871635437012  val loss:  4.667396068572998  val L1 loss:  5.1498\n",
      "epoch:  16   step:  24   train loss:  1.5503981113433838  val loss:  4.901053428649902  val L1 loss:  5.3677\n",
      "epoch:  16   step:  25   train loss:  6.152568817138672  val loss:  4.832976341247559  val L1 loss:  5.2904\n",
      "epoch:  16   step:  26   train loss:  2.506328582763672  val loss:  4.755273342132568  val L1 loss:  5.2282\n",
      "epoch:  16   step:  27   train loss:  4.271549224853516  val loss:  4.572010517120361  val L1 loss:  5.0403\n",
      "epoch:  16   step:  28   train loss:  3.584987163543701  val loss:  4.596578121185303  val L1 loss:  5.0662\n",
      "epoch:  16   step:  29   train loss:  2.1440482139587402  val loss:  4.620701313018799  val L1 loss:  5.0908\n",
      "epoch:  16   step:  30   train loss:  4.828254222869873  val loss:  4.594600200653076  val L1 loss:  5.0647\n",
      "epoch:  16   step:  31   train loss:  1.6911003589630127  val loss:  4.550195217132568  val L1 loss:  5.0122\n",
      "epoch:  16   step:  32   train loss:  3.9070639610290527  val loss:  4.476955890655518  val L1 loss:  4.9306\n",
      "epoch:  16   step:  33   train loss:  2.419286012649536  val loss:  4.462157726287842  val L1 loss:  4.922\n",
      "epoch:  16   step:  34   train loss:  3.9126765727996826  val loss:  4.524228572845459  val L1 loss:  4.9973\n",
      "epoch:  16   step:  35   train loss:  2.1374902725219727  val loss:  4.460099220275879  val L1 loss:  4.9235\n",
      "epoch:  16   step:  36   train loss:  5.190743446350098  val loss:  4.404745578765869  val L1 loss:  4.8711\n",
      "epoch:  16   step:  37   train loss:  3.674017906188965  val loss:  4.400518894195557  val L1 loss:  4.8675\n",
      "epoch:  16   step:  38   train loss:  2.159618616104126  val loss:  4.311531066894531  val L1 loss:  4.7612\n",
      "epoch:  16   step:  39   train loss:  3.360776424407959  val loss:  4.243257999420166  val L1 loss:  4.685\n",
      "epoch:  16   step:  40   train loss:  3.189887285232544  val loss:  4.227133274078369  val L1 loss:  4.6838\n",
      "epoch:  16   step:  41   train loss:  3.2107412815093994  val loss:  4.140181064605713  val L1 loss:  4.6023\n",
      "epoch:  16   step:  42   train loss:  3.3777272701263428  val loss:  4.090426921844482  val L1 loss:  4.5651\n",
      "epoch:  16   step:  43   train loss:  3.7884018421173096  val loss:  4.061380386352539  val L1 loss:  4.5386\n",
      "epoch:  16   step:  44   train loss:  5.759631633758545  val loss:  4.058419704437256  val L1 loss:  4.5408\n",
      "epoch:  16   step:  45   train loss:  3.7426748275756836  val loss:  4.068496227264404  val L1 loss:  4.5469\n",
      "epoch:  16   step:  46   train loss:  2.6293272972106934  val loss:  4.1788225173950195  val L1 loss:  4.6427\n",
      "epoch:  16   step:  47   train loss:  2.1683249473571777  val loss:  4.206521987915039  val L1 loss:  4.6762\n",
      "epoch:  16   step:  48   train loss:  2.6974778175354004  val loss:  4.0955328941345215  val L1 loss:  4.5616\n",
      "epoch:  16   step:  49   train loss:  2.3595199584960938  val loss:  4.214771270751953  val L1 loss:  4.669\n",
      "epoch:  16   step:  50   train loss:  2.9989843368530273  val loss:  4.827268123626709  val L1 loss:  5.2864\n",
      "epoch:  16   step:  51   train loss:  4.099052429199219  val loss:  5.218767166137695  val L1 loss:  5.6826\n",
      "epoch:  16   step:  52   train loss:  2.11592435836792  val loss:  5.142760276794434  val L1 loss:  5.5978\n",
      "epoch:  16   step:  53   train loss:  3.469907283782959  val loss:  4.507978916168213  val L1 loss:  4.9635\n",
      "epoch:  16   step:  54   train loss:  2.443903923034668  val loss:  4.29644775390625  val L1 loss:  4.7586\n",
      "epoch:  16   step:  55   train loss:  3.6865055561065674  val loss:  4.644558429718018  val L1 loss:  5.1115\n",
      "epoch:  16   step:  56   train loss:  3.3544740676879883  val loss:  4.896780967712402  val L1 loss:  5.3722\n",
      "epoch:  16   step:  57   train loss:  3.8184351921081543  val loss:  5.12409782409668  val L1 loss:  5.6039\n",
      "epoch:  16   step:  58   train loss:  2.2806551456451416  val loss:  5.118879318237305  val L1 loss:  5.5915\n",
      "epoch:  16   step:  59   train loss:  2.565249443054199  val loss:  4.909177303314209  val L1 loss:  5.3692\n",
      "epoch:  16   step:  60   train loss:  2.925780773162842  val loss:  4.832836151123047  val L1 loss:  5.3129\n",
      "epoch:  16   step:  61   train loss:  2.2307000160217285  val loss:  4.823550224304199  val L1 loss:  5.3107\n",
      "epoch:  16   step:  62   train loss:  1.8715236186981201  val loss:  4.815929889678955  val L1 loss:  5.3014\n",
      "epoch:  16   step:  63   train loss:  3.972029209136963  val loss:  4.637118339538574  val L1 loss:  5.116\n",
      "epoch:  16   step:  64   train loss:  3.725956916809082  val loss:  4.615199089050293  val L1 loss:  5.0862\n",
      "epoch:  16   step:  65   train loss:  2.793614149093628  val loss:  4.56382417678833  val L1 loss:  5.0242\n",
      "epoch:  16   step:  66   train loss:  2.9073219299316406  val loss:  4.651724338531494  val L1 loss:  5.0868\n",
      "epoch:  16   step:  67   train loss:  2.4360461235046387  val loss:  4.633083820343018  val L1 loss:  5.085\n",
      "epoch:  16   step:  68   train loss:  3.1776654720306396  val loss:  4.494457721710205  val L1 loss:  4.9513\n",
      "epoch:  16   step:  69   train loss:  3.0769333839416504  val loss:  4.476261615753174  val L1 loss:  4.929\n",
      "epoch:  16   step:  70   train loss:  2.082000494003296  val loss:  4.556794166564941  val L1 loss:  5.0316\n",
      "epoch:  16   step:  71   train loss:  2.1111550331115723  val loss:  4.515595436096191  val L1 loss:  4.9865\n",
      "epoch:  16   step:  72   train loss:  2.2887110710144043  val loss:  4.475729465484619  val L1 loss:  4.921\n",
      "epoch:  16   step:  73   train loss:  3.2258381843566895  val loss:  4.247853755950928  val L1 loss:  4.7124\n",
      "epoch:  16   step:  74   train loss:  2.3524904251098633  val loss:  4.331685543060303  val L1 loss:  4.7912\n",
      "epoch:  16   step:  75   train loss:  3.1942176818847656  val loss:  4.443592548370361  val L1 loss:  4.904\n",
      "epoch:  16   step:  76   train loss:  3.080226421356201  val loss:  4.469208717346191  val L1 loss:  4.9152\n",
      "epoch:  16   step:  77   train loss:  3.046476364135742  val loss:  4.683779716491699  val L1 loss:  5.1632\n",
      "epoch:  16   step:  78   train loss:  2.636850357055664  val loss:  5.123110294342041  val L1 loss:  5.609\n",
      "epoch:  16   step:  79   train loss:  1.9768537282943726  val loss:  5.618600845336914  val L1 loss:  6.0916\n",
      "epoch:  16   step:  80   train loss:  7.181902885437012  val loss:  6.378772258758545  val L1 loss:  6.865\n",
      "epoch:  16   step:  81   train loss:  3.1054980754852295  val loss:  6.393393039703369  val L1 loss:  6.8857\n",
      "epoch:  16   step:  82   train loss:  4.927045822143555  val loss:  5.907578468322754  val L1 loss:  6.3971\n",
      "epoch:  16   step:  83   train loss:  2.89817476272583  val loss:  5.01317024230957  val L1 loss:  5.465\n",
      "epoch:  16   step:  84   train loss:  2.475020408630371  val loss:  4.668652534484863  val L1 loss:  5.1353\n",
      "epoch:  16   step:  85   train loss:  2.969224452972412  val loss:  4.753551006317139  val L1 loss:  5.2142\n",
      "epoch:  16   step:  86   train loss:  6.828281879425049  val loss:  4.807785511016846  val L1 loss:  5.2768\n",
      "epoch:  16   step:  87   train loss:  7.025402069091797  val loss:  4.5282206535339355  val L1 loss:  4.9969\n",
      "epoch:  16   step:  88   train loss:  4.254501819610596  val loss:  4.747139930725098  val L1 loss:  5.2247\n",
      "epoch:  16   step:  89   train loss:  1.2562005519866943  val loss:  5.63941764831543  val L1 loss:  6.1161\n",
      "epoch:  16   step:  90   train loss:  3.1416444778442383  val loss:  6.362481594085693  val L1 loss:  6.8559\n",
      "epoch:  16   step:  91   train loss:  4.092545509338379  val loss:  6.425665855407715  val L1 loss:  6.9247\n",
      "epoch:  16   step:  92   train loss:  3.4374303817749023  val loss:  5.796034812927246  val L1 loss:  6.2844\n",
      "epoch:  16   step:  93   train loss:  3.645277500152588  val loss:  5.217062950134277  val L1 loss:  5.6816\n",
      "epoch:  16   step:  94   train loss:  2.746708393096924  val loss:  5.035892009735107  val L1 loss:  5.5087\n",
      "epoch:  16   step:  95   train loss:  1.850340485572815  val loss:  4.9795403480529785  val L1 loss:  5.436\n",
      "epoch:  16   step:  96   train loss:  6.073494911193848  val loss:  5.0940046310424805  val L1 loss:  5.5449\n",
      "epoch:  16   step:  97   train loss:  3.939849376678467  val loss:  5.292975425720215  val L1 loss:  5.7798\n",
      "epoch:  16   step:  98   train loss:  2.987354278564453  val loss:  5.761200904846191  val L1 loss:  6.2479\n",
      "epoch:  16   step:  99   train loss:  3.484501838684082  val loss:  6.006712913513184  val L1 loss:  6.4897\n",
      "epoch:  16   step:  100   train loss:  3.648681163787842  val loss:  5.677610874176025  val L1 loss:  6.1481\n",
      "epoch:  16   step:  101   train loss:  2.7288920879364014  val loss:  5.0819993019104  val L1 loss:  5.5619\n",
      "epoch:  16   step:  102   train loss:  3.501887321472168  val loss:  4.782983303070068  val L1 loss:  5.269\n",
      "epoch:  16   step:  103   train loss:  2.8131346702575684  val loss:  4.700062274932861  val L1 loss:  5.1619\n",
      "epoch:  16   step:  104   train loss:  2.8493595123291016  val loss:  4.972429275512695  val L1 loss:  5.4505\n",
      "epoch:  16   step:  105   train loss:  2.7399725914001465  val loss:  4.966446876525879  val L1 loss:  5.437\n",
      "epoch:  16   step:  106   train loss:  3.8124780654907227  val loss:  4.827099323272705  val L1 loss:  5.3044\n",
      "epoch:  16   step:  107   train loss:  1.4599742889404297  val loss:  4.743225574493408  val L1 loss:  5.2216\n",
      "epoch:  16   step:  108   train loss:  3.5804359912872314  val loss:  4.655686855316162  val L1 loss:  5.1351\n",
      "epoch:  16   step:  109   train loss:  3.794449806213379  val loss:  4.594114303588867  val L1 loss:  5.0619\n",
      "epoch:  16   step:  110   train loss:  2.717653274536133  val loss:  4.651867389678955  val L1 loss:  5.122\n",
      "epoch:  16   step:  111   train loss:  1.974368929862976  val loss:  4.707714557647705  val L1 loss:  5.1575\n",
      "epoch:  16   step:  112   train loss:  1.5510631799697876  val loss:  4.717770099639893  val L1 loss:  5.1875\n",
      "epoch:  16   step:  113   train loss:  5.439608573913574  val loss:  4.702556133270264  val L1 loss:  5.1668\n",
      "epoch:  16   step:  114   train loss:  2.4240541458129883  val loss:  4.669539928436279  val L1 loss:  5.1498\n",
      "epoch:  16   step:  115   train loss:  2.5129828453063965  val loss:  4.9353251457214355  val L1 loss:  5.4034\n",
      "epoch:  16   step:  116   train loss:  3.184091567993164  val loss:  5.27537202835083  val L1 loss:  5.7446\n",
      "epoch:  16   step:  117   train loss:  2.4798169136047363  val loss:  5.367892742156982  val L1 loss:  5.8372\n",
      "epoch:  16   step:  118   train loss:  6.9110565185546875  val loss:  4.984771251678467  val L1 loss:  5.4378\n",
      "epoch:  16   step:  119   train loss:  1.7081921100616455  val loss:  4.909502029418945  val L1 loss:  5.3883\n",
      "epoch:  16   step:  120   train loss:  2.791022777557373  val loss:  4.9475836753845215  val L1 loss:  5.4165\n",
      "epoch:  16   step:  121   train loss:  4.92451810836792  val loss:  4.930884838104248  val L1 loss:  5.3974\n",
      "epoch:  16   step:  122   train loss:  4.667995452880859  val loss:  4.876709938049316  val L1 loss:  5.3515\n",
      "epoch:  16   step:  123   train loss:  4.385725498199463  val loss:  5.041921138763428  val L1 loss:  5.5035\n",
      "epoch:  16   step:  124   train loss:  3.7025628089904785  val loss:  5.415610313415527  val L1 loss:  5.9066\n",
      "epoch:  16   step:  125   train loss:  3.0578858852386475  val loss:  5.218264102935791  val L1 loss:  5.7005\n",
      "epoch:  16   step:  126   train loss:  4.145601749420166  val loss:  4.775259971618652  val L1 loss:  5.2269\n",
      "epoch:  16   step:  127   train loss:  3.484992265701294  val loss:  4.567572116851807  val L1 loss:  5.0409\n",
      "epoch:  16   step:  128   train loss:  3.5002496242523193  val loss:  4.566208362579346  val L1 loss:  5.0129\n",
      "epoch:  16   step:  129   train loss:  6.427644729614258  val loss:  4.65960693359375  val L1 loss:  5.113\n",
      "epoch:  16   step:  130   train loss:  4.533788681030273  val loss:  4.738623142242432  val L1 loss:  5.2009\n",
      "epoch:  16   step:  131   train loss:  2.377838134765625  val loss:  4.670351028442383  val L1 loss:  5.132\n",
      "epoch:  16   step:  132   train loss:  2.2472753524780273  val loss:  4.708929538726807  val L1 loss:  5.1788\n",
      "epoch:  16   step:  133   train loss:  3.584491729736328  val loss:  4.911134719848633  val L1 loss:  5.3789\n",
      "epoch:  16   step:  134   train loss:  2.586940288543701  val loss:  5.18301248550415  val L1 loss:  5.6574\n",
      "epoch:  16   step:  135   train loss:  3.5377840995788574  val loss:  5.160135269165039  val L1 loss:  5.6293\n",
      "epoch:  16   step:  136   train loss:  3.2376785278320312  val loss:  4.815868377685547  val L1 loss:  5.2855\n",
      "epoch:  16   step:  137   train loss:  4.439086437225342  val loss:  4.755702018737793  val L1 loss:  5.2194\n",
      "epoch:  16   step:  138   train loss:  3.3392837047576904  val loss:  5.452666282653809  val L1 loss:  5.9424\n",
      "epoch:  16   step:  139   train loss:  3.2398478984832764  val loss:  5.91323184967041  val L1 loss:  6.4089\n",
      "epoch:  16   step:  140   train loss:  3.992705821990967  val loss:  6.123232364654541  val L1 loss:  6.6192\n",
      "epoch:  16   step:  141   train loss:  3.4309980869293213  val loss:  5.714339256286621  val L1 loss:  6.1968\n",
      "epoch:  16   step:  142   train loss:  4.95359992980957  val loss:  5.127077579498291  val L1 loss:  5.5801\n",
      "epoch:  16   step:  143   train loss:  3.649148464202881  val loss:  4.957841873168945  val L1 loss:  5.426\n",
      "epoch:  16   step:  144   train loss:  4.279930591583252  val loss:  5.000600337982178  val L1 loss:  5.4807\n",
      "epoch:  16   step:  145   train loss:  3.5653419494628906  val loss:  4.907034873962402  val L1 loss:  5.3718\n",
      "epoch:  16   step:  146   train loss:  2.528623104095459  val loss:  4.8873772621154785  val L1 loss:  5.3592\n",
      "epoch:  16   step:  147   train loss:  1.8814020156860352  val loss:  4.966084003448486  val L1 loss:  5.4477\n",
      "epoch:  16   step:  148   train loss:  2.033658266067505  val loss:  4.769877910614014  val L1 loss:  5.2414\n",
      "epoch:  16   step:  149   train loss:  3.648982286453247  val loss:  4.7522358894348145  val L1 loss:  5.2037\n",
      "epoch:  16   step:  150   train loss:  5.169160842895508  val loss:  4.778117656707764  val L1 loss:  5.2436\n",
      "epoch:  16   step:  151   train loss:  2.521590232849121  val loss:  4.725393772125244  val L1 loss:  5.1811\n",
      "epoch:  16   step:  152   train loss:  4.715305328369141  val loss:  4.794137001037598  val L1 loss:  5.2561\n",
      "epoch:  16   step:  153   train loss:  3.5302228927612305  val loss:  5.066580295562744  val L1 loss:  5.5372\n",
      "epoch:  16   step:  154   train loss:  3.0620274543762207  val loss:  4.997073173522949  val L1 loss:  5.4768\n",
      "epoch:  16   step:  155   train loss:  2.9863595962524414  val loss:  4.873631954193115  val L1 loss:  5.3428\n",
      "epoch:  16   step:  156   train loss:  2.5708580017089844  val loss:  4.842180252075195  val L1 loss:  5.3026\n",
      "epoch:  16   step:  157   train loss:  2.0562901496887207  val loss:  5.2832932472229  val L1 loss:  5.7551\n",
      "epoch:  16   step:  158   train loss:  4.7325239181518555  val loss:  5.491403579711914  val L1 loss:  5.9682\n",
      "epoch:  16   step:  159   train loss:  5.099776268005371  val loss:  5.314762592315674  val L1 loss:  5.7881\n",
      "epoch:  16   step:  160   train loss:  4.829227447509766  val loss:  4.939996242523193  val L1 loss:  5.4097\n",
      "epoch:  16   step:  161   train loss:  3.7576544284820557  val loss:  5.441943168640137  val L1 loss:  5.9119\n",
      "epoch:  16   step:  162   train loss:  4.850688934326172  val loss:  6.156698703765869  val L1 loss:  6.6481\n",
      "epoch:  16   step:  163   train loss:  3.0757508277893066  val loss:  6.366662979125977  val L1 loss:  6.8566\n",
      "epoch:  16   step:  164   train loss:  3.4062023162841797  val loss:  5.736643314361572  val L1 loss:  6.2159\n",
      "epoch:  16   step:  165   train loss:  3.4023690223693848  val loss:  5.2200140953063965  val L1 loss:  5.684\n",
      "epoch:  16   step:  166   train loss:  5.008363723754883  val loss:  5.084396839141846  val L1 loss:  5.5528\n",
      "epoch:  16   step:  167   train loss:  3.5090863704681396  val loss:  5.084140300750732  val L1 loss:  5.5457\n",
      "epoch:  16   step:  168   train loss:  2.761357069015503  val loss:  5.050311088562012  val L1 loss:  5.4955\n",
      "epoch:  16   step:  169   train loss:  3.177802324295044  val loss:  5.1392645835876465  val L1 loss:  5.6159\n",
      "epoch:  16   step:  170   train loss:  2.2083873748779297  val loss:  5.465841770172119  val L1 loss:  5.9424\n",
      "epoch:  16   step:  171   train loss:  2.6899006366729736  val loss:  5.513099670410156  val L1 loss:  5.9956\n",
      "epoch:  16   step:  172   train loss:  2.759192943572998  val loss:  5.2909417152404785  val L1 loss:  5.7719\n",
      "epoch:  16   step:  173   train loss:  1.7548439502716064  val loss:  4.974363327026367  val L1 loss:  5.433\n",
      "epoch:  16   step:  174   train loss:  3.1442742347717285  val loss:  5.018470764160156  val L1 loss:  5.5029\n",
      "epoch:  16   step:  175   train loss:  3.7561237812042236  val loss:  5.078410625457764  val L1 loss:  5.5664\n",
      "epoch:  16   step:  176   train loss:  2.7082648277282715  val loss:  5.065276622772217  val L1 loss:  5.5459\n",
      "epoch:  16   step:  177   train loss:  2.435356378555298  val loss:  4.9607110023498535  val L1 loss:  5.4288\n",
      "epoch:  16   step:  178   train loss:  2.7004542350769043  val loss:  4.928200721740723  val L1 loss:  5.3978\n",
      "epoch:  16   step:  179   train loss:  3.158933639526367  val loss:  4.919915676116943  val L1 loss:  5.3912\n",
      "epoch:  16   step:  180   train loss:  1.4176948070526123  val loss:  4.86674165725708  val L1 loss:  5.3462\n",
      "epoch:  16   step:  181   train loss:  1.9753940105438232  val loss:  4.85087776184082  val L1 loss:  5.3084\n",
      "epoch:  16   step:  182   train loss:  2.8976688385009766  val loss:  4.8521881103515625  val L1 loss:  5.3144\n",
      "epoch:  16   step:  183   train loss:  2.7709906101226807  val loss:  4.80042028427124  val L1 loss:  5.2676\n",
      "epoch:  16   step:  184   train loss:  2.4071435928344727  val loss:  4.6433424949646  val L1 loss:  5.1004\n",
      "epoch:  16   step:  185   train loss:  2.1515402793884277  val loss:  4.574246883392334  val L1 loss:  5.0462\n",
      "epoch:  16   step:  186   train loss:  6.1346516609191895  val loss:  4.585053443908691  val L1 loss:  5.0446\n",
      "epoch:  16   step:  187   train loss:  2.748544216156006  val loss:  4.491366863250732  val L1 loss:  4.9424\n",
      "epoch:  16   step:  188   train loss:  3.914910316467285  val loss:  4.443687438964844  val L1 loss:  4.9137\n",
      "epoch:  16   step:  189   train loss:  3.797036647796631  val loss:  4.486128330230713  val L1 loss:  4.9718\n",
      "epoch:  16   step:  190   train loss:  3.1338846683502197  val loss:  4.700362205505371  val L1 loss:  5.1388\n",
      "epoch:  16   step:  191   train loss:  4.59688663482666  val loss:  4.7768449783325195  val L1 loss:  5.2379\n",
      "epoch:  16   step:  192   train loss:  4.539319038391113  val loss:  4.648126125335693  val L1 loss:  5.0981\n",
      "epoch:  16   step:  193   train loss:  3.08686900138855  val loss:  4.555668830871582  val L1 loss:  5.039\n",
      "epoch:  16   step:  194   train loss:  6.616448402404785  val loss:  4.467031002044678  val L1 loss:  4.9258\n",
      "epoch:  16   step:  195   train loss:  2.6880884170532227  val loss:  4.593059062957764  val L1 loss:  5.0681\n",
      "epoch:  16   step:  196   train loss:  3.9332408905029297  val loss:  4.69633150100708  val L1 loss:  5.1681\n",
      "epoch:  16   step:  197   train loss:  2.9784343242645264  val loss:  4.447429180145264  val L1 loss:  4.9132\n",
      "epoch:  16   step:  198   train loss:  3.831629753112793  val loss:  4.403881549835205  val L1 loss:  4.8852\n",
      "epoch:  16   step:  199   train loss:  1.6234500408172607  val loss:  4.4916205406188965  val L1 loss:  4.957\n",
      "epoch:  16   step:  200   train loss:  6.083558082580566  val loss:  4.4533796310424805  val L1 loss:  4.9098\n",
      "epoch:  16   step:  201   train loss:  3.6597931385040283  val loss:  4.318224906921387  val L1 loss:  4.7962\n",
      "epoch:  16   step:  202   train loss:  5.425252437591553  val loss:  4.190405368804932  val L1 loss:  4.6484\n",
      "epoch:  16   step:  203   train loss:  3.176018238067627  val loss:  4.5379157066345215  val L1 loss:  4.9944\n",
      "epoch:  16   step:  204   train loss:  3.96905255317688  val loss:  4.631800174713135  val L1 loss:  5.0926\n",
      "epoch:  16   step:  205   train loss:  3.058962821960449  val loss:  4.32795524597168  val L1 loss:  4.785\n",
      "epoch:  16   step:  206   train loss:  3.913249969482422  val loss:  4.143974304199219  val L1 loss:  4.6297\n",
      "epoch:  16   step:  207   train loss:  2.1367440223693848  val loss:  4.390120506286621  val L1 loss:  4.8395\n",
      "epoch:  16   step:  208   train loss:  2.532076597213745  val loss:  4.681587219238281  val L1 loss:  5.149\n",
      "epoch:  17   step:  0   train loss:  2.060757875442505  val loss:  4.630428791046143  val L1 loss:  5.1064\n",
      "epoch:  17   step:  1   train loss:  2.684513568878174  val loss:  4.228895664215088  val L1 loss:  4.6991\n",
      "epoch:  17   step:  2   train loss:  2.8091135025024414  val loss:  4.160530090332031  val L1 loss:  4.6052\n",
      "epoch:  17   step:  3   train loss:  3.05558443069458  val loss:  4.716017723083496  val L1 loss:  5.2009\n",
      "epoch:  17   step:  4   train loss:  2.430941581726074  val loss:  5.205597400665283  val L1 loss:  5.6816\n",
      "epoch:  17   step:  5   train loss:  3.383049488067627  val loss:  4.94743013381958  val L1 loss:  5.4374\n",
      "epoch:  17   step:  6   train loss:  3.199009418487549  val loss:  4.523030757904053  val L1 loss:  4.9897\n",
      "epoch:  17   step:  7   train loss:  2.00278377532959  val loss:  4.58953857421875  val L1 loss:  5.0538\n",
      "epoch:  17   step:  8   train loss:  2.9906461238861084  val loss:  4.7841572761535645  val L1 loss:  5.2381\n",
      "epoch:  17   step:  9   train loss:  2.519012928009033  val loss:  4.853222370147705  val L1 loss:  5.3192\n",
      "epoch:  17   step:  10   train loss:  2.3578057289123535  val loss:  4.8236002922058105  val L1 loss:  5.2854\n",
      "epoch:  17   step:  11   train loss:  1.954854965209961  val loss:  4.829684734344482  val L1 loss:  5.297\n",
      "epoch:  17   step:  12   train loss:  3.86527681350708  val loss:  4.817622184753418  val L1 loss:  5.2784\n",
      "epoch:  17   step:  13   train loss:  2.398728847503662  val loss:  4.846935272216797  val L1 loss:  5.3345\n",
      "epoch:  17   step:  14   train loss:  2.175008773803711  val loss:  4.956223487854004  val L1 loss:  5.4282\n",
      "epoch:  17   step:  15   train loss:  4.38034725189209  val loss:  5.208630084991455  val L1 loss:  5.6861\n",
      "epoch:  17   step:  16   train loss:  3.3109631538391113  val loss:  5.228781700134277  val L1 loss:  5.7167\n",
      "epoch:  17   step:  17   train loss:  4.072660446166992  val loss:  4.938736438751221  val L1 loss:  5.4181\n",
      "epoch:  17   step:  18   train loss:  2.9212000370025635  val loss:  4.695322513580322  val L1 loss:  5.133\n",
      "epoch:  17   step:  19   train loss:  2.147578239440918  val loss:  4.963703155517578  val L1 loss:  5.4421\n",
      "epoch:  17   step:  20   train loss:  5.046501159667969  val loss:  5.28076171875  val L1 loss:  5.7543\n",
      "epoch:  17   step:  21   train loss:  5.626007556915283  val loss:  5.121373176574707  val L1 loss:  5.6099\n",
      "epoch:  17   step:  22   train loss:  4.931832313537598  val loss:  4.889523029327393  val L1 loss:  5.3459\n",
      "epoch:  17   step:  23   train loss:  3.013110637664795  val loss:  4.8658881187438965  val L1 loss:  5.337\n",
      "epoch:  17   step:  24   train loss:  2.0258939266204834  val loss:  5.068295955657959  val L1 loss:  5.5642\n",
      "epoch:  17   step:  25   train loss:  3.4818081855773926  val loss:  5.142401218414307  val L1 loss:  5.625\n",
      "epoch:  17   step:  26   train loss:  2.7870593070983887  val loss:  5.274057388305664  val L1 loss:  5.7503\n",
      "epoch:  17   step:  27   train loss:  3.2986574172973633  val loss:  5.506757736206055  val L1 loss:  5.9894\n",
      "epoch:  17   step:  28   train loss:  4.78342342376709  val loss:  5.5049614906311035  val L1 loss:  5.9914\n",
      "epoch:  17   step:  29   train loss:  2.498779296875  val loss:  5.498653888702393  val L1 loss:  5.9804\n",
      "epoch:  17   step:  30   train loss:  3.826352119445801  val loss:  5.461097717285156  val L1 loss:  5.9478\n",
      "epoch:  17   step:  31   train loss:  2.658547878265381  val loss:  5.406138896942139  val L1 loss:  5.8979\n",
      "epoch:  17   step:  32   train loss:  2.2579545974731445  val loss:  5.3278489112854  val L1 loss:  5.8102\n",
      "epoch:  17   step:  33   train loss:  3.2614283561706543  val loss:  5.2035675048828125  val L1 loss:  5.6806\n",
      "epoch:  17   step:  34   train loss:  3.008298873901367  val loss:  5.137754917144775  val L1 loss:  5.6242\n",
      "epoch:  17   step:  35   train loss:  2.772890090942383  val loss:  5.0461859703063965  val L1 loss:  5.5322\n",
      "epoch:  17   step:  36   train loss:  4.1068220138549805  val loss:  4.890686511993408  val L1 loss:  5.3568\n",
      "epoch:  17   step:  37   train loss:  3.3006515502929688  val loss:  4.754907608032227  val L1 loss:  5.2195\n",
      "epoch:  17   step:  38   train loss:  3.5406835079193115  val loss:  4.652710914611816  val L1 loss:  5.1307\n",
      "epoch:  17   step:  39   train loss:  2.1087186336517334  val loss:  4.626214981079102  val L1 loss:  5.0936\n",
      "epoch:  17   step:  40   train loss:  3.4930553436279297  val loss:  5.0736308097839355  val L1 loss:  5.5483\n",
      "epoch:  17   step:  41   train loss:  2.296527862548828  val loss:  5.595572471618652  val L1 loss:  6.0798\n",
      "epoch:  17   step:  42   train loss:  3.7183432579040527  val loss:  5.750115871429443  val L1 loss:  6.2375\n",
      "epoch:  17   step:  43   train loss:  5.019298076629639  val loss:  5.755650043487549  val L1 loss:  6.2432\n",
      "epoch:  17   step:  44   train loss:  4.315409183502197  val loss:  5.097739219665527  val L1 loss:  5.5695\n",
      "epoch:  17   step:  45   train loss:  2.7270588874816895  val loss:  4.5251970291137695  val L1 loss:  5.0106\n",
      "epoch:  17   step:  46   train loss:  3.2687478065490723  val loss:  4.789127349853516  val L1 loss:  5.2575\n",
      "epoch:  17   step:  47   train loss:  2.6472771167755127  val loss:  5.342978477478027  val L1 loss:  5.8264\n",
      "epoch:  17   step:  48   train loss:  5.28065299987793  val loss:  5.592464923858643  val L1 loss:  6.0682\n",
      "epoch:  17   step:  49   train loss:  2.8527212142944336  val loss:  5.624349117279053  val L1 loss:  6.0887\n",
      "epoch:  17   step:  50   train loss:  7.42601203918457  val loss:  5.201545715332031  val L1 loss:  5.6603\n",
      "epoch:  17   step:  51   train loss:  4.554281234741211  val loss:  4.94388484954834  val L1 loss:  5.4222\n",
      "epoch:  17   step:  52   train loss:  3.5817983150482178  val loss:  4.900675296783447  val L1 loss:  5.387\n",
      "epoch:  17   step:  53   train loss:  4.247516632080078  val loss:  4.883368968963623  val L1 loss:  5.359\n",
      "epoch:  17   step:  54   train loss:  3.0486679077148438  val loss:  4.676896095275879  val L1 loss:  5.1551\n",
      "epoch:  17   step:  55   train loss:  3.1360361576080322  val loss:  4.518573760986328  val L1 loss:  4.9977\n",
      "epoch:  17   step:  56   train loss:  3.384629249572754  val loss:  4.325598239898682  val L1 loss:  4.7977\n",
      "epoch:  17   step:  57   train loss:  1.7094852924346924  val loss:  4.2327117919921875  val L1 loss:  4.6902\n",
      "epoch:  17   step:  58   train loss:  4.318474769592285  val loss:  4.204305171966553  val L1 loss:  4.6615\n",
      "epoch:  17   step:  59   train loss:  4.512428283691406  val loss:  4.2016730308532715  val L1 loss:  4.6762\n",
      "epoch:  17   step:  60   train loss:  3.050494432449341  val loss:  4.767571926116943  val L1 loss:  5.2388\n",
      "epoch:  17   step:  61   train loss:  3.256362199783325  val loss:  5.184899806976318  val L1 loss:  5.6834\n",
      "epoch:  17   step:  62   train loss:  2.0065979957580566  val loss:  5.252363681793213  val L1 loss:  5.7508\n",
      "epoch:  17   step:  63   train loss:  4.048709392547607  val loss:  4.901094436645508  val L1 loss:  5.3941\n",
      "epoch:  17   step:  64   train loss:  4.192070484161377  val loss:  4.448945999145508  val L1 loss:  4.915\n",
      "epoch:  17   step:  65   train loss:  2.736340284347534  val loss:  4.246712684631348  val L1 loss:  4.7096\n",
      "epoch:  17   step:  66   train loss:  2.8045108318328857  val loss:  5.011058330535889  val L1 loss:  5.487\n",
      "epoch:  17   step:  67   train loss:  5.509343147277832  val loss:  5.512107849121094  val L1 loss:  5.996\n",
      "epoch:  17   step:  68   train loss:  2.57313871383667  val loss:  5.326298713684082  val L1 loss:  5.8058\n",
      "epoch:  17   step:  69   train loss:  6.464016914367676  val loss:  4.84645938873291  val L1 loss:  5.3219\n",
      "epoch:  17   step:  70   train loss:  5.185089111328125  val loss:  4.56866455078125  val L1 loss:  5.0312\n",
      "epoch:  17   step:  71   train loss:  3.3001151084899902  val loss:  5.368989944458008  val L1 loss:  5.8657\n",
      "epoch:  17   step:  72   train loss:  4.09661340713501  val loss:  5.735871315002441  val L1 loss:  6.2229\n",
      "epoch:  17   step:  73   train loss:  4.9996161460876465  val loss:  5.728682518005371  val L1 loss:  6.2125\n",
      "epoch:  17   step:  74   train loss:  2.8037662506103516  val loss:  5.215031623840332  val L1 loss:  5.6985\n",
      "epoch:  17   step:  75   train loss:  2.215787887573242  val loss:  4.720043659210205  val L1 loss:  5.1757\n",
      "epoch:  17   step:  76   train loss:  2.626974105834961  val loss:  4.544605255126953  val L1 loss:  5.02\n",
      "epoch:  17   step:  77   train loss:  3.303501844406128  val loss:  4.548846244812012  val L1 loss:  5.016\n",
      "epoch:  17   step:  78   train loss:  5.631118297576904  val loss:  4.785862445831299  val L1 loss:  5.2634\n",
      "epoch:  17   step:  79   train loss:  2.634587287902832  val loss:  4.979254245758057  val L1 loss:  5.4589\n",
      "epoch:  17   step:  80   train loss:  2.857936382293701  val loss:  5.178638935089111  val L1 loss:  5.6471\n",
      "epoch:  17   step:  81   train loss:  3.2772326469421387  val loss:  5.091649055480957  val L1 loss:  5.5639\n",
      "epoch:  17   step:  82   train loss:  2.262479066848755  val loss:  4.925630569458008  val L1 loss:  5.4003\n",
      "epoch:  17   step:  83   train loss:  2.6872220039367676  val loss:  4.916326522827148  val L1 loss:  5.4021\n",
      "epoch:  17   step:  84   train loss:  2.2313785552978516  val loss:  4.842764377593994  val L1 loss:  5.3293\n",
      "epoch:  17   step:  85   train loss:  2.7341513633728027  val loss:  4.922934055328369  val L1 loss:  5.3962\n",
      "epoch:  17   step:  86   train loss:  4.647039413452148  val loss:  5.215246677398682  val L1 loss:  5.702\n",
      "epoch:  17   step:  87   train loss:  4.446678161621094  val loss:  5.5218000411987305  val L1 loss:  6.0072\n",
      "epoch:  17   step:  88   train loss:  3.9661102294921875  val loss:  5.62556266784668  val L1 loss:  6.1066\n",
      "epoch:  17   step:  89   train loss:  2.812565803527832  val loss:  5.377343654632568  val L1 loss:  5.8488\n",
      "epoch:  17   step:  90   train loss:  5.139919281005859  val loss:  4.905211925506592  val L1 loss:  5.382\n",
      "epoch:  17   step:  91   train loss:  2.7174785137176514  val loss:  4.8166656494140625  val L1 loss:  5.2733\n",
      "epoch:  17   step:  92   train loss:  2.782776355743408  val loss:  4.750117301940918  val L1 loss:  5.2219\n",
      "epoch:  17   step:  93   train loss:  2.9254417419433594  val loss:  4.7865400314331055  val L1 loss:  5.262\n",
      "epoch:  17   step:  94   train loss:  3.5954864025115967  val loss:  5.051048278808594  val L1 loss:  5.5158\n",
      "epoch:  17   step:  95   train loss:  2.3313913345336914  val loss:  5.498497009277344  val L1 loss:  5.9726\n",
      "epoch:  17   step:  96   train loss:  3.666403293609619  val loss:  5.829789161682129  val L1 loss:  6.3132\n",
      "epoch:  17   step:  97   train loss:  3.6778621673583984  val loss:  5.7104082107543945  val L1 loss:  6.1927\n",
      "epoch:  17   step:  98   train loss:  5.0066680908203125  val loss:  5.097681045532227  val L1 loss:  5.5625\n",
      "epoch:  17   step:  99   train loss:  5.000792503356934  val loss:  4.695596694946289  val L1 loss:  5.1603\n",
      "epoch:  17   step:  100   train loss:  2.3772995471954346  val loss:  4.7349534034729  val L1 loss:  5.2103\n",
      "epoch:  17   step:  101   train loss:  4.230897903442383  val loss:  4.701961994171143  val L1 loss:  5.1759\n",
      "epoch:  17   step:  102   train loss:  3.7676796913146973  val loss:  4.620950698852539  val L1 loss:  5.0755\n",
      "epoch:  17   step:  103   train loss:  3.3509273529052734  val loss:  4.548079013824463  val L1 loss:  4.9976\n",
      "epoch:  17   step:  104   train loss:  1.6769278049468994  val loss:  4.509527683258057  val L1 loss:  4.957\n",
      "epoch:  17   step:  105   train loss:  3.2659220695495605  val loss:  4.505019664764404  val L1 loss:  4.9453\n",
      "epoch:  17   step:  106   train loss:  6.072272777557373  val loss:  4.882078647613525  val L1 loss:  5.3633\n",
      "epoch:  17   step:  107   train loss:  4.7578887939453125  val loss:  5.404833793640137  val L1 loss:  5.8991\n",
      "epoch:  17   step:  108   train loss:  2.256507396697998  val loss:  5.945047855377197  val L1 loss:  6.4385\n",
      "epoch:  17   step:  109   train loss:  3.3010993003845215  val loss:  5.937744140625  val L1 loss:  6.4354\n",
      "epoch:  17   step:  110   train loss:  3.6508259773254395  val loss:  5.621521949768066  val L1 loss:  6.1137\n",
      "epoch:  17   step:  111   train loss:  3.312197208404541  val loss:  4.848850727081299  val L1 loss:  5.3416\n",
      "epoch:  17   step:  112   train loss:  3.8151111602783203  val loss:  4.440181732177734  val L1 loss:  4.8752\n",
      "epoch:  17   step:  113   train loss:  2.347773551940918  val loss:  4.352598190307617  val L1 loss:  4.7861\n",
      "epoch:  17   step:  114   train loss:  2.331730365753174  val loss:  4.376380920410156  val L1 loss:  4.8495\n",
      "epoch:  17   step:  115   train loss:  2.573244094848633  val loss:  4.518410682678223  val L1 loss:  5.0037\n",
      "epoch:  17   step:  116   train loss:  4.869451522827148  val loss:  4.688369274139404  val L1 loss:  5.1558\n",
      "epoch:  17   step:  117   train loss:  4.416296005249023  val loss:  4.951371669769287  val L1 loss:  5.4315\n",
      "epoch:  17   step:  118   train loss:  3.197558879852295  val loss:  4.974611282348633  val L1 loss:  5.4512\n",
      "epoch:  17   step:  119   train loss:  4.298937797546387  val loss:  4.777742862701416  val L1 loss:  5.2693\n",
      "epoch:  17   step:  120   train loss:  4.5110392570495605  val loss:  4.5656867027282715  val L1 loss:  5.043\n",
      "epoch:  17   step:  121   train loss:  1.4860810041427612  val loss:  5.032863616943359  val L1 loss:  5.5157\n",
      "epoch:  17   step:  122   train loss:  5.244492530822754  val loss:  5.420324802398682  val L1 loss:  5.8901\n",
      "epoch:  17   step:  123   train loss:  4.540595531463623  val loss:  4.9526591300964355  val L1 loss:  5.4336\n",
      "epoch:  17   step:  124   train loss:  4.768524169921875  val loss:  4.60493278503418  val L1 loss:  5.0642\n",
      "epoch:  17   step:  125   train loss:  4.824624538421631  val loss:  4.939306735992432  val L1 loss:  5.4074\n",
      "epoch:  17   step:  126   train loss:  3.8262600898742676  val loss:  5.418379783630371  val L1 loss:  5.9052\n",
      "epoch:  17   step:  127   train loss:  3.5271506309509277  val loss:  5.282876968383789  val L1 loss:  5.7737\n",
      "epoch:  17   step:  128   train loss:  4.056890487670898  val loss:  4.818305969238281  val L1 loss:  5.2985\n",
      "epoch:  17   step:  129   train loss:  3.1395883560180664  val loss:  4.534519672393799  val L1 loss:  5.0075\n",
      "epoch:  17   step:  130   train loss:  3.064099073410034  val loss:  4.434102535247803  val L1 loss:  4.9087\n",
      "epoch:  17   step:  131   train loss:  2.8321356773376465  val loss:  4.510902404785156  val L1 loss:  4.9665\n",
      "epoch:  17   step:  132   train loss:  2.2537691593170166  val loss:  4.590184211730957  val L1 loss:  5.0406\n",
      "epoch:  17   step:  133   train loss:  6.006054401397705  val loss:  4.435708999633789  val L1 loss:  4.9078\n",
      "epoch:  17   step:  134   train loss:  5.673373222351074  val loss:  4.389871120452881  val L1 loss:  4.8702\n",
      "epoch:  17   step:  135   train loss:  4.2176737785339355  val loss:  4.380027770996094  val L1 loss:  4.8618\n",
      "epoch:  17   step:  136   train loss:  2.0850117206573486  val loss:  4.419577598571777  val L1 loss:  4.8977\n",
      "epoch:  17   step:  137   train loss:  3.3302865028381348  val loss:  4.499483108520508  val L1 loss:  4.9664\n",
      "epoch:  17   step:  138   train loss:  3.9221954345703125  val loss:  4.619268417358398  val L1 loss:  5.0925\n",
      "epoch:  17   step:  139   train loss:  3.237616539001465  val loss:  4.630686283111572  val L1 loss:  5.0809\n",
      "epoch:  17   step:  140   train loss:  2.761754035949707  val loss:  4.648779392242432  val L1 loss:  5.1005\n",
      "epoch:  17   step:  141   train loss:  3.130007743835449  val loss:  4.629069805145264  val L1 loss:  5.0957\n",
      "epoch:  17   step:  142   train loss:  2.128833770751953  val loss:  4.453910827636719  val L1 loss:  4.9384\n",
      "epoch:  17   step:  143   train loss:  4.210116386413574  val loss:  4.458319187164307  val L1 loss:  4.9106\n",
      "epoch:  17   step:  144   train loss:  3.973992347717285  val loss:  4.60835599899292  val L1 loss:  5.081\n",
      "epoch:  17   step:  145   train loss:  2.292792320251465  val loss:  4.569727897644043  val L1 loss:  5.0408\n",
      "epoch:  17   step:  146   train loss:  2.6147589683532715  val loss:  4.564209461212158  val L1 loss:  5.029\n",
      "epoch:  17   step:  147   train loss:  3.016695022583008  val loss:  4.574406147003174  val L1 loss:  5.0471\n",
      "epoch:  17   step:  148   train loss:  3.0732574462890625  val loss:  4.63201904296875  val L1 loss:  5.1074\n",
      "epoch:  17   step:  149   train loss:  3.091731548309326  val loss:  4.6682353019714355  val L1 loss:  5.1456\n",
      "epoch:  17   step:  150   train loss:  1.8422966003417969  val loss:  4.638129234313965  val L1 loss:  5.1091\n",
      "epoch:  17   step:  151   train loss:  2.593714714050293  val loss:  4.615226745605469  val L1 loss:  5.0738\n",
      "epoch:  17   step:  152   train loss:  3.736541748046875  val loss:  4.588634967803955  val L1 loss:  5.0602\n",
      "epoch:  17   step:  153   train loss:  4.124651908874512  val loss:  4.617396354675293  val L1 loss:  5.0891\n",
      "epoch:  17   step:  154   train loss:  5.587704181671143  val loss:  4.670757293701172  val L1 loss:  5.1447\n",
      "epoch:  17   step:  155   train loss:  3.0206332206726074  val loss:  4.789512634277344  val L1 loss:  5.2607\n",
      "epoch:  17   step:  156   train loss:  4.021892547607422  val loss:  4.73586893081665  val L1 loss:  5.2094\n",
      "epoch:  17   step:  157   train loss:  3.4204442501068115  val loss:  4.5824785232543945  val L1 loss:  5.048\n",
      "epoch:  17   step:  158   train loss:  1.7696552276611328  val loss:  4.707639694213867  val L1 loss:  5.1731\n",
      "epoch:  17   step:  159   train loss:  4.812648296356201  val loss:  4.8729777336120605  val L1 loss:  5.3385\n",
      "epoch:  17   step:  160   train loss:  4.312132835388184  val loss:  4.733808994293213  val L1 loss:  5.1937\n",
      "epoch:  17   step:  161   train loss:  1.9715869426727295  val loss:  4.653900146484375  val L1 loss:  5.1249\n",
      "epoch:  17   step:  162   train loss:  2.841047763824463  val loss:  4.889712810516357  val L1 loss:  5.3669\n",
      "epoch:  17   step:  163   train loss:  2.3396785259246826  val loss:  5.190558910369873  val L1 loss:  5.683\n",
      "epoch:  17   step:  164   train loss:  2.901305675506592  val loss:  5.342404842376709  val L1 loss:  5.8293\n",
      "epoch:  17   step:  165   train loss:  3.077458143234253  val loss:  5.103562355041504  val L1 loss:  5.5974\n",
      "epoch:  17   step:  166   train loss:  1.7634015083312988  val loss:  4.77939510345459  val L1 loss:  5.2445\n",
      "epoch:  17   step:  167   train loss:  2.6161999702453613  val loss:  4.805239677429199  val L1 loss:  5.2486\n",
      "epoch:  17   step:  168   train loss:  2.4483041763305664  val loss:  4.974389553070068  val L1 loss:  5.4459\n",
      "epoch:  17   step:  169   train loss:  5.70741605758667  val loss:  4.768689155578613  val L1 loss:  5.2333\n",
      "epoch:  17   step:  170   train loss:  4.4803338050842285  val loss:  4.640954494476318  val L1 loss:  5.0855\n",
      "epoch:  17   step:  171   train loss:  3.8765950202941895  val loss:  4.932456016540527  val L1 loss:  5.3996\n",
      "epoch:  17   step:  172   train loss:  2.243149757385254  val loss:  5.213781833648682  val L1 loss:  5.6873\n",
      "epoch:  17   step:  173   train loss:  3.2494614124298096  val loss:  5.253446102142334  val L1 loss:  5.7362\n",
      "epoch:  17   step:  174   train loss:  3.5276196002960205  val loss:  5.204152584075928  val L1 loss:  5.6825\n",
      "epoch:  17   step:  175   train loss:  2.370710849761963  val loss:  4.872435569763184  val L1 loss:  5.3271\n",
      "epoch:  17   step:  176   train loss:  4.535983085632324  val loss:  4.703495979309082  val L1 loss:  5.1752\n",
      "epoch:  17   step:  177   train loss:  2.4343013763427734  val loss:  4.630926609039307  val L1 loss:  5.104\n",
      "epoch:  17   step:  178   train loss:  3.212228775024414  val loss:  4.567789554595947  val L1 loss:  5.0415\n",
      "epoch:  17   step:  179   train loss:  2.1476266384124756  val loss:  4.590436935424805  val L1 loss:  5.0642\n",
      "epoch:  17   step:  180   train loss:  2.6132028102874756  val loss:  4.6972150802612305  val L1 loss:  5.1642\n",
      "epoch:  17   step:  181   train loss:  1.7350647449493408  val loss:  4.802400588989258  val L1 loss:  5.2798\n",
      "epoch:  17   step:  182   train loss:  3.8877692222595215  val loss:  4.807311058044434  val L1 loss:  5.2797\n",
      "epoch:  17   step:  183   train loss:  3.584292411804199  val loss:  4.467324256896973  val L1 loss:  4.9429\n",
      "epoch:  17   step:  184   train loss:  2.539966106414795  val loss:  4.312979698181152  val L1 loss:  4.7817\n",
      "epoch:  17   step:  185   train loss:  2.3487958908081055  val loss:  4.304067134857178  val L1 loss:  4.769\n",
      "epoch:  17   step:  186   train loss:  2.5910496711730957  val loss:  4.305482387542725  val L1 loss:  4.7722\n",
      "epoch:  17   step:  187   train loss:  2.332408905029297  val loss:  4.478416442871094  val L1 loss:  4.9527\n",
      "epoch:  17   step:  188   train loss:  2.181490182876587  val loss:  4.422019958496094  val L1 loss:  4.8851\n",
      "epoch:  17   step:  189   train loss:  2.264862537384033  val loss:  4.362923622131348  val L1 loss:  4.8411\n",
      "epoch:  17   step:  190   train loss:  1.7832967042922974  val loss:  4.380757808685303  val L1 loss:  4.8474\n",
      "epoch:  17   step:  191   train loss:  2.9588615894317627  val loss:  4.409759521484375  val L1 loss:  4.8809\n",
      "epoch:  17   step:  192   train loss:  3.0479185581207275  val loss:  4.5406084060668945  val L1 loss:  4.9963\n",
      "epoch:  17   step:  193   train loss:  2.701082229614258  val loss:  5.314605712890625  val L1 loss:  5.8018\n",
      "epoch:  17   step:  194   train loss:  4.4860148429870605  val loss:  6.280579566955566  val L1 loss:  6.7704\n",
      "epoch:  17   step:  195   train loss:  4.087070465087891  val loss:  6.432427883148193  val L1 loss:  6.9246\n",
      "epoch:  17   step:  196   train loss:  4.919241905212402  val loss:  5.855730056762695  val L1 loss:  6.3435\n",
      "epoch:  17   step:  197   train loss:  3.149421215057373  val loss:  4.760858535766602  val L1 loss:  5.2421\n",
      "epoch:  17   step:  198   train loss:  2.571840286254883  val loss:  4.200760364532471  val L1 loss:  4.6537\n",
      "epoch:  17   step:  199   train loss:  3.125441074371338  val loss:  4.234244346618652  val L1 loss:  4.7003\n",
      "epoch:  17   step:  200   train loss:  5.059658527374268  val loss:  4.2346625328063965  val L1 loss:  4.6976\n",
      "epoch:  17   step:  201   train loss:  5.715564250946045  val loss:  3.9723284244537354  val L1 loss:  4.4429\n",
      "epoch:  17   step:  202   train loss:  3.9411983489990234  val loss:  4.058471202850342  val L1 loss:  4.5118\n",
      "epoch:  17   step:  203   train loss:  2.720510959625244  val loss:  4.515443801879883  val L1 loss:  4.9837\n",
      "epoch:  17   step:  204   train loss:  2.5620570182800293  val loss:  4.779097080230713  val L1 loss:  5.263\n",
      "epoch:  17   step:  205   train loss:  4.491035461425781  val loss:  4.77110481262207  val L1 loss:  5.2625\n",
      "epoch:  17   step:  206   train loss:  5.259048938751221  val loss:  4.522358417510986  val L1 loss:  5.0009\n",
      "epoch:  17   step:  207   train loss:  4.206085205078125  val loss:  4.018990993499756  val L1 loss:  4.4612\n",
      "epoch:  17   step:  208   train loss:  2.165421724319458  val loss:  4.229785442352295  val L1 loss:  4.6789\n",
      "epoch:  18   step:  0   train loss:  4.624670505523682  val loss:  4.71243143081665  val L1 loss:  5.1744\n",
      "epoch:  18   step:  1   train loss:  3.5772368907928467  val loss:  4.880986213684082  val L1 loss:  5.3473\n",
      "epoch:  18   step:  2   train loss:  3.0865116119384766  val loss:  4.690366268157959  val L1 loss:  5.1601\n",
      "epoch:  18   step:  3   train loss:  4.381069183349609  val loss:  4.579771041870117  val L1 loss:  5.0531\n",
      "epoch:  18   step:  4   train loss:  3.013594627380371  val loss:  4.658841133117676  val L1 loss:  5.134\n",
      "epoch:  18   step:  5   train loss:  3.9998812675476074  val loss:  4.725897789001465  val L1 loss:  5.2078\n",
      "epoch:  18   step:  6   train loss:  3.458312511444092  val loss:  4.669220924377441  val L1 loss:  5.1334\n",
      "epoch:  18   step:  7   train loss:  4.09872579574585  val loss:  4.388223648071289  val L1 loss:  4.8608\n",
      "epoch:  18   step:  8   train loss:  5.123510360717773  val loss:  4.154338359832764  val L1 loss:  4.6437\n",
      "epoch:  18   step:  9   train loss:  1.9791419506072998  val loss:  4.237148284912109  val L1 loss:  4.71\n",
      "epoch:  18   step:  10   train loss:  3.1556334495544434  val loss:  4.383403301239014  val L1 loss:  4.8434\n",
      "epoch:  18   step:  11   train loss:  2.300431728363037  val loss:  4.122835636138916  val L1 loss:  4.5989\n",
      "epoch:  18   step:  12   train loss:  3.892604351043701  val loss:  4.041667938232422  val L1 loss:  4.5007\n",
      "epoch:  18   step:  13   train loss:  3.188964605331421  val loss:  4.571347713470459  val L1 loss:  5.033\n",
      "epoch:  18   step:  14   train loss:  2.6251301765441895  val loss:  5.701579570770264  val L1 loss:  6.1997\n",
      "epoch:  18   step:  15   train loss:  3.385159969329834  val loss:  6.05387020111084  val L1 loss:  6.5515\n",
      "epoch:  18   step:  16   train loss:  5.258974075317383  val loss:  5.672883033752441  val L1 loss:  6.1726\n",
      "epoch:  18   step:  17   train loss:  4.182728290557861  val loss:  5.0491485595703125  val L1 loss:  5.5116\n",
      "epoch:  18   step:  18   train loss:  2.338045120239258  val loss:  4.231795787811279  val L1 loss:  4.7063\n",
      "epoch:  18   step:  19   train loss:  1.8421704769134521  val loss:  3.968994140625  val L1 loss:  4.4389\n",
      "epoch:  18   step:  20   train loss:  2.727336883544922  val loss:  4.358335018157959  val L1 loss:  4.8282\n",
      "epoch:  18   step:  21   train loss:  4.414913177490234  val loss:  4.835738182067871  val L1 loss:  5.3148\n",
      "epoch:  18   step:  22   train loss:  4.315513610839844  val loss:  4.898813247680664  val L1 loss:  5.3845\n",
      "epoch:  18   step:  23   train loss:  5.306713104248047  val loss:  4.533852577209473  val L1 loss:  5.0103\n",
      "epoch:  18   step:  24   train loss:  3.9358155727386475  val loss:  4.211678504943848  val L1 loss:  4.6495\n",
      "epoch:  18   step:  25   train loss:  3.1272635459899902  val loss:  4.373618125915527  val L1 loss:  4.8543\n",
      "epoch:  18   step:  26   train loss:  5.268633842468262  val loss:  4.2948737144470215  val L1 loss:  4.7771\n",
      "epoch:  18   step:  27   train loss:  4.091465950012207  val loss:  4.1705217361450195  val L1 loss:  4.6545\n",
      "epoch:  18   step:  28   train loss:  2.641993999481201  val loss:  4.090914726257324  val L1 loss:  4.5496\n",
      "epoch:  18   step:  29   train loss:  3.4224681854248047  val loss:  4.173260688781738  val L1 loss:  4.6457\n",
      "epoch:  18   step:  30   train loss:  2.2075085639953613  val loss:  4.645588397979736  val L1 loss:  5.136\n",
      "epoch:  18   step:  31   train loss:  2.7174088954925537  val loss:  5.014328479766846  val L1 loss:  5.4862\n",
      "epoch:  18   step:  32   train loss:  5.480398178100586  val loss:  4.808763027191162  val L1 loss:  5.2908\n",
      "epoch:  18   step:  33   train loss:  4.146969318389893  val loss:  4.689631462097168  val L1 loss:  5.1723\n",
      "epoch:  18   step:  34   train loss:  3.2477684020996094  val loss:  4.573209285736084  val L1 loss:  5.0326\n",
      "epoch:  18   step:  35   train loss:  2.1305878162384033  val loss:  4.869108200073242  val L1 loss:  5.3518\n",
      "epoch:  18   step:  36   train loss:  2.8152360916137695  val loss:  5.3733439445495605  val L1 loss:  5.8533\n",
      "epoch:  18   step:  37   train loss:  2.653764247894287  val loss:  5.545194625854492  val L1 loss:  6.0248\n",
      "epoch:  18   step:  38   train loss:  2.1632604598999023  val loss:  5.353110313415527  val L1 loss:  5.8328\n",
      "epoch:  18   step:  39   train loss:  2.882284641265869  val loss:  4.797579288482666  val L1 loss:  5.2765\n",
      "epoch:  18   step:  40   train loss:  2.9365904331207275  val loss:  4.548470973968506  val L1 loss:  5.0291\n",
      "epoch:  18   step:  41   train loss:  2.9592344760894775  val loss:  4.499894142150879  val L1 loss:  4.9725\n",
      "epoch:  18   step:  42   train loss:  3.356565475463867  val loss:  4.475554943084717  val L1 loss:  4.9556\n",
      "epoch:  18   step:  43   train loss:  3.86885142326355  val loss:  4.430068492889404  val L1 loss:  4.9112\n",
      "epoch:  18   step:  44   train loss:  2.462101459503174  val loss:  4.397494792938232  val L1 loss:  4.8739\n",
      "epoch:  18   step:  45   train loss:  2.648402690887451  val loss:  4.424299240112305  val L1 loss:  4.9064\n",
      "epoch:  18   step:  46   train loss:  5.199329376220703  val loss:  4.416567325592041  val L1 loss:  4.9004\n",
      "epoch:  18   step:  47   train loss:  3.6699862480163574  val loss:  4.400588512420654  val L1 loss:  4.8818\n",
      "epoch:  18   step:  48   train loss:  4.281023979187012  val loss:  4.406143665313721  val L1 loss:  4.8733\n",
      "epoch:  18   step:  49   train loss:  2.326296329498291  val loss:  4.416481971740723  val L1 loss:  4.8842\n",
      "epoch:  18   step:  50   train loss:  2.925558567047119  val loss:  4.372490882873535  val L1 loss:  4.8446\n",
      "epoch:  18   step:  51   train loss:  1.880662441253662  val loss:  4.332174777984619  val L1 loss:  4.7991\n",
      "epoch:  18   step:  52   train loss:  1.796055555343628  val loss:  4.293252944946289  val L1 loss:  4.7419\n",
      "epoch:  18   step:  53   train loss:  2.702718496322632  val loss:  4.274402141571045  val L1 loss:  4.7361\n",
      "epoch:  18   step:  54   train loss:  1.638002634048462  val loss:  4.470539569854736  val L1 loss:  4.9606\n",
      "epoch:  18   step:  55   train loss:  2.1899654865264893  val loss:  4.6502790451049805  val L1 loss:  5.1328\n",
      "epoch:  18   step:  56   train loss:  2.1332898139953613  val loss:  4.525210380554199  val L1 loss:  5.0057\n",
      "epoch:  18   step:  57   train loss:  1.761655330657959  val loss:  4.368553161621094  val L1 loss:  4.8369\n",
      "epoch:  18   step:  58   train loss:  2.880049228668213  val loss:  4.293900489807129  val L1 loss:  4.7374\n",
      "epoch:  18   step:  59   train loss:  3.1247787475585938  val loss:  4.299795627593994  val L1 loss:  4.7651\n",
      "epoch:  18   step:  60   train loss:  3.512136220932007  val loss:  4.265310287475586  val L1 loss:  4.701\n",
      "epoch:  18   step:  61   train loss:  2.798372745513916  val loss:  4.381674289703369  val L1 loss:  4.8553\n",
      "epoch:  18   step:  62   train loss:  1.4965381622314453  val loss:  4.449704647064209  val L1 loss:  4.9097\n",
      "epoch:  18   step:  63   train loss:  2.3975987434387207  val loss:  4.3152947425842285  val L1 loss:  4.7809\n",
      "epoch:  18   step:  64   train loss:  3.120612621307373  val loss:  4.208546161651611  val L1 loss:  4.6776\n",
      "epoch:  18   step:  65   train loss:  3.89162015914917  val loss:  4.161533832550049  val L1 loss:  4.6364\n",
      "epoch:  18   step:  66   train loss:  5.0811614990234375  val loss:  4.181228160858154  val L1 loss:  4.6588\n",
      "epoch:  18   step:  67   train loss:  2.733694314956665  val loss:  4.019586563110352  val L1 loss:  4.4812\n",
      "epoch:  18   step:  68   train loss:  2.572134494781494  val loss:  3.9810612201690674  val L1 loss:  4.4223\n",
      "epoch:  18   step:  69   train loss:  1.9967641830444336  val loss:  4.150201797485352  val L1 loss:  4.6251\n",
      "epoch:  18   step:  70   train loss:  2.846017360687256  val loss:  4.371635437011719  val L1 loss:  4.8368\n",
      "epoch:  18   step:  71   train loss:  2.7100720405578613  val loss:  4.304263114929199  val L1 loss:  4.7667\n",
      "epoch:  18   step:  72   train loss:  1.9924458265304565  val loss:  4.1057658195495605  val L1 loss:  4.566\n",
      "epoch:  18   step:  73   train loss:  2.7636146545410156  val loss:  3.9917149543762207  val L1 loss:  4.44\n",
      "epoch:  18   step:  74   train loss:  3.189211130142212  val loss:  3.9620144367218018  val L1 loss:  4.3879\n",
      "epoch:  18   step:  75   train loss:  3.20495867729187  val loss:  3.9254109859466553  val L1 loss:  4.3487\n",
      "epoch:  18   step:  76   train loss:  3.783907890319824  val loss:  3.9212515354156494  val L1 loss:  4.3723\n",
      "epoch:  18   step:  77   train loss:  1.5915720462799072  val loss:  3.901575803756714  val L1 loss:  4.3513\n",
      "epoch:  18   step:  78   train loss:  2.3962061405181885  val loss:  3.947213888168335  val L1 loss:  4.4136\n",
      "epoch:  18   step:  79   train loss:  2.125817060470581  val loss:  4.059285640716553  val L1 loss:  4.5077\n",
      "epoch:  18   step:  80   train loss:  2.426086902618408  val loss:  4.234458923339844  val L1 loss:  4.6878\n",
      "epoch:  18   step:  81   train loss:  2.826948404312134  val loss:  4.376030445098877  val L1 loss:  4.8331\n",
      "epoch:  18   step:  82   train loss:  2.678845167160034  val loss:  4.483222484588623  val L1 loss:  4.9496\n",
      "epoch:  18   step:  83   train loss:  2.125704050064087  val loss:  4.79134464263916  val L1 loss:  5.2581\n",
      "epoch:  18   step:  84   train loss:  4.119765281677246  val loss:  5.207613945007324  val L1 loss:  5.6926\n",
      "epoch:  18   step:  85   train loss:  3.391599655151367  val loss:  5.251798152923584  val L1 loss:  5.735\n",
      "epoch:  18   step:  86   train loss:  3.095573902130127  val loss:  5.136767387390137  val L1 loss:  5.6126\n",
      "epoch:  18   step:  87   train loss:  3.960145950317383  val loss:  4.794951438903809  val L1 loss:  5.2655\n",
      "epoch:  18   step:  88   train loss:  2.437659502029419  val loss:  4.470269680023193  val L1 loss:  4.9591\n",
      "epoch:  18   step:  89   train loss:  1.4231598377227783  val loss:  4.404476165771484  val L1 loss:  4.8785\n",
      "epoch:  18   step:  90   train loss:  4.855191230773926  val loss:  4.419505596160889  val L1 loss:  4.8726\n",
      "epoch:  18   step:  91   train loss:  2.921038866043091  val loss:  4.518144607543945  val L1 loss:  4.9739\n",
      "epoch:  18   step:  92   train loss:  3.71309232711792  val loss:  4.6844377517700195  val L1 loss:  5.1493\n",
      "epoch:  18   step:  93   train loss:  3.981536865234375  val loss:  4.7296271324157715  val L1 loss:  5.1909\n",
      "epoch:  18   step:  94   train loss:  2.2031819820404053  val loss:  4.655501842498779  val L1 loss:  5.1156\n",
      "epoch:  18   step:  95   train loss:  3.544607639312744  val loss:  4.601595878601074  val L1 loss:  5.0631\n",
      "epoch:  18   step:  96   train loss:  1.8480771780014038  val loss:  4.701996803283691  val L1 loss:  5.1674\n",
      "epoch:  18   step:  97   train loss:  1.9238191843032837  val loss:  4.760790824890137  val L1 loss:  5.238\n",
      "epoch:  18   step:  98   train loss:  2.427137613296509  val loss:  4.716543674468994  val L1 loss:  5.1805\n",
      "epoch:  18   step:  99   train loss:  3.2532787322998047  val loss:  4.664733409881592  val L1 loss:  5.1254\n",
      "epoch:  18   step:  100   train loss:  3.2739458084106445  val loss:  4.686864376068115  val L1 loss:  5.1432\n",
      "epoch:  18   step:  101   train loss:  2.3680551052093506  val loss:  4.782835483551025  val L1 loss:  5.2555\n",
      "epoch:  18   step:  102   train loss:  5.691843509674072  val loss:  4.898761749267578  val L1 loss:  5.363\n",
      "epoch:  18   step:  103   train loss:  2.090117931365967  val loss:  4.872623443603516  val L1 loss:  5.3447\n",
      "epoch:  18   step:  104   train loss:  4.269909858703613  val loss:  4.863945960998535  val L1 loss:  5.3329\n",
      "epoch:  18   step:  105   train loss:  2.818726062774658  val loss:  4.709197044372559  val L1 loss:  5.184\n",
      "epoch:  18   step:  106   train loss:  2.3146283626556396  val loss:  4.699829578399658  val L1 loss:  5.179\n",
      "epoch:  18   step:  107   train loss:  4.311898231506348  val loss:  4.636861324310303  val L1 loss:  5.1213\n",
      "epoch:  18   step:  108   train loss:  2.6214137077331543  val loss:  4.560769081115723  val L1 loss:  5.0443\n",
      "epoch:  18   step:  109   train loss:  3.154764175415039  val loss:  4.59797477722168  val L1 loss:  5.0658\n",
      "epoch:  18   step:  110   train loss:  2.392324447631836  val loss:  4.753917694091797  val L1 loss:  5.2173\n",
      "epoch:  18   step:  111   train loss:  5.535316467285156  val loss:  4.669884204864502  val L1 loss:  5.1426\n",
      "epoch:  18   step:  112   train loss:  2.2097630500793457  val loss:  4.554859161376953  val L1 loss:  5.0004\n",
      "epoch:  18   step:  113   train loss:  4.245604991912842  val loss:  4.633016109466553  val L1 loss:  5.1191\n",
      "epoch:  18   step:  114   train loss:  2.104060173034668  val loss:  4.907589435577393  val L1 loss:  5.376\n",
      "epoch:  18   step:  115   train loss:  3.736349582672119  val loss:  5.445187091827393  val L1 loss:  5.9404\n",
      "epoch:  18   step:  116   train loss:  3.5705435276031494  val loss:  5.849020957946777  val L1 loss:  6.3287\n",
      "epoch:  18   step:  117   train loss:  4.736310958862305  val loss:  5.452266693115234  val L1 loss:  5.9392\n",
      "epoch:  18   step:  118   train loss:  4.805079460144043  val loss:  4.714813232421875  val L1 loss:  5.1805\n",
      "epoch:  18   step:  119   train loss:  2.801579475402832  val loss:  4.548960208892822  val L1 loss:  4.9959\n",
      "epoch:  18   step:  120   train loss:  3.7327423095703125  val loss:  4.861794948577881  val L1 loss:  5.3436\n",
      "epoch:  18   step:  121   train loss:  4.068470001220703  val loss:  4.733624458312988  val L1 loss:  5.2163\n",
      "epoch:  18   step:  122   train loss:  2.7807459831237793  val loss:  4.482287406921387  val L1 loss:  4.9577\n",
      "epoch:  18   step:  123   train loss:  2.1633620262145996  val loss:  4.384799957275391  val L1 loss:  4.8634\n",
      "epoch:  18   step:  124   train loss:  3.419057846069336  val loss:  4.741547107696533  val L1 loss:  5.2234\n",
      "epoch:  18   step:  125   train loss:  2.567781925201416  val loss:  5.0564727783203125  val L1 loss:  5.5376\n",
      "epoch:  18   step:  126   train loss:  2.9288792610168457  val loss:  5.076618194580078  val L1 loss:  5.5491\n",
      "epoch:  18   step:  127   train loss:  1.571333885192871  val loss:  4.828362941741943  val L1 loss:  5.3002\n",
      "epoch:  18   step:  128   train loss:  2.782834529876709  val loss:  4.631412982940674  val L1 loss:  5.1072\n",
      "epoch:  18   step:  129   train loss:  3.4516360759735107  val loss:  4.640631198883057  val L1 loss:  5.1129\n",
      "epoch:  18   step:  130   train loss:  2.348423480987549  val loss:  4.633857250213623  val L1 loss:  5.0943\n",
      "epoch:  18   step:  131   train loss:  2.807403087615967  val loss:  4.7212748527526855  val L1 loss:  5.1978\n",
      "epoch:  18   step:  132   train loss:  4.833185195922852  val loss:  4.875293254852295  val L1 loss:  5.3459\n",
      "epoch:  18   step:  133   train loss:  1.4768526554107666  val loss:  4.7995829582214355  val L1 loss:  5.2727\n",
      "epoch:  18   step:  134   train loss:  2.3394131660461426  val loss:  4.472561359405518  val L1 loss:  4.9443\n",
      "epoch:  18   step:  135   train loss:  3.8440465927124023  val loss:  4.372470378875732  val L1 loss:  4.8375\n",
      "epoch:  18   step:  136   train loss:  2.8550448417663574  val loss:  4.3505988121032715  val L1 loss:  4.8193\n",
      "epoch:  18   step:  137   train loss:  3.9239001274108887  val loss:  4.405278205871582  val L1 loss:  4.8581\n",
      "epoch:  18   step:  138   train loss:  3.4097447395324707  val loss:  4.402463436126709  val L1 loss:  4.8711\n",
      "epoch:  18   step:  139   train loss:  3.4810729026794434  val loss:  4.284195423126221  val L1 loss:  4.748\n",
      "epoch:  18   step:  140   train loss:  2.857583999633789  val loss:  4.304909706115723  val L1 loss:  4.7744\n",
      "epoch:  18   step:  141   train loss:  3.178157329559326  val loss:  4.627005577087402  val L1 loss:  5.1018\n",
      "epoch:  18   step:  142   train loss:  4.09541130065918  val loss:  4.826178550720215  val L1 loss:  5.3067\n",
      "epoch:  18   step:  143   train loss:  1.8362746238708496  val loss:  4.932018280029297  val L1 loss:  5.4225\n",
      "epoch:  18   step:  144   train loss:  2.030747413635254  val loss:  4.747852325439453  val L1 loss:  5.2179\n",
      "epoch:  18   step:  145   train loss:  3.458878517150879  val loss:  4.511598587036133  val L1 loss:  4.9875\n",
      "epoch:  18   step:  146   train loss:  3.2024753093719482  val loss:  4.261472225189209  val L1 loss:  4.7222\n",
      "epoch:  18   step:  147   train loss:  2.968662738800049  val loss:  4.21756649017334  val L1 loss:  4.6648\n",
      "epoch:  18   step:  148   train loss:  1.9964783191680908  val loss:  4.268873691558838  val L1 loss:  4.7131\n",
      "epoch:  18   step:  149   train loss:  3.0340752601623535  val loss:  4.403052806854248  val L1 loss:  4.8649\n",
      "epoch:  18   step:  150   train loss:  3.5271406173706055  val loss:  4.885103225708008  val L1 loss:  5.3602\n",
      "epoch:  18   step:  151   train loss:  3.341104030609131  val loss:  5.285750389099121  val L1 loss:  5.7688\n",
      "epoch:  18   step:  152   train loss:  2.845552444458008  val loss:  5.359444618225098  val L1 loss:  5.8472\n",
      "epoch:  18   step:  153   train loss:  3.4097821712493896  val loss:  5.326930046081543  val L1 loss:  5.8144\n",
      "epoch:  18   step:  154   train loss:  4.114124774932861  val loss:  5.141640663146973  val L1 loss:  5.6172\n",
      "epoch:  18   step:  155   train loss:  3.2733845710754395  val loss:  4.961108207702637  val L1 loss:  5.4309\n",
      "epoch:  18   step:  156   train loss:  3.8981175422668457  val loss:  4.998581886291504  val L1 loss:  5.4665\n",
      "epoch:  18   step:  157   train loss:  2.742924451828003  val loss:  4.936481952667236  val L1 loss:  5.4013\n",
      "epoch:  18   step:  158   train loss:  4.671529769897461  val loss:  4.8074822425842285  val L1 loss:  5.2792\n",
      "epoch:  18   step:  159   train loss:  3.8708019256591797  val loss:  4.75177526473999  val L1 loss:  5.2147\n",
      "epoch:  18   step:  160   train loss:  2.0341744422912598  val loss:  4.985380172729492  val L1 loss:  5.4623\n",
      "epoch:  18   step:  161   train loss:  2.5461316108703613  val loss:  5.419320583343506  val L1 loss:  5.9131\n",
      "epoch:  18   step:  162   train loss:  5.047771453857422  val loss:  5.851754188537598  val L1 loss:  6.3415\n",
      "epoch:  18   step:  163   train loss:  3.74662446975708  val loss:  5.794604301452637  val L1 loss:  6.2891\n",
      "epoch:  18   step:  164   train loss:  2.413365364074707  val loss:  5.3009867668151855  val L1 loss:  5.7799\n",
      "epoch:  18   step:  165   train loss:  3.525364875793457  val loss:  4.934360980987549  val L1 loss:  5.4027\n",
      "epoch:  18   step:  166   train loss:  2.382909059524536  val loss:  4.721782684326172  val L1 loss:  5.187\n",
      "epoch:  18   step:  167   train loss:  4.042593002319336  val loss:  4.701781272888184  val L1 loss:  5.1744\n",
      "epoch:  18   step:  168   train loss:  2.810682773590088  val loss:  4.724098205566406  val L1 loss:  5.1791\n",
      "epoch:  18   step:  169   train loss:  3.3984811305999756  val loss:  4.752949237823486  val L1 loss:  5.2155\n",
      "epoch:  18   step:  170   train loss:  2.32230806350708  val loss:  4.8298020362854  val L1 loss:  5.3129\n",
      "epoch:  18   step:  171   train loss:  2.759324312210083  val loss:  4.900732517242432  val L1 loss:  5.3822\n",
      "epoch:  18   step:  172   train loss:  2.441074848175049  val loss:  4.967116355895996  val L1 loss:  5.4478\n",
      "epoch:  18   step:  173   train loss:  4.430185317993164  val loss:  5.098515033721924  val L1 loss:  5.5848\n",
      "epoch:  18   step:  174   train loss:  3.8347434997558594  val loss:  5.212416648864746  val L1 loss:  5.6826\n",
      "epoch:  18   step:  175   train loss:  2.5952987670898438  val loss:  5.336426258087158  val L1 loss:  5.8017\n",
      "epoch:  18   step:  176   train loss:  2.929168462753296  val loss:  5.200416088104248  val L1 loss:  5.6777\n",
      "epoch:  18   step:  177   train loss:  2.917618989944458  val loss:  5.011666297912598  val L1 loss:  5.4759\n",
      "epoch:  18   step:  178   train loss:  2.6906700134277344  val loss:  4.995151996612549  val L1 loss:  5.4518\n",
      "epoch:  18   step:  179   train loss:  2.4059505462646484  val loss:  4.986922264099121  val L1 loss:  5.4665\n",
      "epoch:  18   step:  180   train loss:  2.3656601905822754  val loss:  4.942076206207275  val L1 loss:  5.419\n",
      "epoch:  18   step:  181   train loss:  4.374926567077637  val loss:  4.902947425842285  val L1 loss:  5.3848\n",
      "epoch:  18   step:  182   train loss:  2.7832512855529785  val loss:  4.930909156799316  val L1 loss:  5.4114\n",
      "epoch:  18   step:  183   train loss:  3.876866102218628  val loss:  4.920020580291748  val L1 loss:  5.3997\n",
      "epoch:  18   step:  184   train loss:  3.164670944213867  val loss:  4.840159893035889  val L1 loss:  5.2976\n",
      "epoch:  18   step:  185   train loss:  3.5041019916534424  val loss:  4.777095794677734  val L1 loss:  5.2514\n",
      "epoch:  18   step:  186   train loss:  2.7380661964416504  val loss:  5.0945725440979  val L1 loss:  5.5754\n",
      "epoch:  18   step:  187   train loss:  2.7656795978546143  val loss:  5.604777812957764  val L1 loss:  6.0676\n",
      "epoch:  18   step:  188   train loss:  3.547928810119629  val loss:  6.108911514282227  val L1 loss:  6.5847\n",
      "epoch:  18   step:  189   train loss:  2.668128490447998  val loss:  6.298804759979248  val L1 loss:  6.7771\n",
      "epoch:  18   step:  190   train loss:  1.834412693977356  val loss:  6.213490962982178  val L1 loss:  6.689\n",
      "epoch:  18   step:  191   train loss:  1.3339248895645142  val loss:  6.159934997558594  val L1 loss:  6.6474\n",
      "epoch:  18   step:  192   train loss:  3.6171517372131348  val loss:  5.940463542938232  val L1 loss:  6.4337\n",
      "epoch:  18   step:  193   train loss:  3.8003809452056885  val loss:  5.5296549797058105  val L1 loss:  6.0205\n",
      "epoch:  18   step:  194   train loss:  2.5852341651916504  val loss:  5.028385639190674  val L1 loss:  5.4997\n",
      "epoch:  18   step:  195   train loss:  3.0063424110412598  val loss:  4.746760368347168  val L1 loss:  5.2114\n",
      "epoch:  18   step:  196   train loss:  4.299814224243164  val loss:  4.6361470222473145  val L1 loss:  5.1051\n",
      "epoch:  18   step:  197   train loss:  3.0735902786254883  val loss:  4.670763969421387  val L1 loss:  5.1206\n",
      "epoch:  18   step:  198   train loss:  4.096563339233398  val loss:  4.858942985534668  val L1 loss:  5.3383\n",
      "epoch:  18   step:  199   train loss:  3.3822436332702637  val loss:  4.823148727416992  val L1 loss:  5.3031\n",
      "epoch:  18   step:  200   train loss:  4.942263126373291  val loss:  4.893625259399414  val L1 loss:  5.3729\n",
      "epoch:  18   step:  201   train loss:  1.9071425199508667  val loss:  4.930643081665039  val L1 loss:  5.4183\n",
      "epoch:  18   step:  202   train loss:  3.3668770790100098  val loss:  4.777442932128906  val L1 loss:  5.2515\n",
      "epoch:  18   step:  203   train loss:  2.836299180984497  val loss:  4.627437114715576  val L1 loss:  5.0994\n",
      "epoch:  18   step:  204   train loss:  2.7375175952911377  val loss:  4.534036636352539  val L1 loss:  5.0134\n",
      "epoch:  18   step:  205   train loss:  2.2611842155456543  val loss:  4.66838264465332  val L1 loss:  5.1284\n",
      "epoch:  18   step:  206   train loss:  5.665799140930176  val loss:  4.849829196929932  val L1 loss:  5.3275\n",
      "epoch:  18   step:  207   train loss:  1.7901585102081299  val loss:  4.8017988204956055  val L1 loss:  5.2571\n",
      "epoch:  18   step:  208   train loss:  2.2978010177612305  val loss:  4.525351524353027  val L1 loss:  4.9985\n",
      "epoch:  19   step:  0   train loss:  5.2842817306518555  val loss:  4.326267242431641  val L1 loss:  4.796\n",
      "epoch:  19   step:  1   train loss:  2.2694594860076904  val loss:  4.179372310638428  val L1 loss:  4.6386\n",
      "epoch:  19   step:  2   train loss:  2.7555365562438965  val loss:  4.1617279052734375  val L1 loss:  4.6154\n",
      "epoch:  19   step:  3   train loss:  2.6178054809570312  val loss:  4.266369342803955  val L1 loss:  4.7466\n",
      "epoch:  19   step:  4   train loss:  2.0863490104675293  val loss:  4.212751388549805  val L1 loss:  4.674\n",
      "epoch:  19   step:  5   train loss:  2.991241216659546  val loss:  4.179275035858154  val L1 loss:  4.6332\n",
      "epoch:  19   step:  6   train loss:  2.397310972213745  val loss:  4.181994915008545  val L1 loss:  4.6446\n",
      "epoch:  19   step:  7   train loss:  2.9515604972839355  val loss:  4.13064432144165  val L1 loss:  4.5934\n",
      "epoch:  19   step:  8   train loss:  1.9796303510665894  val loss:  4.089544773101807  val L1 loss:  4.5403\n",
      "epoch:  19   step:  9   train loss:  2.2594025135040283  val loss:  4.07618522644043  val L1 loss:  4.5423\n",
      "epoch:  19   step:  10   train loss:  4.339209079742432  val loss:  4.02620267868042  val L1 loss:  4.4986\n",
      "epoch:  19   step:  11   train loss:  2.652850866317749  val loss:  4.125885963439941  val L1 loss:  4.591\n",
      "epoch:  19   step:  12   train loss:  2.548092842102051  val loss:  4.303218364715576  val L1 loss:  4.79\n",
      "epoch:  19   step:  13   train loss:  2.839979648590088  val loss:  4.390158176422119  val L1 loss:  4.8741\n",
      "epoch:  19   step:  14   train loss:  2.779863119125366  val loss:  4.275704860687256  val L1 loss:  4.7687\n",
      "epoch:  19   step:  15   train loss:  3.426820755004883  val loss:  4.091279029846191  val L1 loss:  4.5661\n",
      "epoch:  19   step:  16   train loss:  2.7525579929351807  val loss:  3.9647819995880127  val L1 loss:  4.4053\n",
      "epoch:  19   step:  17   train loss:  4.164846420288086  val loss:  4.112309455871582  val L1 loss:  4.5694\n",
      "epoch:  19   step:  18   train loss:  2.8536977767944336  val loss:  4.092257022857666  val L1 loss:  4.5409\n",
      "epoch:  19   step:  19   train loss:  3.1627559661865234  val loss:  4.089946269989014  val L1 loss:  4.5465\n",
      "epoch:  19   step:  20   train loss:  3.257183074951172  val loss:  4.581282138824463  val L1 loss:  5.0516\n",
      "epoch:  19   step:  21   train loss:  3.014103412628174  val loss:  4.946702003479004  val L1 loss:  5.4212\n",
      "epoch:  19   step:  22   train loss:  2.0210330486297607  val loss:  4.9113006591796875  val L1 loss:  5.3782\n",
      "epoch:  19   step:  23   train loss:  3.655179977416992  val loss:  4.598813533782959  val L1 loss:  5.079\n",
      "epoch:  19   step:  24   train loss:  4.114104270935059  val loss:  4.357083320617676  val L1 loss:  4.8232\n",
      "epoch:  19   step:  25   train loss:  3.5389373302459717  val loss:  4.191593647003174  val L1 loss:  4.6551\n",
      "epoch:  19   step:  26   train loss:  1.9833002090454102  val loss:  4.083005428314209  val L1 loss:  4.5344\n",
      "epoch:  19   step:  27   train loss:  2.528451442718506  val loss:  4.092792987823486  val L1 loss:  4.5454\n",
      "epoch:  19   step:  28   train loss:  2.198908805847168  val loss:  4.22899055480957  val L1 loss:  4.692\n",
      "epoch:  19   step:  29   train loss:  3.9051902294158936  val loss:  4.333108901977539  val L1 loss:  4.8125\n",
      "epoch:  19   step:  30   train loss:  1.2360179424285889  val loss:  4.530149936676025  val L1 loss:  5.0166\n",
      "epoch:  19   step:  31   train loss:  2.554272174835205  val loss:  4.652679443359375  val L1 loss:  5.1387\n",
      "epoch:  19   step:  32   train loss:  1.4588730335235596  val loss:  4.7947001457214355  val L1 loss:  5.2775\n",
      "epoch:  19   step:  33   train loss:  2.5453402996063232  val loss:  4.948291301727295  val L1 loss:  5.4302\n",
      "epoch:  19   step:  34   train loss:  2.796645164489746  val loss:  5.155005931854248  val L1 loss:  5.6151\n",
      "epoch:  19   step:  35   train loss:  2.2505383491516113  val loss:  5.076900959014893  val L1 loss:  5.5383\n",
      "epoch:  19   step:  36   train loss:  3.0084574222564697  val loss:  4.837218284606934  val L1 loss:  5.3119\n",
      "epoch:  19   step:  37   train loss:  1.8506293296813965  val loss:  4.562276363372803  val L1 loss:  5.0466\n",
      "epoch:  19   step:  38   train loss:  3.1739840507507324  val loss:  4.419716835021973  val L1 loss:  4.9016\n",
      "epoch:  19   step:  39   train loss:  1.9179353713989258  val loss:  4.417571067810059  val L1 loss:  4.8981\n",
      "epoch:  19   step:  40   train loss:  2.7947893142700195  val loss:  4.374274730682373  val L1 loss:  4.8439\n",
      "epoch:  19   step:  41   train loss:  1.9111216068267822  val loss:  4.372441291809082  val L1 loss:  4.8338\n",
      "epoch:  19   step:  42   train loss:  2.262863874435425  val loss:  4.511767387390137  val L1 loss:  4.9826\n",
      "epoch:  19   step:  43   train loss:  2.632802724838257  val loss:  4.722983360290527  val L1 loss:  5.1958\n",
      "epoch:  19   step:  44   train loss:  4.568155288696289  val loss:  4.499513626098633  val L1 loss:  4.9657\n",
      "epoch:  19   step:  45   train loss:  2.546322822570801  val loss:  4.414605140686035  val L1 loss:  4.8653\n",
      "epoch:  19   step:  46   train loss:  3.4037137031555176  val loss:  4.777423858642578  val L1 loss:  5.2451\n",
      "epoch:  19   step:  47   train loss:  1.6537011861801147  val loss:  5.073622703552246  val L1 loss:  5.5511\n",
      "epoch:  19   step:  48   train loss:  5.310329437255859  val loss:  4.8732452392578125  val L1 loss:  5.3426\n",
      "epoch:  19   step:  49   train loss:  4.524534225463867  val loss:  4.595301628112793  val L1 loss:  5.0631\n",
      "epoch:  19   step:  50   train loss:  2.3429653644561768  val loss:  4.540195941925049  val L1 loss:  4.9931\n",
      "epoch:  19   step:  51   train loss:  3.8332600593566895  val loss:  4.554329872131348  val L1 loss:  5.004\n",
      "epoch:  19   step:  52   train loss:  2.498286247253418  val loss:  4.493075370788574  val L1 loss:  4.9515\n",
      "epoch:  19   step:  53   train loss:  2.399099349975586  val loss:  4.474976539611816  val L1 loss:  4.9509\n",
      "epoch:  19   step:  54   train loss:  2.971038818359375  val loss:  4.624284744262695  val L1 loss:  5.088\n",
      "epoch:  19   step:  55   train loss:  2.7636539936065674  val loss:  4.5000152587890625  val L1 loss:  4.9573\n",
      "epoch:  19   step:  56   train loss:  1.9587111473083496  val loss:  4.47432279586792  val L1 loss:  4.9364\n",
      "epoch:  19   step:  57   train loss:  2.2470686435699463  val loss:  4.588003635406494  val L1 loss:  5.0716\n",
      "epoch:  19   step:  58   train loss:  2.5205607414245605  val loss:  4.984745502471924  val L1 loss:  5.4461\n",
      "epoch:  19   step:  59   train loss:  2.882887363433838  val loss:  5.15164041519165  val L1 loss:  5.6139\n",
      "epoch:  19   step:  60   train loss:  4.0453290939331055  val loss:  4.742530345916748  val L1 loss:  5.2009\n",
      "epoch:  19   step:  61   train loss:  2.2555713653564453  val loss:  4.371403217315674  val L1 loss:  4.8539\n",
      "epoch:  19   step:  62   train loss:  4.504312992095947  val loss:  4.136185646057129  val L1 loss:  4.6047\n",
      "epoch:  19   step:  63   train loss:  2.8563051223754883  val loss:  4.102797985076904  val L1 loss:  4.5438\n",
      "epoch:  19   step:  64   train loss:  2.96718692779541  val loss:  4.059831619262695  val L1 loss:  4.5267\n",
      "epoch:  19   step:  65   train loss:  2.9019272327423096  val loss:  4.026876926422119  val L1 loss:  4.4936\n",
      "epoch:  19   step:  66   train loss:  2.2166218757629395  val loss:  3.9640398025512695  val L1 loss:  4.4176\n",
      "epoch:  19   step:  67   train loss:  4.385455131530762  val loss:  3.9721319675445557  val L1 loss:  4.4205\n",
      "epoch:  19   step:  68   train loss:  3.705474376678467  val loss:  3.9558606147766113  val L1 loss:  4.4061\n",
      "epoch:  19   step:  69   train loss:  2.647787094116211  val loss:  3.933199405670166  val L1 loss:  4.3834\n",
      "epoch:  19   step:  70   train loss:  3.5701332092285156  val loss:  3.877791404724121  val L1 loss:  4.3396\n",
      "epoch:  19   step:  71   train loss:  1.9210574626922607  val loss:  3.9389262199401855  val L1 loss:  4.4033\n",
      "epoch:  19   step:  72   train loss:  2.830070972442627  val loss:  4.203419208526611  val L1 loss:  4.6506\n",
      "epoch:  19   step:  73   train loss:  2.805415630340576  val loss:  4.596170425415039  val L1 loss:  5.0576\n",
      "epoch:  19   step:  74   train loss:  3.5305070877075195  val loss:  4.859803199768066  val L1 loss:  5.3252\n",
      "epoch:  19   step:  75   train loss:  1.4595696926116943  val loss:  4.812326908111572  val L1 loss:  5.2759\n",
      "epoch:  19   step:  76   train loss:  2.9670662879943848  val loss:  4.595921039581299  val L1 loss:  5.0208\n",
      "epoch:  19   step:  77   train loss:  4.379427433013916  val loss:  4.649296760559082  val L1 loss:  5.1411\n",
      "epoch:  19   step:  78   train loss:  2.2881782054901123  val loss:  4.827620029449463  val L1 loss:  5.3117\n",
      "epoch:  19   step:  79   train loss:  2.4267797470092773  val loss:  4.804675579071045  val L1 loss:  5.2857\n",
      "epoch:  19   step:  80   train loss:  2.987593412399292  val loss:  4.586825370788574  val L1 loss:  5.0385\n",
      "epoch:  19   step:  81   train loss:  1.7008824348449707  val loss:  4.494705677032471  val L1 loss:  4.9685\n",
      "epoch:  19   step:  82   train loss:  5.649648666381836  val loss:  4.546778678894043  val L1 loss:  5.0197\n",
      "epoch:  19   step:  83   train loss:  3.9582488536834717  val loss:  4.572890758514404  val L1 loss:  5.0503\n",
      "epoch:  19   step:  84   train loss:  5.267349720001221  val loss:  4.542145729064941  val L1 loss:  4.9941\n",
      "epoch:  19   step:  85   train loss:  2.062173366546631  val loss:  4.714450359344482  val L1 loss:  5.1762\n",
      "epoch:  19   step:  86   train loss:  3.3787841796875  val loss:  5.313918113708496  val L1 loss:  5.8047\n",
      "epoch:  19   step:  87   train loss:  2.2959067821502686  val loss:  5.701149940490723  val L1 loss:  6.1939\n",
      "epoch:  19   step:  88   train loss:  4.7179789543151855  val loss:  5.618462562561035  val L1 loss:  6.1156\n",
      "epoch:  19   step:  89   train loss:  4.192100524902344  val loss:  5.2056097984313965  val L1 loss:  5.7019\n",
      "epoch:  19   step:  90   train loss:  2.270643711090088  val loss:  4.788932800292969  val L1 loss:  5.2475\n",
      "epoch:  19   step:  91   train loss:  4.46904182434082  val loss:  4.567233562469482  val L1 loss:  5.0117\n",
      "epoch:  19   step:  92   train loss:  4.105144500732422  val loss:  4.495222091674805  val L1 loss:  4.9381\n",
      "epoch:  19   step:  93   train loss:  1.9333984851837158  val loss:  4.510921478271484  val L1 loss:  4.9613\n",
      "epoch:  19   step:  94   train loss:  1.9870914220809937  val loss:  4.397070407867432  val L1 loss:  4.846\n",
      "epoch:  19   step:  95   train loss:  2.4696807861328125  val loss:  4.307233810424805  val L1 loss:  4.7568\n",
      "epoch:  19   step:  96   train loss:  2.876269578933716  val loss:  4.221078872680664  val L1 loss:  4.6726\n",
      "epoch:  19   step:  97   train loss:  2.5515565872192383  val loss:  4.240952491760254  val L1 loss:  4.6967\n",
      "epoch:  19   step:  98   train loss:  3.1289165019989014  val loss:  4.246148586273193  val L1 loss:  4.7127\n",
      "epoch:  19   step:  99   train loss:  3.856835126876831  val loss:  4.243834972381592  val L1 loss:  4.7119\n",
      "epoch:  19   step:  100   train loss:  4.027657508850098  val loss:  4.284465312957764  val L1 loss:  4.751\n",
      "epoch:  19   step:  101   train loss:  4.38021183013916  val loss:  4.60820198059082  val L1 loss:  5.0956\n",
      "epoch:  19   step:  102   train loss:  2.307705879211426  val loss:  5.168520450592041  val L1 loss:  5.6487\n",
      "epoch:  19   step:  103   train loss:  3.3391923904418945  val loss:  5.400877475738525  val L1 loss:  5.8882\n",
      "epoch:  19   step:  104   train loss:  2.235105514526367  val loss:  5.089934825897217  val L1 loss:  5.5721\n",
      "epoch:  19   step:  105   train loss:  2.7585113048553467  val loss:  4.766549587249756  val L1 loss:  5.2306\n",
      "epoch:  19   step:  106   train loss:  2.3720407485961914  val loss:  4.313430309295654  val L1 loss:  4.7734\n",
      "epoch:  19   step:  107   train loss:  2.4189226627349854  val loss:  3.901560068130493  val L1 loss:  4.3403\n",
      "epoch:  19   step:  108   train loss:  4.811056137084961  val loss:  3.837930202484131  val L1 loss:  4.2824\n",
      "epoch:  19   step:  109   train loss:  2.8744421005249023  val loss:  3.8662614822387695  val L1 loss:  4.3203\n",
      "epoch:  19   step:  110   train loss:  3.428199291229248  val loss:  3.9872119426727295  val L1 loss:  4.4522\n",
      "epoch:  19   step:  111   train loss:  3.5016415119171143  val loss:  4.023684978485107  val L1 loss:  4.4956\n",
      "epoch:  19   step:  112   train loss:  2.8865609169006348  val loss:  4.00748872756958  val L1 loss:  4.4671\n",
      "epoch:  19   step:  113   train loss:  2.025521755218506  val loss:  3.9019036293029785  val L1 loss:  4.3465\n",
      "epoch:  19   step:  114   train loss:  2.5911881923675537  val loss:  4.0347089767456055  val L1 loss:  4.4916\n",
      "epoch:  19   step:  115   train loss:  4.824138164520264  val loss:  3.997222900390625  val L1 loss:  4.4364\n",
      "epoch:  19   step:  116   train loss:  3.6866586208343506  val loss:  3.9756364822387695  val L1 loss:  4.4306\n",
      "epoch:  19   step:  117   train loss:  5.076922416687012  val loss:  4.25068473815918  val L1 loss:  4.7171\n",
      "epoch:  19   step:  118   train loss:  2.2985267639160156  val loss:  4.772261142730713  val L1 loss:  5.2591\n",
      "epoch:  19   step:  119   train loss:  2.7785630226135254  val loss:  5.247835636138916  val L1 loss:  5.7427\n",
      "epoch:  19   step:  120   train loss:  2.988741159439087  val loss:  5.176047325134277  val L1 loss:  5.6707\n",
      "epoch:  19   step:  121   train loss:  2.8901100158691406  val loss:  4.741844177246094  val L1 loss:  5.2095\n",
      "epoch:  19   step:  122   train loss:  2.565911293029785  val loss:  4.407283306121826  val L1 loss:  4.8862\n",
      "epoch:  19   step:  123   train loss:  1.9640430212020874  val loss:  4.352345943450928  val L1 loss:  4.8143\n",
      "epoch:  19   step:  124   train loss:  2.3395724296569824  val loss:  4.389868259429932  val L1 loss:  4.8646\n",
      "epoch:  19   step:  125   train loss:  1.7382954359054565  val loss:  4.343514442443848  val L1 loss:  4.8134\n",
      "epoch:  19   step:  126   train loss:  3.221264362335205  val loss:  4.312406539916992  val L1 loss:  4.7787\n",
      "epoch:  19   step:  127   train loss:  3.9478092193603516  val loss:  4.216201305389404  val L1 loss:  4.6711\n",
      "epoch:  19   step:  128   train loss:  1.7573622465133667  val loss:  4.128300666809082  val L1 loss:  4.5686\n",
      "epoch:  19   step:  129   train loss:  2.748823881149292  val loss:  4.278389930725098  val L1 loss:  4.7327\n",
      "epoch:  19   step:  130   train loss:  2.1314382553100586  val loss:  4.416700839996338  val L1 loss:  4.883\n",
      "epoch:  19   step:  131   train loss:  2.906618118286133  val loss:  4.376352310180664  val L1 loss:  4.8444\n",
      "epoch:  19   step:  132   train loss:  3.2096614837646484  val loss:  4.3181586265563965  val L1 loss:  4.7837\n",
      "epoch:  19   step:  133   train loss:  1.727636694908142  val loss:  4.618582248687744  val L1 loss:  5.0945\n",
      "epoch:  19   step:  134   train loss:  4.24746561050415  val loss:  4.783876895904541  val L1 loss:  5.278\n",
      "epoch:  19   step:  135   train loss:  3.6712822914123535  val loss:  4.637736797332764  val L1 loss:  5.1187\n",
      "epoch:  19   step:  136   train loss:  3.3544936180114746  val loss:  4.470966339111328  val L1 loss:  4.9475\n",
      "epoch:  19   step:  137   train loss:  3.5790412425994873  val loss:  4.435812950134277  val L1 loss:  4.9223\n",
      "epoch:  19   step:  138   train loss:  2.2761776447296143  val loss:  4.510349750518799  val L1 loss:  4.9592\n",
      "epoch:  19   step:  139   train loss:  3.5799612998962402  val loss:  4.561929702758789  val L1 loss:  5.0349\n",
      "epoch:  19   step:  140   train loss:  4.674190521240234  val loss:  4.642406463623047  val L1 loss:  5.1226\n",
      "epoch:  19   step:  141   train loss:  3.5521256923675537  val loss:  4.59501838684082  val L1 loss:  5.0812\n",
      "epoch:  19   step:  142   train loss:  2.4957637786865234  val loss:  4.495743751525879  val L1 loss:  4.9689\n",
      "epoch:  19   step:  143   train loss:  3.4697985649108887  val loss:  4.3943891525268555  val L1 loss:  4.8597\n",
      "epoch:  19   step:  144   train loss:  2.8526484966278076  val loss:  4.372308254241943  val L1 loss:  4.8339\n",
      "epoch:  19   step:  145   train loss:  3.6867337226867676  val loss:  4.384865760803223  val L1 loss:  4.8296\n",
      "epoch:  19   step:  146   train loss:  3.0689780712127686  val loss:  4.565854549407959  val L1 loss:  5.0273\n",
      "epoch:  19   step:  147   train loss:  2.555959939956665  val loss:  4.97451639175415  val L1 loss:  5.4533\n",
      "epoch:  19   step:  148   train loss:  2.9324910640716553  val loss:  5.282539367675781  val L1 loss:  5.7626\n",
      "epoch:  19   step:  149   train loss:  4.536252021789551  val loss:  5.258944988250732  val L1 loss:  5.7465\n",
      "epoch:  19   step:  150   train loss:  2.420322895050049  val loss:  4.96515417098999  val L1 loss:  5.4493\n",
      "epoch:  19   step:  151   train loss:  3.200416088104248  val loss:  4.485016822814941  val L1 loss:  4.9568\n",
      "epoch:  19   step:  152   train loss:  3.2247166633605957  val loss:  4.694896221160889  val L1 loss:  5.1631\n",
      "epoch:  19   step:  153   train loss:  5.211158752441406  val loss:  5.07312536239624  val L1 loss:  5.554\n",
      "epoch:  19   step:  154   train loss:  3.1341609954833984  val loss:  4.870576858520508  val L1 loss:  5.3376\n",
      "epoch:  19   step:  155   train loss:  4.111815929412842  val loss:  4.4853715896606445  val L1 loss:  4.933\n",
      "epoch:  19   step:  156   train loss:  2.6313295364379883  val loss:  4.5028557777404785  val L1 loss:  4.9792\n",
      "epoch:  19   step:  157   train loss:  4.350704669952393  val loss:  4.532744884490967  val L1 loss:  5.0088\n",
      "epoch:  19   step:  158   train loss:  2.976006507873535  val loss:  4.346189975738525  val L1 loss:  4.8366\n",
      "epoch:  19   step:  159   train loss:  1.9642199277877808  val loss:  4.3028717041015625  val L1 loss:  4.7305\n",
      "epoch:  19   step:  160   train loss:  3.2601447105407715  val loss:  4.605703830718994  val L1 loss:  5.0783\n",
      "epoch:  19   step:  161   train loss:  1.8013570308685303  val loss:  4.934529781341553  val L1 loss:  5.4097\n",
      "epoch:  19   step:  162   train loss:  2.289623737335205  val loss:  5.331092357635498  val L1 loss:  5.81\n",
      "epoch:  19   step:  163   train loss:  1.9546290636062622  val loss:  5.26252555847168  val L1 loss:  5.7401\n",
      "epoch:  19   step:  164   train loss:  1.9991085529327393  val loss:  5.223140716552734  val L1 loss:  5.703\n",
      "epoch:  19   step:  165   train loss:  3.530285120010376  val loss:  5.0467329025268555  val L1 loss:  5.5211\n",
      "epoch:  19   step:  166   train loss:  3.6791179180145264  val loss:  4.73505973815918  val L1 loss:  5.1967\n",
      "epoch:  19   step:  167   train loss:  2.9892702102661133  val loss:  4.4642181396484375  val L1 loss:  4.9436\n",
      "epoch:  19   step:  168   train loss:  2.306671619415283  val loss:  4.334543228149414  val L1 loss:  4.8079\n",
      "epoch:  19   step:  169   train loss:  3.219311475753784  val loss:  4.350444316864014  val L1 loss:  4.8156\n",
      "epoch:  19   step:  170   train loss:  4.54708194732666  val loss:  4.39254903793335  val L1 loss:  4.8574\n",
      "epoch:  19   step:  171   train loss:  3.683739423751831  val loss:  4.406808853149414  val L1 loss:  4.8868\n",
      "epoch:  19   step:  172   train loss:  2.48494029045105  val loss:  4.406599044799805  val L1 loss:  4.8911\n",
      "epoch:  19   step:  173   train loss:  2.1814279556274414  val loss:  4.445545673370361  val L1 loss:  4.9215\n",
      "epoch:  19   step:  174   train loss:  1.8562136888504028  val loss:  4.477071285247803  val L1 loss:  4.9368\n",
      "epoch:  19   step:  175   train loss:  2.6947786808013916  val loss:  4.386222839355469  val L1 loss:  4.8551\n",
      "epoch:  19   step:  176   train loss:  2.9192841053009033  val loss:  4.306243896484375  val L1 loss:  4.7699\n",
      "epoch:  19   step:  177   train loss:  2.4658894538879395  val loss:  4.323344707489014  val L1 loss:  4.8064\n",
      "epoch:  19   step:  178   train loss:  2.519681453704834  val loss:  4.411212921142578  val L1 loss:  4.8888\n",
      "epoch:  19   step:  179   train loss:  2.7582459449768066  val loss:  4.435609817504883  val L1 loss:  4.9179\n",
      "epoch:  19   step:  180   train loss:  2.4912667274475098  val loss:  4.4794487953186035  val L1 loss:  4.9588\n",
      "epoch:  19   step:  181   train loss:  3.138099193572998  val loss:  4.480896949768066  val L1 loss:  4.9361\n",
      "epoch:  19   step:  182   train loss:  3.3276000022888184  val loss:  4.537118434906006  val L1 loss:  5.0029\n",
      "epoch:  19   step:  183   train loss:  2.4434404373168945  val loss:  4.55340576171875  val L1 loss:  5.0117\n",
      "epoch:  19   step:  184   train loss:  3.9090237617492676  val loss:  4.573906898498535  val L1 loss:  5.0385\n",
      "epoch:  19   step:  185   train loss:  4.86644172668457  val loss:  4.7175445556640625  val L1 loss:  5.166\n",
      "epoch:  19   step:  186   train loss:  2.8423190116882324  val loss:  4.8915486335754395  val L1 loss:  5.3721\n",
      "epoch:  19   step:  187   train loss:  3.480874538421631  val loss:  4.955409049987793  val L1 loss:  5.432\n",
      "epoch:  19   step:  188   train loss:  2.8872299194335938  val loss:  5.014319896697998  val L1 loss:  5.4771\n",
      "epoch:  19   step:  189   train loss:  2.759366989135742  val loss:  4.806328773498535  val L1 loss:  5.2874\n",
      "epoch:  19   step:  190   train loss:  1.5032198429107666  val loss:  4.69835090637207  val L1 loss:  5.1362\n",
      "epoch:  19   step:  191   train loss:  3.201338768005371  val loss:  4.908181190490723  val L1 loss:  5.3892\n",
      "epoch:  19   step:  192   train loss:  3.7315123081207275  val loss:  5.1210150718688965  val L1 loss:  5.6023\n",
      "epoch:  19   step:  193   train loss:  4.294814109802246  val loss:  4.838534832000732  val L1 loss:  5.3016\n",
      "epoch:  19   step:  194   train loss:  3.157830238342285  val loss:  4.5314555168151855  val L1 loss:  5.0046\n",
      "epoch:  19   step:  195   train loss:  4.1927900314331055  val loss:  4.432713985443115  val L1 loss:  4.8939\n",
      "epoch:  19   step:  196   train loss:  2.763784170150757  val loss:  4.436819553375244  val L1 loss:  4.9097\n",
      "epoch:  19   step:  197   train loss:  2.24725341796875  val loss:  4.363722801208496  val L1 loss:  4.8207\n",
      "epoch:  19   step:  198   train loss:  3.3049538135528564  val loss:  4.2989373207092285  val L1 loss:  4.7674\n",
      "epoch:  19   step:  199   train loss:  2.132673740386963  val loss:  4.298434257507324  val L1 loss:  4.7645\n",
      "epoch:  19   step:  200   train loss:  3.524819850921631  val loss:  4.237424850463867  val L1 loss:  4.6918\n",
      "epoch:  19   step:  201   train loss:  1.3962301015853882  val loss:  4.1961164474487305  val L1 loss:  4.6535\n",
      "epoch:  19   step:  202   train loss:  3.370180130004883  val loss:  4.0416765213012695  val L1 loss:  4.5109\n",
      "epoch:  19   step:  203   train loss:  1.3440423011779785  val loss:  4.049484729766846  val L1 loss:  4.5041\n",
      "epoch:  19   step:  204   train loss:  2.915130138397217  val loss:  4.2242431640625  val L1 loss:  4.6934\n",
      "epoch:  19   step:  205   train loss:  3.4192922115325928  val loss:  4.2195563316345215  val L1 loss:  4.691\n",
      "epoch:  19   step:  206   train loss:  3.0561699867248535  val loss:  4.2374186515808105  val L1 loss:  4.6858\n",
      "epoch:  19   step:  207   train loss:  2.0520033836364746  val loss:  4.3297119140625  val L1 loss:  4.8066\n",
      "epoch:  19   step:  208   train loss:  5.244143486022949  val loss:  4.549198627471924  val L1 loss:  5.0165\n",
      "epoch:  20   step:  0   train loss:  2.200627088546753  val loss:  4.703647136688232  val L1 loss:  5.1729\n",
      "epoch:  20   step:  1   train loss:  3.0797595977783203  val loss:  4.749762058258057  val L1 loss:  5.2156\n",
      "epoch:  20   step:  2   train loss:  4.685512065887451  val loss:  4.790831565856934  val L1 loss:  5.2395\n",
      "epoch:  20   step:  3   train loss:  2.3672947883605957  val loss:  4.781949520111084  val L1 loss:  5.239\n",
      "epoch:  20   step:  4   train loss:  2.6348118782043457  val loss:  4.717749118804932  val L1 loss:  5.1769\n",
      "epoch:  20   step:  5   train loss:  2.4076576232910156  val loss:  4.670253753662109  val L1 loss:  5.1231\n",
      "epoch:  20   step:  6   train loss:  1.9767093658447266  val loss:  4.557299613952637  val L1 loss:  5.0058\n",
      "epoch:  20   step:  7   train loss:  2.813051223754883  val loss:  4.515020847320557  val L1 loss:  4.9902\n",
      "epoch:  20   step:  8   train loss:  2.075549840927124  val loss:  4.555525302886963  val L1 loss:  5.0121\n",
      "epoch:  20   step:  9   train loss:  2.7704968452453613  val loss:  4.65617036819458  val L1 loss:  5.1124\n",
      "epoch:  20   step:  10   train loss:  1.5062017440795898  val loss:  4.742765426635742  val L1 loss:  5.2262\n",
      "epoch:  20   step:  11   train loss:  4.063230514526367  val loss:  4.722966194152832  val L1 loss:  5.2078\n",
      "epoch:  20   step:  12   train loss:  1.2395515441894531  val loss:  4.686361789703369  val L1 loss:  5.1608\n",
      "epoch:  20   step:  13   train loss:  2.661681652069092  val loss:  4.577352523803711  val L1 loss:  5.0346\n",
      "epoch:  20   step:  14   train loss:  2.8343024253845215  val loss:  4.490522861480713  val L1 loss:  4.9443\n",
      "epoch:  20   step:  15   train loss:  1.2292499542236328  val loss:  4.4283857345581055  val L1 loss:  4.8893\n",
      "epoch:  20   step:  16   train loss:  2.881450653076172  val loss:  4.344488620758057  val L1 loss:  4.7971\n",
      "epoch:  20   step:  17   train loss:  2.857246160507202  val loss:  4.322917461395264  val L1 loss:  4.7633\n",
      "epoch:  20   step:  18   train loss:  1.8395605087280273  val loss:  4.274590969085693  val L1 loss:  4.7147\n",
      "epoch:  20   step:  19   train loss:  2.448984146118164  val loss:  4.243131637573242  val L1 loss:  4.6951\n",
      "epoch:  20   step:  20   train loss:  3.3518171310424805  val loss:  4.128412246704102  val L1 loss:  4.5755\n",
      "epoch:  20   step:  21   train loss:  1.7147178649902344  val loss:  4.235539436340332  val L1 loss:  4.7032\n",
      "epoch:  20   step:  22   train loss:  2.0969414710998535  val loss:  4.577264308929443  val L1 loss:  5.0423\n",
      "epoch:  20   step:  23   train loss:  1.8491239547729492  val loss:  4.600870609283447  val L1 loss:  5.0789\n",
      "epoch:  20   step:  24   train loss:  1.7998411655426025  val loss:  4.5955328941345215  val L1 loss:  5.058\n",
      "epoch:  20   step:  25   train loss:  3.2161240577697754  val loss:  4.702918529510498  val L1 loss:  5.173\n",
      "epoch:  20   step:  26   train loss:  4.692188739776611  val loss:  4.520357608795166  val L1 loss:  4.9868\n",
      "epoch:  20   step:  27   train loss:  3.8813064098358154  val loss:  4.325567245483398  val L1 loss:  4.7791\n",
      "epoch:  20   step:  28   train loss:  2.0883193016052246  val loss:  4.303628921508789  val L1 loss:  4.765\n",
      "epoch:  20   step:  29   train loss:  2.2760207653045654  val loss:  4.359886169433594  val L1 loss:  4.8245\n",
      "epoch:  20   step:  30   train loss:  2.3912668228149414  val loss:  4.523243427276611  val L1 loss:  4.983\n",
      "epoch:  20   step:  31   train loss:  2.9012770652770996  val loss:  4.9857892990112305  val L1 loss:  5.4729\n",
      "epoch:  20   step:  32   train loss:  2.275442361831665  val loss:  5.781775951385498  val L1 loss:  6.2805\n",
      "epoch:  20   step:  33   train loss:  2.4266767501831055  val loss:  6.101596355438232  val L1 loss:  6.6011\n",
      "epoch:  20   step:  34   train loss:  3.5751500129699707  val loss:  6.104472637176514  val L1 loss:  6.603\n",
      "epoch:  20   step:  35   train loss:  3.252178907394409  val loss:  5.459517478942871  val L1 loss:  5.9207\n",
      "epoch:  20   step:  36   train loss:  3.0626468658447266  val loss:  4.726749420166016  val L1 loss:  5.1939\n",
      "epoch:  20   step:  37   train loss:  2.457603931427002  val loss:  4.357875347137451  val L1 loss:  4.8203\n",
      "epoch:  20   step:  38   train loss:  2.7739853858947754  val loss:  4.454973220825195  val L1 loss:  4.9201\n",
      "epoch:  20   step:  39   train loss:  2.676516532897949  val loss:  4.7594122886657715  val L1 loss:  5.2359\n",
      "epoch:  20   step:  40   train loss:  3.346435308456421  val loss:  5.129800796508789  val L1 loss:  5.6128\n",
      "epoch:  20   step:  41   train loss:  2.117116928100586  val loss:  5.242098808288574  val L1 loss:  5.7136\n",
      "epoch:  20   step:  42   train loss:  4.0888261795043945  val loss:  5.199189186096191  val L1 loss:  5.6915\n",
      "epoch:  20   step:  43   train loss:  2.3343148231506348  val loss:  5.0806474685668945  val L1 loss:  5.5614\n",
      "epoch:  20   step:  44   train loss:  3.481441020965576  val loss:  4.875789165496826  val L1 loss:  5.3667\n",
      "epoch:  20   step:  45   train loss:  3.384474277496338  val loss:  4.679541110992432  val L1 loss:  5.1576\n",
      "epoch:  20   step:  46   train loss:  3.212299346923828  val loss:  4.490420341491699  val L1 loss:  4.9519\n",
      "epoch:  20   step:  47   train loss:  2.486489772796631  val loss:  4.305647373199463  val L1 loss:  4.7582\n",
      "epoch:  20   step:  48   train loss:  1.6166356801986694  val loss:  4.254975318908691  val L1 loss:  4.714\n",
      "epoch:  20   step:  49   train loss:  2.9282689094543457  val loss:  4.210032939910889  val L1 loss:  4.6707\n",
      "epoch:  20   step:  50   train loss:  2.4891695976257324  val loss:  4.221322536468506  val L1 loss:  4.6822\n",
      "epoch:  20   step:  51   train loss:  3.536627769470215  val loss:  4.260754108428955  val L1 loss:  4.7311\n",
      "epoch:  20   step:  52   train loss:  3.20231294631958  val loss:  4.383103847503662  val L1 loss:  4.8419\n",
      "epoch:  20   step:  53   train loss:  2.740541696548462  val loss:  4.648989677429199  val L1 loss:  5.1217\n",
      "epoch:  20   step:  54   train loss:  1.9873130321502686  val loss:  4.594529628753662  val L1 loss:  5.0674\n",
      "epoch:  20   step:  55   train loss:  2.4871926307678223  val loss:  4.340179443359375  val L1 loss:  4.8111\n",
      "epoch:  20   step:  56   train loss:  3.384697437286377  val loss:  4.262893199920654  val L1 loss:  4.7066\n",
      "epoch:  20   step:  57   train loss:  3.3010551929473877  val loss:  4.402022838592529  val L1 loss:  4.8762\n",
      "epoch:  20   step:  58   train loss:  2.4015614986419678  val loss:  4.489386081695557  val L1 loss:  4.9785\n",
      "epoch:  20   step:  59   train loss:  3.406050682067871  val loss:  4.43722677230835  val L1 loss:  4.9251\n",
      "epoch:  20   step:  60   train loss:  4.394360065460205  val loss:  4.518014430999756  val L1 loss:  5.0029\n",
      "epoch:  20   step:  61   train loss:  3.9092674255371094  val loss:  4.7321672439575195  val L1 loss:  5.2162\n",
      "epoch:  20   step:  62   train loss:  3.895813465118408  val loss:  4.944960117340088  val L1 loss:  5.4175\n",
      "epoch:  20   step:  63   train loss:  2.5614423751831055  val loss:  5.25189733505249  val L1 loss:  5.7165\n",
      "epoch:  20   step:  64   train loss:  2.4179301261901855  val loss:  5.359407901763916  val L1 loss:  5.8088\n",
      "epoch:  20   step:  65   train loss:  2.9636335372924805  val loss:  5.491147518157959  val L1 loss:  5.9442\n",
      "epoch:  20   step:  66   train loss:  2.2447547912597656  val loss:  5.21836519241333  val L1 loss:  5.6504\n",
      "epoch:  20   step:  67   train loss:  2.133667469024658  val loss:  4.955874443054199  val L1 loss:  5.4093\n",
      "epoch:  20   step:  68   train loss:  2.238579750061035  val loss:  4.8251872062683105  val L1 loss:  5.2967\n",
      "epoch:  20   step:  69   train loss:  3.053121328353882  val loss:  4.736715793609619  val L1 loss:  5.1876\n",
      "epoch:  20   step:  70   train loss:  1.731076955795288  val loss:  4.663943767547607  val L1 loss:  5.1156\n",
      "epoch:  20   step:  71   train loss:  1.9284512996673584  val loss:  4.666015148162842  val L1 loss:  5.1216\n",
      "epoch:  20   step:  72   train loss:  2.4232277870178223  val loss:  4.6663618087768555  val L1 loss:  5.1217\n",
      "epoch:  20   step:  73   train loss:  2.1931581497192383  val loss:  4.75098991394043  val L1 loss:  5.2226\n",
      "epoch:  20   step:  74   train loss:  2.7462944984436035  val loss:  4.697579383850098  val L1 loss:  5.1775\n",
      "epoch:  20   step:  75   train loss:  2.4403092861175537  val loss:  4.616449356079102  val L1 loss:  5.0916\n",
      "epoch:  20   step:  76   train loss:  4.316457748413086  val loss:  4.452949523925781  val L1 loss:  4.9295\n",
      "epoch:  20   step:  77   train loss:  1.7216601371765137  val loss:  4.402858734130859  val L1 loss:  4.8871\n",
      "epoch:  20   step:  78   train loss:  1.6077663898468018  val loss:  4.361364841461182  val L1 loss:  4.8251\n",
      "epoch:  20   step:  79   train loss:  3.4299983978271484  val loss:  4.3104472160339355  val L1 loss:  4.777\n",
      "epoch:  20   step:  80   train loss:  2.293180227279663  val loss:  4.260420322418213  val L1 loss:  4.7353\n",
      "epoch:  20   step:  81   train loss:  3.69500732421875  val loss:  4.420165061950684  val L1 loss:  4.8756\n",
      "epoch:  20   step:  82   train loss:  2.3717713356018066  val loss:  4.728999614715576  val L1 loss:  5.1995\n",
      "epoch:  20   step:  83   train loss:  2.747929573059082  val loss:  4.749406814575195  val L1 loss:  5.2182\n",
      "epoch:  20   step:  84   train loss:  2.877291679382324  val loss:  4.747718334197998  val L1 loss:  5.2167\n",
      "epoch:  20   step:  85   train loss:  3.3291208744049072  val loss:  4.6793742179870605  val L1 loss:  5.1514\n",
      "epoch:  20   step:  86   train loss:  2.0296995639801025  val loss:  4.384394645690918  val L1 loss:  4.8583\n",
      "epoch:  20   step:  87   train loss:  1.4926196336746216  val loss:  4.337605953216553  val L1 loss:  4.7858\n",
      "epoch:  20   step:  88   train loss:  2.0647902488708496  val loss:  4.744195938110352  val L1 loss:  5.2239\n",
      "epoch:  20   step:  89   train loss:  2.92461895942688  val loss:  4.914922714233398  val L1 loss:  5.3972\n",
      "epoch:  20   step:  90   train loss:  2.7135324478149414  val loss:  4.815062522888184  val L1 loss:  5.3075\n",
      "epoch:  20   step:  91   train loss:  1.6225327253341675  val loss:  4.599432468414307  val L1 loss:  5.0447\n",
      "epoch:  20   step:  92   train loss:  1.7307288646697998  val loss:  4.533314228057861  val L1 loss:  4.9915\n",
      "epoch:  20   step:  93   train loss:  2.5511579513549805  val loss:  4.552940368652344  val L1 loss:  5.001\n",
      "epoch:  20   step:  94   train loss:  3.7013392448425293  val loss:  4.755070209503174  val L1 loss:  5.2226\n",
      "epoch:  20   step:  95   train loss:  2.9685380458831787  val loss:  4.800201416015625  val L1 loss:  5.2784\n",
      "epoch:  20   step:  96   train loss:  2.5316200256347656  val loss:  4.74469518661499  val L1 loss:  5.2147\n",
      "epoch:  20   step:  97   train loss:  2.279477596282959  val loss:  4.635809898376465  val L1 loss:  5.0996\n",
      "epoch:  20   step:  98   train loss:  3.251596450805664  val loss:  4.750799655914307  val L1 loss:  5.2171\n",
      "epoch:  20   step:  99   train loss:  3.288285255432129  val loss:  4.910353660583496  val L1 loss:  5.3765\n",
      "epoch:  20   step:  100   train loss:  2.4031214714050293  val loss:  5.046520709991455  val L1 loss:  5.5254\n",
      "epoch:  20   step:  101   train loss:  2.3981478214263916  val loss:  5.03169059753418  val L1 loss:  5.4888\n",
      "epoch:  20   step:  102   train loss:  1.7731066942214966  val loss:  5.048071384429932  val L1 loss:  5.5122\n",
      "epoch:  20   step:  103   train loss:  3.4786932468414307  val loss:  5.200756072998047  val L1 loss:  5.6847\n",
      "epoch:  20   step:  104   train loss:  2.26566219329834  val loss:  5.37642765045166  val L1 loss:  5.8677\n",
      "epoch:  20   step:  105   train loss:  3.1803038120269775  val loss:  5.318314075469971  val L1 loss:  5.8047\n",
      "epoch:  20   step:  106   train loss:  3.9363667964935303  val loss:  5.103973388671875  val L1 loss:  5.5865\n",
      "epoch:  20   step:  107   train loss:  3.9462060928344727  val loss:  4.918819904327393  val L1 loss:  5.382\n",
      "epoch:  20   step:  108   train loss:  1.7830004692077637  val loss:  4.940670490264893  val L1 loss:  5.4067\n",
      "epoch:  20   step:  109   train loss:  3.1097865104675293  val loss:  5.065410614013672  val L1 loss:  5.5446\n",
      "epoch:  20   step:  110   train loss:  1.9427295923233032  val loss:  4.904775142669678  val L1 loss:  5.3715\n",
      "epoch:  20   step:  111   train loss:  2.6303775310516357  val loss:  4.671266555786133  val L1 loss:  5.1452\n",
      "epoch:  20   step:  112   train loss:  2.5457839965820312  val loss:  4.732810020446777  val L1 loss:  5.2042\n",
      "epoch:  20   step:  113   train loss:  2.838182210922241  val loss:  4.936440467834473  val L1 loss:  5.4265\n",
      "epoch:  20   step:  114   train loss:  3.9069294929504395  val loss:  4.957272529602051  val L1 loss:  5.4434\n",
      "epoch:  20   step:  115   train loss:  2.7620136737823486  val loss:  4.627621173858643  val L1 loss:  5.0994\n",
      "epoch:  20   step:  116   train loss:  5.405713081359863  val loss:  4.431818962097168  val L1 loss:  4.8935\n",
      "epoch:  20   step:  117   train loss:  3.162518262863159  val loss:  4.521016597747803  val L1 loss:  4.9755\n",
      "epoch:  20   step:  118   train loss:  2.3226265907287598  val loss:  5.201869964599609  val L1 loss:  5.6884\n",
      "epoch:  20   step:  119   train loss:  2.4487361907958984  val loss:  5.264443397521973  val L1 loss:  5.7561\n",
      "epoch:  20   step:  120   train loss:  6.398683071136475  val loss:  4.559134006500244  val L1 loss:  4.9897\n",
      "epoch:  20   step:  121   train loss:  2.639768600463867  val loss:  4.360079765319824  val L1 loss:  4.8273\n",
      "epoch:  20   step:  122   train loss:  3.0491316318511963  val loss:  4.5947747230529785  val L1 loss:  5.0631\n",
      "epoch:  20   step:  123   train loss:  1.8072302341461182  val loss:  4.799569606781006  val L1 loss:  5.2896\n",
      "epoch:  20   step:  124   train loss:  2.5887975692749023  val loss:  4.564464569091797  val L1 loss:  5.0428\n",
      "epoch:  20   step:  125   train loss:  2.663910388946533  val loss:  4.395993709564209  val L1 loss:  4.8721\n",
      "epoch:  20   step:  126   train loss:  2.509848117828369  val loss:  4.343348979949951  val L1 loss:  4.8057\n",
      "epoch:  20   step:  127   train loss:  2.9318623542785645  val loss:  4.305201530456543  val L1 loss:  4.7802\n",
      "epoch:  20   step:  128   train loss:  3.6449320316314697  val loss:  4.311986446380615  val L1 loss:  4.7688\n",
      "epoch:  20   step:  129   train loss:  3.078099489212036  val loss:  4.376046180725098  val L1 loss:  4.8494\n",
      "epoch:  20   step:  130   train loss:  4.096163749694824  val loss:  4.969425678253174  val L1 loss:  5.4509\n",
      "epoch:  20   step:  131   train loss:  2.329873561859131  val loss:  5.314913272857666  val L1 loss:  5.8139\n",
      "epoch:  20   step:  132   train loss:  2.1634721755981445  val loss:  5.147528171539307  val L1 loss:  5.6313\n",
      "epoch:  20   step:  133   train loss:  3.2376842498779297  val loss:  4.929964065551758  val L1 loss:  5.4144\n",
      "epoch:  20   step:  134   train loss:  3.5640859603881836  val loss:  4.512439250946045  val L1 loss:  4.9703\n",
      "epoch:  20   step:  135   train loss:  2.2233712673187256  val loss:  4.410026550292969  val L1 loss:  4.8807\n",
      "epoch:  20   step:  136   train loss:  3.8992605209350586  val loss:  4.533211708068848  val L1 loss:  4.9955\n",
      "epoch:  20   step:  137   train loss:  2.3152122497558594  val loss:  4.6781110763549805  val L1 loss:  5.1542\n",
      "epoch:  20   step:  138   train loss:  5.277408599853516  val loss:  4.502463340759277  val L1 loss:  4.9704\n",
      "epoch:  20   step:  139   train loss:  2.5525381565093994  val loss:  4.2106428146362305  val L1 loss:  4.6849\n",
      "epoch:  20   step:  140   train loss:  2.703523635864258  val loss:  4.277468681335449  val L1 loss:  4.731\n",
      "epoch:  20   step:  141   train loss:  3.2474894523620605  val loss:  4.571509838104248  val L1 loss:  5.0464\n",
      "epoch:  20   step:  142   train loss:  2.541374921798706  val loss:  4.595195293426514  val L1 loss:  5.0671\n",
      "epoch:  20   step:  143   train loss:  3.2096195220947266  val loss:  4.223240852355957  val L1 loss:  4.692\n",
      "epoch:  20   step:  144   train loss:  3.443286895751953  val loss:  4.023232936859131  val L1 loss:  4.491\n",
      "epoch:  20   step:  145   train loss:  1.7630677223205566  val loss:  4.242353916168213  val L1 loss:  4.7056\n",
      "epoch:  20   step:  146   train loss:  3.27862811088562  val loss:  4.317497730255127  val L1 loss:  4.7763\n",
      "epoch:  20   step:  147   train loss:  3.5326528549194336  val loss:  4.096857070922852  val L1 loss:  4.5648\n",
      "epoch:  20   step:  148   train loss:  1.2913532257080078  val loss:  3.96466064453125  val L1 loss:  4.4196\n",
      "epoch:  20   step:  149   train loss:  3.9553732872009277  val loss:  4.039491653442383  val L1 loss:  4.4978\n",
      "epoch:  20   step:  150   train loss:  1.41890549659729  val loss:  4.197241306304932  val L1 loss:  4.6685\n",
      "epoch:  20   step:  151   train loss:  2.388230562210083  val loss:  4.09013557434082  val L1 loss:  4.5395\n",
      "epoch:  20   step:  152   train loss:  2.072755813598633  val loss:  3.9674296379089355  val L1 loss:  4.4265\n",
      "epoch:  20   step:  153   train loss:  3.004065990447998  val loss:  3.9834659099578857  val L1 loss:  4.4377\n",
      "epoch:  20   step:  154   train loss:  4.062091827392578  val loss:  4.085255146026611  val L1 loss:  4.5553\n",
      "epoch:  20   step:  155   train loss:  4.990168571472168  val loss:  4.077301979064941  val L1 loss:  4.5351\n",
      "epoch:  20   step:  156   train loss:  3.232714891433716  val loss:  4.002634048461914  val L1 loss:  4.4472\n",
      "epoch:  20   step:  157   train loss:  2.0723817348480225  val loss:  3.9475865364074707  val L1 loss:  4.4087\n",
      "epoch:  20   step:  158   train loss:  2.733858585357666  val loss:  4.118828773498535  val L1 loss:  4.5754\n",
      "epoch:  20   step:  159   train loss:  1.6028034687042236  val loss:  4.392091751098633  val L1 loss:  4.8699\n",
      "epoch:  20   step:  160   train loss:  3.470381021499634  val loss:  4.388331413269043  val L1 loss:  4.859\n",
      "epoch:  20   step:  161   train loss:  2.0976579189300537  val loss:  4.286221981048584  val L1 loss:  4.7436\n",
      "epoch:  20   step:  162   train loss:  2.470916748046875  val loss:  4.242916584014893  val L1 loss:  4.7019\n",
      "epoch:  20   step:  163   train loss:  1.6044306755065918  val loss:  4.266332626342773  val L1 loss:  4.7317\n",
      "epoch:  20   step:  164   train loss:  3.0625410079956055  val loss:  4.310008525848389  val L1 loss:  4.7754\n",
      "epoch:  20   step:  165   train loss:  5.826709270477295  val loss:  4.4243292808532715  val L1 loss:  4.8859\n",
      "epoch:  20   step:  166   train loss:  2.2255871295928955  val loss:  4.488246440887451  val L1 loss:  4.9491\n",
      "epoch:  20   step:  167   train loss:  2.439387321472168  val loss:  4.525064945220947  val L1 loss:  4.993\n",
      "epoch:  20   step:  168   train loss:  2.8091816902160645  val loss:  4.5355072021484375  val L1 loss:  5.0019\n",
      "epoch:  20   step:  169   train loss:  2.144204616546631  val loss:  4.566625595092773  val L1 loss:  5.0408\n",
      "epoch:  20   step:  170   train loss:  2.1286911964416504  val loss:  4.553109645843506  val L1 loss:  5.0276\n",
      "epoch:  20   step:  171   train loss:  1.9735445976257324  val loss:  4.463160991668701  val L1 loss:  4.9205\n",
      "epoch:  20   step:  172   train loss:  2.4578239917755127  val loss:  4.302907466888428  val L1 loss:  4.7564\n",
      "epoch:  20   step:  173   train loss:  2.3583388328552246  val loss:  4.403134822845459  val L1 loss:  4.8693\n",
      "epoch:  20   step:  174   train loss:  2.5048346519470215  val loss:  4.432241439819336  val L1 loss:  4.8975\n",
      "epoch:  20   step:  175   train loss:  4.648601531982422  val loss:  4.3796868324279785  val L1 loss:  4.8331\n",
      "epoch:  20   step:  176   train loss:  3.2578961849212646  val loss:  4.477476596832275  val L1 loss:  4.9548\n",
      "epoch:  20   step:  177   train loss:  2.5481677055358887  val loss:  4.755817890167236  val L1 loss:  5.2284\n",
      "epoch:  20   step:  178   train loss:  2.7824184894561768  val loss:  4.874008655548096  val L1 loss:  5.343\n",
      "epoch:  20   step:  179   train loss:  3.9521780014038086  val loss:  4.699517726898193  val L1 loss:  5.1648\n",
      "epoch:  20   step:  180   train loss:  1.9102275371551514  val loss:  4.766425132751465  val L1 loss:  5.2263\n",
      "epoch:  20   step:  181   train loss:  7.470935821533203  val loss:  4.951947212219238  val L1 loss:  5.4286\n",
      "epoch:  20   step:  182   train loss:  2.3260722160339355  val loss:  4.930156707763672  val L1 loss:  5.3932\n",
      "epoch:  20   step:  183   train loss:  2.235837936401367  val loss:  4.7115702629089355  val L1 loss:  5.1897\n",
      "epoch:  20   step:  184   train loss:  2.026881217956543  val loss:  4.520566463470459  val L1 loss:  4.9779\n",
      "epoch:  20   step:  185   train loss:  2.8032357692718506  val loss:  4.501180648803711  val L1 loss:  4.9835\n",
      "epoch:  20   step:  186   train loss:  3.8656165599823  val loss:  4.60313081741333  val L1 loss:  5.0656\n",
      "epoch:  20   step:  187   train loss:  2.969569206237793  val loss:  4.5459885597229  val L1 loss:  5.0167\n",
      "epoch:  20   step:  188   train loss:  2.159374952316284  val loss:  4.579675197601318  val L1 loss:  5.0335\n",
      "epoch:  20   step:  189   train loss:  2.133228063583374  val loss:  5.277021408081055  val L1 loss:  5.7581\n",
      "epoch:  20   step:  190   train loss:  4.9711198806762695  val loss:  5.519280910491943  val L1 loss:  6.0066\n",
      "epoch:  20   step:  191   train loss:  3.376218795776367  val loss:  5.286943435668945  val L1 loss:  5.771\n",
      "epoch:  20   step:  192   train loss:  2.7448160648345947  val loss:  5.0256781578063965  val L1 loss:  5.4853\n",
      "epoch:  20   step:  193   train loss:  2.4229700565338135  val loss:  4.819208145141602  val L1 loss:  5.2868\n",
      "epoch:  20   step:  194   train loss:  2.6541333198547363  val loss:  4.674561977386475  val L1 loss:  5.1363\n",
      "epoch:  20   step:  195   train loss:  2.0492517948150635  val loss:  4.633853912353516  val L1 loss:  5.0965\n",
      "epoch:  20   step:  196   train loss:  2.5463249683380127  val loss:  4.564855098724365  val L1 loss:  5.0297\n",
      "epoch:  20   step:  197   train loss:  2.742523431777954  val loss:  4.499325275421143  val L1 loss:  4.9629\n",
      "epoch:  20   step:  198   train loss:  3.756709575653076  val loss:  4.487704277038574  val L1 loss:  4.9531\n",
      "epoch:  20   step:  199   train loss:  1.938645362854004  val loss:  4.523626804351807  val L1 loss:  4.9878\n",
      "epoch:  20   step:  200   train loss:  3.9510130882263184  val loss:  4.50297737121582  val L1 loss:  4.9686\n",
      "epoch:  20   step:  201   train loss:  2.7791495323181152  val loss:  4.4780073165893555  val L1 loss:  4.9462\n",
      "epoch:  20   step:  202   train loss:  2.1484556198120117  val loss:  4.486614227294922  val L1 loss:  4.9648\n",
      "epoch:  20   step:  203   train loss:  3.675950765609741  val loss:  4.483388900756836  val L1 loss:  4.9565\n",
      "epoch:  20   step:  204   train loss:  3.331481456756592  val loss:  4.494609355926514  val L1 loss:  4.9594\n",
      "epoch:  20   step:  205   train loss:  4.767867088317871  val loss:  4.527952671051025  val L1 loss:  4.9902\n",
      "epoch:  20   step:  206   train loss:  2.699310064315796  val loss:  4.577646255493164  val L1 loss:  5.0476\n",
      "epoch:  20   step:  207   train loss:  2.1427764892578125  val loss:  4.664705753326416  val L1 loss:  5.1125\n",
      "epoch:  20   step:  208   train loss:  3.2297606468200684  val loss:  4.7916107177734375  val L1 loss:  5.2429\n",
      "epoch:  21   step:  0   train loss:  6.506261825561523  val loss:  4.8122663497924805  val L1 loss:  5.2692\n",
      "epoch:  21   step:  1   train loss:  1.3588708639144897  val loss:  4.781494617462158  val L1 loss:  5.2274\n",
      "epoch:  21   step:  2   train loss:  2.0216708183288574  val loss:  4.78499174118042  val L1 loss:  5.2196\n",
      "epoch:  21   step:  3   train loss:  2.8502964973449707  val loss:  4.675073623657227  val L1 loss:  5.1088\n",
      "epoch:  21   step:  4   train loss:  1.613163709640503  val loss:  4.5430169105529785  val L1 loss:  5.0025\n",
      "epoch:  21   step:  5   train loss:  2.217862606048584  val loss:  4.492559909820557  val L1 loss:  4.958\n",
      "epoch:  21   step:  6   train loss:  2.367309093475342  val loss:  4.454470157623291  val L1 loss:  4.9147\n",
      "epoch:  21   step:  7   train loss:  1.3575193881988525  val loss:  4.446642875671387  val L1 loss:  4.907\n",
      "epoch:  21   step:  8   train loss:  2.9511594772338867  val loss:  4.437586784362793  val L1 loss:  4.8942\n",
      "epoch:  21   step:  9   train loss:  3.676304578781128  val loss:  4.503632068634033  val L1 loss:  4.9666\n",
      "epoch:  21   step:  10   train loss:  1.6133698225021362  val loss:  4.858652591705322  val L1 loss:  5.3257\n",
      "epoch:  21   step:  11   train loss:  3.218454122543335  val loss:  5.029399394989014  val L1 loss:  5.5028\n",
      "epoch:  21   step:  12   train loss:  3.2820639610290527  val loss:  4.773115634918213  val L1 loss:  5.2535\n",
      "epoch:  21   step:  13   train loss:  2.3879034519195557  val loss:  4.496298313140869  val L1 loss:  4.9355\n",
      "epoch:  21   step:  14   train loss:  2.510746955871582  val loss:  4.557114601135254  val L1 loss:  5.0109\n",
      "epoch:  21   step:  15   train loss:  2.2528836727142334  val loss:  4.525707721710205  val L1 loss:  4.9842\n",
      "epoch:  21   step:  16   train loss:  5.233924865722656  val loss:  4.516110420227051  val L1 loss:  4.9795\n",
      "epoch:  21   step:  17   train loss:  4.278298377990723  val loss:  4.564756393432617  val L1 loss:  5.0363\n",
      "epoch:  21   step:  18   train loss:  1.6830480098724365  val loss:  4.510817050933838  val L1 loss:  4.982\n",
      "epoch:  21   step:  19   train loss:  3.9280147552490234  val loss:  4.533820629119873  val L1 loss:  5.0094\n",
      "epoch:  21   step:  20   train loss:  2.124575138092041  val loss:  4.4772257804870605  val L1 loss:  4.8996\n",
      "epoch:  21   step:  21   train loss:  1.8890633583068848  val loss:  4.707972526550293  val L1 loss:  5.1772\n",
      "epoch:  21   step:  22   train loss:  3.165461778640747  val loss:  4.800621509552002  val L1 loss:  5.2746\n",
      "epoch:  21   step:  23   train loss:  3.1599202156066895  val loss:  4.8580169677734375  val L1 loss:  5.3349\n",
      "epoch:  21   step:  24   train loss:  3.4235525131225586  val loss:  4.659032344818115  val L1 loss:  5.1314\n",
      "epoch:  21   step:  25   train loss:  3.289008617401123  val loss:  4.457589626312256  val L1 loss:  4.9151\n",
      "epoch:  21   step:  26   train loss:  1.9807085990905762  val loss:  4.502375602722168  val L1 loss:  4.9802\n",
      "epoch:  21   step:  27   train loss:  2.7092981338500977  val loss:  4.670104503631592  val L1 loss:  5.1526\n",
      "epoch:  21   step:  28   train loss:  3.5051681995391846  val loss:  4.49061918258667  val L1 loss:  4.9572\n",
      "epoch:  21   step:  29   train loss:  2.9263696670532227  val loss:  4.425746917724609  val L1 loss:  4.9027\n",
      "epoch:  21   step:  30   train loss:  3.8234071731567383  val loss:  4.642075538635254  val L1 loss:  5.1191\n",
      "epoch:  21   step:  31   train loss:  2.432791233062744  val loss:  4.689186096191406  val L1 loss:  5.1651\n",
      "epoch:  21   step:  32   train loss:  2.7605676651000977  val loss:  4.509389877319336  val L1 loss:  4.994\n",
      "epoch:  21   step:  33   train loss:  3.264380693435669  val loss:  4.274373531341553  val L1 loss:  4.7329\n",
      "epoch:  21   step:  34   train loss:  3.025264263153076  val loss:  4.132129192352295  val L1 loss:  4.5668\n",
      "epoch:  21   step:  35   train loss:  3.93280029296875  val loss:  4.120127201080322  val L1 loss:  4.5753\n",
      "epoch:  21   step:  36   train loss:  5.199355602264404  val loss:  4.376720905303955  val L1 loss:  4.8607\n",
      "epoch:  21   step:  37   train loss:  3.4137210845947266  val loss:  4.546889781951904  val L1 loss:  5.0327\n",
      "epoch:  21   step:  38   train loss:  3.3015859127044678  val loss:  4.283150672912598  val L1 loss:  4.7514\n",
      "epoch:  21   step:  39   train loss:  2.6255574226379395  val loss:  4.061264991760254  val L1 loss:  4.5466\n",
      "epoch:  21   step:  40   train loss:  2.4797348976135254  val loss:  3.9596781730651855  val L1 loss:  4.4134\n",
      "epoch:  21   step:  41   train loss:  3.0009560585021973  val loss:  3.964986801147461  val L1 loss:  4.4152\n",
      "epoch:  21   step:  42   train loss:  1.662261962890625  val loss:  3.996757745742798  val L1 loss:  4.4432\n",
      "epoch:  21   step:  43   train loss:  4.662713527679443  val loss:  4.056207180023193  val L1 loss:  4.5101\n",
      "epoch:  21   step:  44   train loss:  2.0580813884735107  val loss:  4.097818374633789  val L1 loss:  4.5513\n",
      "epoch:  21   step:  45   train loss:  2.153205633163452  val loss:  4.178826332092285  val L1 loss:  4.6353\n",
      "epoch:  21   step:  46   train loss:  1.7737473249435425  val loss:  4.270464897155762  val L1 loss:  4.7372\n",
      "epoch:  21   step:  47   train loss:  2.699866533279419  val loss:  4.197652339935303  val L1 loss:  4.6556\n",
      "epoch:  21   step:  48   train loss:  2.876312255859375  val loss:  4.12596321105957  val L1 loss:  4.5833\n",
      "epoch:  21   step:  49   train loss:  2.2857494354248047  val loss:  4.399474620819092  val L1 loss:  4.8449\n",
      "epoch:  21   step:  50   train loss:  2.74924373626709  val loss:  5.182650089263916  val L1 loss:  5.666\n",
      "epoch:  21   step:  51   train loss:  3.4391963481903076  val loss:  5.719038009643555  val L1 loss:  6.2101\n",
      "epoch:  21   step:  52   train loss:  4.304183006286621  val loss:  5.535243988037109  val L1 loss:  6.026\n",
      "epoch:  21   step:  53   train loss:  3.5893311500549316  val loss:  4.732665061950684  val L1 loss:  5.2101\n",
      "epoch:  21   step:  54   train loss:  3.050448417663574  val loss:  4.144221782684326  val L1 loss:  4.6142\n",
      "epoch:  21   step:  55   train loss:  2.7375946044921875  val loss:  4.45806884765625  val L1 loss:  4.943\n",
      "epoch:  21   step:  56   train loss:  3.831162929534912  val loss:  4.899097919464111  val L1 loss:  5.398\n",
      "epoch:  21   step:  57   train loss:  4.878491401672363  val loss:  4.7040114402771  val L1 loss:  5.1899\n",
      "epoch:  21   step:  58   train loss:  1.9352920055389404  val loss:  4.338767051696777  val L1 loss:  4.8073\n",
      "epoch:  21   step:  59   train loss:  2.2263193130493164  val loss:  4.392362117767334  val L1 loss:  4.8542\n",
      "epoch:  21   step:  60   train loss:  2.1719794273376465  val loss:  4.649630546569824  val L1 loss:  5.1298\n",
      "epoch:  21   step:  61   train loss:  2.0023999214172363  val loss:  5.124922752380371  val L1 loss:  5.6014\n",
      "epoch:  21   step:  62   train loss:  2.5168888568878174  val loss:  5.350897312164307  val L1 loss:  5.8339\n",
      "epoch:  21   step:  63   train loss:  3.293349266052246  val loss:  5.267399787902832  val L1 loss:  5.7149\n",
      "epoch:  21   step:  64   train loss:  2.9755373001098633  val loss:  5.100385665893555  val L1 loss:  5.5624\n",
      "epoch:  21   step:  65   train loss:  3.3639416694641113  val loss:  5.047808647155762  val L1 loss:  5.5346\n",
      "epoch:  21   step:  66   train loss:  3.3619418144226074  val loss:  5.044661521911621  val L1 loss:  5.5255\n",
      "epoch:  21   step:  67   train loss:  3.777698040008545  val loss:  5.061553001403809  val L1 loss:  5.5478\n",
      "epoch:  21   step:  68   train loss:  3.129307270050049  val loss:  5.06527042388916  val L1 loss:  5.539\n",
      "epoch:  21   step:  69   train loss:  2.4859232902526855  val loss:  5.061063766479492  val L1 loss:  5.5065\n",
      "epoch:  21   step:  70   train loss:  1.9304370880126953  val loss:  5.22701358795166  val L1 loss:  5.7012\n",
      "epoch:  21   step:  71   train loss:  1.6399531364440918  val loss:  5.226194858551025  val L1 loss:  5.7061\n",
      "epoch:  21   step:  72   train loss:  2.7243621349334717  val loss:  4.892584323883057  val L1 loss:  5.3619\n",
      "epoch:  21   step:  73   train loss:  2.4810619354248047  val loss:  4.5263776779174805  val L1 loss:  4.9892\n",
      "epoch:  21   step:  74   train loss:  2.412571907043457  val loss:  4.5065178871154785  val L1 loss:  4.9744\n",
      "epoch:  21   step:  75   train loss:  2.4418227672576904  val loss:  4.593649387359619  val L1 loss:  5.0608\n",
      "epoch:  21   step:  76   train loss:  2.410921573638916  val loss:  4.546146869659424  val L1 loss:  5.031\n",
      "epoch:  21   step:  77   train loss:  2.4051573276519775  val loss:  4.353962421417236  val L1 loss:  4.8213\n",
      "epoch:  21   step:  78   train loss:  4.013187408447266  val loss:  4.276418209075928  val L1 loss:  4.7502\n",
      "epoch:  21   step:  79   train loss:  1.8221882581710815  val loss:  4.240555286407471  val L1 loss:  4.7129\n",
      "epoch:  21   step:  80   train loss:  2.315732717514038  val loss:  4.6013922691345215  val L1 loss:  5.0791\n",
      "epoch:  21   step:  81   train loss:  4.138219356536865  val loss:  4.76379919052124  val L1 loss:  5.2446\n",
      "epoch:  21   step:  82   train loss:  2.6044483184814453  val loss:  4.585838794708252  val L1 loss:  5.0587\n",
      "epoch:  21   step:  83   train loss:  2.2821273803710938  val loss:  4.393087863922119  val L1 loss:  4.8426\n",
      "epoch:  21   step:  84   train loss:  1.7763175964355469  val loss:  4.27618932723999  val L1 loss:  4.7168\n",
      "epoch:  21   step:  85   train loss:  3.4990124702453613  val loss:  4.271500587463379  val L1 loss:  4.7547\n",
      "epoch:  21   step:  86   train loss:  1.9970309734344482  val loss:  4.236207962036133  val L1 loss:  4.7122\n",
      "epoch:  21   step:  87   train loss:  2.2940585613250732  val loss:  4.140225887298584  val L1 loss:  4.6095\n",
      "epoch:  21   step:  88   train loss:  2.7392842769622803  val loss:  4.192380905151367  val L1 loss:  4.6667\n",
      "epoch:  21   step:  89   train loss:  1.4836528301239014  val loss:  4.4175944328308105  val L1 loss:  4.8926\n",
      "epoch:  21   step:  90   train loss:  2.3763558864593506  val loss:  4.570014476776123  val L1 loss:  5.049\n",
      "epoch:  21   step:  91   train loss:  3.6209051609039307  val loss:  4.373968601226807  val L1 loss:  4.8331\n",
      "epoch:  21   step:  92   train loss:  3.066490888595581  val loss:  4.23085355758667  val L1 loss:  4.691\n",
      "epoch:  21   step:  93   train loss:  2.9143967628479004  val loss:  4.367523670196533  val L1 loss:  4.8389\n",
      "epoch:  21   step:  94   train loss:  2.1653025150299072  val loss:  4.478863716125488  val L1 loss:  4.9569\n",
      "epoch:  21   step:  95   train loss:  3.0766873359680176  val loss:  4.249268054962158  val L1 loss:  4.7066\n",
      "epoch:  21   step:  96   train loss:  2.3802075386047363  val loss:  4.261685848236084  val L1 loss:  4.7214\n",
      "epoch:  21   step:  97   train loss:  2.035491943359375  val loss:  4.599069595336914  val L1 loss:  5.0584\n",
      "epoch:  21   step:  98   train loss:  3.4133052825927734  val loss:  4.971620082855225  val L1 loss:  5.4371\n",
      "epoch:  21   step:  99   train loss:  3.3377199172973633  val loss:  5.038213729858398  val L1 loss:  5.5049\n",
      "epoch:  21   step:  100   train loss:  3.4239041805267334  val loss:  4.810815811157227  val L1 loss:  5.2844\n",
      "epoch:  21   step:  101   train loss:  2.6469385623931885  val loss:  4.63362979888916  val L1 loss:  5.1024\n",
      "epoch:  21   step:  102   train loss:  3.0844297409057617  val loss:  4.508090972900391  val L1 loss:  4.994\n",
      "epoch:  21   step:  103   train loss:  2.797389507293701  val loss:  4.44356107711792  val L1 loss:  4.9215\n",
      "epoch:  21   step:  104   train loss:  3.391523838043213  val loss:  4.408997535705566  val L1 loss:  4.8879\n",
      "epoch:  21   step:  105   train loss:  2.617100954055786  val loss:  4.357544422149658  val L1 loss:  4.8145\n",
      "epoch:  21   step:  106   train loss:  2.590254783630371  val loss:  4.561681270599365  val L1 loss:  5.0466\n",
      "epoch:  21   step:  107   train loss:  2.5646355152130127  val loss:  4.5343828201293945  val L1 loss:  5.0168\n",
      "epoch:  21   step:  108   train loss:  2.895857334136963  val loss:  4.226399898529053  val L1 loss:  4.6823\n",
      "epoch:  21   step:  109   train loss:  2.870656728744507  val loss:  4.062648773193359  val L1 loss:  4.5222\n",
      "epoch:  21   step:  110   train loss:  2.7129087448120117  val loss:  4.116204261779785  val L1 loss:  4.5767\n",
      "epoch:  21   step:  111   train loss:  2.8782405853271484  val loss:  4.5247392654418945  val L1 loss:  5.0061\n",
      "epoch:  21   step:  112   train loss:  3.39601731300354  val loss:  4.842361927032471  val L1 loss:  5.3307\n",
      "epoch:  21   step:  113   train loss:  3.8470985889434814  val loss:  4.626328468322754  val L1 loss:  5.1023\n",
      "epoch:  21   step:  114   train loss:  3.8586490154266357  val loss:  4.246689319610596  val L1 loss:  4.7141\n",
      "epoch:  21   step:  115   train loss:  2.425963878631592  val loss:  4.143909931182861  val L1 loss:  4.6098\n",
      "epoch:  21   step:  116   train loss:  2.5367212295532227  val loss:  4.1219892501831055  val L1 loss:  4.5795\n",
      "epoch:  21   step:  117   train loss:  2.7608957290649414  val loss:  4.154910087585449  val L1 loss:  4.6178\n",
      "epoch:  21   step:  118   train loss:  2.106431722640991  val loss:  4.137885570526123  val L1 loss:  4.5967\n",
      "epoch:  21   step:  119   train loss:  2.7041268348693848  val loss:  4.137246131896973  val L1 loss:  4.6096\n",
      "epoch:  21   step:  120   train loss:  2.907209634780884  val loss:  4.298074722290039  val L1 loss:  4.7736\n",
      "epoch:  21   step:  121   train loss:  2.0757479667663574  val loss:  4.518383502960205  val L1 loss:  4.9885\n",
      "epoch:  21   step:  122   train loss:  1.9993293285369873  val loss:  4.523106575012207  val L1 loss:  4.9919\n",
      "epoch:  21   step:  123   train loss:  2.989745616912842  val loss:  4.439156532287598  val L1 loss:  4.9112\n",
      "epoch:  21   step:  124   train loss:  3.584132194519043  val loss:  4.453406810760498  val L1 loss:  4.9279\n",
      "epoch:  21   step:  125   train loss:  1.937417984008789  val loss:  4.501148700714111  val L1 loss:  4.9672\n",
      "epoch:  21   step:  126   train loss:  1.8636949062347412  val loss:  4.59855842590332  val L1 loss:  5.0747\n",
      "epoch:  21   step:  127   train loss:  2.8548765182495117  val loss:  4.646562576293945  val L1 loss:  5.1078\n",
      "epoch:  21   step:  128   train loss:  3.4140539169311523  val loss:  4.859800815582275  val L1 loss:  5.3339\n",
      "epoch:  21   step:  129   train loss:  1.9802998304367065  val loss:  5.834656715393066  val L1 loss:  6.3211\n",
      "epoch:  21   step:  130   train loss:  3.5415265560150146  val loss:  6.936236381530762  val L1 loss:  7.4326\n",
      "epoch:  21   step:  131   train loss:  2.901926040649414  val loss:  7.481770038604736  val L1 loss:  7.9807\n",
      "epoch:  21   step:  132   train loss:  4.041906356811523  val loss:  7.163188457489014  val L1 loss:  7.6582\n",
      "epoch:  21   step:  133   train loss:  2.7186684608459473  val loss:  6.431935787200928  val L1 loss:  6.9207\n",
      "epoch:  21   step:  134   train loss:  3.1355748176574707  val loss:  5.506892204284668  val L1 loss:  5.9878\n",
      "epoch:  21   step:  135   train loss:  3.009591817855835  val loss:  4.983887672424316  val L1 loss:  5.4415\n",
      "epoch:  21   step:  136   train loss:  4.803716659545898  val loss:  4.924961566925049  val L1 loss:  5.3819\n",
      "epoch:  21   step:  137   train loss:  5.695449352264404  val loss:  4.958846092224121  val L1 loss:  5.4201\n",
      "epoch:  21   step:  138   train loss:  2.0691819190979004  val loss:  5.18535041809082  val L1 loss:  5.6604\n",
      "epoch:  21   step:  139   train loss:  2.26511549949646  val loss:  5.682200908660889  val L1 loss:  6.1538\n",
      "epoch:  21   step:  140   train loss:  2.4712398052215576  val loss:  6.05187463760376  val L1 loss:  6.5308\n",
      "epoch:  21   step:  141   train loss:  1.8406643867492676  val loss:  5.945817947387695  val L1 loss:  6.4251\n",
      "epoch:  21   step:  142   train loss:  2.2693324089050293  val loss:  5.58573055267334  val L1 loss:  6.0574\n",
      "epoch:  21   step:  143   train loss:  3.69327449798584  val loss:  5.110766410827637  val L1 loss:  5.5977\n",
      "epoch:  21   step:  144   train loss:  2.8938510417938232  val loss:  4.757765769958496  val L1 loss:  5.2154\n",
      "epoch:  21   step:  145   train loss:  2.752429723739624  val loss:  4.920507907867432  val L1 loss:  5.383\n",
      "epoch:  21   step:  146   train loss:  4.360944747924805  val loss:  4.839820384979248  val L1 loss:  5.292\n",
      "epoch:  21   step:  147   train loss:  5.318875789642334  val loss:  4.670962333679199  val L1 loss:  5.1477\n",
      "epoch:  21   step:  148   train loss:  2.7474355697631836  val loss:  4.75178337097168  val L1 loss:  5.2376\n",
      "epoch:  21   step:  149   train loss:  2.011317491531372  val loss:  4.78729772567749  val L1 loss:  5.2773\n",
      "epoch:  21   step:  150   train loss:  3.4418158531188965  val loss:  4.7537336349487305  val L1 loss:  5.2309\n",
      "epoch:  21   step:  151   train loss:  2.7881288528442383  val loss:  4.7011847496032715  val L1 loss:  5.1755\n",
      "epoch:  21   step:  152   train loss:  2.69071102142334  val loss:  4.547266006469727  val L1 loss:  5.0276\n",
      "epoch:  21   step:  153   train loss:  3.2419440746307373  val loss:  4.70615291595459  val L1 loss:  5.1778\n",
      "epoch:  21   step:  154   train loss:  1.9110232591629028  val loss:  4.980406284332275  val L1 loss:  5.4547\n",
      "epoch:  21   step:  155   train loss:  2.787053108215332  val loss:  5.086028099060059  val L1 loss:  5.551\n",
      "epoch:  21   step:  156   train loss:  3.1921603679656982  val loss:  4.9212327003479  val L1 loss:  5.3981\n",
      "epoch:  21   step:  157   train loss:  3.4509921073913574  val loss:  4.773974418640137  val L1 loss:  5.2564\n",
      "epoch:  21   step:  158   train loss:  2.1383275985717773  val loss:  4.752632141113281  val L1 loss:  5.2286\n",
      "epoch:  21   step:  159   train loss:  1.4697569608688354  val loss:  4.735265254974365  val L1 loss:  5.2165\n",
      "epoch:  21   step:  160   train loss:  2.390326499938965  val loss:  4.697030544281006  val L1 loss:  5.1611\n",
      "epoch:  21   step:  161   train loss:  2.2610397338867188  val loss:  4.704848289489746  val L1 loss:  5.1818\n",
      "epoch:  21   step:  162   train loss:  2.330775737762451  val loss:  4.804433345794678  val L1 loss:  5.2938\n",
      "epoch:  21   step:  163   train loss:  1.869661808013916  val loss:  4.80825138092041  val L1 loss:  5.2991\n",
      "epoch:  21   step:  164   train loss:  2.144045829772949  val loss:  4.779494285583496  val L1 loss:  5.2625\n",
      "epoch:  21   step:  165   train loss:  2.7545576095581055  val loss:  4.783206939697266  val L1 loss:  5.2628\n",
      "epoch:  21   step:  166   train loss:  2.941349506378174  val loss:  4.846531867980957  val L1 loss:  5.3209\n",
      "epoch:  21   step:  167   train loss:  3.6804487705230713  val loss:  4.902209281921387  val L1 loss:  5.3777\n",
      "epoch:  21   step:  168   train loss:  2.980673313140869  val loss:  4.91697883605957  val L1 loss:  5.3847\n",
      "epoch:  21   step:  169   train loss:  2.018115520477295  val loss:  4.733433723449707  val L1 loss:  5.21\n",
      "epoch:  21   step:  170   train loss:  1.771924376487732  val loss:  4.58070707321167  val L1 loss:  5.03\n",
      "epoch:  21   step:  171   train loss:  2.170344114303589  val loss:  4.688180446624756  val L1 loss:  5.1492\n",
      "epoch:  21   step:  172   train loss:  2.386235237121582  val loss:  4.656641006469727  val L1 loss:  5.1221\n",
      "epoch:  21   step:  173   train loss:  3.0476818084716797  val loss:  4.526495456695557  val L1 loss:  4.9631\n",
      "epoch:  21   step:  174   train loss:  3.1984944343566895  val loss:  4.550750255584717  val L1 loss:  5.0244\n",
      "epoch:  21   step:  175   train loss:  2.256258487701416  val loss:  4.629687786102295  val L1 loss:  5.1026\n",
      "epoch:  21   step:  176   train loss:  3.7493114471435547  val loss:  4.621704578399658  val L1 loss:  5.0956\n",
      "epoch:  21   step:  177   train loss:  2.8920822143554688  val loss:  4.469379901885986  val L1 loss:  4.9464\n",
      "epoch:  21   step:  178   train loss:  3.0402350425720215  val loss:  4.3658270835876465  val L1 loss:  4.8312\n",
      "epoch:  21   step:  179   train loss:  3.724429130554199  val loss:  4.235672950744629  val L1 loss:  4.6953\n",
      "epoch:  21   step:  180   train loss:  1.856909155845642  val loss:  4.175570011138916  val L1 loss:  4.6349\n",
      "epoch:  21   step:  181   train loss:  2.4071524143218994  val loss:  4.225288391113281  val L1 loss:  4.6896\n",
      "epoch:  21   step:  182   train loss:  3.1647815704345703  val loss:  4.699480056762695  val L1 loss:  5.1842\n",
      "epoch:  21   step:  183   train loss:  1.7695107460021973  val loss:  5.429253578186035  val L1 loss:  5.9235\n",
      "epoch:  21   step:  184   train loss:  3.379014492034912  val loss:  5.784736156463623  val L1 loss:  6.2673\n",
      "epoch:  21   step:  185   train loss:  2.278968334197998  val loss:  5.325401306152344  val L1 loss:  5.8126\n",
      "epoch:  21   step:  186   train loss:  3.74005126953125  val loss:  4.980939865112305  val L1 loss:  5.4577\n",
      "epoch:  21   step:  187   train loss:  3.0153400897979736  val loss:  4.515837669372559  val L1 loss:  4.967\n",
      "epoch:  21   step:  188   train loss:  2.4475889205932617  val loss:  4.19887113571167  val L1 loss:  4.6754\n",
      "epoch:  21   step:  189   train loss:  3.777571678161621  val loss:  4.120230197906494  val L1 loss:  4.574\n",
      "epoch:  21   step:  190   train loss:  4.378283977508545  val loss:  4.234144687652588  val L1 loss:  4.6944\n",
      "epoch:  21   step:  191   train loss:  4.074732780456543  val loss:  4.067598819732666  val L1 loss:  4.5471\n",
      "epoch:  21   step:  192   train loss:  2.840479850769043  val loss:  4.086956024169922  val L1 loss:  4.5456\n",
      "epoch:  21   step:  193   train loss:  1.9744315147399902  val loss:  4.292414665222168  val L1 loss:  4.7806\n",
      "epoch:  21   step:  194   train loss:  2.5026891231536865  val loss:  4.456918716430664  val L1 loss:  4.9392\n",
      "epoch:  21   step:  195   train loss:  4.071785926818848  val loss:  4.4184722900390625  val L1 loss:  4.8978\n",
      "epoch:  21   step:  196   train loss:  3.128382921218872  val loss:  4.249526500701904  val L1 loss:  4.7372\n",
      "epoch:  21   step:  197   train loss:  3.3634822368621826  val loss:  3.9973931312561035  val L1 loss:  4.466\n",
      "epoch:  21   step:  198   train loss:  2.8075032234191895  val loss:  4.249954700469971  val L1 loss:  4.7247\n",
      "epoch:  21   step:  199   train loss:  2.4286365509033203  val loss:  4.6009135246276855  val L1 loss:  5.0801\n",
      "epoch:  21   step:  200   train loss:  4.327625274658203  val loss:  4.440789222717285  val L1 loss:  4.9143\n",
      "epoch:  21   step:  201   train loss:  3.099515438079834  val loss:  4.092829704284668  val L1 loss:  4.5643\n",
      "epoch:  21   step:  202   train loss:  2.3688035011291504  val loss:  3.8553647994995117  val L1 loss:  4.3068\n",
      "epoch:  21   step:  203   train loss:  3.3346171379089355  val loss:  3.9008147716522217  val L1 loss:  4.3457\n",
      "epoch:  21   step:  204   train loss:  2.8680310249328613  val loss:  3.994424343109131  val L1 loss:  4.4456\n",
      "epoch:  21   step:  205   train loss:  2.8355050086975098  val loss:  3.8509457111358643  val L1 loss:  4.3168\n",
      "epoch:  21   step:  206   train loss:  2.2126657962799072  val loss:  3.855008363723755  val L1 loss:  4.3097\n",
      "epoch:  21   step:  207   train loss:  1.3040688037872314  val loss:  3.99751877784729  val L1 loss:  4.4675\n",
      "epoch:  21   step:  208   train loss:  5.683520317077637  val loss:  3.9920780658721924  val L1 loss:  4.4608\n",
      "epoch:  22   step:  0   train loss:  1.5771422386169434  val loss:  3.88974666595459  val L1 loss:  4.3521\n",
      "epoch:  22   step:  1   train loss:  3.1227493286132812  val loss:  3.859717845916748  val L1 loss:  4.3167\n",
      "epoch:  22   step:  2   train loss:  2.762031078338623  val loss:  3.8594741821289062  val L1 loss:  4.297\n",
      "epoch:  22   step:  3   train loss:  1.6042468547821045  val loss:  3.924794912338257  val L1 loss:  4.3843\n",
      "epoch:  22   step:  4   train loss:  2.023502826690674  val loss:  4.10783052444458  val L1 loss:  4.5653\n",
      "epoch:  22   step:  5   train loss:  2.078218460083008  val loss:  4.357165336608887  val L1 loss:  4.8416\n",
      "epoch:  22   step:  6   train loss:  1.301137089729309  val loss:  4.482700824737549  val L1 loss:  4.9588\n",
      "epoch:  22   step:  7   train loss:  3.419738292694092  val loss:  4.49635648727417  val L1 loss:  4.9622\n",
      "epoch:  22   step:  8   train loss:  2.797034978866577  val loss:  4.289098262786865  val L1 loss:  4.7324\n",
      "epoch:  22   step:  9   train loss:  3.45231556892395  val loss:  4.048616409301758  val L1 loss:  4.5181\n",
      "epoch:  22   step:  10   train loss:  3.153992176055908  val loss:  4.141010761260986  val L1 loss:  4.6274\n",
      "epoch:  22   step:  11   train loss:  3.1053080558776855  val loss:  4.244829177856445  val L1 loss:  4.7233\n",
      "epoch:  22   step:  12   train loss:  4.0617756843566895  val loss:  4.102608680725098  val L1 loss:  4.5817\n",
      "epoch:  22   step:  13   train loss:  3.5027663707733154  val loss:  3.9734838008880615  val L1 loss:  4.4176\n",
      "epoch:  22   step:  14   train loss:  2.979849338531494  val loss:  4.376217842102051  val L1 loss:  4.8528\n",
      "epoch:  22   step:  15   train loss:  2.6526708602905273  val loss:  4.789913177490234  val L1 loss:  5.2681\n",
      "epoch:  22   step:  16   train loss:  1.8721472024917603  val loss:  5.168167591094971  val L1 loss:  5.6494\n",
      "epoch:  22   step:  17   train loss:  3.9515833854675293  val loss:  4.895380020141602  val L1 loss:  5.3736\n",
      "epoch:  22   step:  18   train loss:  4.769598960876465  val loss:  4.320788383483887  val L1 loss:  4.8005\n",
      "epoch:  22   step:  19   train loss:  3.511889934539795  val loss:  4.048514366149902  val L1 loss:  4.5168\n",
      "epoch:  22   step:  20   train loss:  2.1413345336914062  val loss:  4.280007362365723  val L1 loss:  4.7661\n",
      "epoch:  22   step:  21   train loss:  1.6388239860534668  val loss:  4.5147552490234375  val L1 loss:  4.9874\n",
      "epoch:  22   step:  22   train loss:  2.131152629852295  val loss:  4.42991304397583  val L1 loss:  4.8974\n",
      "epoch:  22   step:  23   train loss:  2.043246030807495  val loss:  4.320776462554932  val L1 loss:  4.7976\n",
      "epoch:  22   step:  24   train loss:  2.02729868888855  val loss:  4.33459997177124  val L1 loss:  4.7882\n",
      "epoch:  22   step:  25   train loss:  2.063922882080078  val loss:  4.553569793701172  val L1 loss:  5.0227\n",
      "epoch:  22   step:  26   train loss:  2.5615086555480957  val loss:  4.815497398376465  val L1 loss:  5.2911\n",
      "epoch:  22   step:  27   train loss:  2.9170660972595215  val loss:  4.695060729980469  val L1 loss:  5.174\n",
      "epoch:  22   step:  28   train loss:  2.772472858428955  val loss:  4.398997783660889  val L1 loss:  4.8614\n",
      "epoch:  22   step:  29   train loss:  1.5406005382537842  val loss:  4.162315368652344  val L1 loss:  4.6129\n",
      "epoch:  22   step:  30   train loss:  4.013360500335693  val loss:  4.1878767013549805  val L1 loss:  4.6743\n",
      "epoch:  22   step:  31   train loss:  2.1035828590393066  val loss:  4.2402567863464355  val L1 loss:  4.7128\n",
      "epoch:  22   step:  32   train loss:  3.7793641090393066  val loss:  4.289729595184326  val L1 loss:  4.7698\n",
      "epoch:  22   step:  33   train loss:  3.8041188716888428  val loss:  4.189910411834717  val L1 loss:  4.6742\n",
      "epoch:  22   step:  34   train loss:  1.9601240158081055  val loss:  4.124384880065918  val L1 loss:  4.5862\n",
      "epoch:  22   step:  35   train loss:  1.540558099746704  val loss:  4.381006717681885  val L1 loss:  4.8708\n",
      "epoch:  22   step:  36   train loss:  3.092254877090454  val loss:  4.541962623596191  val L1 loss:  5.0317\n",
      "epoch:  22   step:  37   train loss:  1.6752607822418213  val loss:  4.519516468048096  val L1 loss:  5.0124\n",
      "epoch:  22   step:  38   train loss:  2.198904037475586  val loss:  4.3829665184021  val L1 loss:  4.8731\n",
      "epoch:  22   step:  39   train loss:  2.0995049476623535  val loss:  4.126661777496338  val L1 loss:  4.5888\n",
      "epoch:  22   step:  40   train loss:  2.2926011085510254  val loss:  4.1345319747924805  val L1 loss:  4.5815\n",
      "epoch:  22   step:  41   train loss:  3.2888309955596924  val loss:  4.158455848693848  val L1 loss:  4.6172\n",
      "epoch:  22   step:  42   train loss:  2.7120003700256348  val loss:  4.180540084838867  val L1 loss:  4.6407\n",
      "epoch:  22   step:  43   train loss:  4.062067985534668  val loss:  4.185730457305908  val L1 loss:  4.6603\n",
      "epoch:  22   step:  44   train loss:  3.2903854846954346  val loss:  4.246298313140869  val L1 loss:  4.6971\n",
      "epoch:  22   step:  45   train loss:  2.4935245513916016  val loss:  4.2600321769714355  val L1 loss:  4.7182\n",
      "epoch:  22   step:  46   train loss:  2.7963240146636963  val loss:  4.267717361450195  val L1 loss:  4.7179\n",
      "epoch:  22   step:  47   train loss:  3.3635849952697754  val loss:  4.230037689208984  val L1 loss:  4.6975\n",
      "epoch:  22   step:  48   train loss:  3.064634323120117  val loss:  4.322906017303467  val L1 loss:  4.7729\n",
      "epoch:  22   step:  49   train loss:  1.7265970706939697  val loss:  4.428670406341553  val L1 loss:  4.8869\n",
      "epoch:  22   step:  50   train loss:  2.3053643703460693  val loss:  4.463655471801758  val L1 loss:  4.9119\n",
      "epoch:  22   step:  51   train loss:  2.189107894897461  val loss:  4.434299945831299  val L1 loss:  4.9017\n",
      "epoch:  22   step:  52   train loss:  1.472257375717163  val loss:  4.525549411773682  val L1 loss:  4.9807\n",
      "epoch:  22   step:  53   train loss:  3.4833858013153076  val loss:  4.610698699951172  val L1 loss:  5.0817\n",
      "epoch:  22   step:  54   train loss:  1.6391582489013672  val loss:  4.7379045486450195  val L1 loss:  5.2062\n",
      "epoch:  22   step:  55   train loss:  4.67932653427124  val loss:  4.788570404052734  val L1 loss:  5.2593\n",
      "epoch:  22   step:  56   train loss:  2.1038084030151367  val loss:  4.732448101043701  val L1 loss:  5.2003\n",
      "epoch:  22   step:  57   train loss:  3.694664478302002  val loss:  4.559450149536133  val L1 loss:  5.0184\n",
      "epoch:  22   step:  58   train loss:  2.5230417251586914  val loss:  4.734225749969482  val L1 loss:  5.187\n",
      "epoch:  22   step:  59   train loss:  2.2721474170684814  val loss:  4.881575584411621  val L1 loss:  5.359\n",
      "epoch:  22   step:  60   train loss:  1.8129209280014038  val loss:  4.778707027435303  val L1 loss:  5.2508\n",
      "epoch:  22   step:  61   train loss:  1.9881911277770996  val loss:  4.543967247009277  val L1 loss:  5.0081\n",
      "epoch:  22   step:  62   train loss:  4.486487865447998  val loss:  4.570057392120361  val L1 loss:  5.0219\n",
      "epoch:  22   step:  63   train loss:  2.226149559020996  val loss:  4.577197074890137  val L1 loss:  5.0418\n",
      "epoch:  22   step:  64   train loss:  2.4242284297943115  val loss:  4.5442352294921875  val L1 loss:  5.0003\n",
      "epoch:  22   step:  65   train loss:  1.8168333768844604  val loss:  4.599025249481201  val L1 loss:  5.0717\n",
      "epoch:  22   step:  66   train loss:  2.0155279636383057  val loss:  4.6677985191345215  val L1 loss:  5.1428\n",
      "epoch:  22   step:  67   train loss:  3.678926944732666  val loss:  4.639148712158203  val L1 loss:  5.1041\n",
      "epoch:  22   step:  68   train loss:  1.8079923391342163  val loss:  4.56203556060791  val L1 loss:  5.042\n",
      "epoch:  22   step:  69   train loss:  1.8145885467529297  val loss:  4.535650253295898  val L1 loss:  5.0196\n",
      "epoch:  22   step:  70   train loss:  2.5929765701293945  val loss:  4.4805827140808105  val L1 loss:  4.9634\n",
      "epoch:  22   step:  71   train loss:  4.11961555480957  val loss:  4.431037902832031  val L1 loss:  4.909\n",
      "epoch:  22   step:  72   train loss:  4.156894207000732  val loss:  4.295310974121094  val L1 loss:  4.7705\n",
      "epoch:  22   step:  73   train loss:  1.9138147830963135  val loss:  4.210404872894287  val L1 loss:  4.6834\n",
      "epoch:  22   step:  74   train loss:  1.6262884140014648  val loss:  4.24222469329834  val L1 loss:  4.7082\n",
      "epoch:  22   step:  75   train loss:  5.560761451721191  val loss:  4.162447452545166  val L1 loss:  4.641\n",
      "epoch:  22   step:  76   train loss:  2.1719226837158203  val loss:  4.05369758605957  val L1 loss:  4.4999\n",
      "epoch:  22   step:  77   train loss:  3.99556565284729  val loss:  3.983997106552124  val L1 loss:  4.4338\n",
      "epoch:  22   step:  78   train loss:  3.0019946098327637  val loss:  4.0900421142578125  val L1 loss:  4.5531\n",
      "epoch:  22   step:  79   train loss:  2.714714527130127  val loss:  4.065170764923096  val L1 loss:  4.5284\n",
      "epoch:  22   step:  80   train loss:  2.046255350112915  val loss:  3.9264211654663086  val L1 loss:  4.3777\n",
      "epoch:  22   step:  81   train loss:  1.788240909576416  val loss:  3.858973979949951  val L1 loss:  4.2755\n",
      "epoch:  22   step:  82   train loss:  1.6003252267837524  val loss:  3.955091714859009  val L1 loss:  4.416\n",
      "epoch:  22   step:  83   train loss:  2.3831284046173096  val loss:  3.9527382850646973  val L1 loss:  4.4072\n",
      "epoch:  22   step:  84   train loss:  2.487628936767578  val loss:  4.005670547485352  val L1 loss:  4.4603\n",
      "epoch:  22   step:  85   train loss:  2.722470283508301  val loss:  4.037951946258545  val L1 loss:  4.5006\n",
      "epoch:  22   step:  86   train loss:  2.909811496734619  val loss:  3.849391460418701  val L1 loss:  4.2928\n",
      "epoch:  22   step:  87   train loss:  5.91801643371582  val loss:  3.9031083583831787  val L1 loss:  4.3751\n",
      "epoch:  22   step:  88   train loss:  2.7691783905029297  val loss:  4.263044357299805  val L1 loss:  4.7466\n",
      "epoch:  22   step:  89   train loss:  3.4421944618225098  val loss:  4.38399076461792  val L1 loss:  4.877\n",
      "epoch:  22   step:  90   train loss:  2.461005449295044  val loss:  4.238248348236084  val L1 loss:  4.7196\n",
      "epoch:  22   step:  91   train loss:  2.1614608764648438  val loss:  3.9870338439941406  val L1 loss:  4.4292\n",
      "epoch:  22   step:  92   train loss:  2.128448009490967  val loss:  4.016499042510986  val L1 loss:  4.48\n",
      "epoch:  22   step:  93   train loss:  3.008666515350342  val loss:  4.240119457244873  val L1 loss:  4.7022\n",
      "epoch:  22   step:  94   train loss:  2.709425449371338  val loss:  4.365626335144043  val L1 loss:  4.8458\n",
      "epoch:  22   step:  95   train loss:  2.4762086868286133  val loss:  4.400020122528076  val L1 loss:  4.875\n",
      "epoch:  22   step:  96   train loss:  2.834977626800537  val loss:  4.374575614929199  val L1 loss:  4.8414\n",
      "epoch:  22   step:  97   train loss:  2.9697508811950684  val loss:  4.425588607788086  val L1 loss:  4.8877\n",
      "epoch:  22   step:  98   train loss:  2.8224525451660156  val loss:  4.476097106933594  val L1 loss:  4.9393\n",
      "epoch:  22   step:  99   train loss:  5.525397300720215  val loss:  4.420731544494629  val L1 loss:  4.8739\n",
      "epoch:  22   step:  100   train loss:  2.5621323585510254  val loss:  4.31767463684082  val L1 loss:  4.7869\n",
      "epoch:  22   step:  101   train loss:  1.8029720783233643  val loss:  4.224315643310547  val L1 loss:  4.6801\n",
      "epoch:  22   step:  102   train loss:  2.0836448669433594  val loss:  4.327763080596924  val L1 loss:  4.789\n",
      "epoch:  22   step:  103   train loss:  1.8153177499771118  val loss:  4.489689826965332  val L1 loss:  4.9688\n",
      "epoch:  22   step:  104   train loss:  2.7119717597961426  val loss:  4.371610641479492  val L1 loss:  4.8439\n",
      "epoch:  22   step:  105   train loss:  2.160576820373535  val loss:  4.236117839813232  val L1 loss:  4.6952\n",
      "epoch:  22   step:  106   train loss:  2.013854742050171  val loss:  4.103379249572754  val L1 loss:  4.5556\n",
      "epoch:  22   step:  107   train loss:  3.3490922451019287  val loss:  4.137612342834473  val L1 loss:  4.597\n",
      "epoch:  22   step:  108   train loss:  2.263853073120117  val loss:  4.335101127624512  val L1 loss:  4.8086\n",
      "epoch:  22   step:  109   train loss:  1.8306450843811035  val loss:  4.5374674797058105  val L1 loss:  5.0265\n",
      "epoch:  22   step:  110   train loss:  1.7351652383804321  val loss:  4.628536224365234  val L1 loss:  5.1131\n",
      "epoch:  22   step:  111   train loss:  1.988410472869873  val loss:  4.705704689025879  val L1 loss:  5.1592\n",
      "epoch:  22   step:  112   train loss:  2.334665298461914  val loss:  4.703348159790039  val L1 loss:  5.1626\n",
      "epoch:  22   step:  113   train loss:  3.4680392742156982  val loss:  4.645534038543701  val L1 loss:  5.1259\n",
      "epoch:  22   step:  114   train loss:  2.9066483974456787  val loss:  4.557999610900879  val L1 loss:  5.0376\n",
      "epoch:  22   step:  115   train loss:  3.755631923675537  val loss:  4.605140209197998  val L1 loss:  5.0673\n",
      "epoch:  22   step:  116   train loss:  2.9442691802978516  val loss:  4.630093097686768  val L1 loss:  5.1021\n",
      "epoch:  22   step:  117   train loss:  2.7084689140319824  val loss:  4.6409912109375  val L1 loss:  5.1082\n",
      "epoch:  22   step:  118   train loss:  1.9105638265609741  val loss:  4.630507946014404  val L1 loss:  5.0872\n",
      "epoch:  22   step:  119   train loss:  1.80820631980896  val loss:  4.5948944091796875  val L1 loss:  5.0636\n",
      "epoch:  22   step:  120   train loss:  2.8913607597351074  val loss:  4.539595127105713  val L1 loss:  5.0066\n",
      "epoch:  22   step:  121   train loss:  3.4002203941345215  val loss:  4.504489421844482  val L1 loss:  4.9686\n",
      "epoch:  22   step:  122   train loss:  3.679311990737915  val loss:  4.539286136627197  val L1 loss:  4.9897\n",
      "epoch:  22   step:  123   train loss:  3.3786022663116455  val loss:  4.518021583557129  val L1 loss:  4.9672\n",
      "epoch:  22   step:  124   train loss:  2.839580535888672  val loss:  4.842884063720703  val L1 loss:  5.3249\n",
      "epoch:  22   step:  125   train loss:  2.9375662803649902  val loss:  5.128615379333496  val L1 loss:  5.6093\n",
      "epoch:  22   step:  126   train loss:  2.8208208084106445  val loss:  5.029400825500488  val L1 loss:  5.5074\n",
      "epoch:  22   step:  127   train loss:  4.055624961853027  val loss:  4.983335971832275  val L1 loss:  5.4594\n",
      "epoch:  22   step:  128   train loss:  2.4745969772338867  val loss:  4.546701431274414  val L1 loss:  5.0267\n",
      "epoch:  22   step:  129   train loss:  4.090850830078125  val loss:  4.156479835510254  val L1 loss:  4.6313\n",
      "epoch:  22   step:  130   train loss:  2.2298412322998047  val loss:  4.132514476776123  val L1 loss:  4.5774\n",
      "epoch:  22   step:  131   train loss:  1.5627422332763672  val loss:  4.173832416534424  val L1 loss:  4.6273\n",
      "epoch:  22   step:  132   train loss:  2.259164333343506  val loss:  4.194342136383057  val L1 loss:  4.6629\n",
      "epoch:  22   step:  133   train loss:  1.9165698289871216  val loss:  4.024058818817139  val L1 loss:  4.4665\n",
      "epoch:  22   step:  134   train loss:  4.22041130065918  val loss:  3.877842426300049  val L1 loss:  4.3461\n",
      "epoch:  22   step:  135   train loss:  1.8616265058517456  val loss:  4.002009868621826  val L1 loss:  4.4657\n",
      "epoch:  22   step:  136   train loss:  3.4215261936187744  val loss:  4.311582088470459  val L1 loss:  4.802\n",
      "epoch:  22   step:  137   train loss:  2.3689091205596924  val loss:  4.228797435760498  val L1 loss:  4.7191\n",
      "epoch:  22   step:  138   train loss:  3.418081283569336  val loss:  4.0352463722229  val L1 loss:  4.4764\n",
      "epoch:  22   step:  139   train loss:  3.605236768722534  val loss:  3.964900016784668  val L1 loss:  4.405\n",
      "epoch:  22   step:  140   train loss:  1.9651620388031006  val loss:  4.024727821350098  val L1 loss:  4.4728\n",
      "epoch:  22   step:  141   train loss:  2.4833855628967285  val loss:  4.201194763183594  val L1 loss:  4.6625\n",
      "epoch:  22   step:  142   train loss:  3.2624173164367676  val loss:  4.262317657470703  val L1 loss:  4.7151\n",
      "epoch:  22   step:  143   train loss:  3.146238327026367  val loss:  4.062612056732178  val L1 loss:  4.5451\n",
      "epoch:  22   step:  144   train loss:  4.054812431335449  val loss:  4.237518310546875  val L1 loss:  4.6907\n",
      "epoch:  22   step:  145   train loss:  3.0649185180664062  val loss:  4.355522632598877  val L1 loss:  4.8163\n",
      "epoch:  22   step:  146   train loss:  1.4507536888122559  val loss:  4.554347991943359  val L1 loss:  5.0046\n",
      "epoch:  22   step:  147   train loss:  2.9986038208007812  val loss:  4.614229202270508  val L1 loss:  5.0883\n",
      "epoch:  22   step:  148   train loss:  2.6818320751190186  val loss:  4.5085906982421875  val L1 loss:  4.9986\n",
      "epoch:  22   step:  149   train loss:  1.764780044555664  val loss:  4.487546443939209  val L1 loss:  4.9576\n",
      "epoch:  22   step:  150   train loss:  3.2867631912231445  val loss:  4.625751972198486  val L1 loss:  5.1113\n",
      "epoch:  22   step:  151   train loss:  2.8412208557128906  val loss:  4.622432231903076  val L1 loss:  5.0951\n",
      "epoch:  22   step:  152   train loss:  4.159602165222168  val loss:  4.526793003082275  val L1 loss:  5.0018\n",
      "epoch:  22   step:  153   train loss:  3.185725688934326  val loss:  4.530614852905273  val L1 loss:  5.0151\n",
      "epoch:  22   step:  154   train loss:  2.1568872928619385  val loss:  4.543899059295654  val L1 loss:  5.0182\n",
      "epoch:  22   step:  155   train loss:  3.214524269104004  val loss:  4.483974456787109  val L1 loss:  4.9562\n",
      "epoch:  22   step:  156   train loss:  3.3704910278320312  val loss:  4.345615863800049  val L1 loss:  4.7998\n",
      "epoch:  22   step:  157   train loss:  1.210740327835083  val loss:  4.369363307952881  val L1 loss:  4.8205\n",
      "epoch:  22   step:  158   train loss:  1.9572569131851196  val loss:  4.4600372314453125  val L1 loss:  4.9272\n",
      "epoch:  22   step:  159   train loss:  2.527560234069824  val loss:  4.549069404602051  val L1 loss:  5.0124\n",
      "epoch:  22   step:  160   train loss:  2.0257906913757324  val loss:  4.565430164337158  val L1 loss:  5.0388\n",
      "epoch:  22   step:  161   train loss:  3.409966468811035  val loss:  4.5627851486206055  val L1 loss:  5.0367\n",
      "epoch:  22   step:  162   train loss:  2.8232455253601074  val loss:  4.561268329620361  val L1 loss:  5.0157\n",
      "epoch:  22   step:  163   train loss:  2.2225751876831055  val loss:  4.567642688751221  val L1 loss:  5.033\n",
      "epoch:  22   step:  164   train loss:  2.533017158508301  val loss:  4.621214389801025  val L1 loss:  5.1015\n",
      "epoch:  22   step:  165   train loss:  1.9230711460113525  val loss:  4.857551574707031  val L1 loss:  5.3424\n",
      "epoch:  22   step:  166   train loss:  2.1070947647094727  val loss:  5.098168849945068  val L1 loss:  5.5795\n",
      "epoch:  22   step:  167   train loss:  2.1788859367370605  val loss:  5.1686601638793945  val L1 loss:  5.6543\n",
      "epoch:  22   step:  168   train loss:  2.2371702194213867  val loss:  5.20167350769043  val L1 loss:  5.6856\n",
      "epoch:  22   step:  169   train loss:  2.781101703643799  val loss:  5.1092634201049805  val L1 loss:  5.5954\n",
      "epoch:  22   step:  170   train loss:  2.634000778198242  val loss:  4.773534297943115  val L1 loss:  5.2419\n",
      "epoch:  22   step:  171   train loss:  2.2657968997955322  val loss:  4.679057598114014  val L1 loss:  5.1511\n",
      "epoch:  22   step:  172   train loss:  3.4001057147979736  val loss:  4.592205047607422  val L1 loss:  5.0526\n",
      "epoch:  22   step:  173   train loss:  1.719876766204834  val loss:  4.567144870758057  val L1 loss:  5.0411\n",
      "epoch:  22   step:  174   train loss:  2.60469388961792  val loss:  4.486055374145508  val L1 loss:  4.9606\n",
      "epoch:  22   step:  175   train loss:  1.6556659936904907  val loss:  4.646847248077393  val L1 loss:  5.1216\n",
      "epoch:  22   step:  176   train loss:  3.0380196571350098  val loss:  4.651599407196045  val L1 loss:  5.1165\n",
      "epoch:  22   step:  177   train loss:  2.6302595138549805  val loss:  4.494355201721191  val L1 loss:  4.9512\n",
      "epoch:  22   step:  178   train loss:  2.751800298690796  val loss:  4.292906284332275  val L1 loss:  4.7653\n",
      "epoch:  22   step:  179   train loss:  3.4789552688598633  val loss:  4.54042387008667  val L1 loss:  4.9903\n",
      "epoch:  22   step:  180   train loss:  4.045247554779053  val loss:  5.01640510559082  val L1 loss:  5.488\n",
      "epoch:  22   step:  181   train loss:  2.9379918575286865  val loss:  5.094381809234619  val L1 loss:  5.5646\n",
      "epoch:  22   step:  182   train loss:  3.3631277084350586  val loss:  4.61055850982666  val L1 loss:  5.0858\n",
      "epoch:  22   step:  183   train loss:  2.9071502685546875  val loss:  4.228464126586914  val L1 loss:  4.6798\n",
      "epoch:  22   step:  184   train loss:  1.7181681394577026  val loss:  4.811598777770996  val L1 loss:  5.2806\n",
      "epoch:  22   step:  185   train loss:  2.481314182281494  val loss:  5.311070442199707  val L1 loss:  5.7971\n",
      "epoch:  22   step:  186   train loss:  3.4198780059814453  val loss:  5.1232171058654785  val L1 loss:  5.6065\n",
      "epoch:  22   step:  187   train loss:  3.7839105129241943  val loss:  4.524251461029053  val L1 loss:  4.9645\n",
      "epoch:  22   step:  188   train loss:  3.4380507469177246  val loss:  4.216215133666992  val L1 loss:  4.6896\n",
      "epoch:  22   step:  189   train loss:  1.6532113552093506  val loss:  4.249029636383057  val L1 loss:  4.7222\n",
      "epoch:  22   step:  190   train loss:  3.1436944007873535  val loss:  4.498240947723389  val L1 loss:  4.9676\n",
      "epoch:  22   step:  191   train loss:  2.799997329711914  val loss:  4.756099224090576  val L1 loss:  5.2289\n",
      "epoch:  22   step:  192   train loss:  2.940434455871582  val loss:  4.760005950927734  val L1 loss:  5.2378\n",
      "epoch:  22   step:  193   train loss:  2.963165521621704  val loss:  4.388802528381348  val L1 loss:  4.8637\n",
      "epoch:  22   step:  194   train loss:  2.5933876037597656  val loss:  4.280727386474609  val L1 loss:  4.7493\n",
      "epoch:  22   step:  195   train loss:  1.995482087135315  val loss:  4.214253902435303  val L1 loss:  4.6776\n",
      "epoch:  22   step:  196   train loss:  1.4247033596038818  val loss:  4.151942253112793  val L1 loss:  4.6002\n",
      "epoch:  22   step:  197   train loss:  3.129106283187866  val loss:  4.144465446472168  val L1 loss:  4.5905\n",
      "epoch:  22   step:  198   train loss:  2.920966148376465  val loss:  4.1367058753967285  val L1 loss:  4.5847\n",
      "epoch:  22   step:  199   train loss:  2.4359688758850098  val loss:  4.171581268310547  val L1 loss:  4.638\n",
      "epoch:  22   step:  200   train loss:  1.5604736804962158  val loss:  4.030433654785156  val L1 loss:  4.4977\n",
      "epoch:  22   step:  201   train loss:  3.133671760559082  val loss:  3.927821636199951  val L1 loss:  4.3846\n",
      "epoch:  22   step:  202   train loss:  2.9902172088623047  val loss:  4.01801061630249  val L1 loss:  4.4785\n",
      "epoch:  22   step:  203   train loss:  2.8152129650115967  val loss:  4.131718635559082  val L1 loss:  4.5908\n",
      "epoch:  22   step:  204   train loss:  2.7736124992370605  val loss:  4.2195725440979  val L1 loss:  4.6823\n",
      "epoch:  22   step:  205   train loss:  2.941528797149658  val loss:  4.200332164764404  val L1 loss:  4.6702\n",
      "epoch:  22   step:  206   train loss:  2.2448174953460693  val loss:  4.280874729156494  val L1 loss:  4.7523\n",
      "epoch:  22   step:  207   train loss:  1.5074372291564941  val loss:  4.369317531585693  val L1 loss:  4.8252\n",
      "epoch:  22   step:  208   train loss:  3.093320608139038  val loss:  4.46890926361084  val L1 loss:  4.9407\n",
      "epoch:  23   step:  0   train loss:  2.322186231613159  val loss:  4.3843560218811035  val L1 loss:  4.8583\n",
      "epoch:  23   step:  1   train loss:  1.2994006872177124  val loss:  4.117335796356201  val L1 loss:  4.5757\n",
      "epoch:  23   step:  2   train loss:  2.987863540649414  val loss:  3.9250688552856445  val L1 loss:  4.3772\n",
      "epoch:  23   step:  3   train loss:  2.357743263244629  val loss:  4.126615047454834  val L1 loss:  4.5877\n",
      "epoch:  23   step:  4   train loss:  3.1243953704833984  val loss:  4.734766960144043  val L1 loss:  5.2303\n",
      "epoch:  23   step:  5   train loss:  3.1750426292419434  val loss:  5.121837139129639  val L1 loss:  5.6054\n",
      "epoch:  23   step:  6   train loss:  2.2962646484375  val loss:  5.077739715576172  val L1 loss:  5.5641\n",
      "epoch:  23   step:  7   train loss:  1.0493165254592896  val loss:  4.90289831161499  val L1 loss:  5.3933\n",
      "epoch:  23   step:  8   train loss:  2.8049538135528564  val loss:  4.53463077545166  val L1 loss:  5.0164\n",
      "epoch:  23   step:  9   train loss:  2.121598243713379  val loss:  4.19791841506958  val L1 loss:  4.6585\n",
      "epoch:  23   step:  10   train loss:  1.613921046257019  val loss:  4.095098972320557  val L1 loss:  4.5582\n",
      "epoch:  23   step:  11   train loss:  3.3146915435791016  val loss:  4.115201473236084  val L1 loss:  4.5709\n",
      "epoch:  23   step:  12   train loss:  1.9969686269760132  val loss:  4.173656463623047  val L1 loss:  4.639\n",
      "epoch:  23   step:  13   train loss:  2.4227468967437744  val loss:  4.293717384338379  val L1 loss:  4.7516\n",
      "epoch:  23   step:  14   train loss:  2.9589643478393555  val loss:  4.328660011291504  val L1 loss:  4.7936\n",
      "epoch:  23   step:  15   train loss:  2.8806068897247314  val loss:  4.360980033874512  val L1 loss:  4.8358\n",
      "epoch:  23   step:  16   train loss:  1.6143014430999756  val loss:  4.378997325897217  val L1 loss:  4.8399\n",
      "epoch:  23   step:  17   train loss:  3.3513059616088867  val loss:  4.392815589904785  val L1 loss:  4.8384\n",
      "epoch:  23   step:  18   train loss:  2.757725715637207  val loss:  4.36262845993042  val L1 loss:  4.8088\n",
      "epoch:  23   step:  19   train loss:  1.8389042615890503  val loss:  4.375403881072998  val L1 loss:  4.8282\n",
      "epoch:  23   step:  20   train loss:  2.082554578781128  val loss:  4.431275367736816  val L1 loss:  4.8756\n",
      "epoch:  23   step:  21   train loss:  2.6181626319885254  val loss:  4.360037326812744  val L1 loss:  4.7966\n",
      "epoch:  23   step:  22   train loss:  2.9072265625  val loss:  4.387355804443359  val L1 loss:  4.8504\n",
      "epoch:  23   step:  23   train loss:  2.233165740966797  val loss:  4.37830924987793  val L1 loss:  4.8348\n",
      "epoch:  23   step:  24   train loss:  1.6787784099578857  val loss:  4.327892303466797  val L1 loss:  4.7832\n",
      "epoch:  23   step:  25   train loss:  1.9165955781936646  val loss:  4.299381732940674  val L1 loss:  4.7559\n",
      "epoch:  23   step:  26   train loss:  2.11356782913208  val loss:  4.293432712554932  val L1 loss:  4.7474\n",
      "epoch:  23   step:  27   train loss:  1.628882646560669  val loss:  4.295604705810547  val L1 loss:  4.748\n",
      "epoch:  23   step:  28   train loss:  3.886807441711426  val loss:  4.3315277099609375  val L1 loss:  4.8078\n",
      "epoch:  23   step:  29   train loss:  2.8840506076812744  val loss:  4.372448444366455  val L1 loss:  4.8533\n",
      "epoch:  23   step:  30   train loss:  3.3438119888305664  val loss:  4.357316970825195  val L1 loss:  4.8263\n",
      "epoch:  23   step:  31   train loss:  2.206066131591797  val loss:  4.472543716430664  val L1 loss:  4.9242\n",
      "epoch:  23   step:  32   train loss:  1.348231554031372  val loss:  4.803049564361572  val L1 loss:  5.2828\n",
      "epoch:  23   step:  33   train loss:  1.1665558815002441  val loss:  5.111913204193115  val L1 loss:  5.5888\n",
      "epoch:  23   step:  34   train loss:  1.3114771842956543  val loss:  5.459971904754639  val L1 loss:  5.9509\n",
      "epoch:  23   step:  35   train loss:  2.8706798553466797  val loss:  5.433005332946777  val L1 loss:  5.923\n",
      "epoch:  23   step:  36   train loss:  2.4773120880126953  val loss:  4.9157819747924805  val L1 loss:  5.3749\n",
      "epoch:  23   step:  37   train loss:  2.869248628616333  val loss:  4.597569942474365  val L1 loss:  5.0758\n",
      "epoch:  23   step:  38   train loss:  4.642183303833008  val loss:  4.525733947753906  val L1 loss:  5.0038\n",
      "epoch:  23   step:  39   train loss:  4.0415544509887695  val loss:  4.486053466796875  val L1 loss:  4.9761\n",
      "epoch:  23   step:  40   train loss:  1.8851194381713867  val loss:  4.581521987915039  val L1 loss:  5.0243\n",
      "epoch:  23   step:  41   train loss:  4.170969486236572  val loss:  4.892097473144531  val L1 loss:  5.3793\n",
      "epoch:  23   step:  42   train loss:  4.048105239868164  val loss:  4.921009540557861  val L1 loss:  5.4073\n",
      "epoch:  23   step:  43   train loss:  3.104964017868042  val loss:  4.539855480194092  val L1 loss:  4.9955\n",
      "epoch:  23   step:  44   train loss:  3.322719097137451  val loss:  4.332362651824951  val L1 loss:  4.8045\n",
      "epoch:  23   step:  45   train loss:  2.3102078437805176  val loss:  4.538847923278809  val L1 loss:  5.0246\n",
      "epoch:  23   step:  46   train loss:  3.360656261444092  val loss:  4.876423358917236  val L1 loss:  5.3298\n",
      "epoch:  23   step:  47   train loss:  3.626408338546753  val loss:  4.745748043060303  val L1 loss:  5.218\n",
      "epoch:  23   step:  48   train loss:  2.700922966003418  val loss:  4.379054069519043  val L1 loss:  4.8494\n",
      "epoch:  23   step:  49   train loss:  2.1758062839508057  val loss:  4.356314182281494  val L1 loss:  4.8144\n",
      "epoch:  23   step:  50   train loss:  2.6569013595581055  val loss:  4.593476295471191  val L1 loss:  5.0461\n",
      "epoch:  23   step:  51   train loss:  3.3016061782836914  val loss:  4.606420993804932  val L1 loss:  5.0723\n",
      "epoch:  23   step:  52   train loss:  3.6874186992645264  val loss:  4.409471035003662  val L1 loss:  4.8818\n",
      "epoch:  23   step:  53   train loss:  2.784383773803711  val loss:  4.502560138702393  val L1 loss:  4.9793\n",
      "epoch:  23   step:  54   train loss:  1.6635351181030273  val loss:  4.853214740753174  val L1 loss:  5.3147\n",
      "epoch:  23   step:  55   train loss:  1.6568197011947632  val loss:  5.094398021697998  val L1 loss:  5.5533\n",
      "epoch:  23   step:  56   train loss:  2.1051549911499023  val loss:  4.891724586486816  val L1 loss:  5.3584\n",
      "epoch:  23   step:  57   train loss:  2.0681028366088867  val loss:  4.564449787139893  val L1 loss:  5.0033\n",
      "epoch:  23   step:  58   train loss:  5.14504337310791  val loss:  4.575840950012207  val L1 loss:  5.0094\n",
      "epoch:  23   step:  59   train loss:  2.516026496887207  val loss:  4.889704704284668  val L1 loss:  5.3698\n",
      "epoch:  23   step:  60   train loss:  2.9166927337646484  val loss:  4.931211471557617  val L1 loss:  5.4043\n",
      "epoch:  23   step:  61   train loss:  2.6753969192504883  val loss:  4.917574405670166  val L1 loss:  5.3886\n",
      "epoch:  23   step:  62   train loss:  3.3782806396484375  val loss:  4.679548740386963  val L1 loss:  5.1465\n",
      "epoch:  23   step:  63   train loss:  2.3785014152526855  val loss:  4.4866838455200195  val L1 loss:  4.9288\n",
      "epoch:  23   step:  64   train loss:  2.265186309814453  val loss:  4.362612724304199  val L1 loss:  4.8429\n",
      "epoch:  23   step:  65   train loss:  2.4597342014312744  val loss:  4.383553981781006  val L1 loss:  4.8669\n",
      "epoch:  23   step:  66   train loss:  2.9811534881591797  val loss:  4.36090087890625  val L1 loss:  4.8293\n",
      "epoch:  23   step:  67   train loss:  3.6213533878326416  val loss:  4.463068962097168  val L1 loss:  4.9481\n",
      "epoch:  23   step:  68   train loss:  1.722488284111023  val loss:  4.607446193695068  val L1 loss:  5.0819\n",
      "epoch:  23   step:  69   train loss:  2.380671977996826  val loss:  4.594379901885986  val L1 loss:  5.069\n",
      "epoch:  23   step:  70   train loss:  3.8012964725494385  val loss:  4.515622138977051  val L1 loss:  4.9965\n",
      "epoch:  23   step:  71   train loss:  2.1421637535095215  val loss:  4.512605667114258  val L1 loss:  4.9855\n",
      "epoch:  23   step:  72   train loss:  2.7325072288513184  val loss:  4.460330486297607  val L1 loss:  4.9105\n",
      "epoch:  23   step:  73   train loss:  1.5012718439102173  val loss:  4.540305137634277  val L1 loss:  4.985\n",
      "epoch:  23   step:  74   train loss:  2.334427833557129  val loss:  4.772664546966553  val L1 loss:  5.2419\n",
      "epoch:  23   step:  75   train loss:  1.6399717330932617  val loss:  5.282954216003418  val L1 loss:  5.7672\n",
      "epoch:  23   step:  76   train loss:  3.44913649559021  val loss:  5.781494140625  val L1 loss:  6.2686\n",
      "epoch:  23   step:  77   train loss:  1.9570746421813965  val loss:  5.935892105102539  val L1 loss:  6.4184\n",
      "epoch:  23   step:  78   train loss:  3.583278179168701  val loss:  5.475625991821289  val L1 loss:  5.9551\n",
      "epoch:  23   step:  79   train loss:  1.673648715019226  val loss:  4.954361915588379  val L1 loss:  5.4398\n",
      "epoch:  23   step:  80   train loss:  2.7569990158081055  val loss:  4.63483190536499  val L1 loss:  5.1267\n",
      "epoch:  23   step:  81   train loss:  2.3299617767333984  val loss:  4.339765548706055  val L1 loss:  4.8145\n",
      "epoch:  23   step:  82   train loss:  2.1606428623199463  val loss:  4.145894527435303  val L1 loss:  4.6336\n",
      "epoch:  23   step:  83   train loss:  1.8840749263763428  val loss:  4.14210844039917  val L1 loss:  4.6129\n",
      "epoch:  23   step:  84   train loss:  2.1581039428710938  val loss:  4.1525559425354  val L1 loss:  4.6359\n",
      "epoch:  23   step:  85   train loss:  2.2819085121154785  val loss:  4.068061828613281  val L1 loss:  4.5425\n",
      "epoch:  23   step:  86   train loss:  2.5129685401916504  val loss:  4.030531406402588  val L1 loss:  4.4889\n",
      "epoch:  23   step:  87   train loss:  2.2025346755981445  val loss:  3.9323344230651855  val L1 loss:  4.3918\n",
      "epoch:  23   step:  88   train loss:  2.2174036502838135  val loss:  3.9256837368011475  val L1 loss:  4.3947\n",
      "epoch:  23   step:  89   train loss:  3.3702995777130127  val loss:  3.962639570236206  val L1 loss:  4.4221\n",
      "epoch:  23   step:  90   train loss:  2.341951847076416  val loss:  4.3946075439453125  val L1 loss:  4.8599\n",
      "epoch:  23   step:  91   train loss:  5.183718681335449  val loss:  5.313838005065918  val L1 loss:  5.7966\n",
      "epoch:  23   step:  92   train loss:  2.2090158462524414  val loss:  6.137111186981201  val L1 loss:  6.6275\n",
      "epoch:  23   step:  93   train loss:  3.390397548675537  val loss:  6.269180774688721  val L1 loss:  6.7509\n",
      "epoch:  23   step:  94   train loss:  3.2367868423461914  val loss:  5.688953876495361  val L1 loss:  6.1774\n",
      "epoch:  23   step:  95   train loss:  2.899097442626953  val loss:  4.853046894073486  val L1 loss:  5.337\n",
      "epoch:  23   step:  96   train loss:  2.060298204421997  val loss:  4.147496223449707  val L1 loss:  4.6125\n",
      "epoch:  23   step:  97   train loss:  2.911625862121582  val loss:  4.002745151519775  val L1 loss:  4.4683\n",
      "epoch:  23   step:  98   train loss:  2.4479317665100098  val loss:  3.901217460632324  val L1 loss:  4.3726\n",
      "epoch:  23   step:  99   train loss:  2.2029354572296143  val loss:  3.8599493503570557  val L1 loss:  4.3409\n",
      "epoch:  23   step:  100   train loss:  2.805419445037842  val loss:  3.944972515106201  val L1 loss:  4.4147\n",
      "epoch:  23   step:  101   train loss:  3.476065158843994  val loss:  4.009378433227539  val L1 loss:  4.487\n",
      "epoch:  23   step:  102   train loss:  1.6503428220748901  val loss:  4.06094217300415  val L1 loss:  4.5401\n",
      "epoch:  23   step:  103   train loss:  3.694520950317383  val loss:  4.051654815673828  val L1 loss:  4.5239\n",
      "epoch:  23   step:  104   train loss:  3.2473201751708984  val loss:  3.8557116985321045  val L1 loss:  4.3162\n",
      "epoch:  23   step:  105   train loss:  3.657961368560791  val loss:  3.737084150314331  val L1 loss:  4.1943\n",
      "min_val_loss_print 3.737084150314331\n",
      "epoch:  23   step:  106   train loss:  3.759934186935425  val loss:  3.797318935394287  val L1 loss:  4.2554\n",
      "epoch:  23   step:  107   train loss:  3.8165438175201416  val loss:  3.869617462158203  val L1 loss:  4.3401\n",
      "epoch:  23   step:  108   train loss:  2.2978460788726807  val loss:  4.035114765167236  val L1 loss:  4.5108\n",
      "epoch:  23   step:  109   train loss:  3.0235090255737305  val loss:  4.2018537521362305  val L1 loss:  4.6705\n",
      "epoch:  23   step:  110   train loss:  2.9227206707000732  val loss:  4.461498737335205  val L1 loss:  4.9115\n",
      "epoch:  23   step:  111   train loss:  4.1640801429748535  val loss:  4.248744487762451  val L1 loss:  4.7157\n",
      "epoch:  23   step:  112   train loss:  2.5213801860809326  val loss:  3.9959514141082764  val L1 loss:  4.4574\n",
      "epoch:  23   step:  113   train loss:  2.700422763824463  val loss:  4.413964748382568  val L1 loss:  4.8973\n",
      "epoch:  23   step:  114   train loss:  2.1649978160858154  val loss:  4.997392654418945  val L1 loss:  5.4768\n",
      "epoch:  23   step:  115   train loss:  3.2079572677612305  val loss:  5.234670162200928  val L1 loss:  5.7201\n",
      "epoch:  23   step:  116   train loss:  2.027435779571533  val loss:  5.052905559539795  val L1 loss:  5.5065\n",
      "epoch:  23   step:  117   train loss:  2.281817674636841  val loss:  4.957415580749512  val L1 loss:  5.4203\n",
      "epoch:  23   step:  118   train loss:  3.376087188720703  val loss:  4.9465789794921875  val L1 loss:  5.4139\n",
      "epoch:  23   step:  119   train loss:  4.222718238830566  val loss:  4.8061041831970215  val L1 loss:  5.2725\n",
      "epoch:  23   step:  120   train loss:  2.728194236755371  val loss:  4.572621822357178  val L1 loss:  5.025\n",
      "epoch:  23   step:  121   train loss:  2.556649684906006  val loss:  4.3689470291137695  val L1 loss:  4.8211\n",
      "epoch:  23   step:  122   train loss:  2.3478140830993652  val loss:  4.300726413726807  val L1 loss:  4.7519\n",
      "epoch:  23   step:  123   train loss:  1.9969558715820312  val loss:  4.313132286071777  val L1 loss:  4.7635\n",
      "epoch:  23   step:  124   train loss:  2.7265896797180176  val loss:  4.426538467407227  val L1 loss:  4.8976\n",
      "epoch:  23   step:  125   train loss:  2.7831621170043945  val loss:  4.547189712524414  val L1 loss:  5.0297\n",
      "epoch:  23   step:  126   train loss:  1.938427448272705  val loss:  4.601724624633789  val L1 loss:  5.0856\n",
      "epoch:  23   step:  127   train loss:  4.259835243225098  val loss:  4.707629680633545  val L1 loss:  5.1862\n",
      "epoch:  23   step:  128   train loss:  2.821913719177246  val loss:  5.027626037597656  val L1 loss:  5.4991\n",
      "epoch:  23   step:  129   train loss:  3.1237168312072754  val loss:  5.484846115112305  val L1 loss:  5.9578\n",
      "epoch:  23   step:  130   train loss:  1.7040870189666748  val loss:  5.952223777770996  val L1 loss:  6.4324\n",
      "epoch:  23   step:  131   train loss:  3.8394246101379395  val loss:  5.838850021362305  val L1 loss:  6.3288\n",
      "epoch:  23   step:  132   train loss:  2.5201926231384277  val loss:  5.300940990447998  val L1 loss:  5.7892\n",
      "epoch:  23   step:  133   train loss:  1.9906675815582275  val loss:  4.822977542877197  val L1 loss:  5.2868\n",
      "epoch:  23   step:  134   train loss:  3.019744873046875  val loss:  4.845734119415283  val L1 loss:  5.3138\n",
      "epoch:  23   step:  135   train loss:  1.66839599609375  val loss:  5.240358829498291  val L1 loss:  5.7218\n",
      "epoch:  23   step:  136   train loss:  5.8388872146606445  val loss:  5.4024152755737305  val L1 loss:  5.8894\n",
      "epoch:  23   step:  137   train loss:  4.194131851196289  val loss:  5.164376258850098  val L1 loss:  5.6165\n",
      "epoch:  23   step:  138   train loss:  1.678114414215088  val loss:  4.939142227172852  val L1 loss:  5.3993\n",
      "epoch:  23   step:  139   train loss:  3.5518646240234375  val loss:  5.034238338470459  val L1 loss:  5.518\n",
      "epoch:  23   step:  140   train loss:  2.3675990104675293  val loss:  5.0977582931518555  val L1 loss:  5.5785\n",
      "epoch:  23   step:  141   train loss:  2.493102788925171  val loss:  5.078464031219482  val L1 loss:  5.5615\n",
      "epoch:  23   step:  142   train loss:  2.0744123458862305  val loss:  4.976866245269775  val L1 loss:  5.4529\n",
      "epoch:  23   step:  143   train loss:  2.3714327812194824  val loss:  4.830908298492432  val L1 loss:  5.2981\n",
      "epoch:  23   step:  144   train loss:  2.30256986618042  val loss:  4.770378112792969  val L1 loss:  5.2382\n",
      "epoch:  23   step:  145   train loss:  2.8680710792541504  val loss:  4.827180862426758  val L1 loss:  5.2893\n",
      "epoch:  23   step:  146   train loss:  1.9516271352767944  val loss:  4.835200309753418  val L1 loss:  5.2999\n",
      "epoch:  23   step:  147   train loss:  2.3814404010772705  val loss:  4.773324012756348  val L1 loss:  5.2489\n",
      "epoch:  23   step:  148   train loss:  3.5248427391052246  val loss:  4.773812294006348  val L1 loss:  5.248\n",
      "epoch:  23   step:  149   train loss:  2.0834364891052246  val loss:  4.723391056060791  val L1 loss:  5.1965\n",
      "epoch:  23   step:  150   train loss:  2.1957926750183105  val loss:  4.703723430633545  val L1 loss:  5.1769\n",
      "epoch:  23   step:  151   train loss:  2.8847384452819824  val loss:  4.698395729064941  val L1 loss:  5.1481\n",
      "epoch:  23   step:  152   train loss:  3.3955564498901367  val loss:  4.750674724578857  val L1 loss:  5.2161\n",
      "epoch:  23   step:  153   train loss:  2.2436537742614746  val loss:  4.7516398429870605  val L1 loss:  5.2253\n",
      "epoch:  23   step:  154   train loss:  2.598156452178955  val loss:  4.7490234375  val L1 loss:  5.2275\n",
      "epoch:  23   step:  155   train loss:  2.173027515411377  val loss:  4.696175575256348  val L1 loss:  5.1682\n",
      "epoch:  23   step:  156   train loss:  3.011120319366455  val loss:  4.56825065612793  val L1 loss:  5.032\n",
      "epoch:  23   step:  157   train loss:  2.7523975372314453  val loss:  4.4862542152404785  val L1 loss:  4.9231\n",
      "epoch:  23   step:  158   train loss:  1.9392104148864746  val loss:  4.470519542694092  val L1 loss:  4.9308\n",
      "epoch:  23   step:  159   train loss:  3.6563549041748047  val loss:  4.517205238342285  val L1 loss:  4.9839\n",
      "epoch:  23   step:  160   train loss:  1.950059413909912  val loss:  4.570637226104736  val L1 loss:  5.0477\n",
      "epoch:  23   step:  161   train loss:  2.447934150695801  val loss:  4.600583553314209  val L1 loss:  5.0778\n",
      "epoch:  23   step:  162   train loss:  2.8251793384552  val loss:  4.586019515991211  val L1 loss:  5.0597\n",
      "epoch:  23   step:  163   train loss:  1.2077258825302124  val loss:  4.599544048309326  val L1 loss:  5.0586\n",
      "epoch:  23   step:  164   train loss:  1.5418590307235718  val loss:  4.815992832183838  val L1 loss:  5.2835\n",
      "epoch:  23   step:  165   train loss:  3.232714891433716  val loss:  4.930360794067383  val L1 loss:  5.3949\n",
      "epoch:  23   step:  166   train loss:  1.562211036682129  val loss:  4.827891826629639  val L1 loss:  5.2833\n",
      "epoch:  23   step:  167   train loss:  2.466961622238159  val loss:  4.735857009887695  val L1 loss:  5.2053\n",
      "epoch:  23   step:  168   train loss:  1.3998254537582397  val loss:  4.675806045532227  val L1 loss:  5.1598\n",
      "epoch:  23   step:  169   train loss:  2.5578153133392334  val loss:  4.624484062194824  val L1 loss:  5.1086\n",
      "epoch:  23   step:  170   train loss:  2.04455828666687  val loss:  4.511756896972656  val L1 loss:  4.9955\n",
      "epoch:  23   step:  171   train loss:  2.3886914253234863  val loss:  4.394144058227539  val L1 loss:  4.8767\n",
      "epoch:  23   step:  172   train loss:  1.9160518646240234  val loss:  4.297128200531006  val L1 loss:  4.7803\n",
      "epoch:  23   step:  173   train loss:  2.3975841999053955  val loss:  4.181762218475342  val L1 loss:  4.6423\n",
      "epoch:  23   step:  174   train loss:  4.49224328994751  val loss:  4.240931987762451  val L1 loss:  4.6851\n",
      "epoch:  23   step:  175   train loss:  1.810373306274414  val loss:  4.406625270843506  val L1 loss:  4.8619\n",
      "epoch:  23   step:  176   train loss:  2.313406467437744  val loss:  4.8013482093811035  val L1 loss:  5.274\n",
      "epoch:  23   step:  177   train loss:  3.2451071739196777  val loss:  4.85200309753418  val L1 loss:  5.3212\n",
      "epoch:  23   step:  178   train loss:  3.181947708129883  val loss:  4.5032877922058105  val L1 loss:  4.97\n",
      "epoch:  23   step:  179   train loss:  2.06752610206604  val loss:  4.22996187210083  val L1 loss:  4.6897\n",
      "epoch:  23   step:  180   train loss:  2.543139934539795  val loss:  4.234652996063232  val L1 loss:  4.7045\n",
      "epoch:  23   step:  181   train loss:  2.5088703632354736  val loss:  4.521413326263428  val L1 loss:  4.9913\n",
      "epoch:  23   step:  182   train loss:  2.018305540084839  val loss:  5.218318462371826  val L1 loss:  5.7016\n",
      "epoch:  23   step:  183   train loss:  2.306558847427368  val loss:  5.43573522567749  val L1 loss:  5.9157\n",
      "epoch:  23   step:  184   train loss:  4.140983581542969  val loss:  4.997379302978516  val L1 loss:  5.4821\n",
      "epoch:  23   step:  185   train loss:  3.1650514602661133  val loss:  4.619327068328857  val L1 loss:  5.0808\n",
      "epoch:  23   step:  186   train loss:  2.312191963195801  val loss:  4.53314733505249  val L1 loss:  5.0029\n",
      "epoch:  23   step:  187   train loss:  2.473848342895508  val loss:  4.341516494750977  val L1 loss:  4.7915\n",
      "epoch:  23   step:  188   train loss:  2.8213400840759277  val loss:  4.245253086090088  val L1 loss:  4.6844\n",
      "epoch:  23   step:  189   train loss:  4.00706672668457  val loss:  4.2582831382751465  val L1 loss:  4.7387\n",
      "epoch:  23   step:  190   train loss:  2.3441991806030273  val loss:  4.245916843414307  val L1 loss:  4.7244\n",
      "epoch:  23   step:  191   train loss:  1.8033779859542847  val loss:  4.278509616851807  val L1 loss:  4.7318\n",
      "epoch:  23   step:  192   train loss:  2.614661931991577  val loss:  4.377831935882568  val L1 loss:  4.8381\n",
      "epoch:  23   step:  193   train loss:  2.3553409576416016  val loss:  4.430721759796143  val L1 loss:  4.8932\n",
      "epoch:  23   step:  194   train loss:  1.7691314220428467  val loss:  4.447667598724365  val L1 loss:  4.9119\n",
      "epoch:  23   step:  195   train loss:  2.6172993183135986  val loss:  4.530892372131348  val L1 loss:  4.9919\n",
      "epoch:  23   step:  196   train loss:  3.7528090476989746  val loss:  5.063077449798584  val L1 loss:  5.5401\n",
      "epoch:  23   step:  197   train loss:  2.1777586936950684  val loss:  5.717939376831055  val L1 loss:  6.2133\n",
      "epoch:  23   step:  198   train loss:  3.3917489051818848  val loss:  5.86641788482666  val L1 loss:  6.3572\n",
      "epoch:  23   step:  199   train loss:  3.27578067779541  val loss:  5.352973461151123  val L1 loss:  5.847\n",
      "epoch:  23   step:  200   train loss:  2.5220460891723633  val loss:  4.556617259979248  val L1 loss:  4.997\n",
      "epoch:  23   step:  201   train loss:  1.9717258214950562  val loss:  4.614951133728027  val L1 loss:  5.0869\n",
      "epoch:  23   step:  202   train loss:  3.8031671047210693  val loss:  4.683962821960449  val L1 loss:  5.1468\n",
      "epoch:  23   step:  203   train loss:  1.4636220932006836  val loss:  4.642488956451416  val L1 loss:  5.1202\n",
      "epoch:  23   step:  204   train loss:  2.567500591278076  val loss:  4.6577324867248535  val L1 loss:  5.1322\n",
      "epoch:  23   step:  205   train loss:  2.1735267639160156  val loss:  4.973350524902344  val L1 loss:  5.451\n",
      "epoch:  23   step:  206   train loss:  3.17208194732666  val loss:  5.341474533081055  val L1 loss:  5.8255\n",
      "epoch:  23   step:  207   train loss:  3.5911612510681152  val loss:  5.599066734313965  val L1 loss:  6.0769\n",
      "epoch:  23   step:  208   train loss:  2.322953701019287  val loss:  5.479208946228027  val L1 loss:  5.9608\n",
      "epoch:  24   step:  0   train loss:  3.6367340087890625  val loss:  5.135304927825928  val L1 loss:  5.5983\n",
      "epoch:  24   step:  1   train loss:  1.9088623523712158  val loss:  5.0096435546875  val L1 loss:  5.4884\n",
      "epoch:  24   step:  2   train loss:  2.5098578929901123  val loss:  4.923190593719482  val L1 loss:  5.3996\n",
      "epoch:  24   step:  3   train loss:  1.8307260274887085  val loss:  4.871999263763428  val L1 loss:  5.3494\n",
      "epoch:  24   step:  4   train loss:  3.3127152919769287  val loss:  4.838331699371338  val L1 loss:  5.3089\n",
      "epoch:  24   step:  5   train loss:  2.8358845710754395  val loss:  4.739258289337158  val L1 loss:  5.2307\n",
      "epoch:  24   step:  6   train loss:  0.92950439453125  val loss:  4.589436054229736  val L1 loss:  5.0711\n",
      "epoch:  24   step:  7   train loss:  1.913425326347351  val loss:  4.430266857147217  val L1 loss:  4.8858\n",
      "epoch:  24   step:  8   train loss:  2.3751168251037598  val loss:  4.3296732902526855  val L1 loss:  4.7915\n",
      "epoch:  24   step:  9   train loss:  3.011885166168213  val loss:  4.3173346519470215  val L1 loss:  4.7847\n",
      "epoch:  24   step:  10   train loss:  1.780818223953247  val loss:  4.331210136413574  val L1 loss:  4.8079\n",
      "epoch:  24   step:  11   train loss:  1.084891438484192  val loss:  4.304781436920166  val L1 loss:  4.7875\n",
      "epoch:  24   step:  12   train loss:  2.8386669158935547  val loss:  4.2660064697265625  val L1 loss:  4.7432\n",
      "epoch:  24   step:  13   train loss:  2.5283665657043457  val loss:  4.051973819732666  val L1 loss:  4.5076\n",
      "epoch:  24   step:  14   train loss:  1.6300363540649414  val loss:  3.8555314540863037  val L1 loss:  4.309\n",
      "epoch:  24   step:  15   train loss:  2.2993969917297363  val loss:  3.79095458984375  val L1 loss:  4.2438\n",
      "epoch:  24   step:  16   train loss:  1.6303722858428955  val loss:  3.7877566814422607  val L1 loss:  4.2479\n",
      "epoch:  24   step:  17   train loss:  2.3806495666503906  val loss:  3.9544684886932373  val L1 loss:  4.4179\n",
      "epoch:  24   step:  18   train loss:  2.0791289806365967  val loss:  4.254364013671875  val L1 loss:  4.7243\n",
      "epoch:  24   step:  19   train loss:  2.5447981357574463  val loss:  4.345296859741211  val L1 loss:  4.8132\n",
      "epoch:  24   step:  20   train loss:  3.754964828491211  val loss:  4.212826251983643  val L1 loss:  4.6733\n",
      "epoch:  24   step:  21   train loss:  3.65478515625  val loss:  3.99326491355896  val L1 loss:  4.4401\n",
      "epoch:  24   step:  22   train loss:  1.90778648853302  val loss:  4.148177146911621  val L1 loss:  4.6078\n",
      "epoch:  24   step:  23   train loss:  2.422701835632324  val loss:  4.475413799285889  val L1 loss:  4.9496\n",
      "epoch:  24   step:  24   train loss:  2.150336742401123  val loss:  4.66596794128418  val L1 loss:  5.131\n",
      "epoch:  24   step:  25   train loss:  2.4168901443481445  val loss:  4.589141845703125  val L1 loss:  5.0475\n",
      "epoch:  24   step:  26   train loss:  2.8287947177886963  val loss:  4.519985675811768  val L1 loss:  4.9769\n",
      "epoch:  24   step:  27   train loss:  2.972792863845825  val loss:  4.398894786834717  val L1 loss:  4.8686\n",
      "epoch:  24   step:  28   train loss:  2.874814510345459  val loss:  4.279253005981445  val L1 loss:  4.7388\n",
      "epoch:  24   step:  29   train loss:  2.192530393600464  val loss:  4.09993839263916  val L1 loss:  4.5819\n",
      "epoch:  24   step:  30   train loss:  2.3375396728515625  val loss:  4.147218704223633  val L1 loss:  4.6112\n",
      "epoch:  24   step:  31   train loss:  3.7541723251342773  val loss:  4.122573375701904  val L1 loss:  4.5834\n",
      "epoch:  24   step:  32   train loss:  3.3059744834899902  val loss:  4.010993003845215  val L1 loss:  4.4887\n",
      "epoch:  24   step:  33   train loss:  2.5133414268493652  val loss:  4.095639705657959  val L1 loss:  4.5633\n",
      "epoch:  24   step:  34   train loss:  2.300173044204712  val loss:  4.316544532775879  val L1 loss:  4.7891\n",
      "epoch:  24   step:  35   train loss:  2.7660675048828125  val loss:  4.55613374710083  val L1 loss:  5.0196\n",
      "epoch:  24   step:  36   train loss:  1.84110426902771  val loss:  4.43739652633667  val L1 loss:  4.902\n",
      "epoch:  24   step:  37   train loss:  2.752694606781006  val loss:  4.086328983306885  val L1 loss:  4.554\n",
      "epoch:  24   step:  38   train loss:  1.8507755994796753  val loss:  4.022287368774414  val L1 loss:  4.4846\n",
      "epoch:  24   step:  39   train loss:  2.3141040802001953  val loss:  4.0016608238220215  val L1 loss:  4.4648\n",
      "epoch:  24   step:  40   train loss:  1.4450987577438354  val loss:  3.915912389755249  val L1 loss:  4.3665\n",
      "epoch:  24   step:  41   train loss:  1.753709077835083  val loss:  3.895798921585083  val L1 loss:  4.3625\n",
      "epoch:  24   step:  42   train loss:  1.3753818273544312  val loss:  3.8034844398498535  val L1 loss:  4.268\n",
      "epoch:  24   step:  43   train loss:  3.2912096977233887  val loss:  3.7275524139404297  val L1 loss:  4.1678\n",
      "min_val_loss_print 3.7275524139404297\n",
      "epoch:  24   step:  44   train loss:  2.8897905349731445  val loss:  3.711859941482544  val L1 loss:  4.1405\n",
      "min_val_loss_print 3.711859941482544\n",
      "epoch:  24   step:  45   train loss:  1.1784095764160156  val loss:  3.75710129737854  val L1 loss:  4.2064\n",
      "epoch:  24   step:  46   train loss:  1.9312357902526855  val loss:  3.800960063934326  val L1 loss:  4.2626\n",
      "epoch:  24   step:  47   train loss:  1.982593297958374  val loss:  3.902583122253418  val L1 loss:  4.3527\n",
      "epoch:  24   step:  48   train loss:  4.111824989318848  val loss:  3.896853446960449  val L1 loss:  4.3524\n",
      "epoch:  24   step:  49   train loss:  5.690025329589844  val loss:  3.8636796474456787  val L1 loss:  4.306\n",
      "epoch:  24   step:  50   train loss:  1.8345423936843872  val loss:  4.163507461547852  val L1 loss:  4.6312\n",
      "epoch:  24   step:  51   train loss:  2.242722511291504  val loss:  4.501497268676758  val L1 loss:  4.9872\n",
      "epoch:  24   step:  52   train loss:  2.5265986919403076  val loss:  4.630013465881348  val L1 loss:  5.1095\n",
      "epoch:  24   step:  53   train loss:  3.852686882019043  val loss:  4.446428298950195  val L1 loss:  4.9205\n",
      "epoch:  24   step:  54   train loss:  2.0718741416931152  val loss:  4.239413738250732  val L1 loss:  4.701\n",
      "epoch:  24   step:  55   train loss:  1.9852609634399414  val loss:  4.124880790710449  val L1 loss:  4.5806\n",
      "epoch:  24   step:  56   train loss:  4.016525745391846  val loss:  4.188747882843018  val L1 loss:  4.6367\n",
      "epoch:  24   step:  57   train loss:  1.674078106880188  val loss:  4.231415271759033  val L1 loss:  4.6948\n",
      "epoch:  24   step:  58   train loss:  1.5768455266952515  val loss:  4.434626579284668  val L1 loss:  4.9198\n",
      "epoch:  24   step:  59   train loss:  2.1028640270233154  val loss:  4.61705207824707  val L1 loss:  5.0884\n",
      "epoch:  24   step:  60   train loss:  2.219242811203003  val loss:  4.701573848724365  val L1 loss:  5.1734\n",
      "epoch:  24   step:  61   train loss:  2.660698413848877  val loss:  4.714139461517334  val L1 loss:  5.1841\n",
      "epoch:  24   step:  62   train loss:  1.7738292217254639  val loss:  4.598079204559326  val L1 loss:  5.0669\n",
      "epoch:  24   step:  63   train loss:  2.2653069496154785  val loss:  4.563925266265869  val L1 loss:  5.027\n",
      "epoch:  24   step:  64   train loss:  3.3612060546875  val loss:  4.467667102813721  val L1 loss:  4.9281\n",
      "epoch:  24   step:  65   train loss:  2.5430681705474854  val loss:  4.427608013153076  val L1 loss:  4.8846\n",
      "epoch:  24   step:  66   train loss:  2.5051727294921875  val loss:  4.37774133682251  val L1 loss:  4.8228\n",
      "epoch:  24   step:  67   train loss:  3.9131391048431396  val loss:  4.298527240753174  val L1 loss:  4.7435\n",
      "epoch:  24   step:  68   train loss:  1.8853631019592285  val loss:  4.25889778137207  val L1 loss:  4.7275\n",
      "epoch:  24   step:  69   train loss:  1.7365517616271973  val loss:  4.274180889129639  val L1 loss:  4.7251\n",
      "epoch:  24   step:  70   train loss:  1.6280851364135742  val loss:  4.346831798553467  val L1 loss:  4.8227\n",
      "epoch:  24   step:  71   train loss:  2.0971083641052246  val loss:  4.342170238494873  val L1 loss:  4.8051\n",
      "epoch:  24   step:  72   train loss:  2.0981991291046143  val loss:  4.38326358795166  val L1 loss:  4.8461\n",
      "epoch:  24   step:  73   train loss:  2.5189337730407715  val loss:  4.424037933349609  val L1 loss:  4.8895\n",
      "epoch:  24   step:  74   train loss:  1.8016397953033447  val loss:  4.461155414581299  val L1 loss:  4.9453\n",
      "epoch:  24   step:  75   train loss:  1.8988558053970337  val loss:  4.4416632652282715  val L1 loss:  4.9224\n",
      "epoch:  24   step:  76   train loss:  3.34047532081604  val loss:  4.391658306121826  val L1 loss:  4.8635\n",
      "epoch:  24   step:  77   train loss:  1.8087279796600342  val loss:  4.379990577697754  val L1 loss:  4.8485\n",
      "epoch:  24   step:  78   train loss:  3.384162664413452  val loss:  4.382050514221191  val L1 loss:  4.8575\n",
      "epoch:  24   step:  79   train loss:  1.6179251670837402  val loss:  4.309206008911133  val L1 loss:  4.785\n",
      "epoch:  24   step:  80   train loss:  1.5693025588989258  val loss:  4.18166971206665  val L1 loss:  4.6424\n",
      "epoch:  24   step:  81   train loss:  2.042738437652588  val loss:  4.135957717895508  val L1 loss:  4.6012\n",
      "epoch:  24   step:  82   train loss:  1.91371750831604  val loss:  4.1490068435668945  val L1 loss:  4.6012\n",
      "epoch:  24   step:  83   train loss:  1.8235752582550049  val loss:  4.208424091339111  val L1 loss:  4.663\n",
      "epoch:  24   step:  84   train loss:  2.635396718978882  val loss:  4.268787860870361  val L1 loss:  4.7223\n",
      "epoch:  24   step:  85   train loss:  1.8778412342071533  val loss:  4.433958530426025  val L1 loss:  4.9013\n",
      "epoch:  24   step:  86   train loss:  2.510080337524414  val loss:  4.579425811767578  val L1 loss:  5.0491\n",
      "epoch:  24   step:  87   train loss:  1.9785919189453125  val loss:  4.504806041717529  val L1 loss:  4.9644\n",
      "epoch:  24   step:  88   train loss:  2.0066189765930176  val loss:  4.46345853805542  val L1 loss:  4.9246\n",
      "epoch:  24   step:  89   train loss:  1.7341980934143066  val loss:  4.494045257568359  val L1 loss:  4.9485\n",
      "epoch:  24   step:  90   train loss:  2.535940408706665  val loss:  4.544112682342529  val L1 loss:  5.0236\n",
      "epoch:  24   step:  91   train loss:  2.6314730644226074  val loss:  4.506258010864258  val L1 loss:  4.9801\n",
      "epoch:  24   step:  92   train loss:  2.4699318408966064  val loss:  4.502658367156982  val L1 loss:  4.9691\n",
      "epoch:  24   step:  93   train loss:  3.843315839767456  val loss:  4.45833158493042  val L1 loss:  4.9159\n",
      "epoch:  24   step:  94   train loss:  1.6314432621002197  val loss:  4.502284049987793  val L1 loss:  4.9578\n",
      "epoch:  24   step:  95   train loss:  2.0461888313293457  val loss:  4.524420261383057  val L1 loss:  5.0043\n",
      "epoch:  24   step:  96   train loss:  2.6805694103240967  val loss:  4.568601131439209  val L1 loss:  5.0516\n",
      "epoch:  24   step:  97   train loss:  1.8114700317382812  val loss:  4.559267997741699  val L1 loss:  5.0417\n",
      "epoch:  24   step:  98   train loss:  2.4128992557525635  val loss:  4.803005695343018  val L1 loss:  5.2851\n",
      "epoch:  24   step:  99   train loss:  2.687985897064209  val loss:  4.858143329620361  val L1 loss:  5.3393\n",
      "epoch:  24   step:  100   train loss:  1.9029148817062378  val loss:  4.939196586608887  val L1 loss:  5.4193\n",
      "epoch:  24   step:  101   train loss:  2.2575151920318604  val loss:  4.6180572509765625  val L1 loss:  5.0911\n",
      "epoch:  24   step:  102   train loss:  2.417928457260132  val loss:  4.385446548461914  val L1 loss:  4.8391\n",
      "epoch:  24   step:  103   train loss:  2.9825000762939453  val loss:  4.34988260269165  val L1 loss:  4.8149\n",
      "epoch:  24   step:  104   train loss:  2.5260202884674072  val loss:  4.416674613952637  val L1 loss:  4.8838\n",
      "epoch:  24   step:  105   train loss:  3.2979848384857178  val loss:  4.391621112823486  val L1 loss:  4.8431\n",
      "epoch:  24   step:  106   train loss:  2.0071568489074707  val loss:  4.554627418518066  val L1 loss:  5.0275\n",
      "epoch:  24   step:  107   train loss:  3.0110695362091064  val loss:  4.694034576416016  val L1 loss:  5.1719\n",
      "epoch:  24   step:  108   train loss:  2.930330991744995  val loss:  4.672235488891602  val L1 loss:  5.1557\n",
      "epoch:  24   step:  109   train loss:  2.7082862854003906  val loss:  4.455462455749512  val L1 loss:  4.9208\n",
      "epoch:  24   step:  110   train loss:  2.472393751144409  val loss:  4.239891052246094  val L1 loss:  4.7039\n",
      "epoch:  24   step:  111   train loss:  2.1038246154785156  val loss:  4.268886089324951  val L1 loss:  4.7387\n",
      "epoch:  24   step:  112   train loss:  1.3305861949920654  val loss:  4.323577880859375  val L1 loss:  4.7843\n",
      "epoch:  24   step:  113   train loss:  3.6253609657287598  val loss:  4.202568531036377  val L1 loss:  4.6549\n",
      "epoch:  24   step:  114   train loss:  1.8431416749954224  val loss:  4.204190731048584  val L1 loss:  4.688\n",
      "epoch:  24   step:  115   train loss:  1.7147982120513916  val loss:  4.316923141479492  val L1 loss:  4.7757\n",
      "epoch:  24   step:  116   train loss:  2.1443915367126465  val loss:  4.3348212242126465  val L1 loss:  4.7867\n",
      "epoch:  24   step:  117   train loss:  4.4460272789001465  val loss:  4.149991512298584  val L1 loss:  4.6116\n",
      "epoch:  24   step:  118   train loss:  4.185912132263184  val loss:  4.076712608337402  val L1 loss:  4.5447\n",
      "epoch:  24   step:  119   train loss:  2.3880720138549805  val loss:  4.073243141174316  val L1 loss:  4.5423\n",
      "epoch:  24   step:  120   train loss:  3.917935371398926  val loss:  4.07479190826416  val L1 loss:  4.5327\n",
      "epoch:  24   step:  121   train loss:  1.9536075592041016  val loss:  3.951428174972534  val L1 loss:  4.4282\n",
      "epoch:  24   step:  122   train loss:  2.306598424911499  val loss:  3.9529237747192383  val L1 loss:  4.3961\n",
      "epoch:  24   step:  123   train loss:  4.188179969787598  val loss:  4.209874629974365  val L1 loss:  4.6728\n",
      "epoch:  24   step:  124   train loss:  2.107827663421631  val loss:  4.382312774658203  val L1 loss:  4.864\n",
      "epoch:  24   step:  125   train loss:  2.7383780479431152  val loss:  4.138391494750977  val L1 loss:  4.5834\n",
      "epoch:  24   step:  126   train loss:  3.216068983078003  val loss:  4.034689903259277  val L1 loss:  4.4941\n",
      "epoch:  24   step:  127   train loss:  2.065453290939331  val loss:  4.0607757568359375  val L1 loss:  4.5261\n",
      "epoch:  24   step:  128   train loss:  2.5978434085845947  val loss:  4.244491100311279  val L1 loss:  4.6901\n",
      "epoch:  24   step:  129   train loss:  4.5840253829956055  val loss:  4.253417491912842  val L1 loss:  4.7119\n",
      "epoch:  24   step:  130   train loss:  2.9304962158203125  val loss:  4.293023109436035  val L1 loss:  4.7592\n",
      "epoch:  24   step:  131   train loss:  2.4949252605438232  val loss:  4.313054084777832  val L1 loss:  4.7709\n",
      "epoch:  24   step:  132   train loss:  3.0947794914245605  val loss:  4.461764812469482  val L1 loss:  4.9033\n",
      "epoch:  24   step:  133   train loss:  1.7953602075576782  val loss:  4.763726711273193  val L1 loss:  5.2354\n",
      "epoch:  24   step:  134   train loss:  3.1144495010375977  val loss:  5.003322124481201  val L1 loss:  5.4778\n",
      "epoch:  24   step:  135   train loss:  2.5459787845611572  val loss:  5.136507511138916  val L1 loss:  5.6067\n",
      "epoch:  24   step:  136   train loss:  3.6434268951416016  val loss:  4.812891006469727  val L1 loss:  5.3035\n",
      "epoch:  24   step:  137   train loss:  3.162064552307129  val loss:  4.511172294616699  val L1 loss:  4.9659\n",
      "epoch:  24   step:  138   train loss:  2.552713394165039  val loss:  4.619511127471924  val L1 loss:  5.1061\n",
      "epoch:  24   step:  139   train loss:  3.7072596549987793  val loss:  4.698825359344482  val L1 loss:  5.1869\n",
      "epoch:  24   step:  140   train loss:  3.1367950439453125  val loss:  4.636253356933594  val L1 loss:  5.0908\n",
      "epoch:  24   step:  141   train loss:  2.36439847946167  val loss:  4.657858371734619  val L1 loss:  5.1042\n",
      "epoch:  24   step:  142   train loss:  2.25834321975708  val loss:  4.80973482131958  val L1 loss:  5.2935\n",
      "epoch:  24   step:  143   train loss:  3.0104615688323975  val loss:  4.854981422424316  val L1 loss:  5.3432\n",
      "epoch:  24   step:  144   train loss:  2.2138800621032715  val loss:  4.872739315032959  val L1 loss:  5.3632\n",
      "epoch:  24   step:  145   train loss:  1.7683398723602295  val loss:  4.752076148986816  val L1 loss:  5.2421\n",
      "epoch:  24   step:  146   train loss:  2.435070276260376  val loss:  4.495246887207031  val L1 loss:  4.9604\n",
      "epoch:  24   step:  147   train loss:  2.843592643737793  val loss:  4.322390556335449  val L1 loss:  4.7698\n",
      "epoch:  24   step:  148   train loss:  2.1335854530334473  val loss:  4.354078769683838  val L1 loss:  4.8175\n",
      "epoch:  24   step:  149   train loss:  2.0716331005096436  val loss:  4.248994827270508  val L1 loss:  4.7211\n",
      "epoch:  24   step:  150   train loss:  2.121128559112549  val loss:  4.220162391662598  val L1 loss:  4.6854\n",
      "epoch:  24   step:  151   train loss:  1.406858205795288  val loss:  4.40852689743042  val L1 loss:  4.8655\n",
      "epoch:  24   step:  152   train loss:  2.277714252471924  val loss:  4.542571067810059  val L1 loss:  5.0044\n",
      "epoch:  24   step:  153   train loss:  2.8438520431518555  val loss:  4.373909950256348  val L1 loss:  4.8381\n",
      "epoch:  24   step:  154   train loss:  2.055335521697998  val loss:  4.420241355895996  val L1 loss:  4.896\n",
      "epoch:  24   step:  155   train loss:  2.552982807159424  val loss:  4.708137512207031  val L1 loss:  5.1735\n",
      "epoch:  24   step:  156   train loss:  3.8953819274902344  val loss:  4.968622207641602  val L1 loss:  5.4394\n",
      "epoch:  24   step:  157   train loss:  2.2974469661712646  val loss:  4.92812967300415  val L1 loss:  5.3919\n",
      "epoch:  24   step:  158   train loss:  3.4643301963806152  val loss:  4.695371627807617  val L1 loss:  5.1658\n",
      "epoch:  24   step:  159   train loss:  1.8735065460205078  val loss:  4.629448890686035  val L1 loss:  5.113\n",
      "epoch:  24   step:  160   train loss:  1.9635708332061768  val loss:  4.668313503265381  val L1 loss:  5.1514\n",
      "epoch:  24   step:  161   train loss:  3.052806854248047  val loss:  4.685769081115723  val L1 loss:  5.1683\n",
      "epoch:  24   step:  162   train loss:  2.2536983489990234  val loss:  4.69214391708374  val L1 loss:  5.1764\n",
      "epoch:  24   step:  163   train loss:  1.996192216873169  val loss:  4.657352924346924  val L1 loss:  5.1397\n",
      "epoch:  24   step:  164   train loss:  2.676501750946045  val loss:  4.724888324737549  val L1 loss:  5.1986\n",
      "epoch:  24   step:  165   train loss:  2.1894309520721436  val loss:  4.8085432052612305  val L1 loss:  5.2928\n",
      "epoch:  24   step:  166   train loss:  1.7990760803222656  val loss:  4.785442352294922  val L1 loss:  5.2629\n",
      "epoch:  24   step:  167   train loss:  4.526644706726074  val loss:  4.749717712402344  val L1 loss:  5.228\n",
      "epoch:  24   step:  168   train loss:  2.1073269844055176  val loss:  4.735443115234375  val L1 loss:  5.2138\n",
      "epoch:  24   step:  169   train loss:  1.829912543296814  val loss:  4.656325817108154  val L1 loss:  5.1371\n",
      "epoch:  24   step:  170   train loss:  2.551257610321045  val loss:  4.5526580810546875  val L1 loss:  5.0146\n",
      "epoch:  24   step:  171   train loss:  1.5278905630111694  val loss:  4.487301349639893  val L1 loss:  4.9586\n",
      "epoch:  24   step:  172   train loss:  1.7418487071990967  val loss:  4.418995380401611  val L1 loss:  4.8906\n",
      "epoch:  24   step:  173   train loss:  1.951214075088501  val loss:  4.358725547790527  val L1 loss:  4.8323\n",
      "epoch:  24   step:  174   train loss:  2.5601868629455566  val loss:  4.427419185638428  val L1 loss:  4.8982\n",
      "epoch:  24   step:  175   train loss:  3.832838535308838  val loss:  4.744758605957031  val L1 loss:  5.204\n",
      "epoch:  24   step:  176   train loss:  2.6023716926574707  val loss:  4.957601070404053  val L1 loss:  5.4454\n",
      "epoch:  24   step:  177   train loss:  2.701850652694702  val loss:  5.136045932769775  val L1 loss:  5.6239\n",
      "epoch:  24   step:  178   train loss:  3.163423538208008  val loss:  5.073287010192871  val L1 loss:  5.546\n",
      "epoch:  24   step:  179   train loss:  1.9800430536270142  val loss:  4.84145975112915  val L1 loss:  5.3168\n",
      "epoch:  24   step:  180   train loss:  1.5872693061828613  val loss:  4.402594566345215  val L1 loss:  4.8616\n",
      "epoch:  24   step:  181   train loss:  2.550201892852783  val loss:  4.306697368621826  val L1 loss:  4.7738\n",
      "epoch:  24   step:  182   train loss:  3.2582061290740967  val loss:  4.24713134765625  val L1 loss:  4.7175\n",
      "epoch:  24   step:  183   train loss:  2.421783447265625  val loss:  4.211126804351807  val L1 loss:  4.6832\n",
      "epoch:  24   step:  184   train loss:  2.949528932571411  val loss:  4.25674295425415  val L1 loss:  4.7183\n",
      "epoch:  24   step:  185   train loss:  4.9933929443359375  val loss:  4.742373466491699  val L1 loss:  5.2287\n",
      "epoch:  24   step:  186   train loss:  2.296330451965332  val loss:  5.456198692321777  val L1 loss:  5.9305\n",
      "epoch:  24   step:  187   train loss:  2.8981528282165527  val loss:  5.789684772491455  val L1 loss:  6.2589\n",
      "epoch:  24   step:  188   train loss:  3.228189706802368  val loss:  5.1990485191345215  val L1 loss:  5.6869\n",
      "epoch:  24   step:  189   train loss:  2.991039514541626  val loss:  4.423952579498291  val L1 loss:  4.9\n",
      "epoch:  24   step:  190   train loss:  2.4004833698272705  val loss:  4.073138236999512  val L1 loss:  4.5514\n",
      "epoch:  24   step:  191   train loss:  2.601499080657959  val loss:  4.057766437530518  val L1 loss:  4.548\n",
      "epoch:  24   step:  192   train loss:  2.4322023391723633  val loss:  4.052127361297607  val L1 loss:  4.5235\n",
      "epoch:  24   step:  193   train loss:  2.3034462928771973  val loss:  4.192241191864014  val L1 loss:  4.653\n",
      "epoch:  24   step:  194   train loss:  1.9791171550750732  val loss:  4.591279029846191  val L1 loss:  5.0783\n",
      "epoch:  24   step:  195   train loss:  1.4342849254608154  val loss:  4.802490711212158  val L1 loss:  5.293\n",
      "epoch:  24   step:  196   train loss:  1.8607124090194702  val loss:  4.532265663146973  val L1 loss:  5.0128\n",
      "epoch:  24   step:  197   train loss:  2.3650479316711426  val loss:  4.1285600662231445  val L1 loss:  4.5978\n",
      "epoch:  24   step:  198   train loss:  3.230522632598877  val loss:  3.893889904022217  val L1 loss:  4.3506\n",
      "epoch:  24   step:  199   train loss:  2.564467191696167  val loss:  4.1004862785339355  val L1 loss:  4.5517\n",
      "epoch:  24   step:  200   train loss:  2.7621843814849854  val loss:  4.136846542358398  val L1 loss:  4.5921\n",
      "epoch:  24   step:  201   train loss:  2.4506282806396484  val loss:  4.039383411407471  val L1 loss:  4.5134\n",
      "epoch:  24   step:  202   train loss:  1.8879544734954834  val loss:  4.121224403381348  val L1 loss:  4.5774\n",
      "epoch:  24   step:  203   train loss:  1.4574363231658936  val loss:  4.407272815704346  val L1 loss:  4.8819\n",
      "epoch:  24   step:  204   train loss:  3.4769937992095947  val loss:  4.707413673400879  val L1 loss:  5.1916\n",
      "epoch:  24   step:  205   train loss:  1.3975074291229248  val loss:  4.690162658691406  val L1 loss:  5.1608\n",
      "epoch:  24   step:  206   train loss:  2.032907009124756  val loss:  4.573024749755859  val L1 loss:  5.0636\n",
      "epoch:  24   step:  207   train loss:  2.5010809898376465  val loss:  4.618137836456299  val L1 loss:  5.0871\n",
      "epoch:  24   step:  208   train loss:  2.227296829223633  val loss:  4.662874221801758  val L1 loss:  5.1374\n",
      "epoch:  25   step:  0   train loss:  2.735811948776245  val loss:  4.552358150482178  val L1 loss:  5.0221\n",
      "epoch:  25   step:  1   train loss:  1.8597362041473389  val loss:  4.493608474731445  val L1 loss:  4.9559\n",
      "epoch:  25   step:  2   train loss:  1.4167143106460571  val loss:  4.596496105194092  val L1 loss:  5.0621\n",
      "epoch:  25   step:  3   train loss:  3.869746685028076  val loss:  4.707879543304443  val L1 loss:  5.1797\n",
      "epoch:  25   step:  4   train loss:  3.086822986602783  val loss:  4.60837984085083  val L1 loss:  5.0563\n",
      "epoch:  25   step:  5   train loss:  1.8897653818130493  val loss:  4.54520845413208  val L1 loss:  5.0103\n",
      "epoch:  25   step:  6   train loss:  1.3709065914154053  val loss:  4.9710917472839355  val L1 loss:  5.4285\n",
      "epoch:  25   step:  7   train loss:  2.6831274032592773  val loss:  5.728437423706055  val L1 loss:  6.2054\n",
      "epoch:  25   step:  8   train loss:  3.1311402320861816  val loss:  5.989859580993652  val L1 loss:  6.4827\n",
      "epoch:  25   step:  9   train loss:  3.1242809295654297  val loss:  5.45860481262207  val L1 loss:  5.9306\n",
      "epoch:  25   step:  10   train loss:  2.2631797790527344  val loss:  4.859530448913574  val L1 loss:  5.3116\n",
      "epoch:  25   step:  11   train loss:  2.9703798294067383  val loss:  4.798165321350098  val L1 loss:  5.2664\n",
      "epoch:  25   step:  12   train loss:  2.23144268989563  val loss:  4.903865337371826  val L1 loss:  5.3702\n",
      "epoch:  25   step:  13   train loss:  1.8932020664215088  val loss:  4.872029781341553  val L1 loss:  5.3522\n",
      "epoch:  25   step:  14   train loss:  1.8483076095581055  val loss:  4.7447638511657715  val L1 loss:  5.2166\n",
      "epoch:  25   step:  15   train loss:  4.027260780334473  val loss:  4.74371862411499  val L1 loss:  5.2218\n",
      "epoch:  25   step:  16   train loss:  2.688516139984131  val loss:  4.810184955596924  val L1 loss:  5.2888\n",
      "epoch:  25   step:  17   train loss:  2.3334507942199707  val loss:  4.866269588470459  val L1 loss:  5.3428\n",
      "epoch:  25   step:  18   train loss:  3.0133883953094482  val loss:  5.018572807312012  val L1 loss:  5.4995\n",
      "epoch:  25   step:  19   train loss:  1.7472450733184814  val loss:  5.30021333694458  val L1 loss:  5.7864\n",
      "epoch:  25   step:  20   train loss:  2.7019824981689453  val loss:  5.34956169128418  val L1 loss:  5.8193\n",
      "epoch:  25   step:  21   train loss:  2.646786689758301  val loss:  4.906017780303955  val L1 loss:  5.3882\n",
      "epoch:  25   step:  22   train loss:  3.106123924255371  val loss:  4.366795539855957  val L1 loss:  4.8413\n",
      "epoch:  25   step:  23   train loss:  2.975043296813965  val loss:  4.3210954666137695  val L1 loss:  4.7829\n",
      "epoch:  25   step:  24   train loss:  3.3654916286468506  val loss:  4.542046546936035  val L1 loss:  5.0144\n",
      "epoch:  25   step:  25   train loss:  2.063518762588501  val loss:  4.523053169250488  val L1 loss:  4.9994\n",
      "epoch:  25   step:  26   train loss:  2.945075750350952  val loss:  4.4140472412109375  val L1 loss:  4.8994\n",
      "epoch:  25   step:  27   train loss:  1.3302741050720215  val loss:  4.340110778808594  val L1 loss:  4.7925\n",
      "epoch:  25   step:  28   train loss:  3.091005325317383  val loss:  4.736970901489258  val L1 loss:  5.2212\n",
      "epoch:  25   step:  29   train loss:  4.308329105377197  val loss:  5.5975189208984375  val L1 loss:  6.087\n",
      "epoch:  25   step:  30   train loss:  3.2530357837677  val loss:  5.818686485290527  val L1 loss:  6.3116\n",
      "epoch:  25   step:  31   train loss:  3.4149513244628906  val loss:  5.657820701599121  val L1 loss:  6.14\n",
      "epoch:  25   step:  32   train loss:  3.9955310821533203  val loss:  5.17957067489624  val L1 loss:  5.6588\n",
      "epoch:  25   step:  33   train loss:  3.1849565505981445  val loss:  4.5381598472595215  val L1 loss:  4.9873\n",
      "epoch:  25   step:  34   train loss:  1.2711974382400513  val loss:  4.633918285369873  val L1 loss:  5.0854\n",
      "epoch:  25   step:  35   train loss:  2.3065760135650635  val loss:  4.971987724304199  val L1 loss:  5.4567\n",
      "epoch:  25   step:  36   train loss:  2.82183837890625  val loss:  4.989140033721924  val L1 loss:  5.4729\n",
      "epoch:  25   step:  37   train loss:  3.817124605178833  val loss:  4.653024673461914  val L1 loss:  5.1123\n",
      "epoch:  25   step:  38   train loss:  4.3544416427612305  val loss:  4.775485038757324  val L1 loss:  5.2488\n",
      "epoch:  25   step:  39   train loss:  3.462407112121582  val loss:  5.878127098083496  val L1 loss:  6.3779\n",
      "epoch:  25   step:  40   train loss:  2.802300453186035  val loss:  6.6478400230407715  val L1 loss:  7.1287\n",
      "epoch:  25   step:  41   train loss:  3.0684685707092285  val loss:  6.53591775894165  val L1 loss:  7.0229\n",
      "epoch:  25   step:  42   train loss:  3.8638477325439453  val loss:  5.868833065032959  val L1 loss:  6.351\n",
      "epoch:  25   step:  43   train loss:  3.186197519302368  val loss:  4.986639976501465  val L1 loss:  5.4737\n",
      "epoch:  25   step:  44   train loss:  2.3720293045043945  val loss:  4.587648391723633  val L1 loss:  5.0569\n",
      "epoch:  25   step:  45   train loss:  2.465811252593994  val loss:  4.615142822265625  val L1 loss:  5.0668\n",
      "epoch:  25   step:  46   train loss:  3.6858553886413574  val loss:  4.819884300231934  val L1 loss:  5.2697\n",
      "epoch:  25   step:  47   train loss:  3.8121986389160156  val loss:  4.613992691040039  val L1 loss:  5.0669\n",
      "epoch:  25   step:  48   train loss:  2.662220001220703  val loss:  4.339634418487549  val L1 loss:  4.7957\n",
      "epoch:  25   step:  49   train loss:  1.1359243392944336  val loss:  4.480775833129883  val L1 loss:  4.9604\n",
      "epoch:  25   step:  50   train loss:  3.875490427017212  val loss:  4.763387203216553  val L1 loss:  5.2524\n",
      "epoch:  25   step:  51   train loss:  1.5111708641052246  val loss:  4.985201358795166  val L1 loss:  5.4574\n",
      "epoch:  25   step:  52   train loss:  2.692732810974121  val loss:  4.9412384033203125  val L1 loss:  5.4174\n",
      "epoch:  25   step:  53   train loss:  3.2107770442962646  val loss:  4.640411376953125  val L1 loss:  5.1171\n",
      "epoch:  25   step:  54   train loss:  2.706657648086548  val loss:  4.44990348815918  val L1 loss:  4.9248\n",
      "epoch:  25   step:  55   train loss:  1.2753150463104248  val loss:  4.427862167358398  val L1 loss:  4.9085\n",
      "epoch:  25   step:  56   train loss:  1.2721922397613525  val loss:  4.568569660186768  val L1 loss:  5.0248\n",
      "epoch:  25   step:  57   train loss:  2.5695724487304688  val loss:  4.758500099182129  val L1 loss:  5.215\n",
      "epoch:  25   step:  58   train loss:  1.947401762008667  val loss:  4.885563373565674  val L1 loss:  5.3524\n",
      "epoch:  25   step:  59   train loss:  3.05318021774292  val loss:  4.723860263824463  val L1 loss:  5.1976\n",
      "epoch:  25   step:  60   train loss:  2.2826638221740723  val loss:  4.632649898529053  val L1 loss:  5.0896\n",
      "epoch:  25   step:  61   train loss:  2.096682548522949  val loss:  4.777698040008545  val L1 loss:  5.2556\n",
      "epoch:  25   step:  62   train loss:  3.282925844192505  val loss:  4.931155681610107  val L1 loss:  5.4225\n",
      "epoch:  25   step:  63   train loss:  2.7148821353912354  val loss:  4.7739105224609375  val L1 loss:  5.254\n",
      "epoch:  25   step:  64   train loss:  2.9035534858703613  val loss:  4.606170654296875  val L1 loss:  5.0646\n",
      "epoch:  25   step:  65   train loss:  2.274505615234375  val loss:  4.699958801269531  val L1 loss:  5.1766\n",
      "epoch:  25   step:  66   train loss:  3.508242607116699  val loss:  4.890161514282227  val L1 loss:  5.3457\n",
      "epoch:  25   step:  67   train loss:  3.6352767944335938  val loss:  4.822931289672852  val L1 loss:  5.2722\n",
      "epoch:  25   step:  68   train loss:  1.4446641206741333  val loss:  4.759476661682129  val L1 loss:  5.2131\n",
      "epoch:  25   step:  69   train loss:  2.0381252765655518  val loss:  4.6000285148620605  val L1 loss:  5.0727\n",
      "epoch:  25   step:  70   train loss:  1.5875036716461182  val loss:  4.536782264709473  val L1 loss:  5.0018\n",
      "epoch:  25   step:  71   train loss:  1.6909898519515991  val loss:  4.515214920043945  val L1 loss:  4.9914\n",
      "epoch:  25   step:  72   train loss:  2.280606269836426  val loss:  4.5674896240234375  val L1 loss:  5.0361\n",
      "epoch:  25   step:  73   train loss:  2.356595993041992  val loss:  4.631796836853027  val L1 loss:  5.1071\n",
      "epoch:  25   step:  74   train loss:  2.6281678676605225  val loss:  4.554567337036133  val L1 loss:  5.0353\n",
      "epoch:  25   step:  75   train loss:  1.5932189226150513  val loss:  4.4741339683532715  val L1 loss:  4.9181\n",
      "epoch:  25   step:  76   train loss:  2.604971408843994  val loss:  4.6134538650512695  val L1 loss:  5.0963\n",
      "epoch:  25   step:  77   train loss:  2.1649651527404785  val loss:  4.989082336425781  val L1 loss:  5.466\n",
      "epoch:  25   step:  78   train loss:  2.5218515396118164  val loss:  5.232882022857666  val L1 loss:  5.7207\n",
      "epoch:  25   step:  79   train loss:  2.245565414428711  val loss:  5.30170202255249  val L1 loss:  5.7954\n",
      "epoch:  25   step:  80   train loss:  2.620377540588379  val loss:  5.17829704284668  val L1 loss:  5.6653\n",
      "epoch:  25   step:  81   train loss:  2.322683572769165  val loss:  5.011343955993652  val L1 loss:  5.4795\n",
      "epoch:  25   step:  82   train loss:  1.4755232334136963  val loss:  4.79222297668457  val L1 loss:  5.2797\n",
      "epoch:  25   step:  83   train loss:  1.9916279315948486  val loss:  4.736584663391113  val L1 loss:  5.2193\n",
      "epoch:  25   step:  84   train loss:  2.0593748092651367  val loss:  4.7924041748046875  val L1 loss:  5.2835\n",
      "epoch:  25   step:  85   train loss:  2.8755269050598145  val loss:  4.858462810516357  val L1 loss:  5.3347\n",
      "epoch:  25   step:  86   train loss:  1.1987354755401611  val loss:  4.8252763748168945  val L1 loss:  5.2884\n",
      "epoch:  25   step:  87   train loss:  3.0011072158813477  val loss:  4.780946731567383  val L1 loss:  5.253\n",
      "epoch:  25   step:  88   train loss:  1.949983835220337  val loss:  4.784334659576416  val L1 loss:  5.2664\n",
      "epoch:  25   step:  89   train loss:  1.8228726387023926  val loss:  4.672633171081543  val L1 loss:  5.1579\n",
      "epoch:  25   step:  90   train loss:  3.8314318656921387  val loss:  4.504158973693848  val L1 loss:  4.9833\n",
      "epoch:  25   step:  91   train loss:  2.5857930183410645  val loss:  4.390403747558594  val L1 loss:  4.8443\n",
      "epoch:  25   step:  92   train loss:  2.613980293273926  val loss:  4.644110679626465  val L1 loss:  5.122\n",
      "epoch:  25   step:  93   train loss:  2.557384490966797  val loss:  4.707973003387451  val L1 loss:  5.1762\n",
      "epoch:  25   step:  94   train loss:  3.2164559364318848  val loss:  4.64732551574707  val L1 loss:  5.115\n",
      "epoch:  25   step:  95   train loss:  1.530239462852478  val loss:  4.516364574432373  val L1 loss:  4.9899\n",
      "epoch:  25   step:  96   train loss:  4.076962947845459  val loss:  4.499215126037598  val L1 loss:  4.9602\n",
      "epoch:  25   step:  97   train loss:  2.726444721221924  val loss:  4.684226989746094  val L1 loss:  5.1558\n",
      "epoch:  25   step:  98   train loss:  3.8260507583618164  val loss:  4.768106460571289  val L1 loss:  5.2429\n",
      "epoch:  25   step:  99   train loss:  2.3314614295959473  val loss:  4.691035270690918  val L1 loss:  5.165\n",
      "epoch:  25   step:  100   train loss:  2.8650002479553223  val loss:  4.614537239074707  val L1 loss:  5.093\n",
      "epoch:  25   step:  101   train loss:  1.71732759475708  val loss:  4.6004486083984375  val L1 loss:  5.0712\n",
      "epoch:  25   step:  102   train loss:  2.17181396484375  val loss:  4.618443965911865  val L1 loss:  5.0939\n",
      "epoch:  25   step:  103   train loss:  1.8286960124969482  val loss:  4.689538478851318  val L1 loss:  5.1567\n",
      "epoch:  25   step:  104   train loss:  2.0870141983032227  val loss:  4.766399383544922  val L1 loss:  5.2477\n",
      "epoch:  25   step:  105   train loss:  1.6531447172164917  val loss:  4.913203716278076  val L1 loss:  5.3886\n",
      "epoch:  25   step:  106   train loss:  1.4279279708862305  val loss:  5.239591121673584  val L1 loss:  5.701\n",
      "epoch:  25   step:  107   train loss:  1.9876469373703003  val loss:  5.437205791473389  val L1 loss:  5.9177\n",
      "epoch:  25   step:  108   train loss:  3.853896141052246  val loss:  5.273567199707031  val L1 loss:  5.7355\n",
      "epoch:  25   step:  109   train loss:  3.5061957836151123  val loss:  5.0358710289001465  val L1 loss:  5.5098\n",
      "epoch:  25   step:  110   train loss:  2.8058393001556396  val loss:  4.985437393188477  val L1 loss:  5.4643\n",
      "epoch:  25   step:  111   train loss:  2.8568148612976074  val loss:  5.1912455558776855  val L1 loss:  5.665\n",
      "epoch:  25   step:  112   train loss:  2.055349111557007  val loss:  5.213590145111084  val L1 loss:  5.6983\n",
      "epoch:  25   step:  113   train loss:  3.0652034282684326  val loss:  5.293285846710205  val L1 loss:  5.7869\n",
      "epoch:  25   step:  114   train loss:  3.249729633331299  val loss:  5.120013236999512  val L1 loss:  5.6022\n",
      "epoch:  25   step:  115   train loss:  3.342599630355835  val loss:  4.988007545471191  val L1 loss:  5.4419\n",
      "epoch:  25   step:  116   train loss:  2.0968098640441895  val loss:  5.080052852630615  val L1 loss:  5.5548\n",
      "epoch:  25   step:  117   train loss:  2.1430907249450684  val loss:  5.303325176239014  val L1 loss:  5.7868\n",
      "epoch:  25   step:  118   train loss:  3.58854341506958  val loss:  5.223779678344727  val L1 loss:  5.7073\n",
      "epoch:  25   step:  119   train loss:  3.506824493408203  val loss:  4.903711318969727  val L1 loss:  5.3716\n",
      "epoch:  25   step:  120   train loss:  2.5956997871398926  val loss:  4.708074569702148  val L1 loss:  5.1878\n",
      "epoch:  25   step:  121   train loss:  3.8551175594329834  val loss:  5.145915985107422  val L1 loss:  5.6284\n",
      "epoch:  25   step:  122   train loss:  2.9887285232543945  val loss:  5.788458347320557  val L1 loss:  6.28\n",
      "epoch:  25   step:  123   train loss:  4.969429969787598  val loss:  5.918956279754639  val L1 loss:  6.4071\n",
      "epoch:  25   step:  124   train loss:  5.218210697174072  val loss:  5.452521800994873  val L1 loss:  5.9522\n",
      "epoch:  25   step:  125   train loss:  3.2045998573303223  val loss:  4.8243536949157715  val L1 loss:  5.2812\n",
      "epoch:  25   step:  126   train loss:  2.542384147644043  val loss:  4.622574806213379  val L1 loss:  5.0728\n",
      "epoch:  25   step:  127   train loss:  3.1932578086853027  val loss:  5.1314215660095215  val L1 loss:  5.6165\n",
      "epoch:  25   step:  128   train loss:  3.6703941822052  val loss:  5.622648239135742  val L1 loss:  6.1089\n",
      "epoch:  25   step:  129   train loss:  4.167769432067871  val loss:  5.513942718505859  val L1 loss:  6.0038\n",
      "epoch:  25   step:  130   train loss:  4.4271392822265625  val loss:  5.053518772125244  val L1 loss:  5.5338\n",
      "epoch:  25   step:  131   train loss:  3.3304765224456787  val loss:  4.792474746704102  val L1 loss:  5.255\n",
      "epoch:  25   step:  132   train loss:  3.6017088890075684  val loss:  4.992373943328857  val L1 loss:  5.4742\n",
      "epoch:  25   step:  133   train loss:  2.57222318649292  val loss:  5.292565822601318  val L1 loss:  5.7488\n",
      "epoch:  25   step:  134   train loss:  2.9229307174682617  val loss:  5.494182109832764  val L1 loss:  5.9662\n",
      "epoch:  25   step:  135   train loss:  4.359238147735596  val loss:  5.153779029846191  val L1 loss:  5.6164\n",
      "epoch:  25   step:  136   train loss:  3.553093433380127  val loss:  4.74238920211792  val L1 loss:  5.2063\n",
      "epoch:  25   step:  137   train loss:  2.19861102104187  val loss:  4.697157382965088  val L1 loss:  5.1712\n",
      "epoch:  25   step:  138   train loss:  2.784669876098633  val loss:  5.163062572479248  val L1 loss:  5.6397\n",
      "epoch:  25   step:  139   train loss:  4.370955944061279  val loss:  5.523159503936768  val L1 loss:  5.9984\n",
      "epoch:  25   step:  140   train loss:  3.2735037803649902  val loss:  5.396833419799805  val L1 loss:  5.866\n",
      "epoch:  25   step:  141   train loss:  7.98150634765625  val loss:  4.890666484832764  val L1 loss:  5.356\n",
      "epoch:  25   step:  142   train loss:  4.167135238647461  val loss:  4.695562839508057  val L1 loss:  5.1599\n",
      "epoch:  25   step:  143   train loss:  3.0396780967712402  val loss:  5.007483005523682  val L1 loss:  5.4722\n",
      "epoch:  25   step:  144   train loss:  2.6339244842529297  val loss:  5.077659606933594  val L1 loss:  5.5514\n",
      "epoch:  25   step:  145   train loss:  2.345212936401367  val loss:  4.956171989440918  val L1 loss:  5.428\n",
      "epoch:  25   step:  146   train loss:  2.1488800048828125  val loss:  4.713064670562744  val L1 loss:  5.177\n",
      "epoch:  25   step:  147   train loss:  2.6465065479278564  val loss:  4.752786636352539  val L1 loss:  5.224\n",
      "epoch:  25   step:  148   train loss:  2.6521730422973633  val loss:  4.876457214355469  val L1 loss:  5.3531\n",
      "epoch:  25   step:  149   train loss:  1.9514108896255493  val loss:  4.822305202484131  val L1 loss:  5.2958\n",
      "epoch:  25   step:  150   train loss:  2.2245166301727295  val loss:  4.762205123901367  val L1 loss:  5.2285\n",
      "epoch:  25   step:  151   train loss:  2.4862236976623535  val loss:  4.589725971221924  val L1 loss:  5.0711\n",
      "epoch:  25   step:  152   train loss:  2.3790125846862793  val loss:  4.665952205657959  val L1 loss:  5.1086\n",
      "epoch:  25   step:  153   train loss:  2.344499349594116  val loss:  5.043287754058838  val L1 loss:  5.5241\n",
      "epoch:  25   step:  154   train loss:  2.383256435394287  val loss:  5.374477386474609  val L1 loss:  5.8625\n",
      "epoch:  25   step:  155   train loss:  2.48820161819458  val loss:  5.147021293640137  val L1 loss:  5.6324\n",
      "epoch:  25   step:  156   train loss:  2.8709776401519775  val loss:  4.673621654510498  val L1 loss:  5.1428\n",
      "epoch:  25   step:  157   train loss:  2.3231041431427  val loss:  4.322627544403076  val L1 loss:  4.7684\n",
      "epoch:  25   step:  158   train loss:  2.8603923320770264  val loss:  4.549856662750244  val L1 loss:  5.032\n",
      "epoch:  25   step:  159   train loss:  2.367154121398926  val loss:  4.654999732971191  val L1 loss:  5.1375\n",
      "epoch:  25   step:  160   train loss:  2.316056489944458  val loss:  4.615167617797852  val L1 loss:  5.0987\n",
      "epoch:  25   step:  161   train loss:  3.3399410247802734  val loss:  4.408761024475098  val L1 loss:  4.8802\n",
      "epoch:  25   step:  162   train loss:  2.363792657852173  val loss:  4.451662540435791  val L1 loss:  4.9231\n",
      "epoch:  25   step:  163   train loss:  2.9322996139526367  val loss:  4.750552654266357  val L1 loss:  5.2172\n",
      "epoch:  25   step:  164   train loss:  2.913844108581543  val loss:  4.765321254730225  val L1 loss:  5.2298\n",
      "epoch:  25   step:  165   train loss:  2.57619571685791  val loss:  4.564332962036133  val L1 loss:  5.0286\n",
      "epoch:  25   step:  166   train loss:  2.3856124877929688  val loss:  4.362353324890137  val L1 loss:  4.8147\n",
      "epoch:  25   step:  167   train loss:  1.512721300125122  val loss:  4.325395584106445  val L1 loss:  4.7757\n",
      "epoch:  25   step:  168   train loss:  1.5115735530853271  val loss:  4.269295692443848  val L1 loss:  4.7231\n",
      "epoch:  25   step:  169   train loss:  1.6234023571014404  val loss:  4.316119194030762  val L1 loss:  4.7904\n",
      "epoch:  25   step:  170   train loss:  2.1252198219299316  val loss:  4.293363571166992  val L1 loss:  4.7614\n",
      "epoch:  25   step:  171   train loss:  3.915891170501709  val loss:  4.260566234588623  val L1 loss:  4.7325\n",
      "epoch:  25   step:  172   train loss:  4.497610092163086  val loss:  4.219356536865234  val L1 loss:  4.698\n",
      "epoch:  25   step:  173   train loss:  2.6570980548858643  val loss:  4.178260326385498  val L1 loss:  4.6596\n",
      "epoch:  25   step:  174   train loss:  2.3763184547424316  val loss:  4.153674602508545  val L1 loss:  4.6222\n",
      "epoch:  25   step:  175   train loss:  2.4727120399475098  val loss:  4.055159091949463  val L1 loss:  4.5051\n",
      "epoch:  25   step:  176   train loss:  1.911604642868042  val loss:  3.90490460395813  val L1 loss:  4.3669\n",
      "epoch:  25   step:  177   train loss:  1.5903542041778564  val loss:  3.814406633377075  val L1 loss:  4.2764\n",
      "epoch:  25   step:  178   train loss:  3.1711087226867676  val loss:  4.007040500640869  val L1 loss:  4.46\n",
      "epoch:  25   step:  179   train loss:  1.9775378704071045  val loss:  3.947483539581299  val L1 loss:  4.4018\n",
      "epoch:  25   step:  180   train loss:  1.7442545890808105  val loss:  3.7716097831726074  val L1 loss:  4.2261\n",
      "epoch:  25   step:  181   train loss:  2.324306011199951  val loss:  3.6500768661499023  val L1 loss:  4.1223\n",
      "min_val_loss_print 3.6500768661499023\n",
      "epoch:  25   step:  182   train loss:  2.477146625518799  val loss:  3.727848529815674  val L1 loss:  4.1974\n",
      "epoch:  25   step:  183   train loss:  2.7503161430358887  val loss:  3.8061046600341797  val L1 loss:  4.2862\n",
      "epoch:  25   step:  184   train loss:  1.5472644567489624  val loss:  3.9006292819976807  val L1 loss:  4.3711\n",
      "epoch:  25   step:  185   train loss:  1.9285128116607666  val loss:  3.9929254055023193  val L1 loss:  4.4683\n",
      "epoch:  25   step:  186   train loss:  1.7931894063949585  val loss:  4.0347795486450195  val L1 loss:  4.5222\n",
      "epoch:  25   step:  187   train loss:  2.8304049968719482  val loss:  4.08022928237915  val L1 loss:  4.5623\n",
      "epoch:  25   step:  188   train loss:  2.4200785160064697  val loss:  4.155956745147705  val L1 loss:  4.6337\n",
      "epoch:  25   step:  189   train loss:  2.2450435161590576  val loss:  4.106053829193115  val L1 loss:  4.5799\n",
      "epoch:  25   step:  190   train loss:  3.037271022796631  val loss:  4.108941078186035  val L1 loss:  4.5866\n",
      "epoch:  25   step:  191   train loss:  2.516627788543701  val loss:  4.176056861877441  val L1 loss:  4.6552\n",
      "epoch:  25   step:  192   train loss:  3.1153736114501953  val loss:  4.1994547843933105  val L1 loss:  4.6627\n",
      "epoch:  25   step:  193   train loss:  2.6170406341552734  val loss:  4.289344310760498  val L1 loss:  4.7581\n",
      "epoch:  25   step:  194   train loss:  1.8169267177581787  val loss:  4.62117862701416  val L1 loss:  5.0987\n",
      "epoch:  25   step:  195   train loss:  2.4883594512939453  val loss:  4.638016700744629  val L1 loss:  5.1075\n",
      "epoch:  25   step:  196   train loss:  2.3340749740600586  val loss:  4.433940410614014  val L1 loss:  4.905\n",
      "epoch:  25   step:  197   train loss:  3.6399013996124268  val loss:  4.39879035949707  val L1 loss:  4.8588\n",
      "epoch:  25   step:  198   train loss:  1.9041078090667725  val loss:  4.691747188568115  val L1 loss:  5.1694\n",
      "epoch:  25   step:  199   train loss:  3.9239821434020996  val loss:  5.012909889221191  val L1 loss:  5.4859\n",
      "epoch:  25   step:  200   train loss:  2.061616897583008  val loss:  5.157779216766357  val L1 loss:  5.635\n",
      "epoch:  25   step:  201   train loss:  2.1137094497680664  val loss:  4.899450778961182  val L1 loss:  5.3581\n",
      "epoch:  25   step:  202   train loss:  3.0322327613830566  val loss:  4.750142574310303  val L1 loss:  5.1855\n",
      "epoch:  25   step:  203   train loss:  2.999363422393799  val loss:  4.705416679382324  val L1 loss:  5.1593\n",
      "epoch:  25   step:  204   train loss:  2.1919779777526855  val loss:  4.630660533905029  val L1 loss:  5.1051\n",
      "epoch:  25   step:  205   train loss:  1.3245782852172852  val loss:  4.543169021606445  val L1 loss:  5.0215\n",
      "epoch:  25   step:  206   train loss:  2.149047613143921  val loss:  4.488481044769287  val L1 loss:  4.9644\n",
      "epoch:  25   step:  207   train loss:  1.3613805770874023  val loss:  4.395676612854004  val L1 loss:  4.8705\n",
      "epoch:  25   step:  208   train loss:  1.9175437688827515  val loss:  4.3410797119140625  val L1 loss:  4.8123\n",
      "epoch:  26   step:  0   train loss:  1.5634021759033203  val loss:  4.286189556121826  val L1 loss:  4.7529\n",
      "epoch:  26   step:  1   train loss:  2.9940991401672363  val loss:  4.331600189208984  val L1 loss:  4.7981\n",
      "epoch:  26   step:  2   train loss:  1.3650991916656494  val loss:  4.378661155700684  val L1 loss:  4.8557\n",
      "epoch:  26   step:  3   train loss:  3.3002383708953857  val loss:  4.492771625518799  val L1 loss:  4.9694\n",
      "epoch:  26   step:  4   train loss:  3.1437551975250244  val loss:  4.367412090301514  val L1 loss:  4.837\n",
      "epoch:  26   step:  5   train loss:  1.6752710342407227  val loss:  4.212358474731445  val L1 loss:  4.6982\n",
      "epoch:  26   step:  6   train loss:  2.8179855346679688  val loss:  4.160835266113281  val L1 loss:  4.6208\n",
      "epoch:  26   step:  7   train loss:  2.217491626739502  val loss:  4.288411617279053  val L1 loss:  4.7583\n",
      "epoch:  26   step:  8   train loss:  1.5637269020080566  val loss:  4.419677734375  val L1 loss:  4.8724\n",
      "epoch:  26   step:  9   train loss:  3.740161418914795  val loss:  4.296238422393799  val L1 loss:  4.764\n",
      "epoch:  26   step:  10   train loss:  1.613478422164917  val loss:  4.239351272583008  val L1 loss:  4.7079\n",
      "epoch:  26   step:  11   train loss:  1.5672377347946167  val loss:  4.302093505859375  val L1 loss:  4.7747\n",
      "epoch:  26   step:  12   train loss:  1.5751967430114746  val loss:  4.505563735961914  val L1 loss:  4.9707\n",
      "epoch:  26   step:  13   train loss:  1.6747663021087646  val loss:  4.715820789337158  val L1 loss:  5.1984\n",
      "epoch:  26   step:  14   train loss:  3.0019659996032715  val loss:  4.950428485870361  val L1 loss:  5.4268\n",
      "epoch:  26   step:  15   train loss:  1.7153379917144775  val loss:  5.076752185821533  val L1 loss:  5.5608\n",
      "epoch:  26   step:  16   train loss:  2.401926040649414  val loss:  5.191083908081055  val L1 loss:  5.6787\n",
      "epoch:  26   step:  17   train loss:  1.6129897832870483  val loss:  5.0338287353515625  val L1 loss:  5.5164\n",
      "epoch:  26   step:  18   train loss:  1.7119112014770508  val loss:  4.74146032333374  val L1 loss:  5.2231\n",
      "epoch:  26   step:  19   train loss:  1.7570827007293701  val loss:  4.530620098114014  val L1 loss:  4.9806\n",
      "epoch:  26   step:  20   train loss:  1.5345772504806519  val loss:  4.427162170410156  val L1 loss:  4.8895\n",
      "epoch:  26   step:  21   train loss:  2.2987518310546875  val loss:  4.38385534286499  val L1 loss:  4.8542\n",
      "epoch:  26   step:  22   train loss:  1.651233196258545  val loss:  4.4096832275390625  val L1 loss:  4.867\n",
      "epoch:  26   step:  23   train loss:  3.7349905967712402  val loss:  4.35061502456665  val L1 loss:  4.8042\n",
      "epoch:  26   step:  24   train loss:  1.9974250793457031  val loss:  4.289219856262207  val L1 loss:  4.7305\n",
      "epoch:  26   step:  25   train loss:  2.2402544021606445  val loss:  4.205786228179932  val L1 loss:  4.6441\n",
      "epoch:  26   step:  26   train loss:  2.1430869102478027  val loss:  4.197064399719238  val L1 loss:  4.6642\n",
      "epoch:  26   step:  27   train loss:  1.9214756488800049  val loss:  4.266218662261963  val L1 loss:  4.7192\n",
      "epoch:  26   step:  28   train loss:  1.8657306432724  val loss:  4.442850112915039  val L1 loss:  4.9128\n",
      "epoch:  26   step:  29   train loss:  3.385613441467285  val loss:  4.739289283752441  val L1 loss:  5.2144\n",
      "epoch:  26   step:  30   train loss:  2.1739702224731445  val loss:  4.7652082443237305  val L1 loss:  5.2364\n",
      "epoch:  26   step:  31   train loss:  2.347188711166382  val loss:  4.351253986358643  val L1 loss:  4.8274\n",
      "epoch:  26   step:  32   train loss:  2.909926652908325  val loss:  4.13078498840332  val L1 loss:  4.5906\n",
      "epoch:  26   step:  33   train loss:  1.4705301523208618  val loss:  4.064174175262451  val L1 loss:  4.5282\n",
      "epoch:  26   step:  34   train loss:  2.285757064819336  val loss:  4.060883522033691  val L1 loss:  4.5238\n",
      "epoch:  26   step:  35   train loss:  1.714985728263855  val loss:  4.040337562561035  val L1 loss:  4.5103\n",
      "epoch:  26   step:  36   train loss:  2.0799050331115723  val loss:  4.042191028594971  val L1 loss:  4.5048\n",
      "epoch:  26   step:  37   train loss:  2.3308000564575195  val loss:  4.06277322769165  val L1 loss:  4.5181\n",
      "epoch:  26   step:  38   train loss:  2.2003562450408936  val loss:  4.190822124481201  val L1 loss:  4.6433\n",
      "epoch:  26   step:  39   train loss:  2.3242881298065186  val loss:  4.519787788391113  val L1 loss:  4.9807\n",
      "epoch:  26   step:  40   train loss:  2.960474967956543  val loss:  4.494263648986816  val L1 loss:  4.9626\n",
      "epoch:  26   step:  41   train loss:  3.8581175804138184  val loss:  4.339510440826416  val L1 loss:  4.8124\n",
      "epoch:  26   step:  42   train loss:  2.509348154067993  val loss:  4.231796741485596  val L1 loss:  4.6938\n",
      "epoch:  26   step:  43   train loss:  4.868282318115234  val loss:  4.207374572753906  val L1 loss:  4.6656\n",
      "epoch:  26   step:  44   train loss:  2.3024282455444336  val loss:  4.1689982414245605  val L1 loss:  4.6322\n",
      "epoch:  26   step:  45   train loss:  2.3092093467712402  val loss:  4.118783473968506  val L1 loss:  4.5912\n",
      "epoch:  26   step:  46   train loss:  3.0040812492370605  val loss:  4.326639652252197  val L1 loss:  4.7963\n",
      "epoch:  26   step:  47   train loss:  3.0484213829040527  val loss:  4.576171875  val L1 loss:  5.0472\n",
      "epoch:  26   step:  48   train loss:  1.771350383758545  val loss:  4.840759754180908  val L1 loss:  5.3254\n",
      "epoch:  26   step:  49   train loss:  2.9917097091674805  val loss:  4.694812774658203  val L1 loss:  5.1761\n",
      "epoch:  26   step:  50   train loss:  2.104116916656494  val loss:  4.289706707000732  val L1 loss:  4.7736\n",
      "epoch:  26   step:  51   train loss:  1.4168660640716553  val loss:  3.9471840858459473  val L1 loss:  4.414\n",
      "epoch:  26   step:  52   train loss:  1.8087143898010254  val loss:  3.8721446990966797  val L1 loss:  4.3318\n",
      "epoch:  26   step:  53   train loss:  1.8439595699310303  val loss:  3.9190053939819336  val L1 loss:  4.3822\n",
      "epoch:  26   step:  54   train loss:  2.7094695568084717  val loss:  4.083232879638672  val L1 loss:  4.5398\n",
      "epoch:  26   step:  55   train loss:  2.553877592086792  val loss:  4.138042449951172  val L1 loss:  4.6114\n",
      "epoch:  26   step:  56   train loss:  1.5471489429473877  val loss:  4.274746417999268  val L1 loss:  4.7533\n",
      "epoch:  26   step:  57   train loss:  1.3278119564056396  val loss:  4.372011184692383  val L1 loss:  4.8433\n",
      "epoch:  26   step:  58   train loss:  2.328434467315674  val loss:  4.16312313079834  val L1 loss:  4.6274\n",
      "epoch:  26   step:  59   train loss:  3.097411632537842  val loss:  3.874852418899536  val L1 loss:  4.3386\n",
      "epoch:  26   step:  60   train loss:  2.282289505004883  val loss:  3.885176420211792  val L1 loss:  4.3592\n",
      "epoch:  26   step:  61   train loss:  1.4344899654388428  val loss:  4.000663757324219  val L1 loss:  4.4505\n",
      "epoch:  26   step:  62   train loss:  2.492274284362793  val loss:  4.084941387176514  val L1 loss:  4.5297\n",
      "epoch:  26   step:  63   train loss:  2.598651170730591  val loss:  4.042337417602539  val L1 loss:  4.5173\n",
      "epoch:  26   step:  64   train loss:  2.227804660797119  val loss:  4.120514869689941  val L1 loss:  4.5858\n",
      "epoch:  26   step:  65   train loss:  2.493929386138916  val loss:  4.170517921447754  val L1 loss:  4.6307\n",
      "epoch:  26   step:  66   train loss:  1.9749927520751953  val loss:  4.312135696411133  val L1 loss:  4.7877\n",
      "epoch:  26   step:  67   train loss:  1.8838086128234863  val loss:  4.24322509765625  val L1 loss:  4.7039\n",
      "epoch:  26   step:  68   train loss:  1.6290932893753052  val loss:  4.204028606414795  val L1 loss:  4.6744\n",
      "epoch:  26   step:  69   train loss:  1.5491902828216553  val loss:  4.241792678833008  val L1 loss:  4.7116\n",
      "epoch:  26   step:  70   train loss:  1.9459627866744995  val loss:  4.2592597007751465  val L1 loss:  4.7188\n",
      "epoch:  26   step:  71   train loss:  2.013763904571533  val loss:  4.341409206390381  val L1 loss:  4.7964\n",
      "epoch:  26   step:  72   train loss:  4.042031288146973  val loss:  4.402511119842529  val L1 loss:  4.8738\n",
      "epoch:  26   step:  73   train loss:  1.789084792137146  val loss:  4.3793182373046875  val L1 loss:  4.8569\n",
      "epoch:  26   step:  74   train loss:  2.0687308311462402  val loss:  4.399333953857422  val L1 loss:  4.8677\n",
      "epoch:  26   step:  75   train loss:  1.9687180519104004  val loss:  4.499659538269043  val L1 loss:  4.9671\n",
      "epoch:  26   step:  76   train loss:  2.313809871673584  val loss:  4.583080768585205  val L1 loss:  5.0332\n",
      "epoch:  26   step:  77   train loss:  2.107074737548828  val loss:  4.579145431518555  val L1 loss:  5.0492\n",
      "epoch:  26   step:  78   train loss:  2.0667343139648438  val loss:  4.434322834014893  val L1 loss:  4.8864\n",
      "epoch:  26   step:  79   train loss:  2.329258441925049  val loss:  4.317722320556641  val L1 loss:  4.7844\n",
      "epoch:  26   step:  80   train loss:  3.8575587272644043  val loss:  4.24747371673584  val L1 loss:  4.7355\n",
      "epoch:  26   step:  81   train loss:  2.298694372177124  val loss:  4.280900955200195  val L1 loss:  4.7684\n",
      "epoch:  26   step:  82   train loss:  1.916032075881958  val loss:  4.3454437255859375  val L1 loss:  4.802\n",
      "epoch:  26   step:  83   train loss:  1.9741859436035156  val loss:  4.471002101898193  val L1 loss:  4.9212\n",
      "epoch:  26   step:  84   train loss:  3.0030739307403564  val loss:  4.751058101654053  val L1 loss:  5.2227\n",
      "epoch:  26   step:  85   train loss:  2.1936371326446533  val loss:  4.829366683959961  val L1 loss:  5.3028\n",
      "epoch:  26   step:  86   train loss:  1.9261881113052368  val loss:  4.645240306854248  val L1 loss:  5.1219\n",
      "epoch:  26   step:  87   train loss:  4.22510290145874  val loss:  4.549591064453125  val L1 loss:  5.0094\n",
      "epoch:  26   step:  88   train loss:  2.545286178588867  val loss:  4.541543006896973  val L1 loss:  5.0083\n",
      "epoch:  26   step:  89   train loss:  2.686361312866211  val loss:  4.776849269866943  val L1 loss:  5.2513\n",
      "epoch:  26   step:  90   train loss:  2.1772966384887695  val loss:  4.944271564483643  val L1 loss:  5.4278\n",
      "epoch:  26   step:  91   train loss:  2.309952735900879  val loss:  5.139050006866455  val L1 loss:  5.6214\n",
      "epoch:  26   step:  92   train loss:  1.838271141052246  val loss:  4.945774555206299  val L1 loss:  5.4172\n",
      "epoch:  26   step:  93   train loss:  2.7643651962280273  val loss:  4.551034450531006  val L1 loss:  5.0297\n",
      "epoch:  26   step:  94   train loss:  1.2989060878753662  val loss:  4.2838592529296875  val L1 loss:  4.7314\n",
      "epoch:  26   step:  95   train loss:  2.3301048278808594  val loss:  4.322726249694824  val L1 loss:  4.7826\n",
      "epoch:  26   step:  96   train loss:  3.280500650405884  val loss:  4.331010818481445  val L1 loss:  4.769\n",
      "epoch:  26   step:  97   train loss:  2.9541754722595215  val loss:  4.527085781097412  val L1 loss:  4.9844\n",
      "epoch:  26   step:  98   train loss:  2.550940990447998  val loss:  5.1504034996032715  val L1 loss:  5.6146\n",
      "epoch:  26   step:  99   train loss:  1.614815354347229  val loss:  5.551451206207275  val L1 loss:  6.0392\n",
      "epoch:  26   step:  100   train loss:  4.25624418258667  val loss:  5.999385833740234  val L1 loss:  6.4929\n",
      "epoch:  26   step:  101   train loss:  2.152101993560791  val loss:  5.985439777374268  val L1 loss:  6.4576\n",
      "epoch:  26   step:  102   train loss:  2.3897740840911865  val loss:  5.704888343811035  val L1 loss:  6.1956\n",
      "epoch:  26   step:  103   train loss:  1.9707896709442139  val loss:  5.263309955596924  val L1 loss:  5.7348\n",
      "epoch:  26   step:  104   train loss:  1.8197458982467651  val loss:  4.988399505615234  val L1 loss:  5.4535\n",
      "epoch:  26   step:  105   train loss:  1.9398534297943115  val loss:  4.977890491485596  val L1 loss:  5.4436\n",
      "epoch:  26   step:  106   train loss:  2.2655718326568604  val loss:  5.029784202575684  val L1 loss:  5.5012\n",
      "epoch:  26   step:  107   train loss:  1.877228021621704  val loss:  5.053524494171143  val L1 loss:  5.5307\n",
      "epoch:  26   step:  108   train loss:  1.8788847923278809  val loss:  5.142590045928955  val L1 loss:  5.6296\n",
      "epoch:  26   step:  109   train loss:  2.2019824981689453  val loss:  5.26875638961792  val L1 loss:  5.7636\n",
      "epoch:  26   step:  110   train loss:  4.677633762359619  val loss:  5.28382682800293  val L1 loss:  5.7736\n",
      "epoch:  26   step:  111   train loss:  2.629667043685913  val loss:  5.197318077087402  val L1 loss:  5.6918\n",
      "epoch:  26   step:  112   train loss:  2.372021198272705  val loss:  5.037430286407471  val L1 loss:  5.5321\n",
      "epoch:  26   step:  113   train loss:  1.6583025455474854  val loss:  4.926053524017334  val L1 loss:  5.3883\n"
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.SGD(params = submodule2_net.parameters(), lr=1e-4, momentum=0.9, weight_decay=2e-5)\n",
    "optimizer = torch.optim.Adam(params = submodule2_net.parameters(), lr=1e-4)\n",
    "#optimizer = torch.optim.SGD(params = submodule2_net.parameters(), lr=1e-4)\n",
    "#loss_func = torch.nn.CrossEntropyLoss(weight=class_weights).cuda()  # the target label is NOT an one-hotted\n",
    "loss_func = torch.nn.SmoothL1Loss(reduce=False, size_average=False).cuda()\n",
    "L1_loss = torch.nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "\n",
    "val_submodule2_data_input_1 = val_submodule2_data_input_1.cuda().float()\n",
    "val_submodule2_data_input_2 = val_submodule2_data_input_2.cuda().float()\n",
    "val_submodule2_data_label = val_submodule2_data_label.cuda().float()\n",
    "\n",
    "min_val_loss_print=float('inf')\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for step, (batch_data_1, batch_data_2, batch_label) in enumerate(loader):\n",
    "        batch_data_1 = batch_data_1.cuda().float()\n",
    "        batch_data_2 = batch_data_2.cuda().float()\n",
    "        batch_label = batch_label.cuda().float()\n",
    "        submodule2_net.train()\n",
    "        output  = submodule2_net(batch_data_1, batch_data_2)\n",
    "        train_loss = loss_func(output,batch_label).mean()\n",
    "        #print(train_loss)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_print = train_loss.data.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            submodule2_net.eval()\n",
    "            val_output  = submodule2_net(val_submodule2_data_input_1, val_submodule2_data_input_2)\n",
    "            val_loss = loss_func(val_output,val_submodule2_data_label).mean()\n",
    "            val_loss_print = val_loss.data.item()\n",
    "            val_L1_loss = L1_loss(val_output,val_submodule2_data_label).mean().data.item()\n",
    "            torch.cuda.empty_cache()\n",
    "        #print('epoch: ', epoch, '  step: ', step, '  train loss: ', train_loss_print, 'a val loss: ', val_loss_print)\n",
    "        print('epoch: ', epoch, '  step: ', step, '  train loss: ', train_loss_print, ' val loss: ', val_loss_print, ' val L1 loss: ', round(val_L1_loss,4))\n",
    "        if val_loss_print < min_val_loss_print:\n",
    "            torch.save(submodule2_net.state_dict(), 'net_params_20210711/net_params_submodule2_20210711_1/epoch_'+str(epoch)+'.pkl') \n",
    "            min_val_loss_print = val_loss_print\n",
    "            print('min_val_loss_print', min_val_loss_print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# round(2.34567890, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
